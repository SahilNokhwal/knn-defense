{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import foolbox\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from lib.dataset_utils import *\n",
    "from lib.mnist_model import *\n",
    "from lib.adv_model import *\n",
    "from lib.dknn_attack import DKNNAttack\n",
    "from lib.dknn_attack_l2 import DKNNL2Attack\n",
    "from lib.cwl2_attack import CWL2Attack\n",
    "from lib.dknn import DKNN, DKNNL2\n",
    "from lib.utils import *\n",
    "from lib.lip_model import *\n",
    "from lib.knn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = 2\n",
    "\n",
    "# model_name = 'train_mnist_exp%d.h5' % exp_id\n",
    "# net = BasicModel()\n",
    "\n",
    "# model_name = 'train_mnist_snnl_exp%d.h5' % exp_id\n",
    "# net = SNNLModel(train_it=True)\n",
    "\n",
    "# model_name = 'train_mnist_hidden_mixup_exp%d.h5' % exp_id\n",
    "# net = HiddenMixupModel()\n",
    "\n",
    "# model_name = 'train_mnist_vae_exp%d.h5' % exp_id\n",
    "# # net = VAE((1, 28, 28), num_classes=10, latent_dim=20)\n",
    "# net = VAE2((1, 28, 28), num_classes=10, latent_dim=128)\n",
    "\n",
    "# model_name = 'train_mnist_cav_exp%d.h5' % exp_id\n",
    "# net = ClassAuxVAE((1, 28, 28), num_classes=10, latent_dim=20)\n",
    "\n",
    "# model_name = 'lip_mnist_exp%d.h5' % exp_id\n",
    "# net = LipschitzModel()\n",
    "\n",
    "# model_name = 'dist_mnist_exp%d.h5' % exp_id\n",
    "# init_it = 1\n",
    "# train_it = False\n",
    "# net = NeighborModel(num_classes=10, init_it=init_it, train_it=train_it)\n",
    "\n",
    "model_name = 'adv_mnist_exp%d.h5' % exp_id\n",
    "basic_net = BasicModel()\n",
    "# basic_net = BasicModelV2()\n",
    "config = {'epsilon': 0.3,\n",
    "          'num_steps': 40,\n",
    "          'step_size': 0.01,\n",
    "          'random_start': True,\n",
    "          'loss_func': 'xent'}\n",
    "net = PGDModel(basic_net, config)\n",
    "\n",
    "# model_name = 'lipae_mnist_exp%d.h5' % exp_id\n",
    "# init_it = 1\n",
    "# train_it = False\n",
    "# latent_dim = 128\n",
    "# alpha = 1e2\n",
    "# net = NCA_AE(latent_dim=latent_dim, init_it=init_it,\n",
    "#              train_it=train_it, alpha=alpha)\n",
    "\n",
    "# orig_model = 'adv_mnist_exp2.h5'\n",
    "# model_name = 'tune%d_%s' % (exp_id, orig_model)\n",
    "# net = BasicModel()\n",
    "# from tune_mnist import Identity\n",
    "# net.fc = Identity()\n",
    "\n",
    "# model_name = 'rot_mnist_exp%d.h5' % exp_id\n",
    "# net = BasicModel(num_classes=4)\n",
    "\n",
    "# model_name = 'adv_rot_mnist_exp%d.h5' % exp_id\n",
    "# basic_net = BasicModel(num_classes=4)\n",
    "# config = {'num_steps': 20,\n",
    "#           'step_size': 0.05,\n",
    "#           'random_start': True,\n",
    "#           'loss_func': 'xent'}\n",
    "# net = PGDL2Model(basic_net, config)\n",
    "\n",
    "# model_name = 'ae_mnist_exp%d.h5' % exp_id\n",
    "# net = Autoencoder((1, 28, 28), 128)\n",
    "\n",
    "# model_name = 'adv_mnist_ae_exp%d.h5' % exp_id\n",
    "# basic_net = Autoencoder((1, 28, 28), latent_dim=128)\n",
    "# config = {'num_steps': 40,\n",
    "#               'step_size': 0.1,\n",
    "#               'random_start': True,\n",
    "#               'loss_func': 'xent'}\n",
    "# net = PGDL2Model(basic_net, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicModel(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(8, 8), stride=(2, 2), padding=(3, 3))\n",
       "  (relu1): ReLU(inplace)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(6, 6), stride=(2, 2), padding=(3, 3))\n",
       "  (relu2): ReLU(inplace)\n",
       "  (conv3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu3): ReLU(inplace)\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set all random seeds\n",
    "seed = 2019\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set up model directory\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "net = net.to(device)\n",
    "# if device == 'cuda':\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "#     cudnn.benchmark = True\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "# net = net.module\n",
    "net = net.basic_net\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test) = load_mnist_all(\n",
    "    '/data', val_size=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net(x_test.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7778"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred.argmax(1).cpu() == y_test).sum().numpy() / y_test.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.to(device)\n",
    "# x_test = x_test.to(device)\n",
    "# x_valid = x_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Identity(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(Identity, self).__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x\n",
    "    \n",
    "# net.conv1 = Identity()\n",
    "# net.relu1 = Identity()\n",
    "# net.conv2 = Identity()\n",
    "# net.relu2 = Identity()\n",
    "# net.conv3 = Identity()\n",
    "# net.relu3 = Identity()\n",
    "# net.fc = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# layers = ['relu1', 'relu2', 'relu3', 'fc']\n",
    "layers = ['relu3']\n",
    "# layers = ['relu1']\n",
    "# layers = ['en_conv3']\n",
    "# layers = ['en_mu']\n",
    "# layers = ['maxpool1', 'maxpool2', 'relu3', 'fc2']\n",
    "# layers = ['maxpool2']\n",
    "\n",
    "# dknn = DKNN(net, x_train, y_train, x_valid, y_valid, layers, \n",
    "#             k=75, num_classes=10)\n",
    "dknn = DKNNL2(net, x_train, y_train, x_valid, y_valid, layers, \n",
    "              k=75, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9726\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = dknn.classify(x_test)\n",
    "    ind = np.where(y_pred.argmax(1) == y_test.numpy())[0]\n",
    "    print((y_pred.argmax(1) == y_test.numpy()).sum() / y_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adv = pickle.load(open('x_adv_' + model_name + '.p', 'rb'))\n",
    "# x_adv = pickle.load(open('x_ba_adv2_mnist_0.5_0.05.p', 'rb'))\n",
    "# x_adv = pickle.load(open('x_ba_adv2_mnist_0.2_0.001.p', 'rb'))\n",
    "# x_adv = pickle.load(open('x_ba_ae_mnist_0.5_0.05.p', 'rb'))\n",
    "# x_adv = pickle.load(open('x_ba_ae_mnist_0.2_0.001.p', 'rb'))\n",
    "# x_adv = torch.tensor(x_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.utils.save_image(x_adv[index], 'x_adv_' + model_name + '.png', 10)\n",
    "torchvision.utils.save_image(x_test[ind][index], 'x_test.png', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005757762697923093\n"
     ]
    }
   ],
   "source": [
    "num = 10000\n",
    "with torch.no_grad():\n",
    "    y_pred = dknn.classify(x_adv)\n",
    "    ind_adv = np.where(y_pred.argmax(1) != y_test[ind][:num].numpy())[0]\n",
    "    print((y_pred.argmax(1) == y_test[ind][:num].numpy()).sum() / y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ind_adv[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0.9387 & 0.8027 & 0.5095\n"
     ]
    }
   ],
   "source": [
    "pert = (x_adv - x_test[ind]).view(x_adv.size(0), -1).norm(2, 1)\n",
    "d1 = (len(ind) - (pert[ind_adv] < 1).sum().numpy()) / y_test.size(0)\n",
    "d2 = (len(ind) - (pert[ind_adv] < 2).sum().numpy()) / y_test.size(0)\n",
    "d3 = (len(ind) - (pert[ind_adv] < 3).sum().numpy()) / y_test.size(0)\n",
    "print('& %.4f & %.4f & %.4f' % (d1, d2, d3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pert = (x_adv - x_test[ind][:num]).view(x_adv.size(0), -1).norm(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pert = (x_adv - x_test[ind]).view(x_adv.size(0), -1).norm(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0378)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert[ind_adv].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_adv1 = ind_adv\n",
    "x_adv1 = x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pert1 = pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25,\n",
       "       26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43,\n",
       "       44, 45, 46, 47, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "       63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n",
       "       81, 82, 84, 85, 86, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25,\n",
       "       26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43,\n",
       "       44, 45, 46, 47, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "       63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n",
       "       81, 82, 84, 85, 86, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_adv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adv_best = torch.zeros((100, 1, 28, 28))\n",
    "\n",
    "for i in range(100):\n",
    "    if (i in ind_adv) & (i in ind_adv1):\n",
    "        if pert[i] < pert1[i]:\n",
    "            x_adv_best[i] = x_adv[i]\n",
    "        else:\n",
    "            x_adv_best[i] = x_adv1[i]\n",
    "    elif i in ind_adv:\n",
    "        x_adv_best[i] = x_adv[i]\n",
    "    elif i in ind_adv1:\n",
    "        x_adv_best[i] = x_adv1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03\n"
     ]
    }
   ],
   "source": [
    "num = 100\n",
    "with torch.no_grad():\n",
    "    y_pred = dknn.classify(x_adv_best)\n",
    "    ind_adv = np.where(y_pred.argmax(1) != y_test[ind][:num].numpy())[0]\n",
    "    print((y_pred.argmax(1) == y_test[ind][:num].numpy()).sum() / y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_suc = x_adv_best.view(100, -1).sum(1) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_best = (x_adv_best - x_test[ind][:num]).view(x_adv.size(0), -1).norm(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "plt.figure(figsize=(6, 6), dpi=300)\n",
    "plt.scatter(np.delete(pert[:100][ind_suc].numpy(), [17, 36]), np.delete(pert_best[ind_suc].numpy(), [17, 36]))\n",
    "plt.plot(np.arange(8), np.arange(8), 'r')\n",
    "\n",
    "plt.ylabel(r'$\\ell_2$-Norm of Boundary Attack', fontsize=24)\n",
    "plt.xlabel(r'$\\ell_2$-Norm of Gradient Attack', fontsize=24)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "# plt.legend(fontsize=20)\n",
    "plt.savefig(\"ba_adv2_mnist.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "plt.figure(figsize=(6, 6), dpi=300)\n",
    "plt.scatter(pert[ind_share].numpy(), pert_best[ind_share].numpy())\n",
    "plt.plot(np.arange(13), np.arange(13), 'r')\n",
    "\n",
    "plt.ylabel(r'$\\ell_2$-Norm of Boundary Attack', fontsize=24)\n",
    "plt.xlabel(r'$\\ell_2$-Norm of Gradient Attack', fontsize=24)\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "# plt.legend(fontsize=20)\n",
    "plt.savefig(\"ba_ae_mnist.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "17, 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_share = np.intersect1d(np.where(ind_suc)[0], ind_adv[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_share.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = net(x_test.cuda()).cpu()\n",
    "    ind = np.where(y_pred.argmax(1) == y_test)[0]\n",
    "    print((y_pred.argmax(1) == y_test).numpy().sum() / y_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 20.000; l2dist: 0.000\n",
      "    step: 50; loss: 14.101; l2dist: 1.825\n",
      "    step: 100; loss: 13.725; l2dist: 1.831\n",
      "    step: 150; loss: 13.266; l2dist: 1.926\n",
      "    step: 200; loss: 12.675; l2dist: 2.021\n",
      "    step: 250; loss: 12.228; l2dist: 2.061\n",
      "    step: 300; loss: 11.910; l2dist: 2.093\n",
      "    step: 350; loss: 11.618; l2dist: 2.095\n",
      "    step: 400; loss: 11.465; l2dist: 2.091\n",
      "    step: 450; loss: 11.375; l2dist: 2.088\n",
      "binary step: 0; number of successful adv: 34/100\n",
      "    step: 0; loss: 124.255; l2dist: 0.000\n",
      "    step: 50; loss: 43.499; l2dist: 4.127\n",
      "    step: 100; loss: 26.650; l2dist: 4.000\n",
      "    step: 150; loss: 16.712; l2dist: 3.677\n",
      "    step: 200; loss: 13.904; l2dist: 3.412\n",
      "    step: 250; loss: 12.742; l2dist: 3.288\n",
      "    step: 300; loss: 12.080; l2dist: 3.195\n",
      "    step: 350; loss: 11.666; l2dist: 3.153\n",
      "    step: 400; loss: 11.197; l2dist: 3.098\n",
      "    step: 450; loss: 10.954; l2dist: 3.060\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 202.556; l2dist: 0.000\n",
      "    step: 50; loss: 44.595; l2dist: 4.378\n",
      "    step: 100; loss: 27.137; l2dist: 4.096\n",
      "    step: 150; loss: 18.907; l2dist: 3.747\n",
      "    step: 200; loss: 15.247; l2dist: 3.507\n",
      "    step: 250; loss: 13.935; l2dist: 3.397\n",
      "    step: 300; loss: 12.785; l2dist: 3.264\n",
      "    step: 350; loss: 12.164; l2dist: 3.193\n",
      "    step: 400; loss: 11.773; l2dist: 3.157\n",
      "    step: 450; loss: 11.540; l2dist: 3.127\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 120.303; l2dist: 0.000\n",
      "    step: 50; loss: 33.370; l2dist: 3.798\n",
      "    step: 100; loss: 23.458; l2dist: 3.545\n",
      "    step: 150; loss: 17.168; l2dist: 3.456\n",
      "    step: 200; loss: 14.068; l2dist: 3.334\n",
      "    step: 250; loss: 12.486; l2dist: 3.226\n",
      "    step: 300; loss: 11.794; l2dist: 3.153\n",
      "    step: 350; loss: 11.371; l2dist: 3.108\n",
      "    step: 400; loss: 10.895; l2dist: 3.051\n",
      "    step: 450; loss: 10.967; l2dist: 3.062\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 82.784; l2dist: 0.000\n",
      "    step: 50; loss: 27.946; l2dist: 3.452\n",
      "    step: 100; loss: 20.833; l2dist: 3.288\n",
      "    step: 150; loss: 16.762; l2dist: 3.265\n",
      "    step: 200; loss: 14.126; l2dist: 3.239\n",
      "    step: 250; loss: 12.860; l2dist: 3.167\n",
      "    step: 300; loss: 11.512; l2dist: 3.106\n",
      "    step: 350; loss: 11.116; l2dist: 3.062\n",
      "    step: 400; loss: 10.669; l2dist: 3.024\n",
      "    step: 450; loss: 10.585; l2dist: 3.014\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.821; l2dist: 0.000\n",
      "    step: 50; loss: 25.706; l2dist: 3.146\n",
      "    step: 100; loss: 20.787; l2dist: 3.042\n",
      "    step: 150; loss: 17.278; l2dist: 3.087\n",
      "    step: 200; loss: 14.908; l2dist: 3.109\n",
      "    step: 250; loss: 13.130; l2dist: 3.095\n",
      "    step: 300; loss: 12.114; l2dist: 3.043\n",
      "    step: 350; loss: 11.765; l2dist: 2.996\n",
      "    step: 400; loss: 11.464; l2dist: 2.978\n",
      "    step: 450; loss: 11.394; l2dist: 2.971\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.137; l2dist: 0.000\n",
      "    step: 50; loss: 24.787; l2dist: 3.130\n",
      "    step: 100; loss: 20.533; l2dist: 3.016\n",
      "    step: 150; loss: 17.175; l2dist: 3.081\n",
      "    step: 200; loss: 15.021; l2dist: 3.098\n",
      "    step: 250; loss: 13.288; l2dist: 3.083\n",
      "    step: 300; loss: 12.248; l2dist: 3.053\n",
      "    step: 350; loss: 11.654; l2dist: 3.005\n",
      "    step: 400; loss: 11.304; l2dist: 2.981\n",
      "    step: 450; loss: 11.149; l2dist: 2.950\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.807; l2dist: 0.000\n",
      "    step: 50; loss: 24.327; l2dist: 3.081\n",
      "    step: 100; loss: 20.288; l2dist: 2.998\n",
      "    step: 150; loss: 17.058; l2dist: 3.065\n",
      "    step: 200; loss: 14.789; l2dist: 3.091\n",
      "    step: 250; loss: 12.986; l2dist: 3.056\n",
      "    step: 300; loss: 12.109; l2dist: 3.032\n",
      "    step: 350; loss: 11.615; l2dist: 2.991\n",
      "    step: 400; loss: 11.319; l2dist: 2.959\n",
      "    step: 450; loss: 11.182; l2dist: 2.956\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.062; l2dist: 0.000\n",
      "    step: 50; loss: 24.130; l2dist: 3.076\n",
      "    step: 100; loss: 20.135; l2dist: 3.001\n",
      "    step: 150; loss: 16.956; l2dist: 3.066\n",
      "    step: 200; loss: 14.811; l2dist: 3.087\n",
      "    step: 250; loss: 13.164; l2dist: 3.063\n",
      "    step: 300; loss: 12.160; l2dist: 3.039\n",
      "    step: 350; loss: 11.710; l2dist: 2.998\n",
      "    step: 400; loss: 11.457; l2dist: 2.977\n",
      "    step: 450; loss: 11.227; l2dist: 2.965\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.685; l2dist: 0.000\n",
      "    step: 50; loss: 24.297; l2dist: 3.084\n",
      "    step: 100; loss: 20.263; l2dist: 3.008\n",
      "    step: 150; loss: 17.151; l2dist: 3.071\n",
      "    step: 200; loss: 14.938; l2dist: 3.100\n",
      "    step: 250; loss: 13.189; l2dist: 3.091\n",
      "    step: 300; loss: 12.228; l2dist: 3.060\n",
      "    step: 350; loss: 11.556; l2dist: 3.007\n",
      "    step: 400; loss: 11.417; l2dist: 2.994\n",
      "    step: 450; loss: 11.095; l2dist: 2.970\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.620; l2dist: 0.000\n",
      "    step: 50; loss: 12.937; l2dist: 1.771\n",
      "    step: 100; loss: 12.562; l2dist: 1.778\n",
      "    step: 150; loss: 11.986; l2dist: 1.895\n",
      "    step: 200; loss: 11.388; l2dist: 1.966\n",
      "    step: 250; loss: 10.955; l2dist: 1.996\n",
      "    step: 300; loss: 10.668; l2dist: 2.002\n",
      "    step: 350; loss: 10.490; l2dist: 2.017\n",
      "    step: 400; loss: 10.382; l2dist: 2.016\n",
      "    step: 450; loss: 10.298; l2dist: 2.029\n",
      "binary step: 0; number of successful adv: 38/100\n",
      "    step: 0; loss: 111.125; l2dist: 0.000\n",
      "    step: 50; loss: 37.033; l2dist: 3.865\n",
      "    step: 100; loss: 24.672; l2dist: 3.560\n",
      "    step: 150; loss: 15.270; l2dist: 3.338\n",
      "    step: 200; loss: 12.846; l2dist: 3.166\n",
      "    step: 250; loss: 12.032; l2dist: 3.096\n",
      "    step: 300; loss: 11.259; l2dist: 3.007\n",
      "    step: 350; loss: 10.904; l2dist: 2.961\n",
      "    step: 400; loss: 10.535; l2dist: 2.925\n",
      "    step: 450; loss: 10.369; l2dist: 2.911\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 178.126; l2dist: 0.000\n",
      "    step: 50; loss: 37.121; l2dist: 4.047\n",
      "    step: 100; loss: 23.077; l2dist: 3.782\n",
      "    step: 150; loss: 16.460; l2dist: 3.473\n",
      "    step: 200; loss: 13.886; l2dist: 3.276\n",
      "    step: 250; loss: 12.371; l2dist: 3.146\n",
      "    step: 300; loss: 11.568; l2dist: 3.053\n",
      "    step: 350; loss: 10.907; l2dist: 2.986\n",
      "    step: 400; loss: 10.668; l2dist: 2.959\n",
      "    step: 450; loss: 10.456; l2dist: 2.922\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.131; l2dist: 0.000\n",
      "    step: 50; loss: 29.424; l2dist: 3.589\n",
      "    step: 100; loss: 20.167; l2dist: 3.410\n",
      "    step: 150; loss: 14.453; l2dist: 3.245\n",
      "    step: 200; loss: 12.333; l2dist: 3.102\n",
      "    step: 250; loss: 11.169; l2dist: 2.986\n",
      "    step: 300; loss: 11.078; l2dist: 2.990\n",
      "    step: 350; loss: 10.444; l2dist: 2.911\n",
      "    step: 400; loss: 9.931; l2dist: 2.864\n",
      "    step: 450; loss: 9.887; l2dist: 2.849\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.901; l2dist: 0.000\n",
      "    step: 50; loss: 24.962; l2dist: 3.348\n",
      "    step: 100; loss: 18.966; l2dist: 3.156\n",
      "    step: 150; loss: 14.232; l2dist: 3.108\n",
      "    step: 200; loss: 12.100; l2dist: 3.019\n",
      "    step: 250; loss: 10.988; l2dist: 2.942\n",
      "    step: 300; loss: 10.492; l2dist: 2.919\n",
      "    step: 350; loss: 10.244; l2dist: 2.870\n",
      "    step: 400; loss: 9.827; l2dist: 2.832\n",
      "    step: 450; loss: 9.747; l2dist: 2.827\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.904; l2dist: 0.000\n",
      "    step: 50; loss: 23.257; l2dist: 3.218\n",
      "    step: 100; loss: 18.324; l2dist: 3.058\n",
      "    step: 150; loss: 13.909; l2dist: 3.042\n",
      "    step: 200; loss: 11.910; l2dist: 2.985\n",
      "    step: 250; loss: 10.851; l2dist: 2.919\n",
      "    step: 300; loss: 10.251; l2dist: 2.874\n",
      "    step: 350; loss: 9.864; l2dist: 2.842\n",
      "    step: 400; loss: 9.619; l2dist: 2.817\n",
      "    step: 450; loss: 9.452; l2dist: 2.795\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.097; l2dist: 0.000\n",
      "    step: 50; loss: 22.241; l2dist: 3.160\n",
      "    step: 100; loss: 17.806; l2dist: 3.004\n",
      "    step: 150; loss: 13.779; l2dist: 3.004\n",
      "    step: 200; loss: 11.920; l2dist: 2.946\n",
      "    step: 250; loss: 11.080; l2dist: 2.913\n",
      "    step: 300; loss: 10.367; l2dist: 2.866\n",
      "    step: 350; loss: 10.163; l2dist: 2.869\n",
      "    step: 400; loss: 9.916; l2dist: 2.837\n",
      "    step: 450; loss: 9.677; l2dist: 2.816\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.890; l2dist: 0.000\n",
      "    step: 50; loss: 22.188; l2dist: 3.123\n",
      "    step: 100; loss: 17.738; l2dist: 2.999\n",
      "    step: 150; loss: 13.870; l2dist: 2.984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 200; loss: 12.025; l2dist: 2.933\n",
      "    step: 250; loss: 11.041; l2dist: 2.898\n",
      "    step: 300; loss: 10.375; l2dist: 2.882\n",
      "    step: 350; loss: 10.039; l2dist: 2.861\n",
      "    step: 400; loss: 9.729; l2dist: 2.822\n",
      "    step: 450; loss: 9.766; l2dist: 2.821\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.010; l2dist: 0.000\n",
      "    step: 50; loss: 21.696; l2dist: 3.136\n",
      "    step: 100; loss: 17.152; l2dist: 3.013\n",
      "    step: 150; loss: 13.589; l2dist: 2.978\n",
      "    step: 200; loss: 11.900; l2dist: 2.927\n",
      "    step: 250; loss: 11.020; l2dist: 2.903\n",
      "    step: 300; loss: 10.527; l2dist: 2.889\n",
      "    step: 350; loss: 10.203; l2dist: 2.868\n",
      "    step: 400; loss: 9.958; l2dist: 2.834\n",
      "    step: 450; loss: 9.746; l2dist: 2.809\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.491; l2dist: 0.000\n",
      "    step: 50; loss: 22.045; l2dist: 3.127\n",
      "    step: 100; loss: 17.522; l2dist: 3.009\n",
      "    step: 150; loss: 13.747; l2dist: 2.990\n",
      "    step: 200; loss: 11.926; l2dist: 2.940\n",
      "    step: 250; loss: 10.936; l2dist: 2.898\n",
      "    step: 300; loss: 10.314; l2dist: 2.873\n",
      "    step: 350; loss: 9.979; l2dist: 2.849\n",
      "    step: 400; loss: 9.723; l2dist: 2.832\n",
      "    step: 450; loss: 9.543; l2dist: 2.810\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.393; l2dist: 0.000\n",
      "    step: 50; loss: 12.621; l2dist: 1.788\n",
      "    step: 100; loss: 12.337; l2dist: 1.791\n",
      "    step: 150; loss: 11.844; l2dist: 1.896\n",
      "    step: 200; loss: 11.421; l2dist: 1.951\n",
      "    step: 250; loss: 10.915; l2dist: 2.006\n",
      "    step: 300; loss: 10.610; l2dist: 2.023\n",
      "    step: 350; loss: 10.465; l2dist: 2.040\n",
      "    step: 400; loss: 10.362; l2dist: 2.052\n",
      "    step: 450; loss: 10.253; l2dist: 2.051\n",
      "binary step: 0; number of successful adv: 31/100\n",
      "    step: 0; loss: 121.147; l2dist: 0.000\n",
      "    step: 50; loss: 37.770; l2dist: 4.004\n",
      "    step: 100; loss: 22.623; l2dist: 3.680\n",
      "    step: 150; loss: 15.017; l2dist: 3.362\n",
      "    step: 200; loss: 12.569; l2dist: 3.164\n",
      "    step: 250; loss: 11.619; l2dist: 3.057\n",
      "    step: 300; loss: 10.957; l2dist: 2.980\n",
      "    step: 350; loss: 10.672; l2dist: 2.961\n",
      "    step: 400; loss: 10.255; l2dist: 2.906\n",
      "    step: 450; loss: 10.101; l2dist: 2.880\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 185.860; l2dist: 0.000\n",
      "    step: 50; loss: 35.592; l2dist: 3.972\n",
      "    step: 100; loss: 23.205; l2dist: 3.774\n",
      "    step: 150; loss: 16.092; l2dist: 3.429\n",
      "    step: 200; loss: 13.101; l2dist: 3.240\n",
      "    step: 250; loss: 11.967; l2dist: 3.106\n",
      "    step: 300; loss: 11.107; l2dist: 3.004\n",
      "    step: 350; loss: 10.833; l2dist: 2.987\n",
      "    step: 400; loss: 10.372; l2dist: 2.923\n",
      "    step: 450; loss: 10.131; l2dist: 2.897\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.594; l2dist: 0.000\n",
      "    step: 50; loss: 27.508; l2dist: 3.472\n",
      "    step: 100; loss: 20.651; l2dist: 3.361\n",
      "    step: 150; loss: 15.057; l2dist: 3.242\n",
      "    step: 200; loss: 12.329; l2dist: 3.147\n",
      "    step: 250; loss: 10.945; l2dist: 2.991\n",
      "    step: 300; loss: 10.368; l2dist: 2.922\n",
      "    step: 350; loss: 10.075; l2dist: 2.892\n",
      "    step: 400; loss: 9.672; l2dist: 2.837\n",
      "    step: 450; loss: 9.555; l2dist: 2.827\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 98.331; l2dist: 0.000\n",
      "    step: 50; loss: 24.440; l2dist: 3.203\n",
      "    step: 100; loss: 19.323; l2dist: 3.091\n",
      "    step: 150; loss: 14.643; l2dist: 3.102\n",
      "    step: 200; loss: 11.924; l2dist: 3.013\n",
      "    step: 250; loss: 10.828; l2dist: 2.944\n",
      "    step: 300; loss: 10.155; l2dist: 2.871\n",
      "    step: 350; loss: 9.989; l2dist: 2.853\n",
      "    step: 400; loss: 9.564; l2dist: 2.814\n",
      "    step: 450; loss: 9.541; l2dist: 2.797\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.473; l2dist: 0.000\n",
      "    step: 50; loss: 22.735; l2dist: 3.078\n",
      "    step: 100; loss: 18.373; l2dist: 2.987\n",
      "    step: 150; loss: 14.192; l2dist: 3.019\n",
      "    step: 200; loss: 11.637; l2dist: 2.960\n",
      "    step: 250; loss: 10.670; l2dist: 2.891\n",
      "    step: 300; loss: 10.125; l2dist: 2.837\n",
      "    step: 350; loss: 9.828; l2dist: 2.811\n",
      "    step: 400; loss: 9.511; l2dist: 2.778\n",
      "    step: 450; loss: 9.363; l2dist: 2.758\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 82.628; l2dist: 0.000\n",
      "    step: 50; loss: 22.291; l2dist: 3.023\n",
      "    step: 100; loss: 18.242; l2dist: 2.934\n",
      "    step: 150; loss: 14.388; l2dist: 2.980\n",
      "    step: 200; loss: 11.817; l2dist: 2.931\n",
      "    step: 250; loss: 10.671; l2dist: 2.868\n",
      "    step: 300; loss: 10.238; l2dist: 2.822\n",
      "    step: 350; loss: 9.906; l2dist: 2.796\n",
      "    step: 400; loss: 9.574; l2dist: 2.744\n",
      "    step: 450; loss: 9.405; l2dist: 2.732\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 81.467; l2dist: 0.000\n",
      "    step: 50; loss: 22.095; l2dist: 3.018\n",
      "    step: 100; loss: 18.028; l2dist: 2.931\n",
      "    step: 150; loss: 14.229; l2dist: 2.974\n",
      "    step: 200; loss: 11.661; l2dist: 2.926\n",
      "    step: 250; loss: 10.602; l2dist: 2.854\n",
      "    step: 300; loss: 10.218; l2dist: 2.830\n",
      "    step: 350; loss: 9.846; l2dist: 2.790\n",
      "    step: 400; loss: 9.583; l2dist: 2.771\n",
      "    step: 450; loss: 9.403; l2dist: 2.741\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.997; l2dist: 0.000\n",
      "    step: 50; loss: 22.093; l2dist: 2.999\n",
      "    step: 100; loss: 18.073; l2dist: 2.923\n",
      "    step: 150; loss: 14.349; l2dist: 2.975\n",
      "    step: 200; loss: 11.785; l2dist: 2.935\n",
      "    step: 250; loss: 10.602; l2dist: 2.864\n",
      "    step: 300; loss: 10.096; l2dist: 2.823\n",
      "    step: 350; loss: 9.784; l2dist: 2.789\n",
      "    step: 400; loss: 9.476; l2dist: 2.757\n",
      "    step: 450; loss: 9.580; l2dist: 2.771\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 80.982; l2dist: 0.000\n",
      "    step: 50; loss: 22.108; l2dist: 3.025\n",
      "    step: 100; loss: 17.970; l2dist: 2.942\n",
      "    step: 150; loss: 14.183; l2dist: 2.987\n",
      "    step: 200; loss: 11.720; l2dist: 2.938\n",
      "    step: 250; loss: 10.718; l2dist: 2.886\n",
      "    step: 300; loss: 10.109; l2dist: 2.823\n",
      "    step: 350; loss: 9.736; l2dist: 2.793\n",
      "    step: 400; loss: 9.553; l2dist: 2.773\n",
      "    step: 450; loss: 9.414; l2dist: 2.759\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.350; l2dist: 0.000\n",
      "    step: 50; loss: 13.064; l2dist: 1.752\n",
      "    step: 100; loss: 12.725; l2dist: 1.786\n",
      "    step: 150; loss: 12.161; l2dist: 1.881\n",
      "    step: 200; loss: 11.803; l2dist: 1.935\n",
      "    step: 250; loss: 11.525; l2dist: 1.976\n",
      "    step: 300; loss: 11.364; l2dist: 1.986\n",
      "    step: 350; loss: 11.252; l2dist: 1.989\n",
      "    step: 400; loss: 11.189; l2dist: 1.988\n",
      "    step: 450; loss: 11.104; l2dist: 1.990\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 122.270; l2dist: 0.000\n",
      "    step: 50; loss: 45.223; l2dist: 4.036\n",
      "    step: 100; loss: 29.169; l2dist: 3.852\n",
      "    step: 150; loss: 16.330; l2dist: 3.528\n",
      "    step: 200; loss: 13.589; l2dist: 3.273\n",
      "    step: 250; loss: 12.240; l2dist: 3.166\n",
      "    step: 300; loss: 11.513; l2dist: 3.079\n",
      "    step: 350; loss: 11.003; l2dist: 3.010\n",
      "    step: 400; loss: 10.689; l2dist: 2.978\n",
      "    step: 450; loss: 10.519; l2dist: 2.948\n",
      "binary step: 1; number of successful adv: 94/100\n",
      "    step: 0; loss: 166.267; l2dist: 0.000\n",
      "    step: 50; loss: 44.660; l2dist: 4.153\n",
      "    step: 100; loss: 26.619; l2dist: 3.928\n",
      "    step: 150; loss: 17.144; l2dist: 3.536\n",
      "    step: 200; loss: 14.182; l2dist: 3.309\n",
      "    step: 250; loss: 12.751; l2dist: 3.183\n",
      "    step: 300; loss: 11.902; l2dist: 3.102\n",
      "    step: 350; loss: 11.249; l2dist: 3.025\n",
      "    step: 400; loss: 10.740; l2dist: 2.978\n",
      "    step: 450; loss: 10.572; l2dist: 2.949\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 103.277; l2dist: 0.000\n",
      "    step: 50; loss: 34.171; l2dist: 3.613\n",
      "    step: 100; loss: 23.865; l2dist: 3.462\n",
      "    step: 150; loss: 16.683; l2dist: 3.366\n",
      "    step: 200; loss: 13.383; l2dist: 3.215\n",
      "    step: 250; loss: 11.926; l2dist: 3.103\n",
      "    step: 300; loss: 10.911; l2dist: 2.995\n",
      "    step: 350; loss: 10.431; l2dist: 2.938\n",
      "    step: 400; loss: 10.202; l2dist: 2.907\n",
      "    step: 450; loss: 9.857; l2dist: 2.872\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.906; l2dist: 0.000\n",
      "    step: 50; loss: 28.434; l2dist: 3.192\n",
      "    step: 100; loss: 21.893; l2dist: 3.058\n",
      "    step: 150; loss: 17.173; l2dist: 3.097\n",
      "    step: 200; loss: 13.197; l2dist: 3.106\n",
      "    step: 250; loss: 11.514; l2dist: 2.996\n",
      "    step: 300; loss: 10.593; l2dist: 2.931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 10.128; l2dist: 2.875\n",
      "    step: 400; loss: 9.681; l2dist: 2.833\n",
      "    step: 450; loss: 9.573; l2dist: 2.818\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.479; l2dist: 0.000\n",
      "    step: 50; loss: 26.772; l2dist: 3.055\n",
      "    step: 100; loss: 20.992; l2dist: 2.978\n",
      "    step: 150; loss: 16.629; l2dist: 3.046\n",
      "    step: 200; loss: 13.499; l2dist: 3.037\n",
      "    step: 250; loss: 11.438; l2dist: 2.968\n",
      "    step: 300; loss: 10.550; l2dist: 2.883\n",
      "    step: 350; loss: 9.987; l2dist: 2.838\n",
      "    step: 400; loss: 9.716; l2dist: 2.797\n",
      "    step: 450; loss: 9.539; l2dist: 2.785\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.580; l2dist: 0.000\n",
      "    step: 50; loss: 25.694; l2dist: 2.996\n",
      "    step: 100; loss: 20.676; l2dist: 2.901\n",
      "    step: 150; loss: 16.866; l2dist: 3.010\n",
      "    step: 200; loss: 13.626; l2dist: 3.027\n",
      "    step: 250; loss: 11.941; l2dist: 2.942\n",
      "    step: 300; loss: 11.116; l2dist: 2.879\n",
      "    step: 350; loss: 10.681; l2dist: 2.833\n",
      "    step: 400; loss: 10.435; l2dist: 2.814\n",
      "    step: 450; loss: 10.257; l2dist: 2.791\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.729; l2dist: 0.000\n",
      "    step: 50; loss: 25.195; l2dist: 2.949\n",
      "    step: 100; loss: 20.454; l2dist: 2.871\n",
      "    step: 150; loss: 16.872; l2dist: 2.982\n",
      "    step: 200; loss: 13.402; l2dist: 3.036\n",
      "    step: 250; loss: 11.523; l2dist: 2.956\n",
      "    step: 300; loss: 10.787; l2dist: 2.912\n",
      "    step: 350; loss: 10.132; l2dist: 2.843\n",
      "    step: 400; loss: 9.830; l2dist: 2.816\n",
      "    step: 450; loss: 9.586; l2dist: 2.795\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.969; l2dist: 0.000\n",
      "    step: 50; loss: 24.793; l2dist: 2.931\n",
      "    step: 100; loss: 20.323; l2dist: 2.856\n",
      "    step: 150; loss: 16.940; l2dist: 2.921\n",
      "    step: 200; loss: 13.103; l2dist: 3.029\n",
      "    step: 250; loss: 11.285; l2dist: 2.961\n",
      "    step: 300; loss: 10.538; l2dist: 2.905\n",
      "    step: 350; loss: 9.957; l2dist: 2.842\n",
      "    step: 400; loss: 9.675; l2dist: 2.802\n",
      "    step: 450; loss: 9.447; l2dist: 2.788\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.793; l2dist: 0.000\n",
      "    step: 50; loss: 24.974; l2dist: 2.948\n",
      "    step: 100; loss: 20.457; l2dist: 2.871\n",
      "    step: 150; loss: 17.102; l2dist: 2.933\n",
      "    step: 200; loss: 13.282; l2dist: 3.047\n",
      "    step: 250; loss: 11.383; l2dist: 2.983\n",
      "    step: 300; loss: 10.656; l2dist: 2.916\n",
      "    step: 350; loss: 10.217; l2dist: 2.869\n",
      "    step: 400; loss: 9.658; l2dist: 2.823\n",
      "    step: 450; loss: 9.464; l2dist: 2.793\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.778; l2dist: 0.000\n",
      "    step: 50; loss: 12.012; l2dist: 1.759\n",
      "    step: 100; loss: 11.716; l2dist: 1.782\n",
      "    step: 150; loss: 11.249; l2dist: 1.874\n",
      "    step: 200; loss: 10.910; l2dist: 1.926\n",
      "    step: 250; loss: 10.672; l2dist: 1.944\n",
      "    step: 300; loss: 10.372; l2dist: 1.960\n",
      "    step: 350; loss: 10.247; l2dist: 1.957\n",
      "    step: 400; loss: 10.174; l2dist: 1.962\n",
      "    step: 450; loss: 10.119; l2dist: 1.964\n",
      "binary step: 0; number of successful adv: 36/100\n",
      "    step: 0; loss: 112.252; l2dist: 0.000\n",
      "    step: 50; loss: 38.774; l2dist: 3.850\n",
      "    step: 100; loss: 23.263; l2dist: 3.606\n",
      "    step: 150; loss: 14.909; l2dist: 3.295\n",
      "    step: 200; loss: 12.453; l2dist: 3.124\n",
      "    step: 250; loss: 11.401; l2dist: 3.004\n",
      "    step: 300; loss: 10.770; l2dist: 2.929\n",
      "    step: 350; loss: 10.410; l2dist: 2.894\n",
      "    step: 400; loss: 10.107; l2dist: 2.853\n",
      "    step: 450; loss: 9.849; l2dist: 2.818\n",
      "binary step: 1; number of successful adv: 85/100\n",
      "    step: 0; loss: 267.739; l2dist: 0.000\n",
      "    step: 50; loss: 52.254; l2dist: 4.445\n",
      "    step: 100; loss: 25.229; l2dist: 4.114\n",
      "    step: 150; loss: 17.630; l2dist: 3.675\n",
      "    step: 200; loss: 14.674; l2dist: 3.412\n",
      "    step: 250; loss: 13.094; l2dist: 3.233\n",
      "    step: 300; loss: 12.363; l2dist: 3.148\n",
      "    step: 350; loss: 11.640; l2dist: 3.079\n",
      "    step: 400; loss: 11.205; l2dist: 3.027\n",
      "    step: 450; loss: 11.043; l2dist: 3.002\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 153.604; l2dist: 0.000\n",
      "    step: 50; loss: 37.260; l2dist: 3.891\n",
      "    step: 100; loss: 21.726; l2dist: 3.588\n",
      "    step: 150; loss: 15.545; l2dist: 3.375\n",
      "    step: 200; loss: 12.893; l2dist: 3.209\n",
      "    step: 250; loss: 12.026; l2dist: 3.114\n",
      "    step: 300; loss: 11.204; l2dist: 3.033\n",
      "    step: 350; loss: 10.792; l2dist: 2.984\n",
      "    step: 400; loss: 10.430; l2dist: 2.927\n",
      "    step: 450; loss: 10.437; l2dist: 2.932\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 98.650; l2dist: 0.000\n",
      "    step: 50; loss: 29.105; l2dist: 3.466\n",
      "    step: 100; loss: 18.928; l2dist: 3.267\n",
      "    step: 150; loss: 14.222; l2dist: 3.180\n",
      "    step: 200; loss: 12.163; l2dist: 3.093\n",
      "    step: 250; loss: 11.107; l2dist: 3.001\n",
      "    step: 300; loss: 10.524; l2dist: 2.933\n",
      "    step: 350; loss: 10.205; l2dist: 2.883\n",
      "    step: 400; loss: 10.039; l2dist: 2.872\n",
      "    step: 450; loss: 9.837; l2dist: 2.853\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.192; l2dist: 0.000\n",
      "    step: 50; loss: 25.204; l2dist: 3.215\n",
      "    step: 100; loss: 17.819; l2dist: 3.110\n",
      "    step: 150; loss: 13.871; l2dist: 3.090\n",
      "    step: 200; loss: 12.006; l2dist: 3.017\n",
      "    step: 250; loss: 11.073; l2dist: 2.971\n",
      "    step: 300; loss: 10.464; l2dist: 2.906\n",
      "    step: 350; loss: 9.951; l2dist: 2.839\n",
      "    step: 400; loss: 9.711; l2dist: 2.822\n",
      "    step: 450; loss: 9.751; l2dist: 2.821\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.403; l2dist: 0.000\n",
      "    step: 50; loss: 23.578; l2dist: 3.116\n",
      "    step: 100; loss: 17.583; l2dist: 3.030\n",
      "    step: 150; loss: 14.022; l2dist: 3.017\n",
      "    step: 200; loss: 12.270; l2dist: 2.962\n",
      "    step: 250; loss: 10.799; l2dist: 2.920\n",
      "    step: 300; loss: 10.173; l2dist: 2.865\n",
      "    step: 350; loss: 9.928; l2dist: 2.833\n",
      "    step: 400; loss: 9.718; l2dist: 2.817\n",
      "    step: 450; loss: 9.537; l2dist: 2.794\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.322; l2dist: 0.000\n",
      "    step: 50; loss: 22.994; l2dist: 3.078\n",
      "    step: 100; loss: 17.133; l2dist: 2.999\n",
      "    step: 150; loss: 14.073; l2dist: 2.978\n",
      "    step: 200; loss: 12.261; l2dist: 2.955\n",
      "    step: 250; loss: 10.839; l2dist: 2.921\n",
      "    step: 300; loss: 10.437; l2dist: 2.900\n",
      "    step: 350; loss: 9.822; l2dist: 2.839\n",
      "    step: 400; loss: 9.657; l2dist: 2.810\n",
      "    step: 450; loss: 9.374; l2dist: 2.785\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.940; l2dist: 0.000\n",
      "    step: 50; loss: 22.620; l2dist: 3.055\n",
      "    step: 100; loss: 17.010; l2dist: 2.980\n",
      "    step: 150; loss: 14.060; l2dist: 2.959\n",
      "    step: 200; loss: 12.248; l2dist: 2.943\n",
      "    step: 250; loss: 10.894; l2dist: 2.911\n",
      "    step: 300; loss: 10.255; l2dist: 2.871\n",
      "    step: 350; loss: 10.010; l2dist: 2.826\n",
      "    step: 400; loss: 9.753; l2dist: 2.805\n",
      "    step: 450; loss: 9.665; l2dist: 2.801\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.959; l2dist: 0.000\n",
      "    step: 50; loss: 22.830; l2dist: 3.073\n",
      "    step: 100; loss: 17.070; l2dist: 2.988\n",
      "    step: 150; loss: 14.092; l2dist: 2.968\n",
      "    step: 200; loss: 12.367; l2dist: 2.961\n",
      "    step: 250; loss: 10.860; l2dist: 2.934\n",
      "    step: 300; loss: 10.247; l2dist: 2.875\n",
      "    step: 350; loss: 9.903; l2dist: 2.834\n",
      "    step: 400; loss: 9.651; l2dist: 2.812\n",
      "    step: 450; loss: 9.596; l2dist: 2.799\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.222; l2dist: 0.000\n",
      "    step: 50; loss: 12.005; l2dist: 1.878\n",
      "    step: 100; loss: 11.573; l2dist: 1.890\n",
      "    step: 150; loss: 10.961; l2dist: 1.966\n",
      "    step: 200; loss: 10.434; l2dist: 2.029\n",
      "    step: 250; loss: 10.100; l2dist: 2.044\n",
      "    step: 300; loss: 9.731; l2dist: 2.058\n",
      "    step: 350; loss: 9.592; l2dist: 2.058\n",
      "    step: 400; loss: 9.390; l2dist: 2.066\n",
      "    step: 450; loss: 9.324; l2dist: 2.066\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 116.936; l2dist: 0.000\n",
      "    step: 50; loss: 27.464; l2dist: 4.239\n",
      "    step: 100; loss: 17.339; l2dist: 3.596\n",
      "    step: 150; loss: 13.630; l2dist: 3.253\n",
      "    step: 200; loss: 11.813; l2dist: 3.066\n",
      "    step: 250; loss: 10.939; l2dist: 2.960\n",
      "    step: 300; loss: 10.405; l2dist: 2.898\n",
      "    step: 350; loss: 10.174; l2dist: 2.861\n",
      "    step: 400; loss: 9.985; l2dist: 2.832\n",
      "    step: 450; loss: 9.705; l2dist: 2.809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 1; number of successful adv: 92/100\n",
      "    step: 0; loss: 183.240; l2dist: 0.000\n",
      "    step: 50; loss: 29.618; l2dist: 4.371\n",
      "    step: 100; loss: 18.314; l2dist: 3.714\n",
      "    step: 150; loss: 14.024; l2dist: 3.342\n",
      "    step: 200; loss: 12.224; l2dist: 3.146\n",
      "    step: 250; loss: 11.551; l2dist: 3.047\n",
      "    step: 300; loss: 10.825; l2dist: 2.973\n",
      "    step: 350; loss: 10.567; l2dist: 2.955\n",
      "    step: 400; loss: 10.175; l2dist: 2.906\n",
      "    step: 450; loss: 10.024; l2dist: 2.880\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 160.541; l2dist: 0.000\n",
      "    step: 50; loss: 24.196; l2dist: 3.904\n",
      "    step: 100; loss: 16.769; l2dist: 3.402\n",
      "    step: 150; loss: 13.582; l2dist: 3.166\n",
      "    step: 200; loss: 12.023; l2dist: 3.028\n",
      "    step: 250; loss: 10.999; l2dist: 2.953\n",
      "    step: 300; loss: 10.478; l2dist: 2.900\n",
      "    step: 350; loss: 10.283; l2dist: 2.879\n",
      "    step: 400; loss: 10.088; l2dist: 2.866\n",
      "    step: 450; loss: 9.824; l2dist: 2.850\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 101.991; l2dist: 0.000\n",
      "    step: 50; loss: 20.472; l2dist: 3.476\n",
      "    step: 100; loss: 15.073; l2dist: 3.124\n",
      "    step: 150; loss: 12.686; l2dist: 2.991\n",
      "    step: 200; loss: 11.400; l2dist: 2.923\n",
      "    step: 250; loss: 10.515; l2dist: 2.853\n",
      "    step: 300; loss: 10.161; l2dist: 2.817\n",
      "    step: 350; loss: 9.679; l2dist: 2.801\n",
      "    step: 400; loss: 9.449; l2dist: 2.780\n",
      "    step: 450; loss: 9.496; l2dist: 2.787\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.029; l2dist: 0.000\n",
      "    step: 50; loss: 18.716; l2dist: 3.230\n",
      "    step: 100; loss: 14.349; l2dist: 3.005\n",
      "    step: 150; loss: 12.207; l2dist: 2.901\n",
      "    step: 200; loss: 11.011; l2dist: 2.844\n",
      "    step: 250; loss: 10.157; l2dist: 2.792\n",
      "    step: 300; loss: 9.629; l2dist: 2.776\n",
      "    step: 350; loss: 9.433; l2dist: 2.764\n",
      "    step: 400; loss: 9.301; l2dist: 2.755\n",
      "    step: 450; loss: 9.166; l2dist: 2.748\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.503; l2dist: 0.000\n",
      "    step: 50; loss: 17.806; l2dist: 3.083\n",
      "    step: 100; loss: 14.178; l2dist: 2.924\n",
      "    step: 150; loss: 12.097; l2dist: 2.856\n",
      "    step: 200; loss: 10.909; l2dist: 2.796\n",
      "    step: 250; loss: 10.071; l2dist: 2.759\n",
      "    step: 300; loss: 9.580; l2dist: 2.751\n",
      "    step: 350; loss: 9.267; l2dist: 2.728\n",
      "    step: 400; loss: 9.179; l2dist: 2.720\n",
      "    step: 450; loss: 9.143; l2dist: 2.719\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.101; l2dist: 0.000\n",
      "    step: 50; loss: 17.545; l2dist: 3.038\n",
      "    step: 100; loss: 14.060; l2dist: 2.896\n",
      "    step: 150; loss: 12.100; l2dist: 2.826\n",
      "    step: 200; loss: 10.878; l2dist: 2.779\n",
      "    step: 250; loss: 10.138; l2dist: 2.748\n",
      "    step: 300; loss: 9.665; l2dist: 2.724\n",
      "    step: 350; loss: 9.372; l2dist: 2.708\n",
      "    step: 400; loss: 9.209; l2dist: 2.704\n",
      "    step: 450; loss: 9.140; l2dist: 2.702\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.990; l2dist: 0.000\n",
      "    step: 50; loss: 17.294; l2dist: 3.016\n",
      "    step: 100; loss: 14.050; l2dist: 2.893\n",
      "    step: 150; loss: 12.099; l2dist: 2.832\n",
      "    step: 200; loss: 10.940; l2dist: 2.782\n",
      "    step: 250; loss: 10.141; l2dist: 2.748\n",
      "    step: 300; loss: 9.652; l2dist: 2.734\n",
      "    step: 350; loss: 9.469; l2dist: 2.725\n",
      "    step: 400; loss: 9.269; l2dist: 2.709\n",
      "    step: 450; loss: 9.158; l2dist: 2.698\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.537; l2dist: 0.000\n",
      "    step: 50; loss: 17.487; l2dist: 3.037\n",
      "    step: 100; loss: 14.112; l2dist: 2.907\n",
      "    step: 150; loss: 12.172; l2dist: 2.848\n",
      "    step: 200; loss: 10.944; l2dist: 2.795\n",
      "    step: 250; loss: 10.167; l2dist: 2.761\n",
      "    step: 300; loss: 9.683; l2dist: 2.745\n",
      "    step: 350; loss: 9.397; l2dist: 2.736\n",
      "    step: 400; loss: 9.299; l2dist: 2.728\n",
      "    step: 450; loss: 9.191; l2dist: 2.722\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.696; l2dist: 0.000\n",
      "    step: 50; loss: 12.619; l2dist: 1.723\n",
      "    step: 100; loss: 12.281; l2dist: 1.725\n",
      "    step: 150; loss: 11.823; l2dist: 1.823\n",
      "    step: 200; loss: 11.355; l2dist: 1.877\n",
      "    step: 250; loss: 10.970; l2dist: 1.903\n",
      "    step: 300; loss: 10.665; l2dist: 1.910\n",
      "    step: 350; loss: 10.521; l2dist: 1.915\n",
      "    step: 400; loss: 10.406; l2dist: 1.916\n",
      "    step: 450; loss: 10.358; l2dist: 1.916\n",
      "binary step: 0; number of successful adv: 29/100\n",
      "    step: 0; loss: 119.961; l2dist: 0.000\n",
      "    step: 50; loss: 38.838; l2dist: 4.082\n",
      "    step: 100; loss: 23.454; l2dist: 3.722\n",
      "    step: 150; loss: 14.589; l2dist: 3.392\n",
      "    step: 200; loss: 12.469; l2dist: 3.170\n",
      "    step: 250; loss: 11.563; l2dist: 3.070\n",
      "    step: 300; loss: 10.941; l2dist: 2.989\n",
      "    step: 350; loss: 10.458; l2dist: 2.930\n",
      "    step: 400; loss: 10.251; l2dist: 2.905\n",
      "    step: 450; loss: 10.044; l2dist: 2.882\n",
      "binary step: 1; number of successful adv: 87/100\n",
      "    step: 0; loss: 198.661; l2dist: 0.000\n",
      "    step: 50; loss: 39.720; l2dist: 4.221\n",
      "    step: 100; loss: 23.851; l2dist: 4.016\n",
      "    step: 150; loss: 16.473; l2dist: 3.591\n",
      "    step: 200; loss: 13.670; l2dist: 3.329\n",
      "    step: 250; loss: 12.495; l2dist: 3.188\n",
      "    step: 300; loss: 11.794; l2dist: 3.114\n",
      "    step: 350; loss: 11.356; l2dist: 3.048\n",
      "    step: 400; loss: 10.768; l2dist: 2.971\n",
      "    step: 450; loss: 10.764; l2dist: 2.965\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.808; l2dist: 0.000\n",
      "    step: 50; loss: 29.727; l2dist: 3.687\n",
      "    step: 100; loss: 20.911; l2dist: 3.527\n",
      "    step: 150; loss: 14.721; l2dist: 3.343\n",
      "    step: 200; loss: 12.426; l2dist: 3.165\n",
      "    step: 250; loss: 11.185; l2dist: 3.019\n",
      "    step: 300; loss: 10.919; l2dist: 2.982\n",
      "    step: 350; loss: 10.405; l2dist: 2.917\n",
      "    step: 400; loss: 10.113; l2dist: 2.879\n",
      "    step: 450; loss: 10.181; l2dist: 2.888\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.971; l2dist: 0.000\n",
      "    step: 50; loss: 25.006; l2dist: 3.274\n",
      "    step: 100; loss: 18.919; l2dist: 3.212\n",
      "    step: 150; loss: 14.237; l2dist: 3.188\n",
      "    step: 200; loss: 11.920; l2dist: 3.071\n",
      "    step: 250; loss: 11.059; l2dist: 2.986\n",
      "    step: 300; loss: 10.541; l2dist: 2.934\n",
      "    step: 350; loss: 10.173; l2dist: 2.890\n",
      "    step: 400; loss: 9.968; l2dist: 2.868\n",
      "    step: 450; loss: 9.862; l2dist: 2.845\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.551; l2dist: 0.000\n",
      "    step: 50; loss: 22.884; l2dist: 3.107\n",
      "    step: 100; loss: 18.020; l2dist: 3.051\n",
      "    step: 150; loss: 14.218; l2dist: 3.075\n",
      "    step: 200; loss: 12.067; l2dist: 3.024\n",
      "    step: 250; loss: 11.137; l2dist: 2.963\n",
      "    step: 300; loss: 10.648; l2dist: 2.918\n",
      "    step: 350; loss: 10.090; l2dist: 2.871\n",
      "    step: 400; loss: 9.990; l2dist: 2.860\n",
      "    step: 450; loss: 9.779; l2dist: 2.837\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 57.873; l2dist: 0.000\n",
      "    step: 50; loss: 22.030; l2dist: 3.035\n",
      "    step: 100; loss: 17.551; l2dist: 3.009\n",
      "    step: 150; loss: 14.296; l2dist: 3.011\n",
      "    step: 200; loss: 12.006; l2dist: 2.997\n",
      "    step: 250; loss: 11.066; l2dist: 2.957\n",
      "    step: 300; loss: 10.601; l2dist: 2.921\n",
      "    step: 350; loss: 10.248; l2dist: 2.890\n",
      "    step: 400; loss: 9.990; l2dist: 2.873\n",
      "    step: 450; loss: 9.657; l2dist: 2.835\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 57.113; l2dist: 0.000\n",
      "    step: 50; loss: 21.852; l2dist: 3.023\n",
      "    step: 100; loss: 17.763; l2dist: 2.956\n",
      "    step: 150; loss: 14.817; l2dist: 2.975\n",
      "    step: 200; loss: 12.180; l2dist: 2.984\n",
      "    step: 250; loss: 11.222; l2dist: 2.946\n",
      "    step: 300; loss: 10.641; l2dist: 2.915\n",
      "    step: 350; loss: 10.324; l2dist: 2.884\n",
      "    step: 400; loss: 9.950; l2dist: 2.852\n",
      "    step: 450; loss: 9.847; l2dist: 2.848\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 57.296; l2dist: 0.000\n",
      "    step: 50; loss: 21.924; l2dist: 3.016\n",
      "    step: 100; loss: 17.749; l2dist: 2.957\n",
      "    step: 150; loss: 14.650; l2dist: 2.990\n",
      "    step: 200; loss: 12.223; l2dist: 2.989\n",
      "    step: 250; loss: 11.248; l2dist: 2.961\n",
      "    step: 300; loss: 10.781; l2dist: 2.929\n",
      "    step: 350; loss: 10.451; l2dist: 2.910\n",
      "    step: 400; loss: 10.146; l2dist: 2.879\n",
      "    step: 450; loss: 9.799; l2dist: 2.842\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.282; l2dist: 0.000\n",
      "    step: 50; loss: 22.063; l2dist: 3.037\n",
      "    step: 100; loss: 17.766; l2dist: 2.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 14.709; l2dist: 3.005\n",
      "    step: 200; loss: 12.206; l2dist: 2.993\n",
      "    step: 250; loss: 11.340; l2dist: 2.962\n",
      "    step: 300; loss: 10.627; l2dist: 2.913\n",
      "    step: 350; loss: 10.271; l2dist: 2.903\n",
      "    step: 400; loss: 10.033; l2dist: 2.876\n",
      "    step: 450; loss: 9.949; l2dist: 2.864\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.153; l2dist: 0.000\n",
      "    step: 50; loss: 13.497; l2dist: 1.604\n",
      "    step: 100; loss: 13.339; l2dist: 1.612\n",
      "    step: 150; loss: 12.949; l2dist: 1.711\n",
      "    step: 200; loss: 12.600; l2dist: 1.790\n",
      "    step: 250; loss: 12.252; l2dist: 1.852\n",
      "    step: 300; loss: 11.930; l2dist: 1.882\n",
      "    step: 350; loss: 11.687; l2dist: 1.888\n",
      "    step: 400; loss: 11.546; l2dist: 1.885\n",
      "    step: 450; loss: 11.471; l2dist: 1.887\n",
      "binary step: 0; number of successful adv: 30/100\n",
      "    step: 0; loss: 122.871; l2dist: 0.000\n",
      "    step: 50; loss: 46.324; l2dist: 4.130\n",
      "    step: 100; loss: 27.957; l2dist: 3.993\n",
      "    step: 150; loss: 16.594; l2dist: 3.604\n",
      "    step: 200; loss: 14.217; l2dist: 3.384\n",
      "    step: 250; loss: 13.109; l2dist: 3.266\n",
      "    step: 300; loss: 12.288; l2dist: 3.177\n",
      "    step: 350; loss: 11.926; l2dist: 3.127\n",
      "    step: 400; loss: 11.671; l2dist: 3.117\n",
      "    step: 450; loss: 11.293; l2dist: 3.067\n",
      "binary step: 1; number of successful adv: 94/100\n",
      "    step: 0; loss: 124.007; l2dist: 0.000\n",
      "    step: 50; loss: 38.021; l2dist: 3.956\n",
      "    step: 100; loss: 24.820; l2dist: 3.879\n",
      "    step: 150; loss: 16.934; l2dist: 3.595\n",
      "    step: 200; loss: 13.988; l2dist: 3.358\n",
      "    step: 250; loss: 12.690; l2dist: 3.244\n",
      "    step: 300; loss: 12.045; l2dist: 3.171\n",
      "    step: 350; loss: 11.629; l2dist: 3.105\n",
      "    step: 400; loss: 11.649; l2dist: 3.122\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 82.239; l2dist: 0.000\n",
      "    step: 50; loss: 31.413; l2dist: 3.510\n",
      "    step: 100; loss: 24.279; l2dist: 3.442\n",
      "    step: 150; loss: 17.001; l2dist: 3.463\n",
      "    step: 200; loss: 13.827; l2dist: 3.295\n",
      "    step: 250; loss: 12.410; l2dist: 3.171\n",
      "    step: 300; loss: 11.775; l2dist: 3.127\n",
      "    step: 350; loss: 11.345; l2dist: 3.067\n",
      "    step: 400; loss: 11.021; l2dist: 3.030\n",
      "    step: 450; loss: 10.882; l2dist: 3.029\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.775; l2dist: 0.000\n",
      "    step: 50; loss: 27.228; l2dist: 3.160\n",
      "    step: 100; loss: 21.975; l2dist: 3.143\n",
      "    step: 150; loss: 17.645; l2dist: 3.203\n",
      "    step: 200; loss: 14.717; l2dist: 3.178\n",
      "    step: 250; loss: 12.318; l2dist: 3.121\n",
      "    step: 300; loss: 11.726; l2dist: 3.079\n",
      "    step: 350; loss: 11.011; l2dist: 2.994\n",
      "    step: 400; loss: 10.775; l2dist: 2.964\n",
      "    step: 450; loss: 10.670; l2dist: 2.953\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 57.677; l2dist: 0.000\n",
      "    step: 50; loss: 25.420; l2dist: 3.045\n",
      "    step: 100; loss: 21.417; l2dist: 3.021\n",
      "    step: 150; loss: 17.422; l2dist: 3.119\n",
      "    step: 200; loss: 13.973; l2dist: 3.138\n",
      "    step: 250; loss: 12.086; l2dist: 3.081\n",
      "    step: 300; loss: 11.456; l2dist: 3.034\n",
      "    step: 350; loss: 10.971; l2dist: 2.984\n",
      "    step: 400; loss: 10.728; l2dist: 2.959\n",
      "    step: 450; loss: 10.521; l2dist: 2.942\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.787; l2dist: 0.000\n",
      "    step: 50; loss: 24.533; l2dist: 2.989\n",
      "    step: 100; loss: 21.050; l2dist: 2.957\n",
      "    step: 150; loss: 17.292; l2dist: 3.075\n",
      "    step: 200; loss: 13.720; l2dist: 3.139\n",
      "    step: 250; loss: 12.094; l2dist: 3.064\n",
      "    step: 300; loss: 11.514; l2dist: 3.046\n",
      "    step: 350; loss: 11.066; l2dist: 3.001\n",
      "    step: 400; loss: 10.763; l2dist: 2.972\n",
      "    step: 450; loss: 10.503; l2dist: 2.955\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.418; l2dist: 0.000\n",
      "    step: 50; loss: 24.225; l2dist: 2.974\n",
      "    step: 100; loss: 20.933; l2dist: 2.943\n",
      "    step: 150; loss: 17.212; l2dist: 3.086\n",
      "    step: 200; loss: 13.732; l2dist: 3.110\n",
      "    step: 250; loss: 12.071; l2dist: 3.077\n",
      "    step: 300; loss: 11.364; l2dist: 3.026\n",
      "    step: 350; loss: 10.974; l2dist: 3.001\n",
      "    step: 400; loss: 10.736; l2dist: 2.977\n",
      "    step: 450; loss: 10.578; l2dist: 2.957\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.689; l2dist: 0.000\n",
      "    step: 50; loss: 23.936; l2dist: 2.958\n",
      "    step: 100; loss: 20.768; l2dist: 2.927\n",
      "    step: 150; loss: 17.450; l2dist: 3.026\n",
      "    step: 200; loss: 14.464; l2dist: 3.095\n",
      "    step: 250; loss: 12.225; l2dist: 3.068\n",
      "    step: 300; loss: 11.425; l2dist: 3.036\n",
      "    step: 350; loss: 10.979; l2dist: 2.997\n",
      "    step: 400; loss: 10.688; l2dist: 2.976\n",
      "    step: 450; loss: 10.561; l2dist: 2.965\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.337; l2dist: 0.000\n",
      "    step: 50; loss: 24.270; l2dist: 2.957\n",
      "    step: 100; loss: 20.891; l2dist: 2.946\n",
      "    step: 150; loss: 17.097; l2dist: 3.081\n",
      "    step: 200; loss: 14.094; l2dist: 3.090\n",
      "    step: 250; loss: 12.125; l2dist: 3.075\n",
      "    step: 300; loss: 11.459; l2dist: 3.046\n",
      "    step: 350; loss: 10.863; l2dist: 2.998\n",
      "    step: 400; loss: 10.627; l2dist: 2.973\n",
      "    step: 450; loss: 10.513; l2dist: 2.960\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.244; l2dist: 0.000\n",
      "    step: 50; loss: 11.899; l2dist: 1.706\n",
      "    step: 100; loss: 11.629; l2dist: 1.726\n",
      "    step: 150; loss: 11.218; l2dist: 1.797\n",
      "    step: 200; loss: 10.904; l2dist: 1.850\n",
      "    step: 250; loss: 10.697; l2dist: 1.887\n",
      "    step: 300; loss: 10.486; l2dist: 1.903\n",
      "    step: 350; loss: 10.421; l2dist: 1.906\n",
      "    step: 400; loss: 10.376; l2dist: 1.909\n",
      "    step: 450; loss: 10.347; l2dist: 1.912\n",
      "binary step: 0; number of successful adv: 31/100\n",
      "    step: 0; loss: 123.706; l2dist: 0.000\n",
      "    step: 50; loss: 37.012; l2dist: 4.010\n",
      "    step: 100; loss: 21.937; l2dist: 3.662\n",
      "    step: 150; loss: 14.752; l2dist: 3.366\n",
      "    step: 200; loss: 12.430; l2dist: 3.170\n",
      "    step: 250; loss: 11.422; l2dist: 3.046\n",
      "    step: 300; loss: 10.825; l2dist: 2.974\n",
      "    step: 350; loss: 10.558; l2dist: 2.941\n",
      "    step: 400; loss: 10.380; l2dist: 2.925\n",
      "    step: 450; loss: 10.227; l2dist: 2.905\n",
      "binary step: 1; number of successful adv: 94/100\n",
      "    step: 0; loss: 149.578; l2dist: 0.000\n",
      "    step: 50; loss: 35.306; l2dist: 4.015\n",
      "    step: 100; loss: 22.057; l2dist: 3.686\n",
      "    step: 150; loss: 14.773; l2dist: 3.440\n",
      "    step: 200; loss: 12.610; l2dist: 3.225\n",
      "    step: 250; loss: 11.540; l2dist: 3.093\n",
      "    step: 300; loss: 10.930; l2dist: 3.022\n",
      "    step: 350; loss: 10.598; l2dist: 2.978\n",
      "    step: 400; loss: 10.414; l2dist: 2.947\n",
      "    step: 450; loss: 10.202; l2dist: 2.928\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.185; l2dist: 0.000\n",
      "    step: 50; loss: 27.731; l2dist: 3.513\n",
      "    step: 100; loss: 20.744; l2dist: 3.287\n",
      "    step: 150; loss: 14.767; l2dist: 3.253\n",
      "    step: 200; loss: 12.053; l2dist: 3.128\n",
      "    step: 250; loss: 11.050; l2dist: 3.014\n",
      "    step: 300; loss: 10.702; l2dist: 2.979\n",
      "    step: 350; loss: 10.319; l2dist: 2.927\n",
      "    step: 400; loss: 10.037; l2dist: 2.889\n",
      "    step: 450; loss: 9.958; l2dist: 2.873\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.965; l2dist: 0.000\n",
      "    step: 50; loss: 23.096; l2dist: 3.125\n",
      "    step: 100; loss: 18.425; l2dist: 2.959\n",
      "    step: 150; loss: 15.106; l2dist: 2.976\n",
      "    step: 200; loss: 13.077; l2dist: 2.947\n",
      "    step: 250; loss: 11.976; l2dist: 2.907\n",
      "    step: 300; loss: 10.535; l2dist: 2.920\n",
      "    step: 350; loss: 10.198; l2dist: 2.896\n",
      "    step: 400; loss: 9.779; l2dist: 2.845\n",
      "    step: 450; loss: 9.717; l2dist: 2.829\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.556; l2dist: 0.000\n",
      "    step: 50; loss: 20.784; l2dist: 2.907\n",
      "    step: 100; loss: 17.098; l2dist: 2.813\n",
      "    step: 150; loss: 14.717; l2dist: 2.826\n",
      "    step: 200; loss: 12.933; l2dist: 2.833\n",
      "    step: 250; loss: 11.586; l2dist: 2.825\n",
      "    step: 300; loss: 10.685; l2dist: 2.820\n",
      "    step: 350; loss: 10.346; l2dist: 2.799\n",
      "    step: 400; loss: 10.093; l2dist: 2.777\n",
      "    step: 450; loss: 9.969; l2dist: 2.765\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.991; l2dist: 0.000\n",
      "    step: 50; loss: 20.053; l2dist: 2.830\n",
      "    step: 100; loss: 16.932; l2dist: 2.762\n",
      "    step: 150; loss: 14.904; l2dist: 2.770\n",
      "    step: 200; loss: 13.128; l2dist: 2.814\n",
      "    step: 250; loss: 11.903; l2dist: 2.833\n",
      "    step: 300; loss: 11.254; l2dist: 2.793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 10.949; l2dist: 2.771\n",
      "    step: 400; loss: 10.730; l2dist: 2.754\n",
      "    step: 450; loss: 10.613; l2dist: 2.732\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.095; l2dist: 0.000\n",
      "    step: 50; loss: 19.556; l2dist: 2.783\n",
      "    step: 100; loss: 16.714; l2dist: 2.737\n",
      "    step: 150; loss: 14.629; l2dist: 2.763\n",
      "    step: 200; loss: 13.059; l2dist: 2.800\n",
      "    step: 250; loss: 11.970; l2dist: 2.806\n",
      "    step: 300; loss: 10.885; l2dist: 2.816\n",
      "    step: 350; loss: 10.505; l2dist: 2.778\n",
      "    step: 400; loss: 10.345; l2dist: 2.745\n",
      "    step: 450; loss: 10.124; l2dist: 2.731\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.758; l2dist: 0.000\n",
      "    step: 50; loss: 19.291; l2dist: 2.755\n",
      "    step: 100; loss: 16.559; l2dist: 2.716\n",
      "    step: 150; loss: 14.614; l2dist: 2.733\n",
      "    step: 200; loss: 13.102; l2dist: 2.781\n",
      "    step: 250; loss: 12.107; l2dist: 2.781\n",
      "    step: 300; loss: 11.555; l2dist: 2.766\n",
      "    step: 350; loss: 11.168; l2dist: 2.750\n",
      "    step: 400; loss: 10.790; l2dist: 2.753\n",
      "    step: 450; loss: 10.593; l2dist: 2.735\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.548; l2dist: 0.000\n",
      "    step: 50; loss: 19.494; l2dist: 2.779\n",
      "    step: 100; loss: 16.679; l2dist: 2.743\n",
      "    step: 150; loss: 14.712; l2dist: 2.749\n",
      "    step: 200; loss: 13.062; l2dist: 2.801\n",
      "    step: 250; loss: 12.287; l2dist: 2.776\n",
      "    step: 300; loss: 11.617; l2dist: 2.773\n",
      "    step: 350; loss: 11.233; l2dist: 2.752\n",
      "    step: 400; loss: 11.011; l2dist: 2.729\n",
      "    step: 450; loss: 10.919; l2dist: 2.722\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.343; l2dist: 0.000\n",
      "    step: 50; loss: 14.091; l2dist: 1.741\n",
      "    step: 100; loss: 13.767; l2dist: 1.765\n",
      "    step: 150; loss: 13.128; l2dist: 1.875\n",
      "    step: 200; loss: 12.650; l2dist: 1.930\n",
      "    step: 250; loss: 12.247; l2dist: 1.945\n",
      "    step: 300; loss: 11.958; l2dist: 1.969\n",
      "    step: 350; loss: 11.735; l2dist: 1.968\n",
      "    step: 400; loss: 11.657; l2dist: 1.962\n",
      "    step: 450; loss: 11.626; l2dist: 1.962\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 124.477; l2dist: 0.000\n",
      "    step: 50; loss: 45.095; l2dist: 4.152\n",
      "    step: 100; loss: 28.795; l2dist: 3.861\n",
      "    step: 150; loss: 16.616; l2dist: 3.530\n",
      "    step: 200; loss: 13.712; l2dist: 3.309\n",
      "    step: 250; loss: 12.424; l2dist: 3.165\n",
      "    step: 300; loss: 11.777; l2dist: 3.086\n",
      "    step: 350; loss: 11.203; l2dist: 3.035\n",
      "    step: 400; loss: 11.055; l2dist: 3.014\n",
      "    step: 450; loss: 10.762; l2dist: 2.971\n",
      "binary step: 1; number of successful adv: 88/100\n",
      "    step: 0; loss: 211.401; l2dist: 0.000\n",
      "    step: 50; loss: 48.324; l2dist: 4.406\n",
      "    step: 100; loss: 27.045; l2dist: 4.260\n",
      "    step: 150; loss: 18.548; l2dist: 3.792\n",
      "    step: 200; loss: 15.071; l2dist: 3.479\n",
      "    step: 250; loss: 13.343; l2dist: 3.295\n",
      "    step: 300; loss: 12.286; l2dist: 3.177\n",
      "    step: 350; loss: 12.086; l2dist: 3.144\n",
      "    step: 400; loss: 11.430; l2dist: 3.070\n",
      "    step: 450; loss: 11.216; l2dist: 3.046\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 125.227; l2dist: 0.000\n",
      "    step: 50; loss: 36.521; l2dist: 3.877\n",
      "    step: 100; loss: 24.164; l2dist: 3.782\n",
      "    step: 150; loss: 16.489; l2dist: 3.541\n",
      "    step: 200; loss: 13.425; l2dist: 3.325\n",
      "    step: 250; loss: 12.264; l2dist: 3.193\n",
      "    step: 300; loss: 11.741; l2dist: 3.117\n",
      "    step: 350; loss: 11.007; l2dist: 3.024\n",
      "    step: 400; loss: 10.923; l2dist: 3.018\n",
      "    step: 450; loss: 10.700; l2dist: 2.982\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.507; l2dist: 0.000\n",
      "    step: 50; loss: 29.818; l2dist: 3.439\n",
      "    step: 100; loss: 22.495; l2dist: 3.376\n",
      "    step: 150; loss: 16.053; l2dist: 3.366\n",
      "    step: 200; loss: 12.790; l2dist: 3.234\n",
      "    step: 250; loss: 11.571; l2dist: 3.083\n",
      "    step: 300; loss: 10.867; l2dist: 3.009\n",
      "    step: 350; loss: 10.680; l2dist: 2.978\n",
      "    step: 400; loss: 10.235; l2dist: 2.924\n",
      "    step: 450; loss: 10.145; l2dist: 2.913\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.661; l2dist: 0.000\n",
      "    step: 50; loss: 26.229; l2dist: 3.196\n",
      "    step: 100; loss: 21.255; l2dist: 3.134\n",
      "    step: 150; loss: 16.667; l2dist: 3.160\n",
      "    step: 200; loss: 13.263; l2dist: 3.155\n",
      "    step: 250; loss: 11.789; l2dist: 3.069\n",
      "    step: 300; loss: 10.907; l2dist: 2.968\n",
      "    step: 350; loss: 10.408; l2dist: 2.920\n",
      "    step: 400; loss: 10.129; l2dist: 2.888\n",
      "    step: 450; loss: 10.144; l2dist: 2.884\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.059; l2dist: 0.000\n",
      "    step: 50; loss: 25.092; l2dist: 3.087\n",
      "    step: 100; loss: 20.835; l2dist: 3.017\n",
      "    step: 150; loss: 17.209; l2dist: 3.081\n",
      "    step: 200; loss: 13.901; l2dist: 3.077\n",
      "    step: 250; loss: 11.417; l2dist: 3.025\n",
      "    step: 300; loss: 10.523; l2dist: 2.928\n",
      "    step: 350; loss: 10.195; l2dist: 2.887\n",
      "    step: 400; loss: 9.884; l2dist: 2.861\n",
      "    step: 450; loss: 9.818; l2dist: 2.859\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.811; l2dist: 0.000\n",
      "    step: 50; loss: 24.428; l2dist: 3.044\n",
      "    step: 100; loss: 20.490; l2dist: 2.993\n",
      "    step: 150; loss: 17.238; l2dist: 3.048\n",
      "    step: 200; loss: 13.799; l2dist: 3.077\n",
      "    step: 250; loss: 11.696; l2dist: 3.012\n",
      "    step: 300; loss: 11.031; l2dist: 2.968\n",
      "    step: 350; loss: 10.605; l2dist: 2.902\n",
      "    step: 400; loss: 10.316; l2dist: 2.888\n",
      "    step: 450; loss: 10.159; l2dist: 2.872\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.382; l2dist: 0.000\n",
      "    step: 50; loss: 24.142; l2dist: 3.017\n",
      "    step: 100; loss: 20.420; l2dist: 2.943\n",
      "    step: 150; loss: 17.439; l2dist: 3.010\n",
      "    step: 200; loss: 13.901; l2dist: 3.044\n",
      "    step: 250; loss: 11.635; l2dist: 3.001\n",
      "    step: 300; loss: 10.814; l2dist: 2.917\n",
      "    step: 350; loss: 10.523; l2dist: 2.882\n",
      "    step: 400; loss: 10.315; l2dist: 2.869\n",
      "    step: 450; loss: 10.022; l2dist: 2.828\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.360; l2dist: 0.000\n",
      "    step: 50; loss: 24.371; l2dist: 3.037\n",
      "    step: 100; loss: 20.585; l2dist: 2.971\n",
      "    step: 150; loss: 17.366; l2dist: 3.042\n",
      "    step: 200; loss: 13.356; l2dist: 3.091\n",
      "    step: 250; loss: 11.535; l2dist: 2.999\n",
      "    step: 300; loss: 10.710; l2dist: 2.926\n",
      "    step: 350; loss: 10.378; l2dist: 2.896\n",
      "    step: 400; loss: 10.095; l2dist: 2.857\n",
      "    step: 450; loss: 9.941; l2dist: 2.846\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 14.889; l2dist: 0.000\n",
      "    step: 50; loss: 10.479; l2dist: 1.602\n",
      "    step: 100; loss: 10.329; l2dist: 1.614\n",
      "    step: 150; loss: 9.962; l2dist: 1.700\n",
      "    step: 200; loss: 9.702; l2dist: 1.744\n",
      "    step: 250; loss: 9.493; l2dist: 1.770\n",
      "    step: 300; loss: 9.363; l2dist: 1.783\n",
      "    step: 350; loss: 9.287; l2dist: 1.794\n",
      "    step: 400; loss: 9.214; l2dist: 1.799\n",
      "    step: 450; loss: 9.099; l2dist: 1.800\n",
      "binary step: 0; number of successful adv: 31/100\n",
      "    step: 0; loss: 98.039; l2dist: 0.000\n",
      "    step: 50; loss: 31.655; l2dist: 3.703\n",
      "    step: 100; loss: 20.282; l2dist: 3.423\n",
      "    step: 150; loss: 14.381; l2dist: 3.088\n",
      "    step: 200; loss: 11.255; l2dist: 2.958\n",
      "    step: 250; loss: 10.135; l2dist: 2.863\n",
      "    step: 300; loss: 9.642; l2dist: 2.795\n",
      "    step: 350; loss: 9.392; l2dist: 2.751\n",
      "    step: 400; loss: 9.088; l2dist: 2.724\n",
      "    step: 450; loss: 8.939; l2dist: 2.710\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 151.730; l2dist: 0.000\n",
      "    step: 50; loss: 31.187; l2dist: 3.719\n",
      "    step: 100; loss: 20.772; l2dist: 3.421\n",
      "    step: 150; loss: 14.080; l2dist: 3.151\n",
      "    step: 200; loss: 11.490; l2dist: 2.952\n",
      "    step: 250; loss: 10.232; l2dist: 2.829\n",
      "    step: 300; loss: 9.671; l2dist: 2.767\n",
      "    step: 350; loss: 9.297; l2dist: 2.709\n",
      "    step: 400; loss: 9.153; l2dist: 2.703\n",
      "    step: 450; loss: 8.940; l2dist: 2.674\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.035; l2dist: 0.000\n",
      "    step: 50; loss: 23.709; l2dist: 3.247\n",
      "    step: 100; loss: 17.494; l2dist: 3.025\n",
      "    step: 150; loss: 13.350; l2dist: 2.951\n",
      "    step: 200; loss: 10.567; l2dist: 2.806\n",
      "    step: 250; loss: 9.535; l2dist: 2.744\n",
      "    step: 300; loss: 9.068; l2dist: 2.684\n",
      "    step: 350; loss: 8.546; l2dist: 2.629\n",
      "    step: 400; loss: 8.351; l2dist: 2.602\n",
      "    step: 450; loss: 8.304; l2dist: 2.598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.878; l2dist: 0.000\n",
      "    step: 50; loss: 19.413; l2dist: 2.938\n",
      "    step: 100; loss: 15.426; l2dist: 2.797\n",
      "    step: 150; loss: 12.302; l2dist: 2.788\n",
      "    step: 200; loss: 10.210; l2dist: 2.724\n",
      "    step: 250; loss: 9.317; l2dist: 2.665\n",
      "    step: 300; loss: 8.795; l2dist: 2.616\n",
      "    step: 350; loss: 8.624; l2dist: 2.612\n",
      "    step: 400; loss: 8.367; l2dist: 2.570\n",
      "    step: 450; loss: 8.274; l2dist: 2.552\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.431; l2dist: 0.000\n",
      "    step: 50; loss: 17.525; l2dist: 2.776\n",
      "    step: 100; loss: 14.389; l2dist: 2.702\n",
      "    step: 150; loss: 11.821; l2dist: 2.724\n",
      "    step: 200; loss: 10.155; l2dist: 2.694\n",
      "    step: 250; loss: 9.258; l2dist: 2.628\n",
      "    step: 300; loss: 8.989; l2dist: 2.614\n",
      "    step: 350; loss: 8.493; l2dist: 2.573\n",
      "    step: 400; loss: 8.249; l2dist: 2.550\n",
      "    step: 450; loss: 8.158; l2dist: 2.541\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.883; l2dist: 0.000\n",
      "    step: 50; loss: 16.556; l2dist: 2.714\n",
      "    step: 100; loss: 13.972; l2dist: 2.650\n",
      "    step: 150; loss: 11.564; l2dist: 2.683\n",
      "    step: 200; loss: 10.201; l2dist: 2.643\n",
      "    step: 250; loss: 9.508; l2dist: 2.619\n",
      "    step: 300; loss: 9.135; l2dist: 2.595\n",
      "    step: 350; loss: 8.846; l2dist: 2.564\n",
      "    step: 400; loss: 8.618; l2dist: 2.548\n",
      "    step: 450; loss: 8.543; l2dist: 2.536\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.977; l2dist: 0.000\n",
      "    step: 50; loss: 16.229; l2dist: 2.683\n",
      "    step: 100; loss: 13.909; l2dist: 2.626\n",
      "    step: 150; loss: 11.402; l2dist: 2.672\n",
      "    step: 200; loss: 10.053; l2dist: 2.633\n",
      "    step: 250; loss: 9.264; l2dist: 2.609\n",
      "    step: 300; loss: 8.813; l2dist: 2.574\n",
      "    step: 350; loss: 8.567; l2dist: 2.565\n",
      "    step: 400; loss: 8.432; l2dist: 2.543\n",
      "    step: 450; loss: 8.226; l2dist: 2.509\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.652; l2dist: 0.000\n",
      "    step: 50; loss: 16.054; l2dist: 2.659\n",
      "    step: 100; loss: 13.840; l2dist: 2.607\n",
      "    step: 150; loss: 11.425; l2dist: 2.659\n",
      "    step: 200; loss: 9.989; l2dist: 2.632\n",
      "    step: 250; loss: 9.088; l2dist: 2.590\n",
      "    step: 300; loss: 8.650; l2dist: 2.556\n",
      "    step: 350; loss: 8.279; l2dist: 2.538\n",
      "    step: 400; loss: 8.240; l2dist: 2.536\n",
      "    step: 450; loss: 7.959; l2dist: 2.511\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.420; l2dist: 0.000\n",
      "    step: 50; loss: 16.118; l2dist: 2.680\n",
      "    step: 100; loss: 13.881; l2dist: 2.622\n",
      "    step: 150; loss: 11.483; l2dist: 2.668\n",
      "    step: 200; loss: 10.029; l2dist: 2.646\n",
      "    step: 250; loss: 9.150; l2dist: 2.609\n",
      "    step: 300; loss: 8.574; l2dist: 2.566\n",
      "    step: 350; loss: 8.250; l2dist: 2.547\n",
      "    step: 400; loss: 8.127; l2dist: 2.531\n",
      "    step: 450; loss: 7.986; l2dist: 2.521\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.217; l2dist: 0.000\n",
      "    step: 50; loss: 12.229; l2dist: 1.850\n",
      "    step: 100; loss: 11.927; l2dist: 1.862\n",
      "    step: 150; loss: 11.432; l2dist: 1.937\n",
      "    step: 200; loss: 10.919; l2dist: 1.971\n",
      "    step: 250; loss: 10.614; l2dist: 1.993\n",
      "    step: 300; loss: 10.407; l2dist: 2.008\n",
      "    step: 350; loss: 10.280; l2dist: 2.013\n",
      "    step: 400; loss: 10.188; l2dist: 2.016\n",
      "    step: 450; loss: 10.055; l2dist: 2.017\n",
      "binary step: 0; number of successful adv: 36/100\n",
      "    step: 0; loss: 116.507; l2dist: 0.000\n",
      "    step: 50; loss: 33.182; l2dist: 3.930\n",
      "    step: 100; loss: 21.894; l2dist: 3.472\n",
      "    step: 150; loss: 14.134; l2dist: 3.188\n",
      "    step: 200; loss: 11.956; l2dist: 3.037\n",
      "    step: 250; loss: 10.979; l2dist: 2.924\n",
      "    step: 300; loss: 10.570; l2dist: 2.878\n",
      "    step: 350; loss: 10.217; l2dist: 2.835\n",
      "    step: 400; loss: 10.000; l2dist: 2.804\n",
      "    step: 450; loss: 9.903; l2dist: 2.804\n",
      "binary step: 1; number of successful adv: 93/100\n",
      "    step: 0; loss: 188.314; l2dist: 0.000\n",
      "    step: 50; loss: 33.666; l2dist: 4.076\n",
      "    step: 100; loss: 19.973; l2dist: 3.573\n",
      "    step: 150; loss: 14.406; l2dist: 3.251\n",
      "    step: 200; loss: 12.197; l2dist: 3.062\n",
      "    step: 250; loss: 11.113; l2dist: 2.947\n",
      "    step: 300; loss: 10.538; l2dist: 2.873\n",
      "    step: 350; loss: 10.304; l2dist: 2.842\n",
      "    step: 400; loss: 10.026; l2dist: 2.808\n",
      "    step: 450; loss: 9.691; l2dist: 2.779\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 190.879; l2dist: 0.000\n",
      "    step: 50; loss: 28.240; l2dist: 3.865\n",
      "    step: 100; loss: 17.786; l2dist: 3.422\n",
      "    step: 150; loss: 13.529; l2dist: 3.146\n",
      "    step: 200; loss: 11.454; l2dist: 2.973\n",
      "    step: 250; loss: 10.504; l2dist: 2.861\n",
      "    step: 300; loss: 9.944; l2dist: 2.790\n",
      "    step: 350; loss: 9.566; l2dist: 2.757\n",
      "    step: 400; loss: 9.314; l2dist: 2.726\n",
      "    step: 450; loss: 9.246; l2dist: 2.699\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 117.108; l2dist: 0.000\n",
      "    step: 50; loss: 23.196; l2dist: 3.426\n",
      "    step: 100; loss: 16.260; l2dist: 3.104\n",
      "    step: 150; loss: 12.384; l2dist: 2.927\n",
      "    step: 200; loss: 10.663; l2dist: 2.799\n",
      "    step: 250; loss: 9.898; l2dist: 2.741\n",
      "    step: 300; loss: 9.406; l2dist: 2.689\n",
      "    step: 350; loss: 9.093; l2dist: 2.658\n",
      "    step: 400; loss: 8.955; l2dist: 2.639\n",
      "    step: 450; loss: 8.909; l2dist: 2.630\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 81.725; l2dist: 0.000\n",
      "    step: 50; loss: 20.297; l2dist: 3.139\n",
      "    step: 100; loss: 15.200; l2dist: 2.920\n",
      "    step: 150; loss: 12.056; l2dist: 2.836\n",
      "    step: 200; loss: 10.505; l2dist: 2.732\n",
      "    step: 250; loss: 9.660; l2dist: 2.678\n",
      "    step: 300; loss: 9.307; l2dist: 2.671\n",
      "    step: 350; loss: 8.962; l2dist: 2.635\n",
      "    step: 400; loss: 8.807; l2dist: 2.624\n",
      "    step: 450; loss: 8.583; l2dist: 2.594\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.856; l2dist: 0.000\n",
      "    step: 50; loss: 18.824; l2dist: 2.985\n",
      "    step: 100; loss: 15.066; l2dist: 2.821\n",
      "    step: 150; loss: 12.433; l2dist: 2.800\n",
      "    step: 200; loss: 10.572; l2dist: 2.746\n",
      "    step: 250; loss: 9.760; l2dist: 2.674\n",
      "    step: 300; loss: 9.297; l2dist: 2.635\n",
      "    step: 350; loss: 8.989; l2dist: 2.621\n",
      "    step: 400; loss: 8.755; l2dist: 2.601\n",
      "    step: 450; loss: 8.635; l2dist: 2.589\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.655; l2dist: 0.000\n",
      "    step: 50; loss: 18.606; l2dist: 2.959\n",
      "    step: 100; loss: 14.894; l2dist: 2.790\n",
      "    step: 150; loss: 12.711; l2dist: 2.750\n",
      "    step: 200; loss: 10.933; l2dist: 2.743\n",
      "    step: 250; loss: 9.966; l2dist: 2.682\n",
      "    step: 300; loss: 9.320; l2dist: 2.639\n",
      "    step: 350; loss: 9.066; l2dist: 2.626\n",
      "    step: 400; loss: 8.853; l2dist: 2.610\n",
      "    step: 450; loss: 8.724; l2dist: 2.610\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.075; l2dist: 0.000\n",
      "    step: 50; loss: 18.400; l2dist: 2.940\n",
      "    step: 100; loss: 14.854; l2dist: 2.775\n",
      "    step: 150; loss: 12.555; l2dist: 2.756\n",
      "    step: 200; loss: 11.003; l2dist: 2.733\n",
      "    step: 250; loss: 10.035; l2dist: 2.682\n",
      "    step: 300; loss: 9.324; l2dist: 2.645\n",
      "    step: 350; loss: 9.036; l2dist: 2.633\n",
      "    step: 400; loss: 8.791; l2dist: 2.603\n",
      "    step: 450; loss: 8.725; l2dist: 2.605\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.282; l2dist: 0.000\n",
      "    step: 50; loss: 18.377; l2dist: 2.951\n",
      "    step: 100; loss: 14.758; l2dist: 2.792\n",
      "    step: 150; loss: 12.669; l2dist: 2.763\n",
      "    step: 200; loss: 10.899; l2dist: 2.739\n",
      "    step: 250; loss: 9.814; l2dist: 2.695\n",
      "    step: 300; loss: 9.316; l2dist: 2.644\n",
      "    step: 350; loss: 9.116; l2dist: 2.621\n",
      "    step: 400; loss: 8.866; l2dist: 2.601\n",
      "    step: 450; loss: 8.728; l2dist: 2.594\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.874; l2dist: 0.000\n",
      "    step: 50; loss: 13.087; l2dist: 1.882\n",
      "    step: 100; loss: 12.700; l2dist: 1.894\n",
      "    step: 150; loss: 12.215; l2dist: 1.937\n",
      "    step: 200; loss: 11.824; l2dist: 1.981\n",
      "    step: 250; loss: 11.511; l2dist: 1.997\n",
      "    step: 300; loss: 11.346; l2dist: 1.997\n",
      "    step: 350; loss: 11.014; l2dist: 2.023\n",
      "    step: 400; loss: 10.938; l2dist: 2.018\n",
      "    step: 450; loss: 10.881; l2dist: 2.016\n",
      "binary step: 0; number of successful adv: 30/100\n",
      "    step: 0; loss: 123.621; l2dist: 0.000\n",
      "    step: 50; loss: 38.590; l2dist: 4.206\n",
      "    step: 100; loss: 24.751; l2dist: 3.776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 14.612; l2dist: 3.396\n",
      "    step: 200; loss: 12.456; l2dist: 3.164\n",
      "    step: 250; loss: 11.522; l2dist: 3.059\n",
      "    step: 300; loss: 10.943; l2dist: 2.984\n",
      "    step: 350; loss: 10.714; l2dist: 2.953\n",
      "    step: 400; loss: 10.417; l2dist: 2.906\n",
      "    step: 450; loss: 10.143; l2dist: 2.891\n",
      "binary step: 1; number of successful adv: 93/100\n",
      "    step: 0; loss: 142.983; l2dist: 0.000\n",
      "    step: 50; loss: 34.511; l2dist: 4.106\n",
      "    step: 100; loss: 22.391; l2dist: 3.848\n",
      "    step: 150; loss: 14.873; l2dist: 3.457\n",
      "    step: 200; loss: 12.542; l2dist: 3.243\n",
      "    step: 250; loss: 11.669; l2dist: 3.130\n",
      "    step: 300; loss: 10.938; l2dist: 3.043\n",
      "    step: 350; loss: 10.595; l2dist: 2.990\n",
      "    step: 400; loss: 10.385; l2dist: 2.956\n",
      "    step: 450; loss: 10.466; l2dist: 2.964\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.738; l2dist: 0.000\n",
      "    step: 50; loss: 27.801; l2dist: 3.613\n",
      "    step: 100; loss: 20.972; l2dist: 3.454\n",
      "    step: 150; loss: 14.888; l2dist: 3.293\n",
      "    step: 200; loss: 12.145; l2dist: 3.138\n",
      "    step: 250; loss: 11.216; l2dist: 3.045\n",
      "    step: 300; loss: 10.627; l2dist: 2.970\n",
      "    step: 350; loss: 10.366; l2dist: 2.944\n",
      "    step: 400; loss: 10.148; l2dist: 2.907\n",
      "    step: 450; loss: 9.916; l2dist: 2.888\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.128; l2dist: 0.000\n",
      "    step: 50; loss: 24.020; l2dist: 3.284\n",
      "    step: 100; loss: 19.528; l2dist: 3.157\n",
      "    step: 150; loss: 15.038; l2dist: 3.169\n",
      "    step: 200; loss: 12.527; l2dist: 3.046\n",
      "    step: 250; loss: 11.537; l2dist: 2.953\n",
      "    step: 300; loss: 10.930; l2dist: 2.899\n",
      "    step: 350; loss: 10.588; l2dist: 2.864\n",
      "    step: 400; loss: 10.489; l2dist: 2.864\n",
      "    step: 450; loss: 9.931; l2dist: 2.877\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.822; l2dist: 0.000\n",
      "    step: 50; loss: 22.264; l2dist: 3.119\n",
      "    step: 100; loss: 18.727; l2dist: 3.021\n",
      "    step: 150; loss: 14.972; l2dist: 3.079\n",
      "    step: 200; loss: 12.590; l2dist: 3.013\n",
      "    step: 250; loss: 11.392; l2dist: 2.940\n",
      "    step: 300; loss: 10.843; l2dist: 2.883\n",
      "    step: 350; loss: 10.544; l2dist: 2.855\n",
      "    step: 400; loss: 10.266; l2dist: 2.816\n",
      "    step: 450; loss: 10.148; l2dist: 2.822\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.840; l2dist: 0.000\n",
      "    step: 50; loss: 21.564; l2dist: 3.063\n",
      "    step: 100; loss: 18.295; l2dist: 2.966\n",
      "    step: 150; loss: 15.305; l2dist: 3.007\n",
      "    step: 200; loss: 12.733; l2dist: 2.985\n",
      "    step: 250; loss: 11.214; l2dist: 2.933\n",
      "    step: 300; loss: 10.387; l2dist: 2.908\n",
      "    step: 350; loss: 9.949; l2dist: 2.861\n",
      "    step: 400; loss: 9.703; l2dist: 2.836\n",
      "    step: 450; loss: 9.655; l2dist: 2.832\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.953; l2dist: 0.000\n",
      "    step: 50; loss: 21.244; l2dist: 3.044\n",
      "    step: 100; loss: 18.221; l2dist: 2.936\n",
      "    step: 150; loss: 15.613; l2dist: 2.966\n",
      "    step: 200; loss: 13.144; l2dist: 2.973\n",
      "    step: 250; loss: 11.460; l2dist: 2.933\n",
      "    step: 300; loss: 10.610; l2dist: 2.924\n",
      "    step: 350; loss: 10.169; l2dist: 2.891\n",
      "    step: 400; loss: 10.003; l2dist: 2.875\n",
      "    step: 450; loss: 9.809; l2dist: 2.843\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.288; l2dist: 0.000\n",
      "    step: 50; loss: 20.975; l2dist: 3.033\n",
      "    step: 100; loss: 18.006; l2dist: 2.924\n",
      "    step: 150; loss: 15.347; l2dist: 2.961\n",
      "    step: 200; loss: 12.904; l2dist: 2.967\n",
      "    step: 250; loss: 11.155; l2dist: 2.924\n",
      "    step: 300; loss: 10.243; l2dist: 2.882\n",
      "    step: 350; loss: 10.106; l2dist: 2.873\n",
      "    step: 400; loss: 9.671; l2dist: 2.835\n",
      "    step: 450; loss: 9.458; l2dist: 2.806\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.931; l2dist: 0.000\n",
      "    step: 50; loss: 21.084; l2dist: 3.050\n",
      "    step: 100; loss: 18.096; l2dist: 2.931\n",
      "    step: 150; loss: 15.261; l2dist: 2.988\n",
      "    step: 200; loss: 12.842; l2dist: 2.981\n",
      "    step: 250; loss: 11.073; l2dist: 2.938\n",
      "    step: 300; loss: 10.355; l2dist: 2.891\n",
      "    step: 350; loss: 10.008; l2dist: 2.866\n",
      "    step: 400; loss: 9.726; l2dist: 2.847\n",
      "    step: 450; loss: 9.674; l2dist: 2.825\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.425; l2dist: 0.000\n",
      "    step: 50; loss: 10.166; l2dist: 1.619\n",
      "    step: 100; loss: 9.932; l2dist: 1.614\n",
      "    step: 150; loss: 9.592; l2dist: 1.683\n",
      "    step: 200; loss: 9.182; l2dist: 1.740\n",
      "    step: 250; loss: 8.879; l2dist: 1.757\n",
      "    step: 300; loss: 8.669; l2dist: 1.776\n",
      "    step: 350; loss: 8.522; l2dist: 1.772\n",
      "    step: 400; loss: 8.468; l2dist: 1.766\n",
      "    step: 450; loss: 8.436; l2dist: 1.764\n",
      "binary step: 0; number of successful adv: 34/100\n",
      "    step: 0; loss: 96.544; l2dist: 0.000\n",
      "    step: 50; loss: 30.224; l2dist: 3.589\n",
      "    step: 100; loss: 18.847; l2dist: 3.231\n",
      "    step: 150; loss: 12.192; l2dist: 2.991\n",
      "    step: 200; loss: 10.457; l2dist: 2.799\n",
      "    step: 250; loss: 9.584; l2dist: 2.699\n",
      "    step: 300; loss: 9.234; l2dist: 2.657\n",
      "    step: 350; loss: 8.955; l2dist: 2.630\n",
      "    step: 400; loss: 8.833; l2dist: 2.610\n",
      "    step: 450; loss: 8.564; l2dist: 2.586\n",
      "binary step: 1; number of successful adv: 92/100\n",
      "    step: 0; loss: 139.179; l2dist: 0.000\n",
      "    step: 50; loss: 30.370; l2dist: 3.543\n",
      "    step: 100; loss: 18.511; l2dist: 3.273\n",
      "    step: 150; loss: 12.884; l2dist: 2.995\n",
      "    step: 200; loss: 10.733; l2dist: 2.833\n",
      "    step: 250; loss: 9.838; l2dist: 2.723\n",
      "    step: 300; loss: 9.325; l2dist: 2.658\n",
      "    step: 350; loss: 8.956; l2dist: 2.620\n",
      "    step: 400; loss: 8.615; l2dist: 2.577\n",
      "    step: 450; loss: 8.541; l2dist: 2.568\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 166.239; l2dist: 0.000\n",
      "    step: 50; loss: 26.772; l2dist: 3.263\n",
      "    step: 100; loss: 18.288; l2dist: 3.052\n",
      "    step: 150; loss: 12.859; l2dist: 2.909\n",
      "    step: 200; loss: 10.748; l2dist: 2.772\n",
      "    step: 250; loss: 9.623; l2dist: 2.683\n",
      "    step: 300; loss: 9.036; l2dist: 2.623\n",
      "    step: 350; loss: 8.668; l2dist: 2.577\n",
      "    step: 400; loss: 8.780; l2dist: 2.587\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.690; l2dist: 0.000\n",
      "    step: 50; loss: 23.266; l2dist: 3.049\n",
      "    step: 100; loss: 16.812; l2dist: 2.891\n",
      "    step: 150; loss: 12.775; l2dist: 2.786\n",
      "    step: 200; loss: 10.813; l2dist: 2.712\n",
      "    step: 250; loss: 9.607; l2dist: 2.626\n",
      "    step: 300; loss: 8.954; l2dist: 2.565\n",
      "    step: 350; loss: 8.886; l2dist: 2.580\n",
      "    step: 400; loss: 8.440; l2dist: 2.521\n",
      "    step: 450; loss: 8.459; l2dist: 2.519\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.645; l2dist: 0.000\n",
      "    step: 50; loss: 20.847; l2dist: 2.883\n",
      "    step: 100; loss: 15.997; l2dist: 2.750\n",
      "    step: 150; loss: 12.542; l2dist: 2.705\n",
      "    step: 200; loss: 10.658; l2dist: 2.657\n",
      "    step: 250; loss: 9.515; l2dist: 2.586\n",
      "    step: 300; loss: 8.793; l2dist: 2.544\n",
      "    step: 350; loss: 8.569; l2dist: 2.528\n",
      "    step: 400; loss: 8.326; l2dist: 2.503\n",
      "    step: 450; loss: 8.247; l2dist: 2.497\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.131; l2dist: 0.000\n",
      "    step: 50; loss: 19.847; l2dist: 2.792\n",
      "    step: 100; loss: 15.550; l2dist: 2.702\n",
      "    step: 150; loss: 12.260; l2dist: 2.686\n",
      "    step: 200; loss: 10.393; l2dist: 2.645\n",
      "    step: 250; loss: 9.316; l2dist: 2.591\n",
      "    step: 300; loss: 8.839; l2dist: 2.553\n",
      "    step: 350; loss: 8.657; l2dist: 2.539\n",
      "    step: 400; loss: 8.225; l2dist: 2.498\n",
      "    step: 450; loss: 8.197; l2dist: 2.497\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.864; l2dist: 0.000\n",
      "    step: 50; loss: 19.338; l2dist: 2.755\n",
      "    step: 100; loss: 15.383; l2dist: 2.686\n",
      "    step: 150; loss: 12.208; l2dist: 2.669\n",
      "    step: 200; loss: 10.489; l2dist: 2.630\n",
      "    step: 250; loss: 9.326; l2dist: 2.587\n",
      "    step: 300; loss: 8.782; l2dist: 2.552\n",
      "    step: 350; loss: 8.606; l2dist: 2.525\n",
      "    step: 400; loss: 8.395; l2dist: 2.507\n",
      "    step: 450; loss: 8.288; l2dist: 2.488\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.223; l2dist: 0.000\n",
      "    step: 50; loss: 18.985; l2dist: 2.736\n",
      "    step: 100; loss: 15.227; l2dist: 2.676\n",
      "    step: 150; loss: 12.003; l2dist: 2.668\n",
      "    step: 200; loss: 10.290; l2dist: 2.620\n",
      "    step: 250; loss: 9.160; l2dist: 2.572\n",
      "    step: 300; loss: 8.704; l2dist: 2.538\n",
      "    step: 350; loss: 8.638; l2dist: 2.536\n",
      "    step: 400; loss: 8.367; l2dist: 2.506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 450; loss: 8.235; l2dist: 2.487\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.053; l2dist: 0.000\n",
      "    step: 50; loss: 19.210; l2dist: 2.753\n",
      "    step: 100; loss: 15.405; l2dist: 2.678\n",
      "    step: 150; loss: 12.136; l2dist: 2.668\n",
      "    step: 200; loss: 10.462; l2dist: 2.612\n",
      "    step: 250; loss: 9.248; l2dist: 2.583\n",
      "    step: 300; loss: 8.860; l2dist: 2.555\n",
      "    step: 350; loss: 8.426; l2dist: 2.520\n",
      "    step: 400; loss: 8.290; l2dist: 2.494\n",
      "    step: 450; loss: 8.255; l2dist: 2.497\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.137; l2dist: 0.000\n",
      "    step: 50; loss: 10.896; l2dist: 1.706\n",
      "    step: 100; loss: 10.694; l2dist: 1.712\n",
      "    step: 150; loss: 10.297; l2dist: 1.788\n",
      "    step: 200; loss: 9.920; l2dist: 1.843\n",
      "    step: 250; loss: 9.615; l2dist: 1.878\n",
      "    step: 300; loss: 9.351; l2dist: 1.896\n",
      "    step: 350; loss: 9.227; l2dist: 1.903\n",
      "    step: 400; loss: 9.085; l2dist: 1.917\n",
      "    step: 450; loss: 9.023; l2dist: 1.914\n",
      "binary step: 0; number of successful adv: 31/100\n",
      "    step: 0; loss: 106.755; l2dist: 0.000\n",
      "    step: 50; loss: 31.414; l2dist: 3.872\n",
      "    step: 100; loss: 18.760; l2dist: 3.454\n",
      "    step: 150; loss: 12.975; l2dist: 3.156\n",
      "    step: 200; loss: 11.152; l2dist: 2.979\n",
      "    step: 250; loss: 10.245; l2dist: 2.872\n",
      "    step: 300; loss: 9.878; l2dist: 2.833\n",
      "    step: 350; loss: 9.509; l2dist: 2.791\n",
      "    step: 400; loss: 9.220; l2dist: 2.755\n",
      "    step: 450; loss: 9.082; l2dist: 2.738\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 155.014; l2dist: 0.000\n",
      "    step: 50; loss: 34.007; l2dist: 3.845\n",
      "    step: 100; loss: 20.638; l2dist: 3.607\n",
      "    step: 150; loss: 14.160; l2dist: 3.279\n",
      "    step: 200; loss: 11.870; l2dist: 3.087\n",
      "    step: 250; loss: 11.003; l2dist: 2.981\n",
      "    step: 300; loss: 10.183; l2dist: 2.895\n",
      "    step: 350; loss: 9.834; l2dist: 2.846\n",
      "    step: 400; loss: 9.408; l2dist: 2.806\n",
      "    step: 450; loss: 9.218; l2dist: 2.783\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.336; l2dist: 0.000\n",
      "    step: 50; loss: 25.480; l2dist: 3.332\n",
      "    step: 100; loss: 18.506; l2dist: 3.158\n",
      "    step: 150; loss: 13.250; l2dist: 3.071\n",
      "    step: 200; loss: 11.232; l2dist: 2.962\n",
      "    step: 250; loss: 10.345; l2dist: 2.887\n",
      "    step: 300; loss: 9.637; l2dist: 2.802\n",
      "    step: 350; loss: 9.285; l2dist: 2.761\n",
      "    step: 400; loss: 9.123; l2dist: 2.755\n",
      "    step: 450; loss: 9.045; l2dist: 2.744\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.253; l2dist: 0.000\n",
      "    step: 50; loss: 20.897; l2dist: 2.960\n",
      "    step: 100; loss: 16.568; l2dist: 2.852\n",
      "    step: 150; loss: 12.766; l2dist: 2.903\n",
      "    step: 200; loss: 10.913; l2dist: 2.857\n",
      "    step: 250; loss: 9.876; l2dist: 2.800\n",
      "    step: 300; loss: 9.418; l2dist: 2.762\n",
      "    step: 350; loss: 9.177; l2dist: 2.740\n",
      "    step: 400; loss: 8.940; l2dist: 2.709\n",
      "    step: 450; loss: 8.849; l2dist: 2.691\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.111; l2dist: 0.000\n",
      "    step: 50; loss: 18.518; l2dist: 2.740\n",
      "    step: 100; loss: 15.664; l2dist: 2.680\n",
      "    step: 150; loss: 12.903; l2dist: 2.756\n",
      "    step: 200; loss: 11.309; l2dist: 2.756\n",
      "    step: 250; loss: 10.097; l2dist: 2.736\n",
      "    step: 300; loss: 9.489; l2dist: 2.702\n",
      "    step: 350; loss: 9.092; l2dist: 2.673\n",
      "    step: 400; loss: 8.994; l2dist: 2.658\n",
      "    step: 450; loss: 8.874; l2dist: 2.645\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.302; l2dist: 0.000\n",
      "    step: 50; loss: 17.900; l2dist: 2.703\n",
      "    step: 100; loss: 15.333; l2dist: 2.645\n",
      "    step: 150; loss: 13.154; l2dist: 2.714\n",
      "    step: 200; loss: 11.712; l2dist: 2.730\n",
      "    step: 250; loss: 10.439; l2dist: 2.749\n",
      "    step: 300; loss: 9.586; l2dist: 2.728\n",
      "    step: 350; loss: 9.340; l2dist: 2.711\n",
      "    step: 400; loss: 8.982; l2dist: 2.680\n",
      "    step: 450; loss: 8.844; l2dist: 2.662\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.249; l2dist: 0.000\n",
      "    step: 50; loss: 17.686; l2dist: 2.691\n",
      "    step: 100; loss: 15.388; l2dist: 2.628\n",
      "    step: 150; loss: 13.330; l2dist: 2.682\n",
      "    step: 200; loss: 11.741; l2dist: 2.726\n",
      "    step: 250; loss: 10.761; l2dist: 2.703\n",
      "    step: 300; loss: 10.241; l2dist: 2.667\n",
      "    step: 350; loss: 9.940; l2dist: 2.642\n",
      "    step: 400; loss: 9.778; l2dist: 2.629\n",
      "    step: 450; loss: 9.714; l2dist: 2.616\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.391; l2dist: 0.000\n",
      "    step: 50; loss: 17.540; l2dist: 2.676\n",
      "    step: 100; loss: 15.331; l2dist: 2.604\n",
      "    step: 150; loss: 13.335; l2dist: 2.669\n",
      "    step: 200; loss: 11.791; l2dist: 2.700\n",
      "    step: 250; loss: 10.605; l2dist: 2.737\n",
      "    step: 300; loss: 9.729; l2dist: 2.737\n",
      "    step: 350; loss: 9.187; l2dist: 2.690\n",
      "    step: 400; loss: 9.026; l2dist: 2.681\n",
      "    step: 450; loss: 8.855; l2dist: 2.660\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.268; l2dist: 0.000\n",
      "    step: 50; loss: 17.547; l2dist: 2.703\n",
      "    step: 100; loss: 15.275; l2dist: 2.621\n",
      "    step: 150; loss: 13.142; l2dist: 2.699\n",
      "    step: 200; loss: 11.534; l2dist: 2.741\n",
      "    step: 250; loss: 10.485; l2dist: 2.733\n",
      "    step: 300; loss: 9.874; l2dist: 2.708\n",
      "    step: 350; loss: 9.603; l2dist: 2.683\n",
      "    step: 400; loss: 9.376; l2dist: 2.664\n",
      "    step: 450; loss: 9.250; l2dist: 2.656\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.148; l2dist: 0.000\n",
      "    step: 50; loss: 11.888; l2dist: 1.674\n",
      "    step: 100; loss: 11.533; l2dist: 1.702\n",
      "    step: 150; loss: 11.050; l2dist: 1.780\n",
      "    step: 200; loss: 10.644; l2dist: 1.836\n",
      "    step: 250; loss: 10.281; l2dist: 1.869\n",
      "    step: 300; loss: 9.984; l2dist: 1.890\n",
      "    step: 350; loss: 9.691; l2dist: 1.896\n",
      "    step: 400; loss: 9.577; l2dist: 1.897\n",
      "    step: 450; loss: 9.521; l2dist: 1.901\n",
      "binary step: 0; number of successful adv: 37/100\n",
      "    step: 0; loss: 97.627; l2dist: 0.000\n",
      "    step: 50; loss: 34.987; l2dist: 3.718\n",
      "    step: 100; loss: 21.476; l2dist: 3.419\n",
      "    step: 150; loss: 14.656; l2dist: 3.148\n",
      "    step: 200; loss: 11.783; l2dist: 2.997\n",
      "    step: 250; loss: 10.837; l2dist: 2.894\n",
      "    step: 300; loss: 10.229; l2dist: 2.823\n",
      "    step: 350; loss: 9.963; l2dist: 2.786\n",
      "    step: 400; loss: 9.643; l2dist: 2.754\n",
      "    step: 450; loss: 9.403; l2dist: 2.722\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 136.707; l2dist: 0.000\n",
      "    step: 50; loss: 33.215; l2dist: 3.626\n",
      "    step: 100; loss: 21.603; l2dist: 3.498\n",
      "    step: 150; loss: 15.031; l2dist: 3.252\n",
      "    step: 200; loss: 12.160; l2dist: 3.022\n",
      "    step: 250; loss: 10.833; l2dist: 2.896\n",
      "    step: 300; loss: 10.177; l2dist: 2.814\n",
      "    step: 350; loss: 9.778; l2dist: 2.764\n",
      "    step: 400; loss: 9.531; l2dist: 2.725\n",
      "    step: 450; loss: 9.456; l2dist: 2.723\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 85.310; l2dist: 0.000\n",
      "    step: 50; loss: 25.930; l2dist: 3.228\n",
      "    step: 100; loss: 19.471; l2dist: 3.131\n",
      "    step: 150; loss: 14.121; l2dist: 3.007\n",
      "    step: 200; loss: 11.348; l2dist: 2.896\n",
      "    step: 250; loss: 10.270; l2dist: 2.814\n",
      "    step: 300; loss: 9.824; l2dist: 2.766\n",
      "    step: 350; loss: 9.467; l2dist: 2.724\n",
      "    step: 400; loss: 9.197; l2dist: 2.684\n",
      "    step: 450; loss: 8.881; l2dist: 2.646\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.012; l2dist: 0.000\n",
      "    step: 50; loss: 22.040; l2dist: 2.932\n",
      "    step: 100; loss: 17.795; l2dist: 2.854\n",
      "    step: 150; loss: 14.366; l2dist: 2.828\n",
      "    step: 200; loss: 11.652; l2dist: 2.793\n",
      "    step: 250; loss: 10.039; l2dist: 2.740\n",
      "    step: 300; loss: 9.496; l2dist: 2.688\n",
      "    step: 350; loss: 9.239; l2dist: 2.652\n",
      "    step: 400; loss: 8.935; l2dist: 2.629\n",
      "    step: 450; loss: 8.729; l2dist: 2.600\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.486; l2dist: 0.000\n",
      "    step: 50; loss: 20.633; l2dist: 2.806\n",
      "    step: 100; loss: 17.212; l2dist: 2.746\n",
      "    step: 150; loss: 14.042; l2dist: 2.771\n",
      "    step: 200; loss: 11.594; l2dist: 2.756\n",
      "    step: 250; loss: 9.892; l2dist: 2.716\n",
      "    step: 300; loss: 9.432; l2dist: 2.680\n",
      "    step: 350; loss: 9.119; l2dist: 2.633\n",
      "    step: 400; loss: 8.897; l2dist: 2.620\n",
      "    step: 450; loss: 8.793; l2dist: 2.610\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.747; l2dist: 0.000\n",
      "    step: 50; loss: 19.702; l2dist: 2.719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 16.770; l2dist: 2.670\n",
      "    step: 150; loss: 14.314; l2dist: 2.685\n",
      "    step: 200; loss: 11.815; l2dist: 2.719\n",
      "    step: 250; loss: 10.235; l2dist: 2.700\n",
      "    step: 300; loss: 9.552; l2dist: 2.651\n",
      "    step: 350; loss: 9.156; l2dist: 2.617\n",
      "    step: 400; loss: 8.970; l2dist: 2.601\n",
      "    step: 450; loss: 8.851; l2dist: 2.591\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.962; l2dist: 0.000\n",
      "    step: 50; loss: 19.402; l2dist: 2.715\n",
      "    step: 100; loss: 16.688; l2dist: 2.668\n",
      "    step: 150; loss: 14.424; l2dist: 2.668\n",
      "    step: 200; loss: 11.844; l2dist: 2.704\n",
      "    step: 250; loss: 10.273; l2dist: 2.701\n",
      "    step: 300; loss: 9.587; l2dist: 2.658\n",
      "    step: 350; loss: 9.326; l2dist: 2.638\n",
      "    step: 400; loss: 9.023; l2dist: 2.623\n",
      "    step: 450; loss: 8.785; l2dist: 2.605\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.218; l2dist: 0.000\n",
      "    step: 50; loss: 19.259; l2dist: 2.701\n",
      "    step: 100; loss: 16.562; l2dist: 2.664\n",
      "    step: 150; loss: 14.200; l2dist: 2.675\n",
      "    step: 200; loss: 12.295; l2dist: 2.667\n",
      "    step: 250; loss: 10.685; l2dist: 2.650\n",
      "    step: 300; loss: 9.559; l2dist: 2.659\n",
      "    step: 350; loss: 9.205; l2dist: 2.644\n",
      "    step: 400; loss: 9.166; l2dist: 2.639\n",
      "    step: 450; loss: 8.728; l2dist: 2.602\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.904; l2dist: 0.000\n",
      "    step: 50; loss: 19.413; l2dist: 2.717\n",
      "    step: 100; loss: 16.655; l2dist: 2.670\n",
      "    step: 150; loss: 14.279; l2dist: 2.686\n",
      "    step: 200; loss: 12.234; l2dist: 2.680\n",
      "    step: 250; loss: 10.419; l2dist: 2.685\n",
      "    step: 300; loss: 9.589; l2dist: 2.663\n",
      "    step: 350; loss: 9.208; l2dist: 2.633\n",
      "    step: 400; loss: 8.932; l2dist: 2.610\n",
      "    step: 450; loss: 8.622; l2dist: 2.581\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.192; l2dist: 0.000\n",
      "    step: 50; loss: 11.554; l2dist: 1.739\n",
      "    step: 100; loss: 11.240; l2dist: 1.742\n",
      "    step: 150; loss: 10.776; l2dist: 1.820\n",
      "    step: 200; loss: 10.440; l2dist: 1.849\n",
      "    step: 250; loss: 10.127; l2dist: 1.869\n",
      "    step: 300; loss: 9.938; l2dist: 1.877\n",
      "    step: 350; loss: 9.805; l2dist: 1.878\n",
      "    step: 400; loss: 9.694; l2dist: 1.874\n",
      "    step: 450; loss: 9.602; l2dist: 1.879\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 110.413; l2dist: 0.000\n",
      "    step: 50; loss: 32.606; l2dist: 3.917\n",
      "    step: 100; loss: 22.019; l2dist: 3.470\n",
      "    step: 150; loss: 14.621; l2dist: 3.196\n",
      "    step: 200; loss: 12.961; l2dist: 3.055\n",
      "    step: 250; loss: 11.417; l2dist: 2.976\n",
      "    step: 300; loss: 10.443; l2dist: 2.929\n",
      "    step: 350; loss: 10.014; l2dist: 2.876\n",
      "    step: 400; loss: 9.792; l2dist: 2.839\n",
      "    step: 450; loss: 9.646; l2dist: 2.817\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 175.785; l2dist: 0.000\n",
      "    step: 50; loss: 37.594; l2dist: 4.072\n",
      "    step: 100; loss: 21.110; l2dist: 3.696\n",
      "    step: 150; loss: 14.429; l2dist: 3.340\n",
      "    step: 200; loss: 12.164; l2dist: 3.106\n",
      "    step: 250; loss: 11.093; l2dist: 2.993\n",
      "    step: 300; loss: 10.470; l2dist: 2.917\n",
      "    step: 350; loss: 10.014; l2dist: 2.862\n",
      "    step: 400; loss: 9.745; l2dist: 2.825\n",
      "    step: 450; loss: 9.459; l2dist: 2.786\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 197.857; l2dist: 0.000\n",
      "    step: 50; loss: 32.277; l2dist: 3.911\n",
      "    step: 100; loss: 19.441; l2dist: 3.584\n",
      "    step: 150; loss: 14.304; l2dist: 3.287\n",
      "    step: 200; loss: 12.175; l2dist: 3.077\n",
      "    step: 250; loss: 11.029; l2dist: 2.955\n",
      "    step: 300; loss: 10.264; l2dist: 2.876\n",
      "    step: 350; loss: 9.876; l2dist: 2.829\n",
      "    step: 400; loss: 9.657; l2dist: 2.799\n",
      "    step: 450; loss: 9.457; l2dist: 2.769\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 127.037; l2dist: 0.000\n",
      "    step: 50; loss: 26.770; l2dist: 3.545\n",
      "    step: 100; loss: 17.996; l2dist: 3.260\n",
      "    step: 150; loss: 13.526; l2dist: 3.123\n",
      "    step: 200; loss: 11.477; l2dist: 2.970\n",
      "    step: 250; loss: 10.602; l2dist: 2.895\n",
      "    step: 300; loss: 10.039; l2dist: 2.841\n",
      "    step: 350; loss: 9.649; l2dist: 2.797\n",
      "    step: 400; loss: 9.513; l2dist: 2.775\n",
      "    step: 450; loss: 9.394; l2dist: 2.756\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.915; l2dist: 0.000\n",
      "    step: 50; loss: 23.145; l2dist: 3.279\n",
      "    step: 100; loss: 16.717; l2dist: 3.072\n",
      "    step: 150; loss: 12.832; l2dist: 2.995\n",
      "    step: 200; loss: 11.026; l2dist: 2.912\n",
      "    step: 250; loss: 10.411; l2dist: 2.854\n",
      "    step: 300; loss: 9.966; l2dist: 2.816\n",
      "    step: 350; loss: 9.688; l2dist: 2.775\n",
      "    step: 400; loss: 9.421; l2dist: 2.747\n",
      "    step: 450; loss: 9.215; l2dist: 2.725\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.553; l2dist: 0.000\n",
      "    step: 50; loss: 22.182; l2dist: 3.152\n",
      "    step: 100; loss: 16.627; l2dist: 3.007\n",
      "    step: 150; loss: 12.730; l2dist: 2.947\n",
      "    step: 200; loss: 10.879; l2dist: 2.882\n",
      "    step: 250; loss: 10.090; l2dist: 2.817\n",
      "    step: 300; loss: 9.820; l2dist: 2.800\n",
      "    step: 350; loss: 9.379; l2dist: 2.749\n",
      "    step: 400; loss: 9.203; l2dist: 2.727\n",
      "    step: 450; loss: 8.983; l2dist: 2.704\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.020; l2dist: 0.000\n",
      "    step: 50; loss: 21.485; l2dist: 3.078\n",
      "    step: 100; loss: 16.753; l2dist: 2.963\n",
      "    step: 150; loss: 13.371; l2dist: 2.880\n",
      "    step: 200; loss: 11.546; l2dist: 2.821\n",
      "    step: 250; loss: 10.317; l2dist: 2.802\n",
      "    step: 300; loss: 9.837; l2dist: 2.764\n",
      "    step: 350; loss: 9.544; l2dist: 2.735\n",
      "    step: 400; loss: 9.167; l2dist: 2.705\n",
      "    step: 450; loss: 9.409; l2dist: 2.733\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.984; l2dist: 0.000\n",
      "    step: 50; loss: 21.081; l2dist: 3.025\n",
      "    step: 100; loss: 16.748; l2dist: 2.932\n",
      "    step: 150; loss: 13.313; l2dist: 2.875\n",
      "    step: 200; loss: 11.735; l2dist: 2.802\n",
      "    step: 250; loss: 10.683; l2dist: 2.774\n",
      "    step: 300; loss: 9.785; l2dist: 2.742\n",
      "    step: 350; loss: 9.594; l2dist: 2.728\n",
      "    step: 400; loss: 9.381; l2dist: 2.719\n",
      "    step: 450; loss: 9.035; l2dist: 2.675\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.692; l2dist: 0.000\n",
      "    step: 50; loss: 21.196; l2dist: 3.033\n",
      "    step: 100; loss: 16.839; l2dist: 2.935\n",
      "    step: 150; loss: 13.389; l2dist: 2.870\n",
      "    step: 200; loss: 11.832; l2dist: 2.799\n",
      "    step: 250; loss: 10.788; l2dist: 2.781\n",
      "    step: 300; loss: 9.820; l2dist: 2.738\n",
      "    step: 350; loss: 9.401; l2dist: 2.713\n",
      "    step: 400; loss: 9.271; l2dist: 2.696\n",
      "    step: 450; loss: 9.082; l2dist: 2.682\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.106; l2dist: 0.000\n",
      "    step: 50; loss: 12.209; l2dist: 1.728\n",
      "    step: 100; loss: 11.955; l2dist: 1.721\n",
      "    step: 150; loss: 11.561; l2dist: 1.796\n",
      "    step: 200; loss: 11.063; l2dist: 1.862\n",
      "    step: 250; loss: 10.783; l2dist: 1.882\n",
      "    step: 300; loss: 10.459; l2dist: 1.936\n",
      "    step: 350; loss: 10.231; l2dist: 1.947\n",
      "    step: 400; loss: 10.099; l2dist: 1.954\n",
      "    step: 450; loss: 10.033; l2dist: 1.951\n",
      "binary step: 0; number of successful adv: 34/100\n",
      "    step: 0; loss: 106.763; l2dist: 0.000\n",
      "    step: 50; loss: 33.733; l2dist: 3.883\n",
      "    step: 100; loss: 21.108; l2dist: 3.481\n",
      "    step: 150; loss: 14.538; l2dist: 3.204\n",
      "    step: 200; loss: 12.079; l2dist: 3.049\n",
      "    step: 250; loss: 11.225; l2dist: 2.960\n",
      "    step: 300; loss: 10.605; l2dist: 2.891\n",
      "    step: 350; loss: 10.288; l2dist: 2.850\n",
      "    step: 400; loss: 10.043; l2dist: 2.815\n",
      "    step: 450; loss: 9.907; l2dist: 2.814\n",
      "binary step: 1; number of successful adv: 88/100\n",
      "    step: 0; loss: 175.522; l2dist: 0.000\n",
      "    step: 50; loss: 36.887; l2dist: 3.981\n",
      "    step: 100; loss: 22.474; l2dist: 3.691\n",
      "    step: 150; loss: 15.678; l2dist: 3.346\n",
      "    step: 200; loss: 13.087; l2dist: 3.122\n",
      "    step: 250; loss: 11.563; l2dist: 2.980\n",
      "    step: 300; loss: 11.009; l2dist: 2.917\n",
      "    step: 350; loss: 10.849; l2dist: 2.902\n",
      "    step: 400; loss: 10.069; l2dist: 2.810\n",
      "    step: 450; loss: 10.333; l2dist: 2.837\n",
      "binary step: 2; number of successful adv: 98/100\n",
      "    step: 0; loss: 351.019; l2dist: 0.000\n",
      "    step: 50; loss: 35.845; l2dist: 4.185\n",
      "    step: 100; loss: 22.026; l2dist: 3.761\n",
      "    step: 150; loss: 16.890; l2dist: 3.462\n",
      "    step: 200; loss: 13.982; l2dist: 3.219\n",
      "    step: 250; loss: 12.478; l2dist: 3.090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 11.528; l2dist: 3.002\n",
      "    step: 350; loss: 11.031; l2dist: 2.944\n",
      "    step: 400; loss: 10.775; l2dist: 2.914\n",
      "    step: 450; loss: 10.426; l2dist: 2.875\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 210.167; l2dist: 0.000\n",
      "    step: 50; loss: 29.361; l2dist: 3.776\n",
      "    step: 100; loss: 19.607; l2dist: 3.425\n",
      "    step: 150; loss: 15.045; l2dist: 3.219\n",
      "    step: 200; loss: 12.872; l2dist: 3.070\n",
      "    step: 250; loss: 11.626; l2dist: 2.977\n",
      "    step: 300; loss: 10.837; l2dist: 2.908\n",
      "    step: 350; loss: 10.455; l2dist: 2.871\n",
      "    step: 400; loss: 10.199; l2dist: 2.844\n",
      "    step: 450; loss: 9.960; l2dist: 2.823\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 141.309; l2dist: 0.000\n",
      "    step: 50; loss: 25.655; l2dist: 3.447\n",
      "    step: 100; loss: 18.210; l2dist: 3.150\n",
      "    step: 150; loss: 14.187; l2dist: 3.028\n",
      "    step: 200; loss: 12.182; l2dist: 2.932\n",
      "    step: 250; loss: 11.136; l2dist: 2.884\n",
      "    step: 300; loss: 10.507; l2dist: 2.852\n",
      "    step: 350; loss: 10.106; l2dist: 2.815\n",
      "    step: 400; loss: 9.898; l2dist: 2.794\n",
      "    step: 450; loss: 9.581; l2dist: 2.764\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 107.764; l2dist: 0.000\n",
      "    step: 50; loss: 23.425; l2dist: 3.240\n",
      "    step: 100; loss: 17.902; l2dist: 3.013\n",
      "    step: 150; loss: 14.148; l2dist: 2.944\n",
      "    step: 200; loss: 12.183; l2dist: 2.899\n",
      "    step: 250; loss: 11.013; l2dist: 2.860\n",
      "    step: 300; loss: 10.356; l2dist: 2.834\n",
      "    step: 350; loss: 10.064; l2dist: 2.806\n",
      "    step: 400; loss: 9.738; l2dist: 2.770\n",
      "    step: 450; loss: 9.582; l2dist: 2.755\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 91.078; l2dist: 0.000\n",
      "    step: 50; loss: 22.287; l2dist: 3.143\n",
      "    step: 100; loss: 17.447; l2dist: 2.966\n",
      "    step: 150; loss: 13.828; l2dist: 2.919\n",
      "    step: 200; loss: 11.956; l2dist: 2.880\n",
      "    step: 250; loss: 10.845; l2dist: 2.844\n",
      "    step: 300; loss: 10.245; l2dist: 2.822\n",
      "    step: 350; loss: 9.859; l2dist: 2.783\n",
      "    step: 400; loss: 9.518; l2dist: 2.741\n",
      "    step: 450; loss: 9.276; l2dist: 2.725\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 82.753; l2dist: 0.000\n",
      "    step: 50; loss: 21.669; l2dist: 3.080\n",
      "    step: 100; loss: 17.238; l2dist: 2.928\n",
      "    step: 150; loss: 13.772; l2dist: 2.892\n",
      "    step: 200; loss: 11.976; l2dist: 2.861\n",
      "    step: 250; loss: 10.886; l2dist: 2.826\n",
      "    step: 300; loss: 10.317; l2dist: 2.807\n",
      "    step: 350; loss: 9.876; l2dist: 2.774\n",
      "    step: 400; loss: 10.012; l2dist: 2.789\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.192; l2dist: 0.000\n",
      "    step: 50; loss: 21.832; l2dist: 3.090\n",
      "    step: 100; loss: 17.340; l2dist: 2.941\n",
      "    step: 150; loss: 13.818; l2dist: 2.906\n",
      "    step: 200; loss: 11.970; l2dist: 2.859\n",
      "    step: 250; loss: 10.863; l2dist: 2.824\n",
      "    step: 300; loss: 10.272; l2dist: 2.807\n",
      "    step: 350; loss: 9.861; l2dist: 2.772\n",
      "    step: 400; loss: 9.770; l2dist: 2.762\n",
      "    step: 450; loss: 9.528; l2dist: 2.736\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.730; l2dist: 0.000\n",
      "    step: 50; loss: 12.364; l2dist: 1.763\n",
      "    step: 100; loss: 12.038; l2dist: 1.773\n",
      "    step: 150; loss: 11.400; l2dist: 1.851\n",
      "    step: 200; loss: 10.928; l2dist: 1.888\n",
      "    step: 250; loss: 10.600; l2dist: 1.910\n",
      "    step: 300; loss: 10.404; l2dist: 1.917\n",
      "    step: 350; loss: 10.293; l2dist: 1.913\n",
      "    step: 400; loss: 10.215; l2dist: 1.927\n",
      "    step: 450; loss: 10.146; l2dist: 1.928\n",
      "binary step: 0; number of successful adv: 34/100\n",
      "    step: 0; loss: 111.109; l2dist: 0.000\n",
      "    step: 50; loss: 34.215; l2dist: 3.899\n",
      "    step: 100; loss: 22.313; l2dist: 3.552\n",
      "    step: 150; loss: 13.995; l2dist: 3.264\n",
      "    step: 200; loss: 11.737; l2dist: 3.068\n",
      "    step: 250; loss: 10.809; l2dist: 2.959\n",
      "    step: 300; loss: 10.330; l2dist: 2.900\n",
      "    step: 350; loss: 10.084; l2dist: 2.875\n",
      "    step: 400; loss: 9.821; l2dist: 2.847\n",
      "    step: 450; loss: 9.578; l2dist: 2.818\n",
      "binary step: 1; number of successful adv: 93/100\n",
      "    step: 0; loss: 162.024; l2dist: 0.000\n",
      "    step: 50; loss: 31.827; l2dist: 3.856\n",
      "    step: 100; loss: 20.348; l2dist: 3.569\n",
      "    step: 150; loss: 14.130; l2dist: 3.275\n",
      "    step: 200; loss: 11.881; l2dist: 3.065\n",
      "    step: 250; loss: 10.851; l2dist: 2.962\n",
      "    step: 300; loss: 10.255; l2dist: 2.881\n",
      "    step: 350; loss: 9.733; l2dist: 2.838\n",
      "    step: 400; loss: 9.478; l2dist: 2.801\n",
      "    step: 450; loss: 9.483; l2dist: 2.804\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 97.490; l2dist: 0.000\n",
      "    step: 50; loss: 24.866; l2dist: 3.383\n",
      "    step: 100; loss: 18.211; l2dist: 3.198\n",
      "    step: 150; loss: 13.504; l2dist: 3.101\n",
      "    step: 200; loss: 11.254; l2dist: 2.965\n",
      "    step: 250; loss: 10.282; l2dist: 2.866\n",
      "    step: 300; loss: 9.856; l2dist: 2.815\n",
      "    step: 350; loss: 9.450; l2dist: 2.769\n",
      "    step: 400; loss: 9.448; l2dist: 2.771\n",
      "    step: 450; loss: 9.178; l2dist: 2.730\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.069; l2dist: 0.000\n",
      "    step: 50; loss: 20.980; l2dist: 3.082\n",
      "    step: 100; loss: 16.805; l2dist: 2.929\n",
      "    step: 150; loss: 13.186; l2dist: 2.948\n",
      "    step: 200; loss: 10.999; l2dist: 2.898\n",
      "    step: 250; loss: 10.017; l2dist: 2.825\n",
      "    step: 300; loss: 9.582; l2dist: 2.789\n",
      "    step: 350; loss: 9.060; l2dist: 2.725\n",
      "    step: 400; loss: 8.944; l2dist: 2.725\n",
      "    step: 450; loss: 8.801; l2dist: 2.701\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.626; l2dist: 0.000\n",
      "    step: 50; loss: 19.482; l2dist: 2.976\n",
      "    step: 100; loss: 16.321; l2dist: 2.852\n",
      "    step: 150; loss: 12.998; l2dist: 2.866\n",
      "    step: 200; loss: 11.072; l2dist: 2.828\n",
      "    step: 250; loss: 10.142; l2dist: 2.799\n",
      "    step: 300; loss: 9.657; l2dist: 2.750\n",
      "    step: 350; loss: 9.402; l2dist: 2.737\n",
      "    step: 400; loss: 9.184; l2dist: 2.718\n",
      "    step: 450; loss: 9.029; l2dist: 2.702\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.284; l2dist: 0.000\n",
      "    step: 50; loss: 18.816; l2dist: 2.897\n",
      "    step: 100; loss: 16.283; l2dist: 2.779\n",
      "    step: 150; loss: 13.115; l2dist: 2.847\n",
      "    step: 200; loss: 11.370; l2dist: 2.810\n",
      "    step: 250; loss: 10.071; l2dist: 2.804\n",
      "    step: 300; loss: 9.599; l2dist: 2.764\n",
      "    step: 350; loss: 9.237; l2dist: 2.729\n",
      "    step: 400; loss: 9.055; l2dist: 2.717\n",
      "    step: 450; loss: 8.965; l2dist: 2.703\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.589; l2dist: 0.000\n",
      "    step: 50; loss: 18.677; l2dist: 2.880\n",
      "    step: 100; loss: 16.084; l2dist: 2.778\n",
      "    step: 150; loss: 13.138; l2dist: 2.839\n",
      "    step: 200; loss: 11.275; l2dist: 2.816\n",
      "    step: 250; loss: 10.212; l2dist: 2.814\n",
      "    step: 300; loss: 9.650; l2dist: 2.776\n",
      "    step: 350; loss: 9.374; l2dist: 2.743\n",
      "    step: 400; loss: 9.140; l2dist: 2.736\n",
      "    step: 450; loss: 9.103; l2dist: 2.717\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.154; l2dist: 0.000\n",
      "    step: 50; loss: 18.611; l2dist: 2.874\n",
      "    step: 100; loss: 16.047; l2dist: 2.782\n",
      "    step: 150; loss: 13.075; l2dist: 2.823\n",
      "    step: 200; loss: 11.180; l2dist: 2.797\n",
      "    step: 250; loss: 10.087; l2dist: 2.784\n",
      "    step: 300; loss: 9.555; l2dist: 2.754\n",
      "    step: 350; loss: 9.291; l2dist: 2.742\n",
      "    step: 400; loss: 9.090; l2dist: 2.721\n",
      "    step: 450; loss: 8.929; l2dist: 2.701\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.882; l2dist: 0.000\n",
      "    step: 50; loss: 18.678; l2dist: 2.885\n",
      "    step: 100; loss: 16.090; l2dist: 2.787\n",
      "    step: 150; loss: 13.249; l2dist: 2.837\n",
      "    step: 200; loss: 11.278; l2dist: 2.817\n",
      "    step: 250; loss: 10.135; l2dist: 2.800\n",
      "    step: 300; loss: 9.615; l2dist: 2.768\n",
      "    step: 350; loss: 9.266; l2dist: 2.735\n",
      "    step: 400; loss: 9.144; l2dist: 2.728\n",
      "    step: 450; loss: 8.986; l2dist: 2.707\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.856; l2dist: 0.000\n",
      "    step: 50; loss: 11.259; l2dist: 1.795\n",
      "    step: 100; loss: 10.971; l2dist: 1.811\n",
      "    step: 150; loss: 10.530; l2dist: 1.907\n",
      "    step: 200; loss: 10.063; l2dist: 1.977\n",
      "    step: 250; loss: 9.690; l2dist: 2.006\n",
      "    step: 300; loss: 9.466; l2dist: 2.015\n",
      "    step: 350; loss: 9.340; l2dist: 2.013\n",
      "    step: 400; loss: 9.254; l2dist: 2.015\n",
      "    step: 450; loss: 9.176; l2dist: 2.025\n",
      "binary step: 0; number of successful adv: 34/100\n",
      "    step: 0; loss: 105.303; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 27.354; l2dist: 3.905\n",
      "    step: 100; loss: 16.680; l2dist: 3.462\n",
      "    step: 150; loss: 12.548; l2dist: 3.158\n",
      "    step: 200; loss: 11.209; l2dist: 3.018\n",
      "    step: 250; loss: 10.614; l2dist: 2.962\n",
      "    step: 300; loss: 10.156; l2dist: 2.910\n",
      "    step: 350; loss: 9.901; l2dist: 2.871\n",
      "    step: 400; loss: 9.798; l2dist: 2.861\n",
      "    step: 450; loss: 9.678; l2dist: 2.842\n",
      "binary step: 1; number of successful adv: 92/100\n",
      "    step: 0; loss: 142.421; l2dist: 0.000\n",
      "    step: 50; loss: 27.713; l2dist: 3.819\n",
      "    step: 100; loss: 18.487; l2dist: 3.499\n",
      "    step: 150; loss: 13.728; l2dist: 3.212\n",
      "    step: 200; loss: 11.938; l2dist: 3.071\n",
      "    step: 250; loss: 11.082; l2dist: 3.010\n",
      "    step: 300; loss: 10.560; l2dist: 2.951\n",
      "    step: 350; loss: 10.124; l2dist: 2.897\n",
      "    step: 400; loss: 10.050; l2dist: 2.900\n",
      "    step: 450; loss: 9.796; l2dist: 2.859\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.161; l2dist: 0.000\n",
      "    step: 50; loss: 22.391; l2dist: 3.398\n",
      "    step: 100; loss: 17.166; l2dist: 3.220\n",
      "    step: 150; loss: 12.771; l2dist: 3.089\n",
      "    step: 200; loss: 11.165; l2dist: 2.965\n",
      "    step: 250; loss: 10.475; l2dist: 2.932\n",
      "    step: 300; loss: 9.895; l2dist: 2.871\n",
      "    step: 350; loss: 9.749; l2dist: 2.860\n",
      "    step: 400; loss: 9.570; l2dist: 2.840\n",
      "    step: 450; loss: 9.466; l2dist: 2.822\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.393; l2dist: 0.000\n",
      "    step: 50; loss: 19.774; l2dist: 3.168\n",
      "    step: 100; loss: 15.798; l2dist: 3.022\n",
      "    step: 150; loss: 13.092; l2dist: 2.948\n",
      "    step: 200; loss: 11.164; l2dist: 2.901\n",
      "    step: 250; loss: 10.237; l2dist: 2.857\n",
      "    step: 300; loss: 9.849; l2dist: 2.846\n",
      "    step: 350; loss: 9.517; l2dist: 2.796\n",
      "    step: 400; loss: 9.385; l2dist: 2.786\n",
      "    step: 450; loss: 9.222; l2dist: 2.777\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 72.247; l2dist: 0.000\n",
      "    step: 50; loss: 19.022; l2dist: 3.124\n",
      "    step: 100; loss: 15.494; l2dist: 2.967\n",
      "    step: 150; loss: 12.660; l2dist: 2.946\n",
      "    step: 200; loss: 11.125; l2dist: 2.879\n",
      "    step: 250; loss: 10.312; l2dist: 2.840\n",
      "    step: 300; loss: 9.822; l2dist: 2.806\n",
      "    step: 350; loss: 9.683; l2dist: 2.802\n",
      "    step: 400; loss: 9.406; l2dist: 2.774\n",
      "    step: 450; loss: 9.369; l2dist: 2.784\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.302; l2dist: 0.000\n",
      "    step: 50; loss: 18.126; l2dist: 3.039\n",
      "    step: 100; loss: 15.115; l2dist: 2.879\n",
      "    step: 150; loss: 12.531; l2dist: 2.880\n",
      "    step: 200; loss: 11.078; l2dist: 2.839\n",
      "    step: 250; loss: 10.305; l2dist: 2.796\n",
      "    step: 300; loss: 9.849; l2dist: 2.785\n",
      "    step: 350; loss: 9.459; l2dist: 2.774\n",
      "    step: 400; loss: 9.477; l2dist: 2.777\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.658; l2dist: 0.000\n",
      "    step: 50; loss: 18.028; l2dist: 3.029\n",
      "    step: 100; loss: 15.134; l2dist: 2.885\n",
      "    step: 150; loss: 12.667; l2dist: 2.891\n",
      "    step: 200; loss: 11.192; l2dist: 2.858\n",
      "    step: 250; loss: 10.359; l2dist: 2.816\n",
      "    step: 300; loss: 9.928; l2dist: 2.803\n",
      "    step: 350; loss: 9.625; l2dist: 2.779\n",
      "    step: 400; loss: 9.409; l2dist: 2.770\n",
      "    step: 450; loss: 9.310; l2dist: 2.770\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.945; l2dist: 0.000\n",
      "    step: 50; loss: 17.883; l2dist: 3.037\n",
      "    step: 100; loss: 15.060; l2dist: 2.887\n",
      "    step: 150; loss: 12.503; l2dist: 2.900\n",
      "    step: 200; loss: 11.064; l2dist: 2.848\n",
      "    step: 250; loss: 10.301; l2dist: 2.808\n",
      "    step: 300; loss: 9.843; l2dist: 2.790\n",
      "    step: 350; loss: 9.551; l2dist: 2.778\n",
      "    step: 400; loss: 9.235; l2dist: 2.761\n",
      "    step: 450; loss: 9.162; l2dist: 2.763\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.774; l2dist: 0.000\n",
      "    step: 50; loss: 17.988; l2dist: 3.051\n",
      "    step: 100; loss: 15.088; l2dist: 2.901\n",
      "    step: 150; loss: 12.615; l2dist: 2.904\n",
      "    step: 200; loss: 11.100; l2dist: 2.856\n",
      "    step: 250; loss: 10.352; l2dist: 2.819\n",
      "    step: 300; loss: 9.894; l2dist: 2.810\n",
      "    step: 350; loss: 9.571; l2dist: 2.786\n",
      "    step: 400; loss: 9.327; l2dist: 2.770\n",
      "    step: 450; loss: 9.155; l2dist: 2.764\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.804; l2dist: 0.000\n",
      "    step: 50; loss: 12.129; l2dist: 1.642\n",
      "    step: 100; loss: 11.868; l2dist: 1.658\n",
      "    step: 150; loss: 11.433; l2dist: 1.745\n",
      "    step: 200; loss: 10.963; l2dist: 1.834\n",
      "    step: 250; loss: 10.526; l2dist: 1.890\n",
      "    step: 300; loss: 10.067; l2dist: 1.930\n",
      "    step: 350; loss: 9.839; l2dist: 1.947\n",
      "    step: 400; loss: 9.631; l2dist: 1.958\n",
      "    step: 450; loss: 9.551; l2dist: 1.955\n",
      "binary step: 0; number of successful adv: 25/100\n",
      "    step: 0; loss: 120.275; l2dist: 0.000\n",
      "    step: 50; loss: 35.835; l2dist: 4.136\n",
      "    step: 100; loss: 21.754; l2dist: 3.755\n",
      "    step: 150; loss: 14.619; l2dist: 3.394\n",
      "    step: 200; loss: 12.485; l2dist: 3.177\n",
      "    step: 250; loss: 11.487; l2dist: 3.059\n",
      "    step: 300; loss: 10.773; l2dist: 2.989\n",
      "    step: 350; loss: 10.432; l2dist: 2.934\n",
      "    step: 400; loss: 10.152; l2dist: 2.908\n",
      "    step: 450; loss: 9.940; l2dist: 2.886\n",
      "binary step: 1; number of successful adv: 95/100\n",
      "    step: 0; loss: 118.519; l2dist: 0.000\n",
      "    step: 50; loss: 32.094; l2dist: 3.907\n",
      "    step: 100; loss: 20.486; l2dist: 3.609\n",
      "    step: 150; loss: 14.453; l2dist: 3.319\n",
      "    step: 200; loss: 12.159; l2dist: 3.138\n",
      "    step: 250; loss: 11.259; l2dist: 3.046\n",
      "    step: 300; loss: 10.695; l2dist: 2.985\n",
      "    step: 350; loss: 10.257; l2dist: 2.932\n",
      "    step: 400; loss: 9.889; l2dist: 2.887\n",
      "    step: 450; loss: 9.687; l2dist: 2.861\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 77.007; l2dist: 0.000\n",
      "    step: 50; loss: 25.683; l2dist: 3.440\n",
      "    step: 100; loss: 18.682; l2dist: 3.267\n",
      "    step: 150; loss: 13.829; l2dist: 3.152\n",
      "    step: 200; loss: 11.642; l2dist: 3.013\n",
      "    step: 250; loss: 10.539; l2dist: 2.926\n",
      "    step: 300; loss: 10.030; l2dist: 2.872\n",
      "    step: 350; loss: 9.661; l2dist: 2.829\n",
      "    step: 400; loss: 9.467; l2dist: 2.824\n",
      "    step: 450; loss: 9.273; l2dist: 2.792\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.631; l2dist: 0.000\n",
      "    step: 50; loss: 22.143; l2dist: 3.164\n",
      "    step: 100; loss: 17.330; l2dist: 3.036\n",
      "    step: 150; loss: 13.681; l2dist: 3.023\n",
      "    step: 200; loss: 11.640; l2dist: 2.939\n",
      "    step: 250; loss: 10.559; l2dist: 2.880\n",
      "    step: 300; loss: 9.959; l2dist: 2.830\n",
      "    step: 350; loss: 9.574; l2dist: 2.798\n",
      "    step: 400; loss: 9.426; l2dist: 2.782\n",
      "    step: 450; loss: 9.254; l2dist: 2.770\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.278; l2dist: 0.000\n",
      "    step: 50; loss: 20.679; l2dist: 3.011\n",
      "    step: 100; loss: 16.553; l2dist: 2.941\n",
      "    step: 150; loss: 13.608; l2dist: 2.930\n",
      "    step: 200; loss: 11.505; l2dist: 2.890\n",
      "    step: 250; loss: 10.449; l2dist: 2.823\n",
      "    step: 300; loss: 9.819; l2dist: 2.771\n",
      "    step: 350; loss: 9.358; l2dist: 2.731\n",
      "    step: 400; loss: 9.252; l2dist: 2.720\n",
      "    step: 450; loss: 9.201; l2dist: 2.717\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.740; l2dist: 0.000\n",
      "    step: 50; loss: 20.107; l2dist: 2.932\n",
      "    step: 100; loss: 16.262; l2dist: 2.884\n",
      "    step: 150; loss: 13.366; l2dist: 2.900\n",
      "    step: 200; loss: 11.460; l2dist: 2.875\n",
      "    step: 250; loss: 10.338; l2dist: 2.807\n",
      "    step: 300; loss: 9.586; l2dist: 2.751\n",
      "    step: 350; loss: 9.157; l2dist: 2.717\n",
      "    step: 400; loss: 9.029; l2dist: 2.712\n",
      "    step: 450; loss: 8.864; l2dist: 2.688\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.097; l2dist: 0.000\n",
      "    step: 50; loss: 19.763; l2dist: 2.915\n",
      "    step: 100; loss: 16.124; l2dist: 2.875\n",
      "    step: 150; loss: 13.174; l2dist: 2.891\n",
      "    step: 200; loss: 11.515; l2dist: 2.845\n",
      "    step: 250; loss: 10.577; l2dist: 2.800\n",
      "    step: 300; loss: 9.944; l2dist: 2.767\n",
      "    step: 350; loss: 9.488; l2dist: 2.727\n",
      "    step: 400; loss: 9.308; l2dist: 2.717\n",
      "    step: 450; loss: 9.172; l2dist: 2.706\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.176; l2dist: 0.000\n",
      "    step: 50; loss: 19.589; l2dist: 2.891\n",
      "    step: 100; loss: 16.024; l2dist: 2.866\n",
      "    step: 150; loss: 13.415; l2dist: 2.855\n",
      "    step: 200; loss: 11.671; l2dist: 2.850\n",
      "    step: 250; loss: 10.647; l2dist: 2.810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 9.927; l2dist: 2.767\n",
      "    step: 350; loss: 9.505; l2dist: 2.732\n",
      "    step: 400; loss: 9.344; l2dist: 2.718\n",
      "    step: 450; loss: 9.187; l2dist: 2.712\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.627; l2dist: 0.000\n",
      "    step: 50; loss: 19.711; l2dist: 2.907\n",
      "    step: 100; loss: 16.074; l2dist: 2.876\n",
      "    step: 150; loss: 13.482; l2dist: 2.865\n",
      "    step: 200; loss: 11.832; l2dist: 2.833\n",
      "    step: 250; loss: 10.653; l2dist: 2.829\n",
      "    step: 300; loss: 9.955; l2dist: 2.777\n",
      "    step: 350; loss: 9.504; l2dist: 2.736\n",
      "    step: 400; loss: 9.329; l2dist: 2.733\n",
      "    step: 450; loss: 9.151; l2dist: 2.715\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.773; l2dist: 0.000\n",
      "    step: 50; loss: 12.555; l2dist: 1.712\n",
      "    step: 100; loss: 12.293; l2dist: 1.726\n",
      "    step: 150; loss: 11.756; l2dist: 1.824\n",
      "    step: 200; loss: 11.260; l2dist: 1.879\n",
      "    step: 250; loss: 10.884; l2dist: 1.910\n",
      "    step: 300; loss: 10.631; l2dist: 1.928\n",
      "    step: 350; loss: 10.433; l2dist: 1.935\n",
      "    step: 400; loss: 10.299; l2dist: 1.950\n",
      "    step: 450; loss: 10.210; l2dist: 1.944\n",
      "binary step: 0; number of successful adv: 30/100\n",
      "    step: 0; loss: 120.912; l2dist: 0.000\n",
      "    step: 50; loss: 38.446; l2dist: 4.115\n",
      "    step: 100; loss: 23.642; l2dist: 3.771\n",
      "    step: 150; loss: 14.491; l2dist: 3.466\n",
      "    step: 200; loss: 12.650; l2dist: 3.268\n",
      "    step: 250; loss: 11.835; l2dist: 3.141\n",
      "    step: 300; loss: 11.343; l2dist: 3.104\n",
      "    step: 350; loss: 11.128; l2dist: 3.087\n",
      "    step: 400; loss: 10.773; l2dist: 3.047\n",
      "    step: 450; loss: 10.504; l2dist: 3.013\n",
      "binary step: 1; number of successful adv: 89/100\n",
      "    step: 0; loss: 215.981; l2dist: 0.000\n",
      "    step: 50; loss: 43.489; l2dist: 4.490\n",
      "    step: 100; loss: 23.561; l2dist: 4.107\n",
      "    step: 150; loss: 16.506; l2dist: 3.712\n",
      "    step: 200; loss: 14.127; l2dist: 3.462\n",
      "    step: 250; loss: 12.868; l2dist: 3.296\n",
      "    step: 300; loss: 11.936; l2dist: 3.195\n",
      "    step: 350; loss: 11.251; l2dist: 3.106\n",
      "    step: 400; loss: 11.380; l2dist: 3.130\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 125.670; l2dist: 0.000\n",
      "    step: 50; loss: 32.483; l2dist: 3.951\n",
      "    step: 100; loss: 20.977; l2dist: 3.648\n",
      "    step: 150; loss: 14.481; l2dist: 3.437\n",
      "    step: 200; loss: 12.687; l2dist: 3.261\n",
      "    step: 250; loss: 11.691; l2dist: 3.144\n",
      "    step: 300; loss: 11.124; l2dist: 3.081\n",
      "    step: 350; loss: 10.906; l2dist: 3.054\n",
      "    step: 400; loss: 10.553; l2dist: 3.003\n",
      "    step: 450; loss: 10.510; l2dist: 2.984\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 93.232; l2dist: 0.000\n",
      "    step: 50; loss: 27.071; l2dist: 3.645\n",
      "    step: 100; loss: 19.742; l2dist: 3.390\n",
      "    step: 150; loss: 14.196; l2dist: 3.291\n",
      "    step: 200; loss: 12.167; l2dist: 3.173\n",
      "    step: 250; loss: 11.181; l2dist: 3.065\n",
      "    step: 300; loss: 10.631; l2dist: 3.008\n",
      "    step: 350; loss: 10.322; l2dist: 2.969\n",
      "    step: 400; loss: 10.244; l2dist: 2.948\n",
      "    step: 450; loss: 10.053; l2dist: 2.937\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.129; l2dist: 0.000\n",
      "    step: 50; loss: 24.847; l2dist: 3.421\n",
      "    step: 100; loss: 18.640; l2dist: 3.189\n",
      "    step: 150; loss: 13.996; l2dist: 3.148\n",
      "    step: 200; loss: 11.828; l2dist: 3.054\n",
      "    step: 250; loss: 11.026; l2dist: 3.012\n",
      "    step: 300; loss: 10.480; l2dist: 2.950\n",
      "    step: 350; loss: 10.011; l2dist: 2.886\n",
      "    step: 400; loss: 9.715; l2dist: 2.877\n",
      "    step: 450; loss: 9.692; l2dist: 2.870\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.808; l2dist: 0.000\n",
      "    step: 50; loss: 23.724; l2dist: 3.239\n",
      "    step: 100; loss: 18.736; l2dist: 3.045\n",
      "    step: 150; loss: 14.991; l2dist: 3.013\n",
      "    step: 200; loss: 12.302; l2dist: 3.018\n",
      "    step: 250; loss: 11.050; l2dist: 2.956\n",
      "    step: 300; loss: 10.561; l2dist: 2.930\n",
      "    step: 350; loss: 10.143; l2dist: 2.884\n",
      "    step: 400; loss: 9.765; l2dist: 2.838\n",
      "    step: 450; loss: 9.551; l2dist: 2.819\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.051; l2dist: 0.000\n",
      "    step: 50; loss: 22.884; l2dist: 3.178\n",
      "    step: 100; loss: 17.817; l2dist: 3.024\n",
      "    step: 150; loss: 14.396; l2dist: 3.001\n",
      "    step: 200; loss: 12.130; l2dist: 2.980\n",
      "    step: 250; loss: 10.939; l2dist: 2.943\n",
      "    step: 300; loss: 10.574; l2dist: 2.932\n",
      "    step: 350; loss: 10.165; l2dist: 2.887\n",
      "    step: 400; loss: 9.810; l2dist: 2.855\n",
      "    step: 450; loss: 9.497; l2dist: 2.819\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.966; l2dist: 0.000\n",
      "    step: 50; loss: 22.483; l2dist: 3.137\n",
      "    step: 100; loss: 17.708; l2dist: 2.999\n",
      "    step: 150; loss: 14.497; l2dist: 2.971\n",
      "    step: 200; loss: 12.249; l2dist: 2.958\n",
      "    step: 250; loss: 11.092; l2dist: 2.935\n",
      "    step: 300; loss: 10.579; l2dist: 2.916\n",
      "    step: 350; loss: 10.129; l2dist: 2.879\n",
      "    step: 400; loss: 9.802; l2dist: 2.854\n",
      "    step: 450; loss: 9.663; l2dist: 2.834\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.792; l2dist: 0.000\n",
      "    step: 50; loss: 22.640; l2dist: 3.158\n",
      "    step: 100; loss: 17.975; l2dist: 3.003\n",
      "    step: 150; loss: 14.877; l2dist: 2.976\n",
      "    step: 200; loss: 12.412; l2dist: 2.982\n",
      "    step: 250; loss: 11.181; l2dist: 2.975\n",
      "    step: 300; loss: 10.437; l2dist: 2.932\n",
      "    step: 350; loss: 10.162; l2dist: 2.901\n",
      "    step: 400; loss: 9.727; l2dist: 2.855\n",
      "    step: 450; loss: 9.525; l2dist: 2.831\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.899; l2dist: 0.000\n",
      "    step: 50; loss: 10.709; l2dist: 1.789\n",
      "    step: 100; loss: 10.453; l2dist: 1.799\n",
      "    step: 150; loss: 10.070; l2dist: 1.857\n",
      "    step: 200; loss: 9.743; l2dist: 1.890\n",
      "    step: 250; loss: 9.517; l2dist: 1.900\n",
      "    step: 300; loss: 9.318; l2dist: 1.909\n",
      "    step: 350; loss: 9.155; l2dist: 1.924\n",
      "    step: 400; loss: 9.072; l2dist: 1.928\n",
      "    step: 450; loss: 8.958; l2dist: 1.931\n",
      "binary step: 0; number of successful adv: 38/100\n",
      "    step: 0; loss: 103.769; l2dist: 0.000\n",
      "    step: 50; loss: 28.849; l2dist: 3.725\n",
      "    step: 100; loss: 19.261; l2dist: 3.330\n",
      "    step: 150; loss: 12.221; l2dist: 3.087\n",
      "    step: 200; loss: 10.782; l2dist: 2.927\n",
      "    step: 250; loss: 9.935; l2dist: 2.836\n",
      "    step: 300; loss: 9.530; l2dist: 2.778\n",
      "    step: 350; loss: 9.146; l2dist: 2.734\n",
      "    step: 400; loss: 9.040; l2dist: 2.718\n",
      "    step: 450; loss: 8.858; l2dist: 2.696\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 168.789; l2dist: 0.000\n",
      "    step: 50; loss: 33.505; l2dist: 3.908\n",
      "    step: 100; loss: 19.665; l2dist: 3.565\n",
      "    step: 150; loss: 13.862; l2dist: 3.209\n",
      "    step: 200; loss: 11.549; l2dist: 3.028\n",
      "    step: 250; loss: 10.498; l2dist: 2.901\n",
      "    step: 300; loss: 10.261; l2dist: 2.866\n",
      "    step: 350; loss: 9.657; l2dist: 2.797\n",
      "    step: 400; loss: 9.355; l2dist: 2.748\n",
      "    step: 450; loss: 9.148; l2dist: 2.722\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 182.823; l2dist: 0.000\n",
      "    step: 50; loss: 27.915; l2dist: 3.728\n",
      "    step: 100; loss: 18.129; l2dist: 3.382\n",
      "    step: 150; loss: 13.148; l2dist: 3.130\n",
      "    step: 200; loss: 10.977; l2dist: 2.960\n",
      "    step: 250; loss: 10.363; l2dist: 2.879\n",
      "    step: 300; loss: 9.825; l2dist: 2.809\n",
      "    step: 350; loss: 9.443; l2dist: 2.754\n",
      "    step: 400; loss: 9.101; l2dist: 2.714\n",
      "    step: 450; loss: 9.388; l2dist: 2.742\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 116.017; l2dist: 0.000\n",
      "    step: 50; loss: 22.972; l2dist: 3.370\n",
      "    step: 100; loss: 16.296; l2dist: 3.099\n",
      "    step: 150; loss: 12.339; l2dist: 2.938\n",
      "    step: 200; loss: 10.544; l2dist: 2.825\n",
      "    step: 250; loss: 10.077; l2dist: 2.791\n",
      "    step: 300; loss: 9.513; l2dist: 2.748\n",
      "    step: 350; loss: 9.182; l2dist: 2.703\n",
      "    step: 400; loss: 9.012; l2dist: 2.691\n",
      "    step: 450; loss: 8.840; l2dist: 2.665\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.256; l2dist: 0.000\n",
      "    step: 50; loss: 20.227; l2dist: 3.149\n",
      "    step: 100; loss: 15.426; l2dist: 2.933\n",
      "    step: 150; loss: 12.025; l2dist: 2.850\n",
      "    step: 200; loss: 10.531; l2dist: 2.772\n",
      "    step: 250; loss: 9.692; l2dist: 2.714\n",
      "    step: 300; loss: 9.183; l2dist: 2.664\n",
      "    step: 350; loss: 8.807; l2dist: 2.637\n",
      "    step: 400; loss: 8.655; l2dist: 2.621\n",
      "    step: 450; loss: 8.574; l2dist: 2.613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.385; l2dist: 0.000\n",
      "    step: 50; loss: 19.262; l2dist: 3.038\n",
      "    step: 100; loss: 15.104; l2dist: 2.844\n",
      "    step: 150; loss: 11.872; l2dist: 2.814\n",
      "    step: 200; loss: 10.447; l2dist: 2.742\n",
      "    step: 250; loss: 9.514; l2dist: 2.680\n",
      "    step: 300; loss: 8.945; l2dist: 2.643\n",
      "    step: 350; loss: 8.786; l2dist: 2.629\n",
      "    step: 400; loss: 8.634; l2dist: 2.611\n",
      "    step: 450; loss: 8.579; l2dist: 2.605\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.730; l2dist: 0.000\n",
      "    step: 50; loss: 19.027; l2dist: 3.015\n",
      "    step: 100; loss: 14.923; l2dist: 2.821\n",
      "    step: 150; loss: 11.916; l2dist: 2.807\n",
      "    step: 200; loss: 10.462; l2dist: 2.730\n",
      "    step: 250; loss: 9.564; l2dist: 2.684\n",
      "    step: 300; loss: 9.157; l2dist: 2.663\n",
      "    step: 350; loss: 8.961; l2dist: 2.639\n",
      "    step: 400; loss: 8.834; l2dist: 2.647\n",
      "    step: 450; loss: 8.505; l2dist: 2.601\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.075; l2dist: 0.000\n",
      "    step: 50; loss: 18.993; l2dist: 3.034\n",
      "    step: 100; loss: 14.810; l2dist: 2.841\n",
      "    step: 150; loss: 11.749; l2dist: 2.799\n",
      "    step: 200; loss: 10.400; l2dist: 2.722\n",
      "    step: 250; loss: 9.566; l2dist: 2.677\n",
      "    step: 300; loss: 9.086; l2dist: 2.634\n",
      "    step: 350; loss: 8.725; l2dist: 2.616\n",
      "    step: 400; loss: 8.753; l2dist: 2.623\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 77.231; l2dist: 0.000\n",
      "    step: 50; loss: 19.207; l2dist: 3.054\n",
      "    step: 100; loss: 14.886; l2dist: 2.863\n",
      "    step: 150; loss: 11.791; l2dist: 2.809\n",
      "    step: 200; loss: 10.448; l2dist: 2.726\n",
      "    step: 250; loss: 9.605; l2dist: 2.683\n",
      "    step: 300; loss: 9.156; l2dist: 2.655\n",
      "    step: 350; loss: 8.803; l2dist: 2.629\n",
      "    step: 400; loss: 8.628; l2dist: 2.620\n",
      "    step: 450; loss: 8.626; l2dist: 2.619\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.581; l2dist: 0.000\n",
      "    step: 50; loss: 11.530; l2dist: 1.736\n",
      "    step: 100; loss: 11.222; l2dist: 1.745\n",
      "    step: 150; loss: 10.717; l2dist: 1.811\n",
      "    step: 200; loss: 10.326; l2dist: 1.866\n",
      "    step: 250; loss: 9.955; l2dist: 1.891\n",
      "    step: 300; loss: 9.765; l2dist: 1.895\n",
      "    step: 350; loss: 9.681; l2dist: 1.901\n",
      "    step: 400; loss: 9.565; l2dist: 1.908\n",
      "    step: 450; loss: 9.517; l2dist: 1.910\n",
      "binary step: 0; number of successful adv: 26/100\n",
      "    step: 0; loss: 114.404; l2dist: 0.000\n",
      "    step: 50; loss: 32.009; l2dist: 4.035\n",
      "    step: 100; loss: 19.566; l2dist: 3.542\n",
      "    step: 150; loss: 12.824; l2dist: 3.194\n",
      "    step: 200; loss: 11.348; l2dist: 3.022\n",
      "    step: 250; loss: 10.549; l2dist: 2.921\n",
      "    step: 300; loss: 9.956; l2dist: 2.852\n",
      "    step: 350; loss: 9.701; l2dist: 2.817\n",
      "    step: 400; loss: 9.505; l2dist: 2.800\n",
      "    step: 450; loss: 9.412; l2dist: 2.783\n",
      "binary step: 1; number of successful adv: 88/100\n",
      "    step: 0; loss: 227.761; l2dist: 0.000\n",
      "    step: 50; loss: 40.108; l2dist: 4.311\n",
      "    step: 100; loss: 21.822; l2dist: 3.880\n",
      "    step: 150; loss: 15.085; l2dist: 3.442\n",
      "    step: 200; loss: 12.662; l2dist: 3.170\n",
      "    step: 250; loss: 11.580; l2dist: 3.050\n",
      "    step: 300; loss: 10.997; l2dist: 2.967\n",
      "    step: 350; loss: 10.560; l2dist: 2.914\n",
      "    step: 400; loss: 10.072; l2dist: 2.860\n",
      "    step: 450; loss: 10.076; l2dist: 2.856\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 133.809; l2dist: 0.000\n",
      "    step: 50; loss: 30.158; l2dist: 3.777\n",
      "    step: 100; loss: 19.410; l2dist: 3.445\n",
      "    step: 150; loss: 13.543; l2dist: 3.207\n",
      "    step: 200; loss: 11.386; l2dist: 3.009\n",
      "    step: 250; loss: 10.540; l2dist: 2.914\n",
      "    step: 300; loss: 10.133; l2dist: 2.860\n",
      "    step: 350; loss: 9.750; l2dist: 2.818\n",
      "    step: 400; loss: 9.486; l2dist: 2.785\n",
      "    step: 450; loss: 9.247; l2dist: 2.762\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.406; l2dist: 0.000\n",
      "    step: 50; loss: 24.292; l2dist: 3.402\n",
      "    step: 100; loss: 17.495; l2dist: 3.157\n",
      "    step: 150; loss: 13.096; l2dist: 3.069\n",
      "    step: 200; loss: 11.022; l2dist: 2.942\n",
      "    step: 250; loss: 10.266; l2dist: 2.869\n",
      "    step: 300; loss: 9.620; l2dist: 2.786\n",
      "    step: 350; loss: 9.329; l2dist: 2.743\n",
      "    step: 400; loss: 9.126; l2dist: 2.730\n",
      "    step: 450; loss: 8.944; l2dist: 2.701\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 72.328; l2dist: 0.000\n",
      "    step: 50; loss: 21.227; l2dist: 3.175\n",
      "    step: 100; loss: 16.383; l2dist: 2.994\n",
      "    step: 150; loss: 12.821; l2dist: 2.962\n",
      "    step: 200; loss: 10.840; l2dist: 2.880\n",
      "    step: 250; loss: 9.911; l2dist: 2.796\n",
      "    step: 300; loss: 9.342; l2dist: 2.738\n",
      "    step: 350; loss: 9.063; l2dist: 2.703\n",
      "    step: 400; loss: 8.934; l2dist: 2.680\n",
      "    step: 450; loss: 8.748; l2dist: 2.661\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.641; l2dist: 0.000\n",
      "    step: 50; loss: 20.066; l2dist: 3.098\n",
      "    step: 100; loss: 15.832; l2dist: 2.957\n",
      "    step: 150; loss: 12.639; l2dist: 2.917\n",
      "    step: 200; loss: 10.870; l2dist: 2.844\n",
      "    step: 250; loss: 10.012; l2dist: 2.782\n",
      "    step: 300; loss: 9.475; l2dist: 2.716\n",
      "    step: 350; loss: 9.147; l2dist: 2.691\n",
      "    step: 400; loss: 9.013; l2dist: 2.677\n",
      "    step: 450; loss: 8.840; l2dist: 2.663\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.842; l2dist: 0.000\n",
      "    step: 50; loss: 19.362; l2dist: 3.063\n",
      "    step: 100; loss: 15.566; l2dist: 2.939\n",
      "    step: 150; loss: 12.538; l2dist: 2.898\n",
      "    step: 200; loss: 10.990; l2dist: 2.830\n",
      "    step: 250; loss: 10.022; l2dist: 2.772\n",
      "    step: 300; loss: 9.556; l2dist: 2.713\n",
      "    step: 350; loss: 9.186; l2dist: 2.688\n",
      "    step: 400; loss: 9.034; l2dist: 2.675\n",
      "    step: 450; loss: 8.951; l2dist: 2.670\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.079; l2dist: 0.000\n",
      "    step: 50; loss: 19.083; l2dist: 3.047\n",
      "    step: 100; loss: 15.551; l2dist: 2.921\n",
      "    step: 150; loss: 12.473; l2dist: 2.886\n",
      "    step: 200; loss: 10.841; l2dist: 2.828\n",
      "    step: 250; loss: 9.904; l2dist: 2.767\n",
      "    step: 300; loss: 9.360; l2dist: 2.712\n",
      "    step: 350; loss: 9.047; l2dist: 2.682\n",
      "    step: 400; loss: 8.924; l2dist: 2.663\n",
      "    step: 450; loss: 8.730; l2dist: 2.649\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.429; l2dist: 0.000\n",
      "    step: 50; loss: 19.361; l2dist: 3.055\n",
      "    step: 100; loss: 15.593; l2dist: 2.923\n",
      "    step: 150; loss: 12.548; l2dist: 2.893\n",
      "    step: 200; loss: 10.926; l2dist: 2.841\n",
      "    step: 250; loss: 10.038; l2dist: 2.783\n",
      "    step: 300; loss: 9.567; l2dist: 2.731\n",
      "    step: 350; loss: 9.201; l2dist: 2.701\n",
      "    step: 400; loss: 9.164; l2dist: 2.689\n",
      "    step: 450; loss: 8.924; l2dist: 2.663\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.169; l2dist: 0.000\n",
      "    step: 50; loss: 12.577; l2dist: 1.759\n",
      "    step: 100; loss: 12.329; l2dist: 1.768\n",
      "    step: 150; loss: 11.745; l2dist: 1.843\n",
      "    step: 200; loss: 11.294; l2dist: 1.910\n",
      "    step: 250; loss: 10.890; l2dist: 1.962\n",
      "    step: 300; loss: 10.486; l2dist: 2.010\n",
      "    step: 350; loss: 10.267; l2dist: 2.011\n",
      "    step: 400; loss: 10.171; l2dist: 2.009\n",
      "    step: 450; loss: 10.121; l2dist: 2.013\n",
      "binary step: 0; number of successful adv: 26/100\n",
      "    step: 0; loss: 128.216; l2dist: 0.000\n",
      "    step: 50; loss: 35.594; l2dist: 4.283\n",
      "    step: 100; loss: 21.105; l2dist: 3.778\n",
      "    step: 150; loss: 14.567; l2dist: 3.430\n",
      "    step: 200; loss: 12.751; l2dist: 3.226\n",
      "    step: 250; loss: 11.739; l2dist: 3.112\n",
      "    step: 300; loss: 11.179; l2dist: 3.049\n",
      "    step: 350; loss: 10.777; l2dist: 3.004\n",
      "    step: 400; loss: 10.587; l2dist: 2.977\n",
      "    step: 450; loss: 10.322; l2dist: 2.940\n",
      "binary step: 1; number of successful adv: 92/100\n",
      "    step: 0; loss: 164.667; l2dist: 0.000\n",
      "    step: 50; loss: 35.113; l2dist: 4.188\n",
      "    step: 100; loss: 20.022; l2dist: 3.761\n",
      "    step: 150; loss: 14.471; l2dist: 3.423\n",
      "    step: 200; loss: 12.490; l2dist: 3.192\n",
      "    step: 250; loss: 11.514; l2dist: 3.079\n",
      "    step: 300; loss: 10.946; l2dist: 3.008\n",
      "    step: 350; loss: 10.795; l2dist: 2.995\n",
      "    step: 400; loss: 10.538; l2dist: 2.946\n",
      "    step: 450; loss: 10.470; l2dist: 2.942\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 100.926; l2dist: 0.000\n",
      "    step: 50; loss: 27.616; l2dist: 3.722\n",
      "    step: 100; loss: 17.975; l2dist: 3.427\n",
      "    step: 150; loss: 13.364; l2dist: 3.258\n",
      "    step: 200; loss: 11.714; l2dist: 3.084\n",
      "    step: 250; loss: 10.987; l2dist: 3.005\n",
      "    step: 300; loss: 10.484; l2dist: 2.946\n",
      "    step: 350; loss: 10.164; l2dist: 2.893\n",
      "    step: 400; loss: 10.111; l2dist: 2.887\n",
      "    step: 450; loss: 9.945; l2dist: 2.868\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.027; l2dist: 0.000\n",
      "    step: 50; loss: 23.303; l2dist: 3.366\n",
      "    step: 100; loss: 16.884; l2dist: 3.155\n",
      "    step: 150; loss: 12.913; l2dist: 3.108\n",
      "    step: 200; loss: 11.420; l2dist: 3.016\n",
      "    step: 250; loss: 10.577; l2dist: 2.942\n",
      "    step: 300; loss: 10.123; l2dist: 2.891\n",
      "    step: 350; loss: 9.823; l2dist: 2.851\n",
      "    step: 400; loss: 9.733; l2dist: 2.843\n",
      "    step: 450; loss: 9.600; l2dist: 2.828\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.608; l2dist: 0.000\n",
      "    step: 50; loss: 21.705; l2dist: 3.187\n",
      "    step: 100; loss: 16.126; l2dist: 3.021\n",
      "    step: 150; loss: 12.982; l2dist: 3.036\n",
      "    step: 200; loss: 11.359; l2dist: 2.967\n",
      "    step: 250; loss: 10.686; l2dist: 2.918\n",
      "    step: 300; loss: 10.248; l2dist: 2.885\n",
      "    step: 350; loss: 9.984; l2dist: 2.868\n",
      "    step: 400; loss: 9.849; l2dist: 2.856\n",
      "    step: 450; loss: 9.642; l2dist: 2.832\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.801; l2dist: 0.000\n",
      "    step: 50; loss: 21.020; l2dist: 3.087\n",
      "    step: 100; loss: 16.070; l2dist: 2.950\n",
      "    step: 150; loss: 13.401; l2dist: 2.966\n",
      "    step: 200; loss: 11.532; l2dist: 2.941\n",
      "    step: 250; loss: 10.728; l2dist: 2.894\n",
      "    step: 300; loss: 10.340; l2dist: 2.871\n",
      "    step: 350; loss: 10.095; l2dist: 2.842\n",
      "    step: 400; loss: 9.778; l2dist: 2.826\n",
      "    step: 450; loss: 9.632; l2dist: 2.815\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.359; l2dist: 0.000\n",
      "    step: 50; loss: 20.682; l2dist: 3.058\n",
      "    step: 100; loss: 16.036; l2dist: 2.943\n",
      "    step: 150; loss: 13.455; l2dist: 2.958\n",
      "    step: 200; loss: 11.530; l2dist: 2.952\n",
      "    step: 250; loss: 10.686; l2dist: 2.900\n",
      "    step: 300; loss: 10.277; l2dist: 2.870\n",
      "    step: 350; loss: 9.933; l2dist: 2.832\n",
      "    step: 400; loss: 9.728; l2dist: 2.810\n",
      "    step: 450; loss: 9.626; l2dist: 2.807\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.313; l2dist: 0.000\n",
      "    step: 50; loss: 20.531; l2dist: 3.044\n",
      "    step: 100; loss: 16.010; l2dist: 2.931\n",
      "    step: 150; loss: 13.644; l2dist: 2.924\n",
      "    step: 200; loss: 11.582; l2dist: 2.956\n",
      "    step: 250; loss: 10.679; l2dist: 2.911\n",
      "    step: 300; loss: 10.253; l2dist: 2.877\n",
      "    step: 350; loss: 9.856; l2dist: 2.847\n",
      "    step: 400; loss: 9.708; l2dist: 2.827\n",
      "    step: 450; loss: 9.545; l2dist: 2.820\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.392; l2dist: 0.000\n",
      "    step: 50; loss: 20.744; l2dist: 3.071\n",
      "    step: 100; loss: 16.080; l2dist: 2.964\n",
      "    step: 150; loss: 13.124; l2dist: 2.987\n",
      "    step: 200; loss: 11.454; l2dist: 2.939\n",
      "    step: 250; loss: 10.599; l2dist: 2.903\n",
      "    step: 300; loss: 10.144; l2dist: 2.868\n",
      "    step: 350; loss: 9.825; l2dist: 2.842\n",
      "    step: 400; loss: 9.621; l2dist: 2.823\n",
      "    step: 450; loss: 9.557; l2dist: 2.807\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.532; l2dist: 0.000\n",
      "    step: 50; loss: 13.974; l2dist: 1.974\n",
      "    step: 100; loss: 13.564; l2dist: 1.994\n",
      "    step: 150; loss: 12.792; l2dist: 2.119\n",
      "    step: 200; loss: 12.227; l2dist: 2.206\n",
      "    step: 250; loss: 11.584; l2dist: 2.232\n",
      "    step: 300; loss: 11.188; l2dist: 2.242\n",
      "    step: 350; loss: 10.967; l2dist: 2.243\n",
      "    step: 400; loss: 10.808; l2dist: 2.244\n",
      "    step: 450; loss: 10.754; l2dist: 2.241\n",
      "binary step: 0; number of successful adv: 42/100\n",
      "    step: 0; loss: 112.129; l2dist: 0.000\n",
      "    step: 50; loss: 36.071; l2dist: 4.100\n",
      "    step: 100; loss: 22.032; l2dist: 3.703\n",
      "    step: 150; loss: 14.885; l2dist: 3.399\n",
      "    step: 200; loss: 12.628; l2dist: 3.206\n",
      "    step: 250; loss: 11.601; l2dist: 3.083\n",
      "    step: 300; loss: 10.878; l2dist: 2.997\n",
      "    step: 350; loss: 10.540; l2dist: 2.957\n",
      "    step: 400; loss: 10.267; l2dist: 2.915\n",
      "    step: 450; loss: 10.079; l2dist: 2.894\n",
      "binary step: 1; number of successful adv: 94/100\n",
      "    step: 0; loss: 145.723; l2dist: 0.000\n",
      "    step: 50; loss: 35.226; l2dist: 4.065\n",
      "    step: 100; loss: 21.338; l2dist: 3.839\n",
      "    step: 150; loss: 15.277; l2dist: 3.452\n",
      "    step: 200; loss: 12.813; l2dist: 3.229\n",
      "    step: 250; loss: 11.581; l2dist: 3.102\n",
      "    step: 300; loss: 11.220; l2dist: 3.051\n",
      "    step: 350; loss: 10.661; l2dist: 2.974\n",
      "    step: 400; loss: 10.394; l2dist: 2.942\n",
      "    step: 450; loss: 10.063; l2dist: 2.906\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 95.925; l2dist: 0.000\n",
      "    step: 50; loss: 28.226; l2dist: 3.671\n",
      "    step: 100; loss: 19.403; l2dist: 3.497\n",
      "    step: 150; loss: 14.362; l2dist: 3.307\n",
      "    step: 200; loss: 12.217; l2dist: 3.143\n",
      "    step: 250; loss: 11.213; l2dist: 3.043\n",
      "    step: 300; loss: 10.586; l2dist: 2.969\n",
      "    step: 350; loss: 10.252; l2dist: 2.924\n",
      "    step: 400; loss: 10.132; l2dist: 2.906\n",
      "    step: 450; loss: 9.935; l2dist: 2.875\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.604; l2dist: 0.000\n",
      "    step: 50; loss: 24.370; l2dist: 3.367\n",
      "    step: 100; loss: 18.767; l2dist: 3.219\n",
      "    step: 150; loss: 14.574; l2dist: 3.187\n",
      "    step: 200; loss: 12.038; l2dist: 3.099\n",
      "    step: 250; loss: 11.099; l2dist: 3.029\n",
      "    step: 300; loss: 10.460; l2dist: 2.944\n",
      "    step: 350; loss: 10.274; l2dist: 2.922\n",
      "    step: 400; loss: 10.013; l2dist: 2.896\n",
      "    step: 450; loss: 9.915; l2dist: 2.884\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.892; l2dist: 0.000\n",
      "    step: 50; loss: 22.853; l2dist: 3.196\n",
      "    step: 100; loss: 18.334; l2dist: 3.103\n",
      "    step: 150; loss: 15.395; l2dist: 3.027\n",
      "    step: 200; loss: 12.463; l2dist: 3.064\n",
      "    step: 250; loss: 11.167; l2dist: 2.991\n",
      "    step: 300; loss: 10.439; l2dist: 2.915\n",
      "    step: 350; loss: 10.198; l2dist: 2.902\n",
      "    step: 400; loss: 9.906; l2dist: 2.871\n",
      "    step: 450; loss: 9.775; l2dist: 2.852\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.767; l2dist: 0.000\n",
      "    step: 50; loss: 21.754; l2dist: 3.142\n",
      "    step: 100; loss: 17.942; l2dist: 3.046\n",
      "    step: 150; loss: 14.845; l2dist: 3.003\n",
      "    step: 200; loss: 13.116; l2dist: 2.961\n",
      "    step: 250; loss: 11.653; l2dist: 2.942\n",
      "    step: 300; loss: 10.797; l2dist: 2.926\n",
      "    step: 350; loss: 10.294; l2dist: 2.879\n",
      "    step: 400; loss: 10.101; l2dist: 2.862\n",
      "    step: 450; loss: 9.884; l2dist: 2.838\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.836; l2dist: 0.000\n",
      "    step: 50; loss: 21.480; l2dist: 3.150\n",
      "    step: 100; loss: 17.837; l2dist: 3.028\n",
      "    step: 150; loss: 14.930; l2dist: 2.983\n",
      "    step: 200; loss: 13.067; l2dist: 2.950\n",
      "    step: 250; loss: 11.446; l2dist: 2.943\n",
      "    step: 300; loss: 10.615; l2dist: 2.899\n",
      "    step: 350; loss: 10.174; l2dist: 2.865\n",
      "    step: 400; loss: 9.864; l2dist: 2.829\n",
      "    step: 450; loss: 9.847; l2dist: 2.823\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.328; l2dist: 0.000\n",
      "    step: 50; loss: 21.328; l2dist: 3.141\n",
      "    step: 100; loss: 17.528; l2dist: 3.034\n",
      "    step: 150; loss: 14.645; l2dist: 2.992\n",
      "    step: 200; loss: 13.090; l2dist: 2.928\n",
      "    step: 250; loss: 11.510; l2dist: 2.943\n",
      "    step: 300; loss: 10.661; l2dist: 2.901\n",
      "    step: 350; loss: 10.239; l2dist: 2.858\n",
      "    step: 400; loss: 10.015; l2dist: 2.836\n",
      "    step: 450; loss: 9.873; l2dist: 2.817\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.007; l2dist: 0.000\n",
      "    step: 50; loss: 21.424; l2dist: 3.156\n",
      "    step: 100; loss: 17.719; l2dist: 3.053\n",
      "    step: 150; loss: 14.769; l2dist: 3.005\n",
      "    step: 200; loss: 13.217; l2dist: 2.970\n",
      "    step: 250; loss: 11.396; l2dist: 2.965\n",
      "    step: 300; loss: 10.594; l2dist: 2.906\n",
      "    step: 350; loss: 10.243; l2dist: 2.881\n",
      "    step: 400; loss: 10.066; l2dist: 2.853\n",
      "    step: 450; loss: 9.905; l2dist: 2.844\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.328; l2dist: 0.000\n",
      "    step: 50; loss: 11.436; l2dist: 1.662\n",
      "    step: 100; loss: 11.216; l2dist: 1.663\n",
      "    step: 150; loss: 10.840; l2dist: 1.744\n",
      "    step: 200; loss: 10.456; l2dist: 1.805\n",
      "    step: 250; loss: 10.156; l2dist: 1.849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 9.831; l2dist: 1.871\n",
      "    step: 350; loss: 9.584; l2dist: 1.884\n",
      "    step: 400; loss: 9.449; l2dist: 1.889\n",
      "    step: 450; loss: 9.370; l2dist: 1.889\n",
      "binary step: 0; number of successful adv: 25/100\n",
      "    step: 0; loss: 118.402; l2dist: 0.000\n",
      "    step: 50; loss: 37.113; l2dist: 4.034\n",
      "    step: 100; loss: 22.620; l2dist: 3.722\n",
      "    step: 150; loss: 14.018; l2dist: 3.393\n",
      "    step: 200; loss: 12.274; l2dist: 3.199\n",
      "    step: 250; loss: 11.376; l2dist: 3.098\n",
      "    step: 300; loss: 10.772; l2dist: 3.030\n",
      "    step: 350; loss: 10.402; l2dist: 2.973\n",
      "    step: 400; loss: 10.225; l2dist: 2.954\n",
      "    step: 450; loss: 10.012; l2dist: 2.929\n",
      "binary step: 1; number of successful adv: 84/100\n",
      "    step: 0; loss: 257.175; l2dist: 0.000\n",
      "    step: 50; loss: 48.119; l2dist: 4.494\n",
      "    step: 100; loss: 23.949; l2dist: 4.152\n",
      "    step: 150; loss: 16.990; l2dist: 3.694\n",
      "    step: 200; loss: 14.183; l2dist: 3.438\n",
      "    step: 250; loss: 12.744; l2dist: 3.280\n",
      "    step: 300; loss: 12.217; l2dist: 3.192\n",
      "    step: 350; loss: 11.529; l2dist: 3.141\n",
      "    step: 400; loss: 11.324; l2dist: 3.114\n",
      "    step: 450; loss: 10.907; l2dist: 3.071\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 149.817; l2dist: 0.000\n",
      "    step: 50; loss: 35.642; l2dist: 3.982\n",
      "    step: 100; loss: 20.385; l2dist: 3.692\n",
      "    step: 150; loss: 14.755; l2dist: 3.429\n",
      "    step: 200; loss: 12.724; l2dist: 3.263\n",
      "    step: 250; loss: 11.638; l2dist: 3.139\n",
      "    step: 300; loss: 11.032; l2dist: 3.074\n",
      "    step: 350; loss: 10.653; l2dist: 3.029\n",
      "    step: 400; loss: 10.430; l2dist: 3.000\n",
      "    step: 450; loss: 10.209; l2dist: 2.978\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 107.733; l2dist: 0.000\n",
      "    step: 50; loss: 29.351; l2dist: 3.624\n",
      "    step: 100; loss: 18.850; l2dist: 3.402\n",
      "    step: 150; loss: 14.103; l2dist: 3.301\n",
      "    step: 200; loss: 11.995; l2dist: 3.165\n",
      "    step: 250; loss: 11.207; l2dist: 3.090\n",
      "    step: 300; loss: 10.471; l2dist: 3.013\n",
      "    step: 350; loss: 10.057; l2dist: 2.967\n",
      "    step: 400; loss: 9.999; l2dist: 2.951\n",
      "    step: 450; loss: 9.783; l2dist: 2.931\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.934; l2dist: 0.000\n",
      "    step: 50; loss: 26.273; l2dist: 3.451\n",
      "    step: 100; loss: 18.269; l2dist: 3.251\n",
      "    step: 150; loss: 13.966; l2dist: 3.184\n",
      "    step: 200; loss: 11.993; l2dist: 3.087\n",
      "    step: 250; loss: 10.920; l2dist: 3.021\n",
      "    step: 300; loss: 10.403; l2dist: 2.987\n",
      "    step: 350; loss: 10.289; l2dist: 2.946\n",
      "    step: 400; loss: 9.685; l2dist: 2.895\n",
      "    step: 450; loss: 9.760; l2dist: 2.903\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 81.745; l2dist: 0.000\n",
      "    step: 50; loss: 24.751; l2dist: 3.349\n",
      "    step: 100; loss: 17.589; l2dist: 3.206\n",
      "    step: 150; loss: 13.583; l2dist: 3.154\n",
      "    step: 200; loss: 11.789; l2dist: 3.072\n",
      "    step: 250; loss: 10.876; l2dist: 3.010\n",
      "    step: 300; loss: 10.342; l2dist: 2.948\n",
      "    step: 350; loss: 9.977; l2dist: 2.900\n",
      "    step: 400; loss: 9.736; l2dist: 2.879\n",
      "    step: 450; loss: 9.639; l2dist: 2.881\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.917; l2dist: 0.000\n",
      "    step: 50; loss: 23.819; l2dist: 3.280\n",
      "    step: 100; loss: 17.581; l2dist: 3.173\n",
      "    step: 150; loss: 13.613; l2dist: 3.135\n",
      "    step: 200; loss: 11.677; l2dist: 3.065\n",
      "    step: 250; loss: 10.813; l2dist: 3.000\n",
      "    step: 300; loss: 10.230; l2dist: 2.946\n",
      "    step: 350; loss: 9.851; l2dist: 2.907\n",
      "    step: 400; loss: 9.701; l2dist: 2.888\n",
      "    step: 450; loss: 9.579; l2dist: 2.868\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.924; l2dist: 0.000\n",
      "    step: 50; loss: 23.467; l2dist: 3.250\n",
      "    step: 100; loss: 17.585; l2dist: 3.162\n",
      "    step: 150; loss: 13.551; l2dist: 3.140\n",
      "    step: 200; loss: 11.681; l2dist: 3.055\n",
      "    step: 250; loss: 10.805; l2dist: 2.980\n",
      "    step: 300; loss: 10.479; l2dist: 2.947\n",
      "    step: 350; loss: 9.980; l2dist: 2.919\n",
      "    step: 400; loss: 9.847; l2dist: 2.906\n",
      "    step: 450; loss: 9.530; l2dist: 2.869\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.256; l2dist: 0.000\n",
      "    step: 50; loss: 23.682; l2dist: 3.276\n",
      "    step: 100; loss: 17.749; l2dist: 3.173\n",
      "    step: 150; loss: 13.571; l2dist: 3.151\n",
      "    step: 200; loss: 11.748; l2dist: 3.070\n",
      "    step: 250; loss: 10.810; l2dist: 3.001\n",
      "    step: 300; loss: 10.298; l2dist: 2.956\n",
      "    step: 350; loss: 9.824; l2dist: 2.888\n",
      "    step: 400; loss: 9.734; l2dist: 2.896\n",
      "    step: 450; loss: 9.730; l2dist: 2.895\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.005; l2dist: 0.000\n",
      "    step: 50; loss: 11.993; l2dist: 1.665\n",
      "    step: 100; loss: 11.709; l2dist: 1.691\n",
      "    step: 150; loss: 11.159; l2dist: 1.774\n",
      "    step: 200; loss: 10.673; l2dist: 1.846\n",
      "    step: 250; loss: 10.176; l2dist: 1.858\n",
      "    step: 300; loss: 9.998; l2dist: 1.869\n",
      "    step: 350; loss: 9.886; l2dist: 1.871\n",
      "    step: 400; loss: 9.826; l2dist: 1.875\n",
      "    step: 450; loss: 9.780; l2dist: 1.882\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 109.302; l2dist: 0.000\n",
      "    step: 50; loss: 37.218; l2dist: 3.809\n",
      "    step: 100; loss: 21.689; l2dist: 3.569\n",
      "    step: 150; loss: 13.732; l2dist: 3.225\n",
      "    step: 200; loss: 11.803; l2dist: 3.036\n",
      "    step: 250; loss: 10.679; l2dist: 2.905\n",
      "    step: 300; loss: 10.156; l2dist: 2.838\n",
      "    step: 350; loss: 9.773; l2dist: 2.789\n",
      "    step: 400; loss: 9.510; l2dist: 2.762\n",
      "    step: 450; loss: 9.589; l2dist: 2.761\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 194.599; l2dist: 0.000\n",
      "    step: 50; loss: 40.894; l2dist: 4.084\n",
      "    step: 100; loss: 21.705; l2dist: 3.790\n",
      "    step: 150; loss: 15.564; l2dist: 3.352\n",
      "    step: 200; loss: 12.908; l2dist: 3.157\n",
      "    step: 250; loss: 11.747; l2dist: 3.020\n",
      "    step: 300; loss: 10.836; l2dist: 2.925\n",
      "    step: 350; loss: 10.273; l2dist: 2.870\n",
      "    step: 400; loss: 10.256; l2dist: 2.856\n",
      "    step: 450; loss: 9.979; l2dist: 2.825\n",
      "binary step: 2; number of successful adv: 98/100\n",
      "    step: 0; loss: 244.386; l2dist: 0.000\n",
      "    step: 50; loss: 32.864; l2dist: 3.893\n",
      "    step: 100; loss: 21.263; l2dist: 3.595\n",
      "    step: 150; loss: 16.217; l2dist: 3.308\n",
      "    step: 200; loss: 13.415; l2dist: 3.097\n",
      "    step: 250; loss: 11.529; l2dist: 2.992\n",
      "    step: 300; loss: 11.134; l2dist: 2.940\n",
      "    step: 350; loss: 10.313; l2dist: 2.851\n",
      "    step: 400; loss: 10.066; l2dist: 2.830\n",
      "    step: 450; loss: 9.965; l2dist: 2.804\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 149.496; l2dist: 0.000\n",
      "    step: 50; loss: 26.499; l2dist: 3.524\n",
      "    step: 100; loss: 18.763; l2dist: 3.219\n",
      "    step: 150; loss: 14.550; l2dist: 3.049\n",
      "    step: 200; loss: 12.192; l2dist: 2.908\n",
      "    step: 250; loss: 10.999; l2dist: 2.841\n",
      "    step: 300; loss: 10.494; l2dist: 2.834\n",
      "    step: 350; loss: 9.815; l2dist: 2.770\n",
      "    step: 400; loss: 9.630; l2dist: 2.744\n",
      "    step: 450; loss: 9.386; l2dist: 2.723\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 142.936; l2dist: 0.000\n",
      "    step: 50; loss: 24.994; l2dist: 3.412\n",
      "    step: 100; loss: 18.314; l2dist: 3.141\n",
      "    step: 150; loss: 14.655; l2dist: 2.994\n",
      "    step: 200; loss: 12.299; l2dist: 2.908\n",
      "    step: 250; loss: 10.977; l2dist: 2.841\n",
      "    step: 300; loss: 10.254; l2dist: 2.806\n",
      "    step: 350; loss: 9.833; l2dist: 2.766\n",
      "    step: 400; loss: 9.498; l2dist: 2.736\n",
      "    step: 450; loss: 9.672; l2dist: 2.735\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 120.914; l2dist: 0.000\n",
      "    step: 50; loss: 23.693; l2dist: 3.282\n",
      "    step: 100; loss: 17.536; l2dist: 3.048\n",
      "    step: 150; loss: 13.919; l2dist: 2.954\n",
      "    step: 200; loss: 11.862; l2dist: 2.857\n",
      "    step: 250; loss: 10.579; l2dist: 2.796\n",
      "    step: 300; loss: 10.036; l2dist: 2.765\n",
      "    step: 350; loss: 9.594; l2dist: 2.722\n",
      "    step: 400; loss: 9.553; l2dist: 2.720\n",
      "    step: 450; loss: 9.444; l2dist: 2.712\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.673; l2dist: 0.000\n",
      "    step: 50; loss: 22.814; l2dist: 3.204\n",
      "    step: 100; loss: 17.115; l2dist: 2.981\n",
      "    step: 150; loss: 13.861; l2dist: 2.876\n",
      "    step: 200; loss: 11.696; l2dist: 2.835\n",
      "    step: 250; loss: 10.575; l2dist: 2.796\n",
      "    step: 300; loss: 9.824; l2dist: 2.752\n",
      "    step: 350; loss: 9.495; l2dist: 2.721\n",
      "    step: 400; loss: 9.441; l2dist: 2.721\n",
      "    step: 450; loss: 9.128; l2dist: 2.686\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 105.390; l2dist: 0.000\n",
      "    step: 50; loss: 22.467; l2dist: 3.162\n",
      "    step: 100; loss: 16.894; l2dist: 2.960\n",
      "    step: 150; loss: 13.439; l2dist: 2.862\n",
      "    step: 200; loss: 11.589; l2dist: 2.806\n",
      "    step: 250; loss: 10.375; l2dist: 2.778\n",
      "    step: 300; loss: 9.688; l2dist: 2.732\n",
      "    step: 350; loss: 9.519; l2dist: 2.721\n",
      "    step: 400; loss: 9.115; l2dist: 2.671\n",
      "    step: 450; loss: 8.918; l2dist: 2.656\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 106.137; l2dist: 0.000\n",
      "    step: 50; loss: 22.649; l2dist: 3.178\n",
      "    step: 100; loss: 17.075; l2dist: 2.968\n",
      "    step: 150; loss: 13.380; l2dist: 2.886\n",
      "    step: 200; loss: 11.458; l2dist: 2.841\n",
      "    step: 250; loss: 10.353; l2dist: 2.792\n",
      "    step: 300; loss: 9.670; l2dist: 2.739\n",
      "    step: 350; loss: 9.451; l2dist: 2.713\n",
      "    step: 400; loss: 9.479; l2dist: 2.721\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.878; l2dist: 0.000\n",
      "    step: 50; loss: 12.929; l2dist: 1.827\n",
      "    step: 100; loss: 12.577; l2dist: 1.838\n",
      "    step: 150; loss: 12.124; l2dist: 1.918\n",
      "    step: 200; loss: 11.709; l2dist: 2.020\n",
      "    step: 250; loss: 11.184; l2dist: 2.058\n",
      "    step: 300; loss: 10.897; l2dist: 2.071\n",
      "    step: 350; loss: 10.740; l2dist: 2.079\n",
      "    step: 400; loss: 10.633; l2dist: 2.076\n",
      "    step: 450; loss: 10.524; l2dist: 2.079\n",
      "binary step: 0; number of successful adv: 27/100\n",
      "    step: 0; loss: 133.138; l2dist: 0.000\n",
      "    step: 50; loss: 37.695; l2dist: 4.348\n",
      "    step: 100; loss: 21.864; l2dist: 3.930\n",
      "    step: 150; loss: 15.071; l2dist: 3.527\n",
      "    step: 200; loss: 13.080; l2dist: 3.324\n",
      "    step: 250; loss: 12.027; l2dist: 3.189\n",
      "    step: 300; loss: 11.347; l2dist: 3.108\n",
      "    step: 350; loss: 11.029; l2dist: 3.062\n",
      "    step: 400; loss: 10.841; l2dist: 3.033\n",
      "    step: 450; loss: 10.627; l2dist: 3.003\n",
      "binary step: 1; number of successful adv: 93/100\n",
      "    step: 0; loss: 159.948; l2dist: 0.000\n",
      "    step: 50; loss: 35.192; l2dist: 4.216\n",
      "    step: 100; loss: 21.742; l2dist: 3.939\n",
      "    step: 150; loss: 15.704; l2dist: 3.582\n",
      "    step: 200; loss: 13.433; l2dist: 3.367\n",
      "    step: 250; loss: 12.306; l2dist: 3.254\n",
      "    step: 300; loss: 11.755; l2dist: 3.179\n",
      "    step: 350; loss: 11.134; l2dist: 3.106\n",
      "    step: 400; loss: 11.061; l2dist: 3.083\n",
      "    step: 450; loss: 10.750; l2dist: 3.063\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 100.497; l2dist: 0.000\n",
      "    step: 50; loss: 28.541; l2dist: 3.731\n",
      "    step: 100; loss: 20.599; l2dist: 3.535\n",
      "    step: 150; loss: 15.571; l2dist: 3.370\n",
      "    step: 200; loss: 12.801; l2dist: 3.238\n",
      "    step: 250; loss: 11.711; l2dist: 3.133\n",
      "    step: 300; loss: 11.122; l2dist: 3.069\n",
      "    step: 350; loss: 10.734; l2dist: 3.030\n",
      "    step: 400; loss: 10.637; l2dist: 3.022\n",
      "    step: 450; loss: 10.480; l2dist: 3.002\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.989; l2dist: 0.000\n",
      "    step: 50; loss: 24.834; l2dist: 3.444\n",
      "    step: 100; loss: 19.068; l2dist: 3.324\n",
      "    step: 150; loss: 14.975; l2dist: 3.238\n",
      "    step: 200; loss: 12.997; l2dist: 3.113\n",
      "    step: 250; loss: 11.468; l2dist: 3.056\n",
      "    step: 300; loss: 11.075; l2dist: 3.026\n",
      "    step: 350; loss: 10.707; l2dist: 2.995\n",
      "    step: 400; loss: 10.657; l2dist: 2.968\n",
      "    step: 450; loss: 10.315; l2dist: 2.945\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 72.163; l2dist: 0.000\n",
      "    step: 50; loss: 23.668; l2dist: 3.314\n",
      "    step: 100; loss: 18.633; l2dist: 3.234\n",
      "    step: 150; loss: 14.977; l2dist: 3.208\n",
      "    step: 200; loss: 12.511; l2dist: 3.124\n",
      "    step: 250; loss: 11.382; l2dist: 3.056\n",
      "    step: 300; loss: 10.933; l2dist: 3.016\n",
      "    step: 350; loss: 10.573; l2dist: 2.983\n",
      "    step: 400; loss: 10.293; l2dist: 2.947\n",
      "    step: 450; loss: 10.182; l2dist: 2.933\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.889; l2dist: 0.000\n",
      "    step: 50; loss: 22.989; l2dist: 3.260\n",
      "    step: 100; loss: 18.569; l2dist: 3.161\n",
      "    step: 150; loss: 15.267; l2dist: 3.166\n",
      "    step: 200; loss: 12.795; l2dist: 3.116\n",
      "    step: 250; loss: 11.493; l2dist: 3.037\n",
      "    step: 300; loss: 11.082; l2dist: 3.024\n",
      "    step: 350; loss: 10.738; l2dist: 2.980\n",
      "    step: 400; loss: 10.504; l2dist: 2.986\n",
      "    step: 450; loss: 10.345; l2dist: 2.964\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.028; l2dist: 0.000\n",
      "    step: 50; loss: 22.781; l2dist: 3.221\n",
      "    step: 100; loss: 18.506; l2dist: 3.136\n",
      "    step: 150; loss: 15.259; l2dist: 3.142\n",
      "    step: 200; loss: 12.727; l2dist: 3.095\n",
      "    step: 250; loss: 11.610; l2dist: 3.034\n",
      "    step: 300; loss: 10.986; l2dist: 3.004\n",
      "    step: 350; loss: 10.779; l2dist: 2.990\n",
      "    step: 400; loss: 10.391; l2dist: 2.967\n",
      "    step: 450; loss: 10.226; l2dist: 2.929\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.003; l2dist: 0.000\n",
      "    step: 50; loss: 22.537; l2dist: 3.201\n",
      "    step: 100; loss: 18.409; l2dist: 3.123\n",
      "    step: 150; loss: 15.300; l2dist: 3.133\n",
      "    step: 200; loss: 12.879; l2dist: 3.095\n",
      "    step: 250; loss: 11.653; l2dist: 3.041\n",
      "    step: 300; loss: 11.068; l2dist: 3.016\n",
      "    step: 350; loss: 10.789; l2dist: 3.005\n",
      "    step: 400; loss: 10.615; l2dist: 2.972\n",
      "    step: 450; loss: 10.276; l2dist: 2.947\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.712; l2dist: 0.000\n",
      "    step: 50; loss: 22.650; l2dist: 3.216\n",
      "    step: 100; loss: 18.560; l2dist: 3.120\n",
      "    step: 150; loss: 15.239; l2dist: 3.143\n",
      "    step: 200; loss: 12.789; l2dist: 3.107\n",
      "    step: 250; loss: 11.654; l2dist: 3.046\n",
      "    step: 300; loss: 10.984; l2dist: 3.016\n",
      "    step: 350; loss: 10.650; l2dist: 2.987\n",
      "    step: 400; loss: 10.381; l2dist: 2.960\n",
      "    step: 450; loss: 10.129; l2dist: 2.934\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.128; l2dist: 0.000\n",
      "    step: 50; loss: 11.944; l2dist: 1.792\n",
      "    step: 100; loss: 11.702; l2dist: 1.806\n",
      "    step: 150; loss: 11.360; l2dist: 1.867\n",
      "    step: 200; loss: 11.043; l2dist: 1.908\n",
      "    step: 250; loss: 10.757; l2dist: 1.944\n",
      "    step: 300; loss: 10.569; l2dist: 1.959\n",
      "    step: 350; loss: 10.437; l2dist: 1.966\n",
      "    step: 400; loss: 10.325; l2dist: 1.976\n",
      "    step: 450; loss: 10.202; l2dist: 1.986\n",
      "binary step: 0; number of successful adv: 34/100\n",
      "    step: 0; loss: 121.151; l2dist: 0.000\n",
      "    step: 50; loss: 33.551; l2dist: 4.121\n",
      "    step: 100; loss: 21.295; l2dist: 3.666\n",
      "    step: 150; loss: 14.128; l2dist: 3.334\n",
      "    step: 200; loss: 12.310; l2dist: 3.118\n",
      "    step: 250; loss: 11.214; l2dist: 3.010\n",
      "    step: 300; loss: 10.669; l2dist: 2.946\n",
      "    step: 350; loss: 10.340; l2dist: 2.904\n",
      "    step: 400; loss: 10.063; l2dist: 2.870\n",
      "    step: 450; loss: 9.808; l2dist: 2.842\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 185.384; l2dist: 0.000\n",
      "    step: 50; loss: 33.640; l2dist: 4.203\n",
      "    step: 100; loss: 21.473; l2dist: 3.759\n",
      "    step: 150; loss: 15.234; l2dist: 3.404\n",
      "    step: 200; loss: 12.620; l2dist: 3.178\n",
      "    step: 250; loss: 11.389; l2dist: 3.050\n",
      "    step: 300; loss: 10.623; l2dist: 2.959\n",
      "    step: 350; loss: 10.373; l2dist: 2.924\n",
      "    step: 400; loss: 10.029; l2dist: 2.871\n",
      "    step: 450; loss: 9.573; l2dist: 2.819\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 177.695; l2dist: 0.000\n",
      "    step: 50; loss: 28.247; l2dist: 3.850\n",
      "    step: 100; loss: 19.722; l2dist: 3.513\n",
      "    step: 150; loss: 14.542; l2dist: 3.245\n",
      "    step: 200; loss: 12.123; l2dist: 3.074\n",
      "    step: 250; loss: 10.792; l2dist: 2.954\n",
      "    step: 300; loss: 10.005; l2dist: 2.867\n",
      "    step: 350; loss: 10.028; l2dist: 2.866\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.338; l2dist: 0.000\n",
      "    step: 50; loss: 23.298; l2dist: 3.455\n",
      "    step: 100; loss: 17.536; l2dist: 3.199\n",
      "    step: 150; loss: 13.591; l2dist: 3.080\n",
      "    step: 200; loss: 11.488; l2dist: 2.947\n",
      "    step: 250; loss: 10.388; l2dist: 2.858\n",
      "    step: 300; loss: 9.751; l2dist: 2.802\n",
      "    step: 350; loss: 9.700; l2dist: 2.804\n",
      "    step: 400; loss: 9.376; l2dist: 2.774\n",
      "    step: 450; loss: 9.354; l2dist: 2.768\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 81.548; l2dist: 0.000\n",
      "    step: 50; loss: 20.927; l2dist: 3.214\n",
      "    step: 100; loss: 16.782; l2dist: 3.040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 13.260; l2dist: 2.981\n",
      "    step: 200; loss: 11.280; l2dist: 2.907\n",
      "    step: 250; loss: 10.189; l2dist: 2.817\n",
      "    step: 300; loss: 9.641; l2dist: 2.770\n",
      "    step: 350; loss: 9.277; l2dist: 2.746\n",
      "    step: 400; loss: 9.302; l2dist: 2.765\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.327; l2dist: 0.000\n",
      "    step: 50; loss: 19.983; l2dist: 3.108\n",
      "    step: 100; loss: 16.433; l2dist: 2.950\n",
      "    step: 150; loss: 13.221; l2dist: 2.931\n",
      "    step: 200; loss: 11.332; l2dist: 2.872\n",
      "    step: 250; loss: 10.135; l2dist: 2.821\n",
      "    step: 300; loss: 9.514; l2dist: 2.773\n",
      "    step: 350; loss: 9.464; l2dist: 2.758\n",
      "    step: 400; loss: 9.085; l2dist: 2.735\n",
      "    step: 450; loss: 8.903; l2dist: 2.708\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.530; l2dist: 0.000\n",
      "    step: 50; loss: 19.544; l2dist: 3.064\n",
      "    step: 100; loss: 16.284; l2dist: 2.912\n",
      "    step: 150; loss: 13.272; l2dist: 2.910\n",
      "    step: 200; loss: 11.393; l2dist: 2.851\n",
      "    step: 250; loss: 10.267; l2dist: 2.809\n",
      "    step: 300; loss: 9.686; l2dist: 2.759\n",
      "    step: 350; loss: 9.220; l2dist: 2.720\n",
      "    step: 400; loss: 9.125; l2dist: 2.720\n",
      "    step: 450; loss: 8.999; l2dist: 2.707\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.149; l2dist: 0.000\n",
      "    step: 50; loss: 19.261; l2dist: 3.015\n",
      "    step: 100; loss: 16.249; l2dist: 2.887\n",
      "    step: 150; loss: 13.550; l2dist: 2.892\n",
      "    step: 200; loss: 11.566; l2dist: 2.849\n",
      "    step: 250; loss: 10.351; l2dist: 2.799\n",
      "    step: 300; loss: 9.620; l2dist: 2.752\n",
      "    step: 350; loss: 9.428; l2dist: 2.727\n",
      "    step: 400; loss: 9.283; l2dist: 2.719\n",
      "    step: 450; loss: 9.052; l2dist: 2.702\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.598; l2dist: 0.000\n",
      "    step: 50; loss: 19.395; l2dist: 3.042\n",
      "    step: 100; loss: 16.326; l2dist: 2.903\n",
      "    step: 150; loss: 13.445; l2dist: 2.910\n",
      "    step: 200; loss: 11.547; l2dist: 2.852\n",
      "    step: 250; loss: 10.415; l2dist: 2.810\n",
      "    step: 300; loss: 9.701; l2dist: 2.753\n",
      "    step: 350; loss: 9.426; l2dist: 2.734\n",
      "    step: 400; loss: 9.168; l2dist: 2.722\n",
      "    step: 450; loss: 9.052; l2dist: 2.711\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.197; l2dist: 0.000\n",
      "    step: 50; loss: 13.159; l2dist: 1.858\n",
      "    step: 100; loss: 12.928; l2dist: 1.854\n",
      "    step: 150; loss: 12.430; l2dist: 1.962\n",
      "    step: 200; loss: 12.007; l2dist: 2.036\n",
      "    step: 250; loss: 11.561; l2dist: 2.068\n",
      "    step: 300; loss: 11.288; l2dist: 2.074\n",
      "    step: 350; loss: 11.121; l2dist: 2.082\n",
      "    step: 400; loss: 11.012; l2dist: 2.086\n",
      "    step: 450; loss: 10.957; l2dist: 2.087\n",
      "binary step: 0; number of successful adv: 27/100\n",
      "    step: 0; loss: 136.582; l2dist: 0.000\n",
      "    step: 50; loss: 40.255; l2dist: 4.327\n",
      "    step: 100; loss: 24.110; l2dist: 3.961\n",
      "    step: 150; loss: 15.487; l2dist: 3.565\n",
      "    step: 200; loss: 13.290; l2dist: 3.342\n",
      "    step: 250; loss: 12.234; l2dist: 3.220\n",
      "    step: 300; loss: 11.611; l2dist: 3.138\n",
      "    step: 350; loss: 11.240; l2dist: 3.087\n",
      "    step: 400; loss: 11.190; l2dist: 3.083\n",
      "    step: 450; loss: 10.822; l2dist: 3.031\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 189.459; l2dist: 0.000\n",
      "    step: 50; loss: 42.221; l2dist: 4.439\n",
      "    step: 100; loss: 26.212; l2dist: 4.143\n",
      "    step: 150; loss: 17.153; l2dist: 3.724\n",
      "    step: 200; loss: 14.375; l2dist: 3.479\n",
      "    step: 250; loss: 13.033; l2dist: 3.331\n",
      "    step: 300; loss: 12.411; l2dist: 3.251\n",
      "    step: 350; loss: 11.911; l2dist: 3.196\n",
      "    step: 400; loss: 11.334; l2dist: 3.134\n",
      "    step: 450; loss: 11.187; l2dist: 3.106\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.863; l2dist: 0.000\n",
      "    step: 50; loss: 32.197; l2dist: 3.894\n",
      "    step: 100; loss: 22.739; l2dist: 3.696\n",
      "    step: 150; loss: 15.672; l2dist: 3.490\n",
      "    step: 200; loss: 12.848; l2dist: 3.285\n",
      "    step: 250; loss: 12.041; l2dist: 3.178\n",
      "    step: 300; loss: 11.512; l2dist: 3.135\n",
      "    step: 350; loss: 11.012; l2dist: 3.069\n",
      "    step: 400; loss: 10.774; l2dist: 3.042\n",
      "    step: 450; loss: 10.527; l2dist: 3.024\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.609; l2dist: 0.000\n",
      "    step: 50; loss: 27.325; l2dist: 3.531\n",
      "    step: 100; loss: 20.780; l2dist: 3.399\n",
      "    step: 150; loss: 15.047; l2dist: 3.345\n",
      "    step: 200; loss: 12.447; l2dist: 3.205\n",
      "    step: 250; loss: 11.486; l2dist: 3.108\n",
      "    step: 300; loss: 10.788; l2dist: 3.025\n",
      "    step: 350; loss: 10.505; l2dist: 2.994\n",
      "    step: 400; loss: 10.255; l2dist: 2.959\n",
      "    step: 450; loss: 10.020; l2dist: 2.937\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.392; l2dist: 0.000\n",
      "    step: 50; loss: 25.769; l2dist: 3.410\n",
      "    step: 100; loss: 20.453; l2dist: 3.281\n",
      "    step: 150; loss: 15.382; l2dist: 3.259\n",
      "    step: 200; loss: 12.761; l2dist: 3.189\n",
      "    step: 250; loss: 11.677; l2dist: 3.108\n",
      "    step: 300; loss: 11.200; l2dist: 3.065\n",
      "    step: 350; loss: 10.828; l2dist: 3.033\n",
      "    step: 400; loss: 10.359; l2dist: 2.974\n",
      "    step: 450; loss: 10.151; l2dist: 2.941\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.537; l2dist: 0.000\n",
      "    step: 50; loss: 24.663; l2dist: 3.327\n",
      "    step: 100; loss: 19.661; l2dist: 3.214\n",
      "    step: 150; loss: 15.009; l2dist: 3.203\n",
      "    step: 200; loss: 12.270; l2dist: 3.121\n",
      "    step: 250; loss: 11.206; l2dist: 3.039\n",
      "    step: 300; loss: 10.677; l2dist: 2.975\n",
      "    step: 350; loss: 10.337; l2dist: 2.947\n",
      "    step: 400; loss: 10.195; l2dist: 2.934\n",
      "    step: 450; loss: 10.011; l2dist: 2.898\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.338; l2dist: 0.000\n",
      "    step: 50; loss: 24.030; l2dist: 3.285\n",
      "    step: 100; loss: 19.541; l2dist: 3.181\n",
      "    step: 150; loss: 15.041; l2dist: 3.180\n",
      "    step: 200; loss: 12.606; l2dist: 3.128\n",
      "    step: 250; loss: 11.312; l2dist: 3.059\n",
      "    step: 300; loss: 10.578; l2dist: 2.995\n",
      "    step: 350; loss: 10.472; l2dist: 2.975\n",
      "    step: 400; loss: 10.216; l2dist: 2.940\n",
      "    step: 450; loss: 10.031; l2dist: 2.918\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.725; l2dist: 0.000\n",
      "    step: 50; loss: 23.645; l2dist: 3.257\n",
      "    step: 100; loss: 19.537; l2dist: 3.132\n",
      "    step: 150; loss: 15.176; l2dist: 3.168\n",
      "    step: 200; loss: 12.741; l2dist: 3.102\n",
      "    step: 250; loss: 11.409; l2dist: 3.057\n",
      "    step: 300; loss: 10.950; l2dist: 3.022\n",
      "    step: 350; loss: 10.457; l2dist: 2.976\n",
      "    step: 400; loss: 10.028; l2dist: 2.927\n",
      "    step: 450; loss: 9.943; l2dist: 2.906\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.388; l2dist: 0.000\n",
      "    step: 50; loss: 23.768; l2dist: 3.267\n",
      "    step: 100; loss: 19.438; l2dist: 3.169\n",
      "    step: 150; loss: 15.232; l2dist: 3.163\n",
      "    step: 200; loss: 12.543; l2dist: 3.112\n",
      "    step: 250; loss: 11.384; l2dist: 3.062\n",
      "    step: 300; loss: 10.675; l2dist: 3.002\n",
      "    step: 350; loss: 10.303; l2dist: 2.965\n",
      "    step: 400; loss: 10.258; l2dist: 2.960\n",
      "    step: 450; loss: 9.841; l2dist: 2.919\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.188; l2dist: 0.000\n",
      "    step: 50; loss: 12.927; l2dist: 1.923\n",
      "    step: 100; loss: 12.612; l2dist: 1.932\n",
      "    step: 150; loss: 12.191; l2dist: 1.986\n",
      "    step: 200; loss: 11.781; l2dist: 2.048\n",
      "    step: 250; loss: 11.454; l2dist: 2.091\n",
      "    step: 300; loss: 11.136; l2dist: 2.102\n",
      "    step: 350; loss: 10.890; l2dist: 2.109\n",
      "    step: 400; loss: 10.739; l2dist: 2.117\n",
      "    step: 450; loss: 10.621; l2dist: 2.122\n",
      "binary step: 0; number of successful adv: 30/100\n",
      "    step: 0; loss: 129.247; l2dist: 0.000\n",
      "    step: 50; loss: 36.765; l2dist: 4.221\n",
      "    step: 100; loss: 23.286; l2dist: 3.819\n",
      "    step: 150; loss: 14.835; l2dist: 3.458\n",
      "    step: 200; loss: 12.890; l2dist: 3.248\n",
      "    step: 250; loss: 11.865; l2dist: 3.121\n",
      "    step: 300; loss: 11.183; l2dist: 3.036\n",
      "    step: 350; loss: 10.835; l2dist: 2.998\n",
      "    step: 400; loss: 10.472; l2dist: 2.951\n",
      "    step: 450; loss: 10.322; l2dist: 2.925\n",
      "binary step: 1; number of successful adv: 88/100\n",
      "    step: 0; loss: 232.791; l2dist: 0.000\n",
      "    step: 50; loss: 39.689; l2dist: 4.378\n",
      "    step: 100; loss: 23.403; l2dist: 4.007\n",
      "    step: 150; loss: 16.665; l2dist: 3.628\n",
      "    step: 200; loss: 13.765; l2dist: 3.377\n",
      "    step: 250; loss: 12.697; l2dist: 3.257\n",
      "    step: 300; loss: 11.942; l2dist: 3.168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 11.361; l2dist: 3.086\n",
      "    step: 400; loss: 10.860; l2dist: 3.036\n",
      "    step: 450; loss: 10.574; l2dist: 3.021\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 137.897; l2dist: 0.000\n",
      "    step: 50; loss: 31.222; l2dist: 3.874\n",
      "    step: 100; loss: 20.601; l2dist: 3.622\n",
      "    step: 150; loss: 15.224; l2dist: 3.422\n",
      "    step: 200; loss: 12.982; l2dist: 3.232\n",
      "    step: 250; loss: 11.632; l2dist: 3.110\n",
      "    step: 300; loss: 11.207; l2dist: 3.064\n",
      "    step: 350; loss: 10.456; l2dist: 2.971\n",
      "    step: 400; loss: 10.345; l2dist: 2.964\n",
      "    step: 450; loss: 10.048; l2dist: 2.923\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 95.670; l2dist: 0.000\n",
      "    step: 50; loss: 26.363; l2dist: 3.590\n",
      "    step: 100; loss: 18.624; l2dist: 3.413\n",
      "    step: 150; loss: 14.420; l2dist: 3.278\n",
      "    step: 200; loss: 12.172; l2dist: 3.146\n",
      "    step: 250; loss: 11.235; l2dist: 3.050\n",
      "    step: 300; loss: 10.414; l2dist: 2.964\n",
      "    step: 350; loss: 10.291; l2dist: 2.940\n",
      "    step: 400; loss: 9.857; l2dist: 2.886\n",
      "    step: 450; loss: 9.902; l2dist: 2.880\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.891; l2dist: 0.000\n",
      "    step: 50; loss: 24.123; l2dist: 3.404\n",
      "    step: 100; loss: 18.609; l2dist: 3.276\n",
      "    step: 150; loss: 14.433; l2dist: 3.221\n",
      "    step: 200; loss: 12.307; l2dist: 3.110\n",
      "    step: 250; loss: 11.038; l2dist: 3.022\n",
      "    step: 300; loss: 10.561; l2dist: 2.953\n",
      "    step: 350; loss: 10.131; l2dist: 2.915\n",
      "    step: 400; loss: 9.826; l2dist: 2.876\n",
      "    step: 450; loss: 9.549; l2dist: 2.843\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.720; l2dist: 0.000\n",
      "    step: 50; loss: 22.870; l2dist: 3.283\n",
      "    step: 100; loss: 18.434; l2dist: 3.174\n",
      "    step: 150; loss: 14.645; l2dist: 3.149\n",
      "    step: 200; loss: 12.120; l2dist: 3.078\n",
      "    step: 250; loss: 11.014; l2dist: 2.996\n",
      "    step: 300; loss: 10.336; l2dist: 2.937\n",
      "    step: 350; loss: 9.949; l2dist: 2.896\n",
      "    step: 400; loss: 9.649; l2dist: 2.857\n",
      "    step: 450; loss: 9.570; l2dist: 2.844\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.686; l2dist: 0.000\n",
      "    step: 50; loss: 22.362; l2dist: 3.225\n",
      "    step: 100; loss: 18.355; l2dist: 3.127\n",
      "    step: 150; loss: 14.946; l2dist: 3.107\n",
      "    step: 200; loss: 12.574; l2dist: 3.062\n",
      "    step: 250; loss: 11.083; l2dist: 2.996\n",
      "    step: 300; loss: 10.364; l2dist: 2.923\n",
      "    step: 350; loss: 9.962; l2dist: 2.889\n",
      "    step: 400; loss: 9.685; l2dist: 2.850\n",
      "    step: 450; loss: 9.608; l2dist: 2.838\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.594; l2dist: 0.000\n",
      "    step: 50; loss: 22.032; l2dist: 3.201\n",
      "    step: 100; loss: 18.195; l2dist: 3.116\n",
      "    step: 150; loss: 14.633; l2dist: 3.107\n",
      "    step: 200; loss: 12.164; l2dist: 3.069\n",
      "    step: 250; loss: 11.004; l2dist: 2.996\n",
      "    step: 300; loss: 10.406; l2dist: 2.935\n",
      "    step: 350; loss: 9.974; l2dist: 2.883\n",
      "    step: 400; loss: 9.690; l2dist: 2.859\n",
      "    step: 450; loss: 9.598; l2dist: 2.844\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.290; l2dist: 0.000\n",
      "    step: 50; loss: 22.169; l2dist: 3.215\n",
      "    step: 100; loss: 18.282; l2dist: 3.126\n",
      "    step: 150; loss: 14.414; l2dist: 3.130\n",
      "    step: 200; loss: 11.980; l2dist: 3.074\n",
      "    step: 250; loss: 10.951; l2dist: 2.987\n",
      "    step: 300; loss: 10.379; l2dist: 2.939\n",
      "    step: 350; loss: 9.825; l2dist: 2.882\n",
      "    step: 400; loss: 9.640; l2dist: 2.849\n",
      "    step: 450; loss: 9.487; l2dist: 2.837\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.118; l2dist: 0.000\n",
      "    step: 50; loss: 11.034; l2dist: 1.643\n",
      "    step: 100; loss: 10.685; l2dist: 1.640\n",
      "    step: 150; loss: 10.238; l2dist: 1.709\n",
      "    step: 200; loss: 9.931; l2dist: 1.746\n",
      "    step: 250; loss: 9.697; l2dist: 1.788\n",
      "    step: 300; loss: 9.521; l2dist: 1.809\n",
      "    step: 350; loss: 9.426; l2dist: 1.810\n",
      "    step: 400; loss: 9.254; l2dist: 1.831\n",
      "    step: 450; loss: 9.203; l2dist: 1.832\n",
      "binary step: 0; number of successful adv: 28/100\n",
      "    step: 0; loss: 113.158; l2dist: 0.000\n",
      "    step: 50; loss: 36.205; l2dist: 3.872\n",
      "    step: 100; loss: 20.054; l2dist: 3.547\n",
      "    step: 150; loss: 13.524; l2dist: 3.208\n",
      "    step: 200; loss: 11.795; l2dist: 3.021\n",
      "    step: 250; loss: 10.806; l2dist: 2.910\n",
      "    step: 300; loss: 10.207; l2dist: 2.848\n",
      "    step: 350; loss: 9.663; l2dist: 2.779\n",
      "    step: 400; loss: 9.560; l2dist: 2.767\n",
      "    step: 450; loss: 9.367; l2dist: 2.747\n",
      "binary step: 1; number of successful adv: 88/100\n",
      "    step: 0; loss: 191.734; l2dist: 0.000\n",
      "    step: 50; loss: 37.460; l2dist: 4.042\n",
      "    step: 100; loss: 20.964; l2dist: 3.703\n",
      "    step: 150; loss: 14.925; l2dist: 3.345\n",
      "    step: 200; loss: 12.248; l2dist: 3.099\n",
      "    step: 250; loss: 11.194; l2dist: 2.995\n",
      "    step: 300; loss: 10.560; l2dist: 2.902\n",
      "    step: 350; loss: 10.127; l2dist: 2.856\n",
      "    step: 400; loss: 9.796; l2dist: 2.816\n",
      "    step: 450; loss: 9.782; l2dist: 2.823\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.092; l2dist: 0.000\n",
      "    step: 50; loss: 28.581; l2dist: 3.515\n",
      "    step: 100; loss: 18.413; l2dist: 3.301\n",
      "    step: 150; loss: 13.466; l2dist: 3.105\n",
      "    step: 200; loss: 11.186; l2dist: 2.955\n",
      "    step: 250; loss: 10.403; l2dist: 2.885\n",
      "    step: 300; loss: 9.869; l2dist: 2.821\n",
      "    step: 350; loss: 9.583; l2dist: 2.794\n",
      "    step: 400; loss: 9.724; l2dist: 2.789\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.579; l2dist: 0.000\n",
      "    step: 50; loss: 24.232; l2dist: 3.176\n",
      "    step: 100; loss: 17.435; l2dist: 3.087\n",
      "    step: 150; loss: 13.215; l2dist: 3.040\n",
      "    step: 200; loss: 10.966; l2dist: 2.928\n",
      "    step: 250; loss: 10.220; l2dist: 2.851\n",
      "    step: 300; loss: 9.648; l2dist: 2.793\n",
      "    step: 350; loss: 9.472; l2dist: 2.768\n",
      "    step: 400; loss: 9.210; l2dist: 2.748\n",
      "    step: 450; loss: 8.961; l2dist: 2.711\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.318; l2dist: 0.000\n",
      "    step: 50; loss: 21.635; l2dist: 2.982\n",
      "    step: 100; loss: 16.845; l2dist: 2.916\n",
      "    step: 150; loss: 13.087; l2dist: 2.958\n",
      "    step: 200; loss: 10.981; l2dist: 2.884\n",
      "    step: 250; loss: 10.059; l2dist: 2.801\n",
      "    step: 300; loss: 9.518; l2dist: 2.760\n",
      "    step: 350; loss: 9.332; l2dist: 2.737\n",
      "    step: 400; loss: 9.121; l2dist: 2.709\n",
      "    step: 450; loss: 8.896; l2dist: 2.689\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.617; l2dist: 0.000\n",
      "    step: 50; loss: 20.359; l2dist: 2.895\n",
      "    step: 100; loss: 16.191; l2dist: 2.855\n",
      "    step: 150; loss: 12.929; l2dist: 2.900\n",
      "    step: 200; loss: 10.853; l2dist: 2.828\n",
      "    step: 250; loss: 9.899; l2dist: 2.769\n",
      "    step: 300; loss: 9.474; l2dist: 2.728\n",
      "    step: 350; loss: 9.089; l2dist: 2.689\n",
      "    step: 400; loss: 8.913; l2dist: 2.668\n",
      "    step: 450; loss: 8.812; l2dist: 2.666\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.414; l2dist: 0.000\n",
      "    step: 50; loss: 19.884; l2dist: 2.822\n",
      "    step: 100; loss: 15.989; l2dist: 2.811\n",
      "    step: 150; loss: 12.861; l2dist: 2.861\n",
      "    step: 200; loss: 10.843; l2dist: 2.806\n",
      "    step: 250; loss: 9.913; l2dist: 2.753\n",
      "    step: 300; loss: 9.460; l2dist: 2.716\n",
      "    step: 350; loss: 9.119; l2dist: 2.682\n",
      "    step: 400; loss: 9.020; l2dist: 2.669\n",
      "    step: 450; loss: 8.839; l2dist: 2.653\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.938; l2dist: 0.000\n",
      "    step: 50; loss: 19.538; l2dist: 2.809\n",
      "    step: 100; loss: 15.896; l2dist: 2.785\n",
      "    step: 150; loss: 12.801; l2dist: 2.855\n",
      "    step: 200; loss: 10.861; l2dist: 2.799\n",
      "    step: 250; loss: 9.935; l2dist: 2.739\n",
      "    step: 300; loss: 9.454; l2dist: 2.707\n",
      "    step: 350; loss: 9.163; l2dist: 2.688\n",
      "    step: 400; loss: 9.038; l2dist: 2.664\n",
      "    step: 450; loss: 8.816; l2dist: 2.652\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.627; l2dist: 0.000\n",
      "    step: 50; loss: 19.709; l2dist: 2.821\n",
      "    step: 100; loss: 15.956; l2dist: 2.808\n",
      "    step: 150; loss: 12.776; l2dist: 2.858\n",
      "    step: 200; loss: 10.779; l2dist: 2.799\n",
      "    step: 250; loss: 9.902; l2dist: 2.745\n",
      "    step: 300; loss: 9.298; l2dist: 2.697\n",
      "    step: 350; loss: 9.101; l2dist: 2.679\n",
      "    step: 400; loss: 8.897; l2dist: 2.664\n",
      "    step: 450; loss: 8.689; l2dist: 2.643\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.312; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 13.499; l2dist: 1.828\n",
      "    step: 100; loss: 13.249; l2dist: 1.835\n",
      "    step: 150; loss: 12.763; l2dist: 1.924\n",
      "    step: 200; loss: 12.307; l2dist: 2.010\n",
      "    step: 250; loss: 11.924; l2dist: 2.055\n",
      "    step: 300; loss: 11.573; l2dist: 2.094\n",
      "    step: 350; loss: 11.329; l2dist: 2.110\n",
      "    step: 400; loss: 11.096; l2dist: 2.132\n",
      "    step: 450; loss: 10.876; l2dist: 2.127\n",
      "binary step: 0; number of successful adv: 33/100\n",
      "    step: 0; loss: 123.442; l2dist: 0.000\n",
      "    step: 50; loss: 40.375; l2dist: 4.235\n",
      "    step: 100; loss: 24.187; l2dist: 3.909\n",
      "    step: 150; loss: 15.433; l2dist: 3.504\n",
      "    step: 200; loss: 13.297; l2dist: 3.274\n",
      "    step: 250; loss: 12.257; l2dist: 3.155\n",
      "    step: 300; loss: 11.498; l2dist: 3.069\n",
      "    step: 350; loss: 11.185; l2dist: 3.032\n",
      "    step: 400; loss: 10.840; l2dist: 2.985\n",
      "    step: 450; loss: 10.693; l2dist: 2.969\n",
      "binary step: 1; number of successful adv: 96/100\n",
      "    step: 0; loss: 105.775; l2dist: 0.000\n",
      "    step: 50; loss: 32.861; l2dist: 3.858\n",
      "    step: 100; loss: 22.461; l2dist: 3.725\n",
      "    step: 150; loss: 15.372; l2dist: 3.432\n",
      "    step: 200; loss: 13.030; l2dist: 3.265\n",
      "    step: 250; loss: 11.752; l2dist: 3.116\n",
      "    step: 300; loss: 11.308; l2dist: 3.065\n",
      "    step: 350; loss: 10.893; l2dist: 3.016\n",
      "    step: 400; loss: 10.566; l2dist: 2.975\n",
      "    step: 450; loss: 10.597; l2dist: 2.976\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.619; l2dist: 0.000\n",
      "    step: 50; loss: 26.411; l2dist: 3.425\n",
      "    step: 100; loss: 20.794; l2dist: 3.360\n",
      "    step: 150; loss: 15.151; l2dist: 3.239\n",
      "    step: 200; loss: 12.613; l2dist: 3.117\n",
      "    step: 250; loss: 11.310; l2dist: 3.038\n",
      "    step: 300; loss: 10.721; l2dist: 2.973\n",
      "    step: 350; loss: 10.404; l2dist: 2.938\n",
      "    step: 400; loss: 10.155; l2dist: 2.906\n",
      "    step: 450; loss: 9.921; l2dist: 2.874\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.085; l2dist: 0.000\n",
      "    step: 50; loss: 23.915; l2dist: 3.179\n",
      "    step: 100; loss: 19.924; l2dist: 3.121\n",
      "    step: 150; loss: 15.301; l2dist: 3.152\n",
      "    step: 200; loss: 13.162; l2dist: 3.037\n",
      "    step: 250; loss: 11.790; l2dist: 2.980\n",
      "    step: 300; loss: 11.257; l2dist: 2.930\n",
      "    step: 350; loss: 10.461; l2dist: 2.921\n",
      "    step: 400; loss: 10.211; l2dist: 2.891\n",
      "    step: 450; loss: 10.142; l2dist: 2.879\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.843; l2dist: 0.000\n",
      "    step: 50; loss: 22.020; l2dist: 3.007\n",
      "    step: 100; loss: 18.866; l2dist: 2.979\n",
      "    step: 150; loss: 15.593; l2dist: 3.039\n",
      "    step: 200; loss: 13.036; l2dist: 2.999\n",
      "    step: 250; loss: 11.613; l2dist: 2.956\n",
      "    step: 300; loss: 11.046; l2dist: 2.909\n",
      "    step: 350; loss: 10.662; l2dist: 2.865\n",
      "    step: 400; loss: 10.449; l2dist: 2.853\n",
      "    step: 450; loss: 10.249; l2dist: 2.829\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.044; l2dist: 0.000\n",
      "    step: 50; loss: 21.621; l2dist: 2.956\n",
      "    step: 100; loss: 18.790; l2dist: 2.907\n",
      "    step: 150; loss: 15.732; l2dist: 2.976\n",
      "    step: 200; loss: 13.446; l2dist: 2.926\n",
      "    step: 250; loss: 12.269; l2dist: 2.897\n",
      "    step: 300; loss: 11.568; l2dist: 2.874\n",
      "    step: 350; loss: 11.134; l2dist: 2.839\n",
      "    step: 400; loss: 10.759; l2dist: 2.846\n",
      "    step: 450; loss: 10.595; l2dist: 2.825\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.442; l2dist: 0.000\n",
      "    step: 50; loss: 21.248; l2dist: 2.949\n",
      "    step: 100; loss: 18.395; l2dist: 2.900\n",
      "    step: 150; loss: 15.665; l2dist: 2.948\n",
      "    step: 200; loss: 13.470; l2dist: 2.936\n",
      "    step: 250; loss: 12.375; l2dist: 2.887\n",
      "    step: 300; loss: 11.542; l2dist: 2.875\n",
      "    step: 350; loss: 11.170; l2dist: 2.847\n",
      "    step: 400; loss: 10.860; l2dist: 2.818\n",
      "    step: 450; loss: 10.686; l2dist: 2.800\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.622; l2dist: 0.000\n",
      "    step: 50; loss: 21.193; l2dist: 2.948\n",
      "    step: 100; loss: 18.348; l2dist: 2.892\n",
      "    step: 150; loss: 15.634; l2dist: 2.935\n",
      "    step: 200; loss: 13.584; l2dist: 2.927\n",
      "    step: 250; loss: 12.435; l2dist: 2.893\n",
      "    step: 300; loss: 11.631; l2dist: 2.875\n",
      "    step: 350; loss: 11.122; l2dist: 2.881\n",
      "    step: 400; loss: 10.762; l2dist: 2.860\n",
      "    step: 450; loss: 10.586; l2dist: 2.839\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.213; l2dist: 0.000\n",
      "    step: 50; loss: 21.347; l2dist: 2.960\n",
      "    step: 100; loss: 18.430; l2dist: 2.915\n",
      "    step: 150; loss: 15.690; l2dist: 2.964\n",
      "    step: 200; loss: 13.707; l2dist: 2.941\n",
      "    step: 250; loss: 12.157; l2dist: 2.924\n",
      "    step: 300; loss: 11.531; l2dist: 2.886\n",
      "    step: 350; loss: 11.118; l2dist: 2.870\n",
      "    step: 400; loss: 10.641; l2dist: 2.870\n",
      "    step: 450; loss: 10.369; l2dist: 2.841\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.942; l2dist: 0.000\n",
      "    step: 50; loss: 12.961; l2dist: 1.888\n",
      "    step: 100; loss: 12.621; l2dist: 1.901\n",
      "    step: 150; loss: 12.064; l2dist: 1.981\n",
      "    step: 200; loss: 11.600; l2dist: 2.031\n",
      "    step: 250; loss: 11.253; l2dist: 2.065\n",
      "    step: 300; loss: 11.001; l2dist: 2.085\n",
      "    step: 350; loss: 10.824; l2dist: 2.088\n",
      "    step: 400; loss: 10.733; l2dist: 2.096\n",
      "    step: 450; loss: 10.700; l2dist: 2.094\n",
      "binary step: 0; number of successful adv: 41/100\n",
      "    step: 0; loss: 110.516; l2dist: 0.000\n",
      "    step: 50; loss: 37.222; l2dist: 3.901\n",
      "    step: 100; loss: 23.828; l2dist: 3.588\n",
      "    step: 150; loss: 14.609; l2dist: 3.269\n",
      "    step: 200; loss: 12.107; l2dist: 3.062\n",
      "    step: 250; loss: 11.113; l2dist: 2.954\n",
      "    step: 300; loss: 10.684; l2dist: 2.907\n",
      "    step: 350; loss: 10.369; l2dist: 2.865\n",
      "    step: 400; loss: 10.061; l2dist: 2.847\n",
      "    step: 450; loss: 9.959; l2dist: 2.827\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 160.394; l2dist: 0.000\n",
      "    step: 50; loss: 36.050; l2dist: 3.987\n",
      "    step: 100; loss: 22.692; l2dist: 3.776\n",
      "    step: 150; loss: 15.990; l2dist: 3.396\n",
      "    step: 200; loss: 12.802; l2dist: 3.158\n",
      "    step: 250; loss: 11.663; l2dist: 3.031\n",
      "    step: 300; loss: 11.162; l2dist: 2.961\n",
      "    step: 350; loss: 10.656; l2dist: 2.908\n",
      "    step: 400; loss: 10.480; l2dist: 2.883\n",
      "    step: 450; loss: 10.460; l2dist: 2.872\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 155.439; l2dist: 0.000\n",
      "    step: 50; loss: 30.370; l2dist: 3.681\n",
      "    step: 100; loss: 21.464; l2dist: 3.482\n",
      "    step: 150; loss: 15.780; l2dist: 3.281\n",
      "    step: 200; loss: 12.608; l2dist: 3.076\n",
      "    step: 250; loss: 11.336; l2dist: 2.994\n",
      "    step: 300; loss: 10.913; l2dist: 2.943\n",
      "    step: 350; loss: 10.424; l2dist: 2.885\n",
      "    step: 400; loss: 10.242; l2dist: 2.860\n",
      "    step: 450; loss: 10.024; l2dist: 2.845\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 106.937; l2dist: 0.000\n",
      "    step: 50; loss: 26.393; l2dist: 3.395\n",
      "    step: 100; loss: 19.824; l2dist: 3.209\n",
      "    step: 150; loss: 15.033; l2dist: 3.129\n",
      "    step: 200; loss: 12.444; l2dist: 3.017\n",
      "    step: 250; loss: 11.396; l2dist: 2.942\n",
      "    step: 300; loss: 10.687; l2dist: 2.887\n",
      "    step: 350; loss: 10.349; l2dist: 2.878\n",
      "    step: 400; loss: 10.011; l2dist: 2.836\n",
      "    step: 450; loss: 9.841; l2dist: 2.821\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 87.418; l2dist: 0.000\n",
      "    step: 50; loss: 24.693; l2dist: 3.246\n",
      "    step: 100; loss: 19.401; l2dist: 3.092\n",
      "    step: 150; loss: 15.085; l2dist: 3.053\n",
      "    step: 200; loss: 12.696; l2dist: 2.995\n",
      "    step: 250; loss: 11.257; l2dist: 2.921\n",
      "    step: 300; loss: 10.659; l2dist: 2.881\n",
      "    step: 350; loss: 10.219; l2dist: 2.836\n",
      "    step: 400; loss: 10.010; l2dist: 2.822\n",
      "    step: 450; loss: 10.024; l2dist: 2.823\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.350; l2dist: 0.000\n",
      "    step: 50; loss: 23.624; l2dist: 3.189\n",
      "    step: 100; loss: 19.249; l2dist: 3.037\n",
      "    step: 150; loss: 15.161; l2dist: 3.021\n",
      "    step: 200; loss: 12.977; l2dist: 2.963\n",
      "    step: 250; loss: 11.208; l2dist: 2.925\n",
      "    step: 300; loss: 10.546; l2dist: 2.887\n",
      "    step: 350; loss: 10.288; l2dist: 2.855\n",
      "    step: 400; loss: 10.258; l2dist: 2.855\n",
      "    step: 450; loss: 9.880; l2dist: 2.799\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.443; l2dist: 0.000\n",
      "    step: 50; loss: 23.133; l2dist: 3.152\n",
      "    step: 100; loss: 19.004; l2dist: 3.020\n",
      "    step: 150; loss: 14.964; l2dist: 3.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 200; loss: 12.565; l2dist: 2.974\n",
      "    step: 250; loss: 11.231; l2dist: 2.909\n",
      "    step: 300; loss: 10.394; l2dist: 2.865\n",
      "    step: 350; loss: 10.051; l2dist: 2.833\n",
      "    step: 400; loss: 10.040; l2dist: 2.836\n",
      "    step: 450; loss: 9.715; l2dist: 2.799\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.339; l2dist: 0.000\n",
      "    step: 50; loss: 22.805; l2dist: 3.127\n",
      "    step: 100; loss: 18.867; l2dist: 2.989\n",
      "    step: 150; loss: 15.103; l2dist: 3.000\n",
      "    step: 200; loss: 12.710; l2dist: 2.973\n",
      "    step: 250; loss: 11.122; l2dist: 2.921\n",
      "    step: 300; loss: 10.485; l2dist: 2.876\n",
      "    step: 350; loss: 10.150; l2dist: 2.838\n",
      "    step: 400; loss: 9.972; l2dist: 2.818\n",
      "    step: 450; loss: 9.717; l2dist: 2.805\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.007; l2dist: 0.000\n",
      "    step: 50; loss: 22.916; l2dist: 3.136\n",
      "    step: 100; loss: 18.893; l2dist: 3.005\n",
      "    step: 150; loss: 15.035; l2dist: 3.005\n",
      "    step: 200; loss: 12.580; l2dist: 2.978\n",
      "    step: 250; loss: 11.113; l2dist: 2.927\n",
      "    step: 300; loss: 10.561; l2dist: 2.867\n",
      "    step: 350; loss: 10.107; l2dist: 2.836\n",
      "    step: 400; loss: 9.975; l2dist: 2.827\n",
      "    step: 450; loss: 9.753; l2dist: 2.799\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.750; l2dist: 0.000\n",
      "    step: 50; loss: 12.419; l2dist: 1.836\n",
      "    step: 100; loss: 12.062; l2dist: 1.842\n",
      "    step: 150; loss: 11.607; l2dist: 1.904\n",
      "    step: 200; loss: 11.208; l2dist: 1.962\n",
      "    step: 250; loss: 10.864; l2dist: 1.979\n",
      "    step: 300; loss: 10.606; l2dist: 1.995\n",
      "    step: 350; loss: 10.427; l2dist: 1.993\n",
      "    step: 400; loss: 10.360; l2dist: 1.997\n",
      "    step: 450; loss: 10.293; l2dist: 2.002\n",
      "binary step: 0; number of successful adv: 31/100\n",
      "    step: 0; loss: 123.489; l2dist: 0.000\n",
      "    step: 50; loss: 37.976; l2dist: 4.052\n",
      "    step: 100; loss: 23.844; l2dist: 3.697\n",
      "    step: 150; loss: 15.940; l2dist: 3.356\n",
      "    step: 200; loss: 12.895; l2dist: 3.215\n",
      "    step: 250; loss: 11.626; l2dist: 3.083\n",
      "    step: 300; loss: 11.085; l2dist: 3.028\n",
      "    step: 350; loss: 10.694; l2dist: 2.987\n",
      "    step: 400; loss: 10.353; l2dist: 2.954\n",
      "    step: 450; loss: 10.163; l2dist: 2.922\n",
      "binary step: 1; number of successful adv: 92/100\n",
      "    step: 0; loss: 165.867; l2dist: 0.000\n",
      "    step: 50; loss: 45.599; l2dist: 4.461\n",
      "    step: 100; loss: 24.900; l2dist: 3.944\n",
      "    step: 150; loss: 15.964; l2dist: 3.501\n",
      "    step: 200; loss: 13.270; l2dist: 3.286\n",
      "    step: 250; loss: 11.930; l2dist: 3.164\n",
      "    step: 300; loss: 11.359; l2dist: 3.096\n",
      "    step: 350; loss: 10.719; l2dist: 3.015\n",
      "    step: 400; loss: 10.599; l2dist: 2.994\n",
      "    step: 450; loss: 10.474; l2dist: 2.986\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 102.578; l2dist: 0.000\n",
      "    step: 50; loss: 34.551; l2dist: 3.802\n",
      "    step: 100; loss: 22.601; l2dist: 3.583\n",
      "    step: 150; loss: 14.679; l2dist: 3.309\n",
      "    step: 200; loss: 12.212; l2dist: 3.152\n",
      "    step: 250; loss: 11.254; l2dist: 3.048\n",
      "    step: 300; loss: 10.688; l2dist: 3.000\n",
      "    step: 350; loss: 10.336; l2dist: 2.949\n",
      "    step: 400; loss: 10.015; l2dist: 2.913\n",
      "    step: 450; loss: 9.872; l2dist: 2.885\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.086; l2dist: 0.000\n",
      "    step: 50; loss: 27.780; l2dist: 3.315\n",
      "    step: 100; loss: 20.674; l2dist: 3.230\n",
      "    step: 150; loss: 14.213; l2dist: 3.150\n",
      "    step: 200; loss: 11.872; l2dist: 3.014\n",
      "    step: 250; loss: 10.810; l2dist: 2.931\n",
      "    step: 300; loss: 10.356; l2dist: 2.891\n",
      "    step: 350; loss: 9.908; l2dist: 2.843\n",
      "    step: 400; loss: 9.686; l2dist: 2.820\n",
      "    step: 450; loss: 9.445; l2dist: 2.813\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.964; l2dist: 0.000\n",
      "    step: 50; loss: 24.950; l2dist: 3.099\n",
      "    step: 100; loss: 19.961; l2dist: 3.060\n",
      "    step: 150; loss: 15.105; l2dist: 3.041\n",
      "    step: 200; loss: 12.161; l2dist: 2.984\n",
      "    step: 250; loss: 10.948; l2dist: 2.904\n",
      "    step: 300; loss: 10.324; l2dist: 2.863\n",
      "    step: 350; loss: 10.028; l2dist: 2.836\n",
      "    step: 400; loss: 9.687; l2dist: 2.801\n",
      "    step: 450; loss: 9.507; l2dist: 2.777\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.619; l2dist: 0.000\n",
      "    step: 50; loss: 23.563; l2dist: 3.037\n",
      "    step: 100; loss: 19.122; l2dist: 3.000\n",
      "    step: 150; loss: 15.175; l2dist: 2.972\n",
      "    step: 200; loss: 11.892; l2dist: 2.953\n",
      "    step: 250; loss: 10.767; l2dist: 2.861\n",
      "    step: 300; loss: 10.096; l2dist: 2.829\n",
      "    step: 350; loss: 9.753; l2dist: 2.803\n",
      "    step: 400; loss: 9.509; l2dist: 2.775\n",
      "    step: 450; loss: 9.396; l2dist: 2.764\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.340; l2dist: 0.000\n",
      "    step: 50; loss: 22.634; l2dist: 2.996\n",
      "    step: 100; loss: 18.869; l2dist: 2.940\n",
      "    step: 150; loss: 14.927; l2dist: 2.942\n",
      "    step: 200; loss: 12.041; l2dist: 2.931\n",
      "    step: 250; loss: 10.659; l2dist: 2.888\n",
      "    step: 300; loss: 10.120; l2dist: 2.834\n",
      "    step: 350; loss: 9.794; l2dist: 2.807\n",
      "    step: 400; loss: 9.544; l2dist: 2.789\n",
      "    step: 450; loss: 9.412; l2dist: 2.768\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.660; l2dist: 0.000\n",
      "    step: 50; loss: 22.303; l2dist: 2.980\n",
      "    step: 100; loss: 18.662; l2dist: 2.935\n",
      "    step: 150; loss: 14.913; l2dist: 2.937\n",
      "    step: 200; loss: 12.127; l2dist: 2.933\n",
      "    step: 250; loss: 10.973; l2dist: 2.867\n",
      "    step: 300; loss: 10.346; l2dist: 2.831\n",
      "    step: 350; loss: 10.095; l2dist: 2.821\n",
      "    step: 400; loss: 9.717; l2dist: 2.785\n",
      "    step: 450; loss: 9.597; l2dist: 2.775\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.483; l2dist: 0.000\n",
      "    step: 50; loss: 22.420; l2dist: 2.993\n",
      "    step: 100; loss: 18.789; l2dist: 2.938\n",
      "    step: 150; loss: 15.128; l2dist: 2.927\n",
      "    step: 200; loss: 12.128; l2dist: 2.935\n",
      "    step: 250; loss: 10.900; l2dist: 2.884\n",
      "    step: 300; loss: 10.131; l2dist: 2.845\n",
      "    step: 350; loss: 9.951; l2dist: 2.814\n",
      "    step: 400; loss: 9.618; l2dist: 2.792\n",
      "    step: 450; loss: 9.370; l2dist: 2.776\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.006; l2dist: 0.000\n",
      "    step: 50; loss: 12.537; l2dist: 1.860\n",
      "    step: 100; loss: 12.215; l2dist: 1.864\n",
      "    step: 150; loss: 11.613; l2dist: 1.942\n",
      "    step: 200; loss: 11.206; l2dist: 1.972\n",
      "    step: 250; loss: 10.925; l2dist: 2.004\n",
      "    step: 300; loss: 10.666; l2dist: 2.021\n",
      "    step: 350; loss: 10.526; l2dist: 2.036\n",
      "    step: 400; loss: 10.421; l2dist: 2.041\n",
      "    step: 450; loss: 10.332; l2dist: 2.046\n",
      "binary step: 0; number of successful adv: 37/100\n",
      "    step: 0; loss: 122.686; l2dist: 0.000\n",
      "    step: 50; loss: 33.657; l2dist: 4.036\n",
      "    step: 100; loss: 21.314; l2dist: 3.600\n",
      "    step: 150; loss: 15.394; l2dist: 3.272\n",
      "    step: 200; loss: 12.707; l2dist: 3.092\n",
      "    step: 250; loss: 11.424; l2dist: 2.995\n",
      "    step: 300; loss: 10.889; l2dist: 2.926\n",
      "    step: 350; loss: 10.573; l2dist: 2.892\n",
      "    step: 400; loss: 10.282; l2dist: 2.853\n",
      "    step: 450; loss: 10.131; l2dist: 2.837\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 206.857; l2dist: 0.000\n",
      "    step: 50; loss: 39.234; l2dist: 4.529\n",
      "    step: 100; loss: 22.496; l2dist: 3.919\n",
      "    step: 150; loss: 15.740; l2dist: 3.473\n",
      "    step: 200; loss: 13.230; l2dist: 3.223\n",
      "    step: 250; loss: 12.205; l2dist: 3.124\n",
      "    step: 300; loss: 11.518; l2dist: 3.031\n",
      "    step: 350; loss: 11.235; l2dist: 2.984\n",
      "    step: 400; loss: 10.587; l2dist: 2.920\n",
      "    step: 450; loss: 10.615; l2dist: 2.919\n",
      "binary step: 2; number of successful adv: 98/100\n",
      "    step: 0; loss: 534.670; l2dist: 0.000\n",
      "    step: 50; loss: 48.305; l2dist: 5.244\n",
      "    step: 100; loss: 25.932; l2dist: 4.650\n",
      "    step: 150; loss: 20.692; l2dist: 4.163\n",
      "    step: 200; loss: 17.314; l2dist: 3.793\n",
      "    step: 250; loss: 15.244; l2dist: 3.547\n",
      "    step: 300; loss: 13.863; l2dist: 3.380\n",
      "    step: 350; loss: 12.901; l2dist: 3.270\n",
      "    step: 400; loss: 12.209; l2dist: 3.185\n",
      "    step: 450; loss: 11.728; l2dist: 3.125\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 307.295; l2dist: 0.000\n",
      "    step: 50; loss: 37.394; l2dist: 4.754\n",
      "    step: 100; loss: 21.505; l2dist: 4.090\n",
      "    step: 150; loss: 16.883; l2dist: 3.632\n",
      "    step: 200; loss: 14.450; l2dist: 3.387\n",
      "    step: 250; loss: 12.930; l2dist: 3.227\n",
      "    step: 300; loss: 12.074; l2dist: 3.128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 11.629; l2dist: 3.080\n",
      "    step: 400; loss: 11.179; l2dist: 3.021\n",
      "    step: 450; loss: 11.175; l2dist: 3.021\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 203.372; l2dist: 0.000\n",
      "    step: 50; loss: 30.994; l2dist: 4.290\n",
      "    step: 100; loss: 18.921; l2dist: 3.676\n",
      "    step: 150; loss: 14.785; l2dist: 3.321\n",
      "    step: 200; loss: 12.871; l2dist: 3.159\n",
      "    step: 250; loss: 11.801; l2dist: 3.053\n",
      "    step: 300; loss: 11.165; l2dist: 2.981\n",
      "    step: 350; loss: 10.980; l2dist: 2.962\n",
      "    step: 400; loss: 10.656; l2dist: 2.922\n",
      "    step: 450; loss: 10.453; l2dist: 2.903\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 149.167; l2dist: 0.000\n",
      "    step: 50; loss: 27.482; l2dist: 3.965\n",
      "    step: 100; loss: 17.541; l2dist: 3.456\n",
      "    step: 150; loss: 14.032; l2dist: 3.195\n",
      "    step: 200; loss: 12.441; l2dist: 3.075\n",
      "    step: 250; loss: 11.527; l2dist: 2.994\n",
      "    step: 300; loss: 10.918; l2dist: 2.925\n",
      "    step: 350; loss: 10.675; l2dist: 2.903\n",
      "    step: 400; loss: 10.364; l2dist: 2.863\n",
      "    step: 450; loss: 10.146; l2dist: 2.852\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 122.975; l2dist: 0.000\n",
      "    step: 50; loss: 25.627; l2dist: 3.776\n",
      "    step: 100; loss: 17.065; l2dist: 3.345\n",
      "    step: 150; loss: 13.660; l2dist: 3.137\n",
      "    step: 200; loss: 12.145; l2dist: 3.026\n",
      "    step: 250; loss: 11.293; l2dist: 2.956\n",
      "    step: 300; loss: 10.905; l2dist: 2.915\n",
      "    step: 350; loss: 10.607; l2dist: 2.882\n",
      "    step: 400; loss: 10.287; l2dist: 2.852\n",
      "    step: 450; loss: 10.080; l2dist: 2.841\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 109.857; l2dist: 0.000\n",
      "    step: 50; loss: 24.678; l2dist: 3.657\n",
      "    step: 100; loss: 16.917; l2dist: 3.287\n",
      "    step: 150; loss: 13.571; l2dist: 3.109\n",
      "    step: 200; loss: 12.088; l2dist: 3.009\n",
      "    step: 250; loss: 11.247; l2dist: 2.934\n",
      "    step: 300; loss: 10.817; l2dist: 2.900\n",
      "    step: 350; loss: 10.622; l2dist: 2.869\n",
      "    step: 400; loss: 10.242; l2dist: 2.842\n",
      "    step: 450; loss: 9.981; l2dist: 2.807\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.190; l2dist: 0.000\n",
      "    step: 50; loss: 24.674; l2dist: 3.668\n",
      "    step: 100; loss: 17.010; l2dist: 3.298\n",
      "    step: 150; loss: 13.650; l2dist: 3.117\n",
      "    step: 200; loss: 12.137; l2dist: 3.024\n",
      "    step: 250; loss: 11.330; l2dist: 2.957\n",
      "    step: 300; loss: 10.789; l2dist: 2.897\n",
      "    step: 350; loss: 10.387; l2dist: 2.866\n",
      "    step: 400; loss: 10.154; l2dist: 2.840\n",
      "    step: 450; loss: 10.085; l2dist: 2.829\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.349; l2dist: 0.000\n",
      "    step: 50; loss: 13.399; l2dist: 1.666\n",
      "    step: 100; loss: 13.019; l2dist: 1.695\n",
      "    step: 150; loss: 12.581; l2dist: 1.789\n",
      "    step: 200; loss: 12.002; l2dist: 1.868\n",
      "    step: 250; loss: 11.612; l2dist: 1.907\n",
      "    step: 300; loss: 11.335; l2dist: 1.926\n",
      "    step: 350; loss: 11.110; l2dist: 1.933\n",
      "    step: 400; loss: 11.021; l2dist: 1.936\n",
      "    step: 450; loss: 10.919; l2dist: 1.935\n",
      "binary step: 0; number of successful adv: 28/100\n",
      "    step: 0; loss: 127.100; l2dist: 0.000\n",
      "    step: 50; loss: 44.249; l2dist: 4.115\n",
      "    step: 100; loss: 26.631; l2dist: 3.878\n",
      "    step: 150; loss: 16.564; l2dist: 3.546\n",
      "    step: 200; loss: 13.894; l2dist: 3.341\n",
      "    step: 250; loss: 12.927; l2dist: 3.235\n",
      "    step: 300; loss: 12.293; l2dist: 3.157\n",
      "    step: 350; loss: 11.942; l2dist: 3.114\n",
      "    step: 400; loss: 11.668; l2dist: 3.086\n",
      "    step: 450; loss: 11.275; l2dist: 3.044\n",
      "binary step: 1; number of successful adv: 92/100\n",
      "    step: 0; loss: 173.225; l2dist: 0.000\n",
      "    step: 50; loss: 46.352; l2dist: 4.344\n",
      "    step: 100; loss: 25.079; l2dist: 4.011\n",
      "    step: 150; loss: 16.898; l2dist: 3.618\n",
      "    step: 200; loss: 14.201; l2dist: 3.392\n",
      "    step: 250; loss: 12.818; l2dist: 3.245\n",
      "    step: 300; loss: 11.919; l2dist: 3.148\n",
      "    step: 350; loss: 11.543; l2dist: 3.102\n",
      "    step: 400; loss: 11.154; l2dist: 3.057\n",
      "    step: 450; loss: 11.019; l2dist: 3.039\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 105.815; l2dist: 0.000\n",
      "    step: 50; loss: 35.059; l2dist: 3.786\n",
      "    step: 100; loss: 22.383; l2dist: 3.571\n",
      "    step: 150; loss: 15.932; l2dist: 3.418\n",
      "    step: 200; loss: 13.380; l2dist: 3.240\n",
      "    step: 250; loss: 12.211; l2dist: 3.154\n",
      "    step: 300; loss: 11.466; l2dist: 3.068\n",
      "    step: 350; loss: 11.222; l2dist: 3.045\n",
      "    step: 400; loss: 10.986; l2dist: 3.011\n",
      "    step: 450; loss: 10.757; l2dist: 2.987\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.319; l2dist: 0.000\n",
      "    step: 50; loss: 29.076; l2dist: 3.318\n",
      "    step: 100; loss: 20.708; l2dist: 3.183\n",
      "    step: 150; loss: 16.688; l2dist: 3.153\n",
      "    step: 200; loss: 13.641; l2dist: 3.127\n",
      "    step: 250; loss: 11.689; l2dist: 3.063\n",
      "    step: 300; loss: 10.994; l2dist: 2.994\n",
      "    step: 350; loss: 10.508; l2dist: 2.929\n",
      "    step: 400; loss: 10.202; l2dist: 2.907\n",
      "    step: 450; loss: 9.913; l2dist: 2.872\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.117; l2dist: 0.000\n",
      "    step: 50; loss: 25.867; l2dist: 3.071\n",
      "    step: 100; loss: 19.952; l2dist: 2.982\n",
      "    step: 150; loss: 16.730; l2dist: 2.991\n",
      "    step: 200; loss: 14.487; l2dist: 2.985\n",
      "    step: 250; loss: 12.894; l2dist: 2.964\n",
      "    step: 300; loss: 11.589; l2dist: 2.967\n",
      "    step: 350; loss: 11.147; l2dist: 2.936\n",
      "    step: 400; loss: 10.521; l2dist: 2.925\n",
      "    step: 450; loss: 10.362; l2dist: 2.914\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.701; l2dist: 0.000\n",
      "    step: 50; loss: 24.472; l2dist: 2.984\n",
      "    step: 100; loss: 19.958; l2dist: 2.881\n",
      "    step: 150; loss: 16.488; l2dist: 2.957\n",
      "    step: 200; loss: 14.194; l2dist: 2.933\n",
      "    step: 250; loss: 12.768; l2dist: 2.929\n",
      "    step: 300; loss: 11.741; l2dist: 2.909\n",
      "    step: 350; loss: 11.053; l2dist: 2.903\n",
      "    step: 400; loss: 10.770; l2dist: 2.868\n",
      "    step: 450; loss: 10.521; l2dist: 2.842\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.166; l2dist: 0.000\n",
      "    step: 50; loss: 23.724; l2dist: 2.955\n",
      "    step: 100; loss: 19.759; l2dist: 2.877\n",
      "    step: 150; loss: 16.295; l2dist: 2.953\n",
      "    step: 200; loss: 13.939; l2dist: 2.950\n",
      "    step: 250; loss: 12.680; l2dist: 2.920\n",
      "    step: 300; loss: 11.905; l2dist: 2.898\n",
      "    step: 350; loss: 11.251; l2dist: 2.909\n",
      "    step: 400; loss: 10.905; l2dist: 2.887\n",
      "    step: 450; loss: 10.782; l2dist: 2.866\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.274; l2dist: 0.000\n",
      "    step: 50; loss: 23.446; l2dist: 2.932\n",
      "    step: 100; loss: 19.610; l2dist: 2.874\n",
      "    step: 150; loss: 16.509; l2dist: 2.924\n",
      "    step: 200; loss: 14.219; l2dist: 2.926\n",
      "    step: 250; loss: 12.793; l2dist: 2.927\n",
      "    step: 300; loss: 11.796; l2dist: 2.934\n",
      "    step: 350; loss: 11.296; l2dist: 2.875\n",
      "    step: 400; loss: 10.985; l2dist: 2.869\n",
      "    step: 450; loss: 10.474; l2dist: 2.842\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.839; l2dist: 0.000\n",
      "    step: 50; loss: 23.609; l2dist: 2.948\n",
      "    step: 100; loss: 19.822; l2dist: 2.872\n",
      "    step: 150; loss: 16.586; l2dist: 2.931\n",
      "    step: 200; loss: 14.046; l2dist: 2.955\n",
      "    step: 250; loss: 12.801; l2dist: 2.934\n",
      "    step: 300; loss: 11.863; l2dist: 2.921\n",
      "    step: 350; loss: 11.315; l2dist: 2.879\n",
      "    step: 400; loss: 10.951; l2dist: 2.858\n",
      "    step: 450; loss: 10.807; l2dist: 2.869\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.596; l2dist: 0.000\n",
      "    step: 50; loss: 13.230; l2dist: 1.788\n",
      "    step: 100; loss: 12.818; l2dist: 1.828\n",
      "    step: 150; loss: 12.286; l2dist: 1.910\n",
      "    step: 200; loss: 11.867; l2dist: 1.988\n",
      "    step: 250; loss: 11.376; l2dist: 2.031\n",
      "    step: 300; loss: 10.895; l2dist: 2.040\n",
      "    step: 350; loss: 10.669; l2dist: 2.045\n",
      "    step: 400; loss: 10.572; l2dist: 2.047\n",
      "    step: 450; loss: 10.506; l2dist: 2.045\n",
      "binary step: 0; number of successful adv: 38/100\n",
      "    step: 0; loss: 117.897; l2dist: 0.000\n",
      "    step: 50; loss: 37.872; l2dist: 3.897\n",
      "    step: 100; loss: 23.274; l2dist: 3.581\n",
      "    step: 150; loss: 14.402; l2dist: 3.281\n",
      "    step: 200; loss: 12.676; l2dist: 3.106\n",
      "    step: 250; loss: 11.599; l2dist: 2.990\n",
      "    step: 300; loss: 10.876; l2dist: 2.904\n",
      "    step: 350; loss: 10.467; l2dist: 2.869\n",
      "    step: 400; loss: 10.252; l2dist: 2.843\n",
      "    step: 450; loss: 10.000; l2dist: 2.810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 157.914; l2dist: 0.000\n",
      "    step: 50; loss: 40.247; l2dist: 4.301\n",
      "    step: 100; loss: 22.317; l2dist: 3.760\n",
      "    step: 150; loss: 15.761; l2dist: 3.383\n",
      "    step: 200; loss: 12.839; l2dist: 3.161\n",
      "    step: 250; loss: 11.755; l2dist: 3.048\n",
      "    step: 300; loss: 11.057; l2dist: 2.964\n",
      "    step: 350; loss: 10.682; l2dist: 2.916\n",
      "    step: 400; loss: 10.180; l2dist: 2.853\n",
      "    step: 450; loss: 9.890; l2dist: 2.820\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 98.357; l2dist: 0.000\n",
      "    step: 50; loss: 31.764; l2dist: 3.738\n",
      "    step: 100; loss: 19.793; l2dist: 3.406\n",
      "    step: 150; loss: 14.465; l2dist: 3.197\n",
      "    step: 200; loss: 11.991; l2dist: 3.061\n",
      "    step: 250; loss: 10.915; l2dist: 2.944\n",
      "    step: 300; loss: 10.452; l2dist: 2.897\n",
      "    step: 350; loss: 10.024; l2dist: 2.835\n",
      "    step: 400; loss: 10.010; l2dist: 2.825\n",
      "    step: 450; loss: 9.645; l2dist: 2.786\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.880; l2dist: 0.000\n",
      "    step: 50; loss: 27.375; l2dist: 3.436\n",
      "    step: 100; loss: 18.968; l2dist: 3.201\n",
      "    step: 150; loss: 14.317; l2dist: 3.087\n",
      "    step: 200; loss: 11.779; l2dist: 2.983\n",
      "    step: 250; loss: 10.944; l2dist: 2.927\n",
      "    step: 300; loss: 10.499; l2dist: 2.879\n",
      "    step: 350; loss: 10.012; l2dist: 2.831\n",
      "    step: 400; loss: 9.778; l2dist: 2.794\n",
      "    step: 450; loss: 9.585; l2dist: 2.774\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 72.925; l2dist: 0.000\n",
      "    step: 50; loss: 25.209; l2dist: 3.256\n",
      "    step: 100; loss: 18.614; l2dist: 3.098\n",
      "    step: 150; loss: 14.378; l2dist: 3.015\n",
      "    step: 200; loss: 11.774; l2dist: 2.962\n",
      "    step: 250; loss: 10.772; l2dist: 2.879\n",
      "    step: 300; loss: 10.201; l2dist: 2.830\n",
      "    step: 350; loss: 9.977; l2dist: 2.823\n",
      "    step: 400; loss: 9.608; l2dist: 2.779\n",
      "    step: 450; loss: 9.619; l2dist: 2.772\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.318; l2dist: 0.000\n",
      "    step: 50; loss: 24.166; l2dist: 3.218\n",
      "    step: 100; loss: 18.244; l2dist: 3.083\n",
      "    step: 150; loss: 13.969; l2dist: 3.014\n",
      "    step: 200; loss: 11.876; l2dist: 2.938\n",
      "    step: 250; loss: 11.029; l2dist: 2.896\n",
      "    step: 300; loss: 10.345; l2dist: 2.857\n",
      "    step: 350; loss: 9.970; l2dist: 2.812\n",
      "    step: 400; loss: 9.806; l2dist: 2.787\n",
      "    step: 450; loss: 9.648; l2dist: 2.767\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.173; l2dist: 0.000\n",
      "    step: 50; loss: 23.741; l2dist: 3.190\n",
      "    step: 100; loss: 18.239; l2dist: 3.075\n",
      "    step: 150; loss: 14.241; l2dist: 2.994\n",
      "    step: 200; loss: 11.889; l2dist: 2.931\n",
      "    step: 250; loss: 10.786; l2dist: 2.877\n",
      "    step: 300; loss: 10.173; l2dist: 2.832\n",
      "    step: 350; loss: 9.829; l2dist: 2.794\n",
      "    step: 400; loss: 9.708; l2dist: 2.783\n",
      "    step: 450; loss: 9.437; l2dist: 2.750\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.441; l2dist: 0.000\n",
      "    step: 50; loss: 23.360; l2dist: 3.182\n",
      "    step: 100; loss: 18.092; l2dist: 3.068\n",
      "    step: 150; loss: 14.179; l2dist: 2.990\n",
      "    step: 200; loss: 11.714; l2dist: 2.924\n",
      "    step: 250; loss: 10.892; l2dist: 2.880\n",
      "    step: 300; loss: 10.229; l2dist: 2.820\n",
      "    step: 350; loss: 9.722; l2dist: 2.779\n",
      "    step: 400; loss: 9.538; l2dist: 2.754\n",
      "    step: 450; loss: 9.567; l2dist: 2.755\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.434; l2dist: 0.000\n",
      "    step: 50; loss: 23.462; l2dist: 3.196\n",
      "    step: 100; loss: 18.235; l2dist: 3.069\n",
      "    step: 150; loss: 14.077; l2dist: 3.008\n",
      "    step: 200; loss: 11.874; l2dist: 2.934\n",
      "    step: 250; loss: 10.907; l2dist: 2.886\n",
      "    step: 300; loss: 10.231; l2dist: 2.832\n",
      "    step: 350; loss: 10.011; l2dist: 2.823\n",
      "    step: 400; loss: 9.761; l2dist: 2.784\n",
      "    step: 450; loss: 9.428; l2dist: 2.756\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.640; l2dist: 0.000\n",
      "    step: 50; loss: 11.835; l2dist: 1.614\n",
      "    step: 100; loss: 11.562; l2dist: 1.630\n",
      "    step: 150; loss: 11.110; l2dist: 1.702\n",
      "    step: 200; loss: 10.747; l2dist: 1.749\n",
      "    step: 250; loss: 10.551; l2dist: 1.768\n",
      "    step: 300; loss: 10.419; l2dist: 1.788\n",
      "    step: 350; loss: 10.278; l2dist: 1.810\n",
      "    step: 400; loss: 10.179; l2dist: 1.808\n",
      "    step: 450; loss: 10.115; l2dist: 1.818\n",
      "binary step: 0; number of successful adv: 26/100\n",
      "    step: 0; loss: 120.992; l2dist: 0.000\n",
      "    step: 50; loss: 39.165; l2dist: 4.020\n",
      "    step: 100; loss: 23.195; l2dist: 3.683\n",
      "    step: 150; loss: 15.168; l2dist: 3.368\n",
      "    step: 200; loss: 12.420; l2dist: 3.160\n",
      "    step: 250; loss: 11.179; l2dist: 3.014\n",
      "    step: 300; loss: 10.532; l2dist: 2.937\n",
      "    step: 350; loss: 10.179; l2dist: 2.893\n",
      "    step: 400; loss: 9.927; l2dist: 2.857\n",
      "    step: 450; loss: 9.750; l2dist: 2.837\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 171.048; l2dist: 0.000\n",
      "    step: 50; loss: 38.278; l2dist: 4.021\n",
      "    step: 100; loss: 22.457; l2dist: 3.781\n",
      "    step: 150; loss: 15.689; l2dist: 3.444\n",
      "    step: 200; loss: 12.799; l2dist: 3.216\n",
      "    step: 250; loss: 11.780; l2dist: 3.096\n",
      "    step: 300; loss: 10.885; l2dist: 2.990\n",
      "    step: 350; loss: 10.539; l2dist: 2.940\n",
      "    step: 400; loss: 10.076; l2dist: 2.894\n",
      "    step: 450; loss: 9.694; l2dist: 2.842\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 188.275; l2dist: 0.000\n",
      "    step: 50; loss: 31.658; l2dist: 3.778\n",
      "    step: 100; loss: 20.556; l2dist: 3.520\n",
      "    step: 150; loss: 15.481; l2dist: 3.321\n",
      "    step: 200; loss: 12.869; l2dist: 3.101\n",
      "    step: 250; loss: 11.473; l2dist: 2.966\n",
      "    step: 300; loss: 10.753; l2dist: 2.886\n",
      "    step: 350; loss: 10.144; l2dist: 2.835\n",
      "    step: 400; loss: 9.727; l2dist: 2.805\n",
      "    step: 450; loss: 9.469; l2dist: 2.789\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 119.558; l2dist: 0.000\n",
      "    step: 50; loss: 25.858; l2dist: 3.400\n",
      "    step: 100; loss: 18.389; l2dist: 3.207\n",
      "    step: 150; loss: 14.280; l2dist: 3.102\n",
      "    step: 200; loss: 12.336; l2dist: 3.013\n",
      "    step: 250; loss: 11.120; l2dist: 2.919\n",
      "    step: 300; loss: 10.462; l2dist: 2.865\n",
      "    step: 350; loss: 9.920; l2dist: 2.812\n",
      "    step: 400; loss: 9.534; l2dist: 2.770\n",
      "    step: 450; loss: 9.253; l2dist: 2.758\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 87.205; l2dist: 0.000\n",
      "    step: 50; loss: 23.131; l2dist: 3.199\n",
      "    step: 100; loss: 17.216; l2dist: 3.064\n",
      "    step: 150; loss: 13.666; l2dist: 2.994\n",
      "    step: 200; loss: 11.829; l2dist: 2.911\n",
      "    step: 250; loss: 10.780; l2dist: 2.847\n",
      "    step: 300; loss: 10.079; l2dist: 2.774\n",
      "    step: 350; loss: 9.846; l2dist: 2.757\n",
      "    step: 400; loss: 9.431; l2dist: 2.739\n",
      "    step: 450; loss: 9.215; l2dist: 2.723\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.662; l2dist: 0.000\n",
      "    step: 50; loss: 22.126; l2dist: 3.131\n",
      "    step: 100; loss: 16.742; l2dist: 2.987\n",
      "    step: 150; loss: 13.656; l2dist: 2.915\n",
      "    step: 200; loss: 11.782; l2dist: 2.869\n",
      "    step: 250; loss: 10.533; l2dist: 2.776\n",
      "    step: 300; loss: 9.935; l2dist: 2.726\n",
      "    step: 350; loss: 9.643; l2dist: 2.686\n",
      "    step: 400; loss: 9.359; l2dist: 2.658\n",
      "    step: 450; loss: 9.110; l2dist: 2.648\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 82.685; l2dist: 0.000\n",
      "    step: 50; loss: 21.730; l2dist: 3.087\n",
      "    step: 100; loss: 16.525; l2dist: 2.956\n",
      "    step: 150; loss: 13.438; l2dist: 2.881\n",
      "    step: 200; loss: 11.780; l2dist: 2.838\n",
      "    step: 250; loss: 10.449; l2dist: 2.759\n",
      "    step: 300; loss: 9.885; l2dist: 2.707\n",
      "    step: 350; loss: 9.530; l2dist: 2.682\n",
      "    step: 400; loss: 9.209; l2dist: 2.656\n",
      "    step: 450; loss: 8.909; l2dist: 2.656\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.200; l2dist: 0.000\n",
      "    step: 50; loss: 21.405; l2dist: 3.058\n",
      "    step: 100; loss: 16.246; l2dist: 2.954\n",
      "    step: 150; loss: 13.416; l2dist: 2.883\n",
      "    step: 200; loss: 11.871; l2dist: 2.814\n",
      "    step: 250; loss: 10.606; l2dist: 2.778\n",
      "    step: 300; loss: 10.014; l2dist: 2.714\n",
      "    step: 350; loss: 9.661; l2dist: 2.690\n",
      "    step: 400; loss: 9.319; l2dist: 2.664\n",
      "    step: 450; loss: 9.165; l2dist: 2.704\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.607; l2dist: 0.000\n",
      "    step: 50; loss: 21.499; l2dist: 3.071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 16.368; l2dist: 2.960\n",
      "    step: 150; loss: 13.529; l2dist: 2.887\n",
      "    step: 200; loss: 11.925; l2dist: 2.833\n",
      "    step: 250; loss: 10.535; l2dist: 2.778\n",
      "    step: 300; loss: 9.856; l2dist: 2.724\n",
      "    step: 350; loss: 9.437; l2dist: 2.680\n",
      "    step: 400; loss: 9.160; l2dist: 2.660\n",
      "    step: 450; loss: 8.934; l2dist: 2.682\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.525; l2dist: 0.000\n",
      "    step: 50; loss: 12.031; l2dist: 1.888\n",
      "    step: 100; loss: 11.639; l2dist: 1.892\n",
      "    step: 150; loss: 11.203; l2dist: 1.943\n",
      "    step: 200; loss: 10.776; l2dist: 1.993\n",
      "    step: 250; loss: 10.397; l2dist: 2.015\n",
      "    step: 300; loss: 10.120; l2dist: 2.024\n",
      "    step: 350; loss: 9.990; l2dist: 2.046\n",
      "    step: 400; loss: 9.857; l2dist: 2.057\n",
      "    step: 450; loss: 9.803; l2dist: 2.055\n",
      "binary step: 0; number of successful adv: 37/100\n",
      "    step: 0; loss: 113.661; l2dist: 0.000\n",
      "    step: 50; loss: 29.268; l2dist: 3.995\n",
      "    step: 100; loss: 17.924; l2dist: 3.495\n",
      "    step: 150; loss: 13.294; l2dist: 3.182\n",
      "    step: 200; loss: 11.681; l2dist: 3.002\n",
      "    step: 250; loss: 10.885; l2dist: 2.907\n",
      "    step: 300; loss: 10.308; l2dist: 2.847\n",
      "    step: 350; loss: 10.032; l2dist: 2.809\n",
      "    step: 400; loss: 9.850; l2dist: 2.790\n",
      "    step: 450; loss: 9.779; l2dist: 2.783\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 178.283; l2dist: 0.000\n",
      "    step: 50; loss: 29.002; l2dist: 4.020\n",
      "    step: 100; loss: 19.115; l2dist: 3.607\n",
      "    step: 150; loss: 14.258; l2dist: 3.279\n",
      "    step: 200; loss: 12.415; l2dist: 3.104\n",
      "    step: 250; loss: 11.468; l2dist: 2.998\n",
      "    step: 300; loss: 10.976; l2dist: 2.928\n",
      "    step: 350; loss: 10.474; l2dist: 2.862\n",
      "    step: 400; loss: 10.401; l2dist: 2.857\n",
      "    step: 450; loss: 10.305; l2dist: 2.831\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 105.922; l2dist: 0.000\n",
      "    step: 50; loss: 23.072; l2dist: 3.482\n",
      "    step: 100; loss: 17.245; l2dist: 3.215\n",
      "    step: 150; loss: 13.162; l2dist: 3.075\n",
      "    step: 200; loss: 11.539; l2dist: 2.973\n",
      "    step: 250; loss: 10.721; l2dist: 2.888\n",
      "    step: 300; loss: 10.137; l2dist: 2.823\n",
      "    step: 350; loss: 9.861; l2dist: 2.777\n",
      "    step: 400; loss: 9.678; l2dist: 2.760\n",
      "    step: 450; loss: 9.455; l2dist: 2.740\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.638; l2dist: 0.000\n",
      "    step: 50; loss: 20.082; l2dist: 3.147\n",
      "    step: 100; loss: 16.224; l2dist: 3.010\n",
      "    step: 150; loss: 13.008; l2dist: 3.020\n",
      "    step: 200; loss: 11.220; l2dist: 2.928\n",
      "    step: 250; loss: 10.517; l2dist: 2.862\n",
      "    step: 300; loss: 9.994; l2dist: 2.795\n",
      "    step: 350; loss: 9.659; l2dist: 2.758\n",
      "    step: 400; loss: 9.502; l2dist: 2.736\n",
      "    step: 450; loss: 9.308; l2dist: 2.731\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.889; l2dist: 0.000\n",
      "    step: 50; loss: 18.301; l2dist: 2.985\n",
      "    step: 100; loss: 15.230; l2dist: 2.858\n",
      "    step: 150; loss: 12.656; l2dist: 2.903\n",
      "    step: 200; loss: 11.034; l2dist: 2.856\n",
      "    step: 250; loss: 10.305; l2dist: 2.790\n",
      "    step: 300; loss: 9.902; l2dist: 2.757\n",
      "    step: 350; loss: 9.611; l2dist: 2.733\n",
      "    step: 400; loss: 9.403; l2dist: 2.709\n",
      "    step: 450; loss: 9.278; l2dist: 2.695\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.086; l2dist: 0.000\n",
      "    step: 50; loss: 17.866; l2dist: 2.940\n",
      "    step: 100; loss: 15.432; l2dist: 2.832\n",
      "    step: 150; loss: 12.730; l2dist: 2.893\n",
      "    step: 200; loss: 11.122; l2dist: 2.859\n",
      "    step: 250; loss: 10.448; l2dist: 2.805\n",
      "    step: 300; loss: 9.907; l2dist: 2.750\n",
      "    step: 350; loss: 9.560; l2dist: 2.702\n",
      "    step: 400; loss: 9.390; l2dist: 2.683\n",
      "    step: 450; loss: 9.289; l2dist: 2.670\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.835; l2dist: 0.000\n",
      "    step: 50; loss: 17.540; l2dist: 2.905\n",
      "    step: 100; loss: 15.287; l2dist: 2.795\n",
      "    step: 150; loss: 12.705; l2dist: 2.860\n",
      "    step: 200; loss: 11.239; l2dist: 2.839\n",
      "    step: 250; loss: 10.369; l2dist: 2.792\n",
      "    step: 300; loss: 9.986; l2dist: 2.752\n",
      "    step: 350; loss: 9.618; l2dist: 2.707\n",
      "    step: 400; loss: 9.409; l2dist: 2.690\n",
      "    step: 450; loss: 9.282; l2dist: 2.682\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.341; l2dist: 0.000\n",
      "    step: 50; loss: 17.457; l2dist: 2.888\n",
      "    step: 100; loss: 15.196; l2dist: 2.785\n",
      "    step: 150; loss: 12.649; l2dist: 2.847\n",
      "    step: 200; loss: 11.231; l2dist: 2.833\n",
      "    step: 250; loss: 10.501; l2dist: 2.786\n",
      "    step: 300; loss: 9.906; l2dist: 2.734\n",
      "    step: 350; loss: 9.516; l2dist: 2.693\n",
      "    step: 400; loss: 9.448; l2dist: 2.692\n",
      "    step: 450; loss: 9.328; l2dist: 2.680\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.120; l2dist: 0.000\n",
      "    step: 50; loss: 17.616; l2dist: 2.904\n",
      "    step: 100; loss: 15.256; l2dist: 2.808\n",
      "    step: 150; loss: 12.693; l2dist: 2.860\n",
      "    step: 200; loss: 11.312; l2dist: 2.846\n",
      "    step: 250; loss: 10.586; l2dist: 2.793\n",
      "    step: 300; loss: 9.985; l2dist: 2.736\n",
      "    step: 350; loss: 9.853; l2dist: 2.716\n",
      "    step: 400; loss: 9.530; l2dist: 2.703\n",
      "    step: 450; loss: 9.381; l2dist: 2.701\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.436; l2dist: 0.000\n",
      "    step: 50; loss: 12.312; l2dist: 1.852\n",
      "    step: 100; loss: 11.950; l2dist: 1.867\n",
      "    step: 150; loss: 11.407; l2dist: 1.957\n",
      "    step: 200; loss: 11.009; l2dist: 2.016\n",
      "    step: 250; loss: 10.650; l2dist: 2.043\n",
      "    step: 300; loss: 10.409; l2dist: 2.057\n",
      "    step: 350; loss: 10.234; l2dist: 2.066\n",
      "    step: 400; loss: 10.110; l2dist: 2.071\n",
      "    step: 450; loss: 10.060; l2dist: 2.071\n",
      "binary step: 0; number of successful adv: 29/100\n",
      "    step: 0; loss: 122.813; l2dist: 0.000\n",
      "    step: 50; loss: 36.798; l2dist: 4.119\n",
      "    step: 100; loss: 23.863; l2dist: 3.764\n",
      "    step: 150; loss: 15.711; l2dist: 3.414\n",
      "    step: 200; loss: 13.007; l2dist: 3.259\n",
      "    step: 250; loss: 11.981; l2dist: 3.167\n",
      "    step: 300; loss: 11.505; l2dist: 3.108\n",
      "    step: 350; loss: 11.012; l2dist: 3.038\n",
      "    step: 400; loss: 10.819; l2dist: 3.022\n",
      "    step: 450; loss: 10.604; l2dist: 3.006\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 159.332; l2dist: 0.000\n",
      "    step: 50; loss: 35.020; l2dist: 3.945\n",
      "    step: 100; loss: 22.911; l2dist: 3.798\n",
      "    step: 150; loss: 16.263; l2dist: 3.487\n",
      "    step: 200; loss: 13.289; l2dist: 3.290\n",
      "    step: 250; loss: 12.151; l2dist: 3.182\n",
      "    step: 300; loss: 11.457; l2dist: 3.098\n",
      "    step: 350; loss: 10.971; l2dist: 3.038\n",
      "    step: 400; loss: 10.689; l2dist: 3.010\n",
      "    step: 450; loss: 10.676; l2dist: 3.008\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 99.505; l2dist: 0.000\n",
      "    step: 50; loss: 28.370; l2dist: 3.532\n",
      "    step: 100; loss: 20.862; l2dist: 3.402\n",
      "    step: 150; loss: 15.156; l2dist: 3.285\n",
      "    step: 200; loss: 12.614; l2dist: 3.177\n",
      "    step: 250; loss: 11.413; l2dist: 3.091\n",
      "    step: 300; loss: 10.729; l2dist: 3.012\n",
      "    step: 350; loss: 10.341; l2dist: 2.967\n",
      "    step: 400; loss: 10.265; l2dist: 2.961\n",
      "    step: 450; loss: 10.064; l2dist: 2.941\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.899; l2dist: 0.000\n",
      "    step: 50; loss: 24.606; l2dist: 3.248\n",
      "    step: 100; loss: 19.299; l2dist: 3.152\n",
      "    step: 150; loss: 15.024; l2dist: 3.142\n",
      "    step: 200; loss: 12.582; l2dist: 3.109\n",
      "    step: 250; loss: 11.338; l2dist: 3.074\n",
      "    step: 300; loss: 10.631; l2dist: 2.986\n",
      "    step: 350; loss: 10.310; l2dist: 2.952\n",
      "    step: 400; loss: 9.984; l2dist: 2.914\n",
      "    step: 450; loss: 9.814; l2dist: 2.914\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.196; l2dist: 0.000\n",
      "    step: 50; loss: 23.016; l2dist: 3.132\n",
      "    step: 100; loss: 18.531; l2dist: 3.061\n",
      "    step: 150; loss: 14.769; l2dist: 3.097\n",
      "    step: 200; loss: 12.418; l2dist: 3.052\n",
      "    step: 250; loss: 11.136; l2dist: 3.004\n",
      "    step: 300; loss: 10.423; l2dist: 2.948\n",
      "    step: 350; loss: 9.979; l2dist: 2.908\n",
      "    step: 400; loss: 9.756; l2dist: 2.883\n",
      "    step: 450; loss: 9.559; l2dist: 2.859\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.879; l2dist: 0.000\n",
      "    step: 50; loss: 22.423; l2dist: 3.107\n",
      "    step: 100; loss: 18.103; l2dist: 3.019\n",
      "    step: 150; loss: 14.806; l2dist: 3.044\n",
      "    step: 200; loss: 12.477; l2dist: 3.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 250; loss: 11.145; l2dist: 2.996\n",
      "    step: 300; loss: 10.497; l2dist: 2.939\n",
      "    step: 350; loss: 10.124; l2dist: 2.917\n",
      "    step: 400; loss: 9.924; l2dist: 2.898\n",
      "    step: 450; loss: 9.805; l2dist: 2.879\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.077; l2dist: 0.000\n",
      "    step: 50; loss: 22.205; l2dist: 3.106\n",
      "    step: 100; loss: 18.032; l2dist: 3.011\n",
      "    step: 150; loss: 14.457; l2dist: 3.045\n",
      "    step: 200; loss: 12.451; l2dist: 3.014\n",
      "    step: 250; loss: 11.187; l2dist: 2.992\n",
      "    step: 300; loss: 10.659; l2dist: 2.964\n",
      "    step: 350; loss: 10.205; l2dist: 2.938\n",
      "    step: 400; loss: 9.957; l2dist: 2.904\n",
      "    step: 450; loss: 9.756; l2dist: 2.881\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.526; l2dist: 0.000\n",
      "    step: 50; loss: 22.114; l2dist: 3.097\n",
      "    step: 100; loss: 17.936; l2dist: 2.999\n",
      "    step: 150; loss: 14.539; l2dist: 3.026\n",
      "    step: 200; loss: 12.639; l2dist: 3.007\n",
      "    step: 250; loss: 11.359; l2dist: 3.000\n",
      "    step: 300; loss: 10.605; l2dist: 2.958\n",
      "    step: 350; loss: 10.383; l2dist: 2.935\n",
      "    step: 400; loss: 10.112; l2dist: 2.908\n",
      "    step: 450; loss: 10.004; l2dist: 2.888\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.431; l2dist: 0.000\n",
      "    step: 50; loss: 22.287; l2dist: 3.116\n",
      "    step: 100; loss: 18.009; l2dist: 3.012\n",
      "    step: 150; loss: 14.573; l2dist: 3.043\n",
      "    step: 200; loss: 12.661; l2dist: 3.024\n",
      "    step: 250; loss: 11.375; l2dist: 3.000\n",
      "    step: 300; loss: 10.558; l2dist: 2.966\n",
      "    step: 350; loss: 10.142; l2dist: 2.938\n",
      "    step: 400; loss: 9.966; l2dist: 2.908\n",
      "    step: 450; loss: 9.782; l2dist: 2.892\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.064; l2dist: 0.000\n",
      "    step: 50; loss: 12.844; l2dist: 1.852\n",
      "    step: 100; loss: 12.538; l2dist: 1.865\n",
      "    step: 150; loss: 12.107; l2dist: 1.953\n",
      "    step: 200; loss: 11.650; l2dist: 2.022\n",
      "    step: 250; loss: 11.246; l2dist: 2.072\n",
      "    step: 300; loss: 10.824; l2dist: 2.106\n",
      "    step: 350; loss: 10.487; l2dist: 2.113\n",
      "    step: 400; loss: 10.262; l2dist: 2.120\n",
      "    step: 450; loss: 10.098; l2dist: 2.120\n",
      "binary step: 0; number of successful adv: 39/100\n",
      "    step: 0; loss: 107.237; l2dist: 0.000\n",
      "    step: 50; loss: 34.159; l2dist: 3.954\n",
      "    step: 100; loss: 19.006; l2dist: 3.607\n",
      "    step: 150; loss: 13.970; l2dist: 3.305\n",
      "    step: 200; loss: 12.341; l2dist: 3.114\n",
      "    step: 250; loss: 11.458; l2dist: 3.025\n",
      "    step: 300; loss: 10.888; l2dist: 2.955\n",
      "    step: 350; loss: 10.614; l2dist: 2.918\n",
      "    step: 400; loss: 10.215; l2dist: 2.870\n",
      "    step: 450; loss: 10.030; l2dist: 2.850\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 156.471; l2dist: 0.000\n",
      "    step: 50; loss: 42.231; l2dist: 4.420\n",
      "    step: 100; loss: 22.307; l2dist: 3.823\n",
      "    step: 150; loss: 15.502; l2dist: 3.464\n",
      "    step: 200; loss: 13.191; l2dist: 3.245\n",
      "    step: 250; loss: 12.319; l2dist: 3.142\n",
      "    step: 300; loss: 11.738; l2dist: 3.072\n",
      "    step: 350; loss: 11.240; l2dist: 3.008\n",
      "    step: 400; loss: 10.890; l2dist: 2.970\n",
      "    step: 450; loss: 10.543; l2dist: 2.922\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 96.783; l2dist: 0.000\n",
      "    step: 50; loss: 32.367; l2dist: 3.754\n",
      "    step: 100; loss: 19.856; l2dist: 3.440\n",
      "    step: 150; loss: 14.445; l2dist: 3.265\n",
      "    step: 200; loss: 12.313; l2dist: 3.142\n",
      "    step: 250; loss: 11.484; l2dist: 3.029\n",
      "    step: 300; loss: 10.895; l2dist: 2.962\n",
      "    step: 350; loss: 10.450; l2dist: 2.898\n",
      "    step: 400; loss: 10.202; l2dist: 2.885\n",
      "    step: 450; loss: 10.077; l2dist: 2.865\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.674; l2dist: 0.000\n",
      "    step: 50; loss: 26.629; l2dist: 3.263\n",
      "    step: 100; loss: 18.685; l2dist: 3.184\n",
      "    step: 150; loss: 14.363; l2dist: 3.121\n",
      "    step: 200; loss: 12.267; l2dist: 3.060\n",
      "    step: 250; loss: 11.445; l2dist: 3.008\n",
      "    step: 300; loss: 10.742; l2dist: 2.921\n",
      "    step: 350; loss: 10.295; l2dist: 2.872\n",
      "    step: 400; loss: 9.905; l2dist: 2.833\n",
      "    step: 450; loss: 9.726; l2dist: 2.812\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.227; l2dist: 0.000\n",
      "    step: 50; loss: 23.295; l2dist: 3.064\n",
      "    step: 100; loss: 17.420; l2dist: 3.010\n",
      "    step: 150; loss: 14.318; l2dist: 2.992\n",
      "    step: 200; loss: 12.370; l2dist: 2.955\n",
      "    step: 250; loss: 11.146; l2dist: 2.922\n",
      "    step: 300; loss: 10.435; l2dist: 2.883\n",
      "    step: 350; loss: 10.160; l2dist: 2.862\n",
      "    step: 400; loss: 9.779; l2dist: 2.823\n",
      "    step: 450; loss: 9.612; l2dist: 2.810\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.703; l2dist: 0.000\n",
      "    step: 50; loss: 21.984; l2dist: 2.990\n",
      "    step: 100; loss: 17.418; l2dist: 2.916\n",
      "    step: 150; loss: 14.624; l2dist: 2.929\n",
      "    step: 200; loss: 12.745; l2dist: 2.899\n",
      "    step: 250; loss: 11.431; l2dist: 2.862\n",
      "    step: 300; loss: 10.829; l2dist: 2.806\n",
      "    step: 350; loss: 10.413; l2dist: 2.786\n",
      "    step: 400; loss: 10.059; l2dist: 2.762\n",
      "    step: 450; loss: 9.920; l2dist: 2.749\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.083; l2dist: 0.000\n",
      "    step: 50; loss: 21.457; l2dist: 2.960\n",
      "    step: 100; loss: 17.574; l2dist: 2.888\n",
      "    step: 150; loss: 14.955; l2dist: 2.901\n",
      "    step: 200; loss: 12.893; l2dist: 2.876\n",
      "    step: 250; loss: 11.711; l2dist: 2.884\n",
      "    step: 300; loss: 11.178; l2dist: 2.853\n",
      "    step: 350; loss: 10.631; l2dist: 2.801\n",
      "    step: 400; loss: 10.268; l2dist: 2.772\n",
      "    step: 450; loss: 10.054; l2dist: 2.754\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.894; l2dist: 0.000\n",
      "    step: 50; loss: 21.248; l2dist: 2.965\n",
      "    step: 100; loss: 17.450; l2dist: 2.900\n",
      "    step: 150; loss: 14.565; l2dist: 2.930\n",
      "    step: 200; loss: 12.470; l2dist: 2.921\n",
      "    step: 250; loss: 11.353; l2dist: 2.923\n",
      "    step: 300; loss: 10.658; l2dist: 2.890\n",
      "    step: 350; loss: 10.171; l2dist: 2.841\n",
      "    step: 400; loss: 9.945; l2dist: 2.820\n",
      "    step: 450; loss: 9.698; l2dist: 2.800\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.903; l2dist: 0.000\n",
      "    step: 50; loss: 21.361; l2dist: 2.986\n",
      "    step: 100; loss: 17.501; l2dist: 2.916\n",
      "    step: 150; loss: 14.595; l2dist: 2.935\n",
      "    step: 200; loss: 12.521; l2dist: 2.916\n",
      "    step: 250; loss: 11.631; l2dist: 2.891\n",
      "    step: 300; loss: 10.997; l2dist: 2.855\n",
      "    step: 350; loss: 10.587; l2dist: 2.821\n",
      "    step: 400; loss: 10.278; l2dist: 2.787\n",
      "    step: 450; loss: 9.990; l2dist: 2.769\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.101; l2dist: 0.000\n",
      "    step: 50; loss: 12.380; l2dist: 1.668\n",
      "    step: 100; loss: 12.071; l2dist: 1.684\n",
      "    step: 150; loss: 11.634; l2dist: 1.783\n",
      "    step: 200; loss: 11.172; l2dist: 1.828\n",
      "    step: 250; loss: 10.905; l2dist: 1.841\n",
      "    step: 300; loss: 10.672; l2dist: 1.854\n",
      "    step: 350; loss: 10.535; l2dist: 1.860\n",
      "    step: 400; loss: 10.440; l2dist: 1.862\n",
      "    step: 450; loss: 10.357; l2dist: 1.868\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 114.164; l2dist: 0.000\n",
      "    step: 50; loss: 40.670; l2dist: 3.858\n",
      "    step: 100; loss: 25.147; l2dist: 3.711\n",
      "    step: 150; loss: 15.530; l2dist: 3.381\n",
      "    step: 200; loss: 12.516; l2dist: 3.174\n",
      "    step: 250; loss: 11.475; l2dist: 3.055\n",
      "    step: 300; loss: 10.809; l2dist: 2.972\n",
      "    step: 350; loss: 10.372; l2dist: 2.910\n",
      "    step: 400; loss: 9.979; l2dist: 2.859\n",
      "    step: 450; loss: 9.801; l2dist: 2.836\n",
      "binary step: 1; number of successful adv: 94/100\n",
      "    step: 0; loss: 144.010; l2dist: 0.000\n",
      "    step: 50; loss: 44.785; l2dist: 4.073\n",
      "    step: 100; loss: 23.261; l2dist: 3.836\n",
      "    step: 150; loss: 15.838; l2dist: 3.500\n",
      "    step: 200; loss: 13.119; l2dist: 3.264\n",
      "    step: 250; loss: 11.795; l2dist: 3.118\n",
      "    step: 300; loss: 11.051; l2dist: 3.024\n",
      "    step: 350; loss: 10.639; l2dist: 2.964\n",
      "    step: 400; loss: 10.215; l2dist: 2.912\n",
      "    step: 450; loss: 10.057; l2dist: 2.886\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.328; l2dist: 0.000\n",
      "    step: 50; loss: 33.953; l2dist: 3.574\n",
      "    step: 100; loss: 21.880; l2dist: 3.360\n",
      "    step: 150; loss: 14.923; l2dist: 3.261\n",
      "    step: 200; loss: 12.253; l2dist: 3.107\n",
      "    step: 250; loss: 10.843; l2dist: 2.961\n",
      "    step: 300; loss: 10.271; l2dist: 2.886\n",
      "    step: 350; loss: 9.891; l2dist: 2.833\n",
      "    step: 400; loss: 9.522; l2dist: 2.785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 450; loss: 9.576; l2dist: 2.785\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.873; l2dist: 0.000\n",
      "    step: 50; loss: 27.838; l2dist: 3.158\n",
      "    step: 100; loss: 19.950; l2dist: 3.017\n",
      "    step: 150; loss: 15.228; l2dist: 3.061\n",
      "    step: 200; loss: 12.137; l2dist: 3.036\n",
      "    step: 250; loss: 10.727; l2dist: 2.914\n",
      "    step: 300; loss: 9.959; l2dist: 2.834\n",
      "    step: 350; loss: 9.681; l2dist: 2.777\n",
      "    step: 400; loss: 9.473; l2dist: 2.759\n",
      "    step: 450; loss: 9.137; l2dist: 2.726\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.895; l2dist: 0.000\n",
      "    step: 50; loss: 24.517; l2dist: 2.872\n",
      "    step: 100; loss: 18.718; l2dist: 2.811\n",
      "    step: 150; loss: 15.606; l2dist: 2.816\n",
      "    step: 200; loss: 13.275; l2dist: 2.856\n",
      "    step: 250; loss: 11.138; l2dist: 2.862\n",
      "    step: 300; loss: 10.237; l2dist: 2.792\n",
      "    step: 350; loss: 9.808; l2dist: 2.753\n",
      "    step: 400; loss: 9.468; l2dist: 2.716\n",
      "    step: 450; loss: 9.275; l2dist: 2.691\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.678; l2dist: 0.000\n",
      "    step: 50; loss: 23.049; l2dist: 2.774\n",
      "    step: 100; loss: 18.251; l2dist: 2.737\n",
      "    step: 150; loss: 15.346; l2dist: 2.757\n",
      "    step: 200; loss: 13.199; l2dist: 2.800\n",
      "    step: 250; loss: 11.188; l2dist: 2.821\n",
      "    step: 300; loss: 10.250; l2dist: 2.777\n",
      "    step: 350; loss: 9.721; l2dist: 2.735\n",
      "    step: 400; loss: 9.422; l2dist: 2.705\n",
      "    step: 450; loss: 9.375; l2dist: 2.704\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.503; l2dist: 0.000\n",
      "    step: 50; loss: 22.365; l2dist: 2.726\n",
      "    step: 100; loss: 18.114; l2dist: 2.727\n",
      "    step: 150; loss: 15.203; l2dist: 2.765\n",
      "    step: 200; loss: 13.479; l2dist: 2.770\n",
      "    step: 250; loss: 11.765; l2dist: 2.775\n",
      "    step: 300; loss: 10.565; l2dist: 2.774\n",
      "    step: 350; loss: 9.813; l2dist: 2.758\n",
      "    step: 400; loss: 9.490; l2dist: 2.736\n",
      "    step: 450; loss: 9.266; l2dist: 2.710\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.109; l2dist: 0.000\n",
      "    step: 50; loss: 21.912; l2dist: 2.698\n",
      "    step: 100; loss: 18.080; l2dist: 2.693\n",
      "    step: 150; loss: 15.228; l2dist: 2.726\n",
      "    step: 200; loss: 13.475; l2dist: 2.728\n",
      "    step: 250; loss: 11.684; l2dist: 2.764\n",
      "    step: 300; loss: 10.204; l2dist: 2.784\n",
      "    step: 350; loss: 9.637; l2dist: 2.736\n",
      "    step: 400; loss: 9.361; l2dist: 2.715\n",
      "    step: 450; loss: 9.069; l2dist: 2.692\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.596; l2dist: 0.000\n",
      "    step: 50; loss: 22.039; l2dist: 2.702\n",
      "    step: 100; loss: 18.282; l2dist: 2.694\n",
      "    step: 150; loss: 15.283; l2dist: 2.749\n",
      "    step: 200; loss: 13.357; l2dist: 2.764\n",
      "    step: 250; loss: 11.677; l2dist: 2.783\n",
      "    step: 300; loss: 10.523; l2dist: 2.771\n",
      "    step: 350; loss: 10.038; l2dist: 2.734\n",
      "    step: 400; loss: 9.727; l2dist: 2.702\n",
      "    step: 450; loss: 9.459; l2dist: 2.672\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.528; l2dist: 0.000\n",
      "    step: 50; loss: 10.364; l2dist: 1.628\n",
      "    step: 100; loss: 10.026; l2dist: 1.641\n",
      "    step: 150; loss: 9.688; l2dist: 1.704\n",
      "    step: 200; loss: 9.448; l2dist: 1.761\n",
      "    step: 250; loss: 9.100; l2dist: 1.801\n",
      "    step: 300; loss: 8.891; l2dist: 1.820\n",
      "    step: 350; loss: 8.738; l2dist: 1.832\n",
      "    step: 400; loss: 8.620; l2dist: 1.827\n",
      "    step: 450; loss: 8.541; l2dist: 1.840\n",
      "binary step: 0; number of successful adv: 33/100\n",
      "    step: 0; loss: 98.462; l2dist: 0.000\n",
      "    step: 50; loss: 32.565; l2dist: 3.622\n",
      "    step: 100; loss: 20.636; l2dist: 3.390\n",
      "    step: 150; loss: 13.096; l2dist: 3.082\n",
      "    step: 200; loss: 11.058; l2dist: 2.916\n",
      "    step: 250; loss: 9.963; l2dist: 2.793\n",
      "    step: 300; loss: 9.322; l2dist: 2.720\n",
      "    step: 350; loss: 9.101; l2dist: 2.689\n",
      "    step: 400; loss: 8.930; l2dist: 2.672\n",
      "    step: 450; loss: 8.754; l2dist: 2.639\n",
      "binary step: 1; number of successful adv: 88/100\n",
      "    step: 0; loss: 156.787; l2dist: 0.000\n",
      "    step: 50; loss: 33.560; l2dist: 3.733\n",
      "    step: 100; loss: 20.989; l2dist: 3.497\n",
      "    step: 150; loss: 14.782; l2dist: 3.198\n",
      "    step: 200; loss: 11.683; l2dist: 2.962\n",
      "    step: 250; loss: 10.402; l2dist: 2.825\n",
      "    step: 300; loss: 9.944; l2dist: 2.778\n",
      "    step: 350; loss: 9.629; l2dist: 2.745\n",
      "    step: 400; loss: 9.182; l2dist: 2.684\n",
      "    step: 450; loss: 8.953; l2dist: 2.653\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 95.545; l2dist: 0.000\n",
      "    step: 50; loss: 25.300; l2dist: 3.266\n",
      "    step: 100; loss: 17.405; l2dist: 3.095\n",
      "    step: 150; loss: 13.039; l2dist: 2.919\n",
      "    step: 200; loss: 10.728; l2dist: 2.784\n",
      "    step: 250; loss: 9.725; l2dist: 2.681\n",
      "    step: 300; loss: 9.209; l2dist: 2.631\n",
      "    step: 350; loss: 8.991; l2dist: 2.604\n",
      "    step: 400; loss: 8.745; l2dist: 2.589\n",
      "    step: 450; loss: 8.736; l2dist: 2.601\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.126; l2dist: 0.000\n",
      "    step: 50; loss: 21.128; l2dist: 2.999\n",
      "    step: 100; loss: 15.585; l2dist: 2.866\n",
      "    step: 150; loss: 12.087; l2dist: 2.759\n",
      "    step: 200; loss: 10.370; l2dist: 2.685\n",
      "    step: 250; loss: 9.566; l2dist: 2.622\n",
      "    step: 300; loss: 9.025; l2dist: 2.592\n",
      "    step: 350; loss: 8.924; l2dist: 2.592\n",
      "    step: 400; loss: 8.728; l2dist: 2.582\n",
      "    step: 450; loss: 8.553; l2dist: 2.570\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.957; l2dist: 0.000\n",
      "    step: 50; loss: 19.174; l2dist: 2.852\n",
      "    step: 100; loss: 15.085; l2dist: 2.756\n",
      "    step: 150; loss: 11.991; l2dist: 2.712\n",
      "    step: 200; loss: 10.338; l2dist: 2.646\n",
      "    step: 250; loss: 9.340; l2dist: 2.587\n",
      "    step: 300; loss: 8.857; l2dist: 2.555\n",
      "    step: 350; loss: 8.607; l2dist: 2.560\n",
      "    step: 400; loss: 8.325; l2dist: 2.536\n",
      "    step: 450; loss: 8.228; l2dist: 2.528\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.126; l2dist: 0.000\n",
      "    step: 50; loss: 18.678; l2dist: 2.833\n",
      "    step: 100; loss: 14.962; l2dist: 2.731\n",
      "    step: 150; loss: 11.802; l2dist: 2.703\n",
      "    step: 200; loss: 10.252; l2dist: 2.638\n",
      "    step: 250; loss: 9.426; l2dist: 2.589\n",
      "    step: 300; loss: 9.060; l2dist: 2.567\n",
      "    step: 350; loss: 8.691; l2dist: 2.534\n",
      "    step: 400; loss: 8.493; l2dist: 2.524\n",
      "    step: 450; loss: 8.448; l2dist: 2.517\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.976; l2dist: 0.000\n",
      "    step: 50; loss: 18.403; l2dist: 2.809\n",
      "    step: 100; loss: 14.714; l2dist: 2.720\n",
      "    step: 150; loss: 11.832; l2dist: 2.692\n",
      "    step: 200; loss: 10.121; l2dist: 2.633\n",
      "    step: 250; loss: 9.291; l2dist: 2.572\n",
      "    step: 300; loss: 8.930; l2dist: 2.556\n",
      "    step: 350; loss: 8.616; l2dist: 2.537\n",
      "    step: 400; loss: 8.565; l2dist: 2.539\n",
      "    step: 450; loss: 8.225; l2dist: 2.521\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.299; l2dist: 0.000\n",
      "    step: 50; loss: 18.421; l2dist: 2.784\n",
      "    step: 100; loss: 14.959; l2dist: 2.700\n",
      "    step: 150; loss: 12.252; l2dist: 2.668\n",
      "    step: 200; loss: 10.586; l2dist: 2.616\n",
      "    step: 250; loss: 9.435; l2dist: 2.588\n",
      "    step: 300; loss: 8.890; l2dist: 2.569\n",
      "    step: 350; loss: 8.552; l2dist: 2.545\n",
      "    step: 400; loss: 8.371; l2dist: 2.534\n",
      "    step: 450; loss: 8.234; l2dist: 2.521\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.063; l2dist: 0.000\n",
      "    step: 50; loss: 18.567; l2dist: 2.787\n",
      "    step: 100; loss: 15.015; l2dist: 2.708\n",
      "    step: 150; loss: 12.301; l2dist: 2.678\n",
      "    step: 200; loss: 10.373; l2dist: 2.646\n",
      "    step: 250; loss: 9.458; l2dist: 2.595\n",
      "    step: 300; loss: 8.876; l2dist: 2.589\n",
      "    step: 350; loss: 8.572; l2dist: 2.571\n",
      "    step: 400; loss: 8.343; l2dist: 2.544\n",
      "    step: 450; loss: 8.212; l2dist: 2.531\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.882; l2dist: 0.000\n",
      "    step: 50; loss: 13.813; l2dist: 1.702\n",
      "    step: 100; loss: 13.578; l2dist: 1.718\n",
      "    step: 150; loss: 13.154; l2dist: 1.818\n",
      "    step: 200; loss: 12.634; l2dist: 1.906\n",
      "    step: 250; loss: 12.207; l2dist: 1.943\n",
      "    step: 300; loss: 11.885; l2dist: 1.951\n",
      "    step: 350; loss: 11.740; l2dist: 1.946\n",
      "    step: 400; loss: 11.665; l2dist: 1.948\n",
      "    step: 450; loss: 11.588; l2dist: 1.952\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 125.477; l2dist: 0.000\n",
      "    step: 50; loss: 46.732; l2dist: 4.197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 29.076; l2dist: 4.004\n",
      "    step: 150; loss: 17.263; l2dist: 3.688\n",
      "    step: 200; loss: 14.813; l2dist: 3.502\n",
      "    step: 250; loss: 13.432; l2dist: 3.347\n",
      "    step: 300; loss: 12.633; l2dist: 3.250\n",
      "    step: 350; loss: 12.004; l2dist: 3.181\n",
      "    step: 400; loss: 11.725; l2dist: 3.144\n",
      "    step: 450; loss: 11.421; l2dist: 3.106\n",
      "binary step: 1; number of successful adv: 93/100\n",
      "    step: 0; loss: 149.324; l2dist: 0.000\n",
      "    step: 50; loss: 42.510; l2dist: 4.253\n",
      "    step: 100; loss: 28.371; l2dist: 4.043\n",
      "    step: 150; loss: 19.006; l2dist: 3.671\n",
      "    step: 200; loss: 15.092; l2dist: 3.435\n",
      "    step: 250; loss: 13.099; l2dist: 3.304\n",
      "    step: 300; loss: 12.386; l2dist: 3.231\n",
      "    step: 350; loss: 11.687; l2dist: 3.145\n",
      "    step: 400; loss: 11.307; l2dist: 3.108\n",
      "    step: 450; loss: 11.253; l2dist: 3.100\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 94.277; l2dist: 0.000\n",
      "    step: 50; loss: 33.611; l2dist: 3.723\n",
      "    step: 100; loss: 25.853; l2dist: 3.589\n",
      "    step: 150; loss: 17.824; l2dist: 3.496\n",
      "    step: 200; loss: 14.763; l2dist: 3.285\n",
      "    step: 250; loss: 12.491; l2dist: 3.185\n",
      "    step: 300; loss: 11.896; l2dist: 3.127\n",
      "    step: 350; loss: 11.042; l2dist: 3.045\n",
      "    step: 400; loss: 10.989; l2dist: 3.036\n",
      "    step: 450; loss: 10.602; l2dist: 2.995\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.226; l2dist: 0.000\n",
      "    step: 50; loss: 27.872; l2dist: 3.312\n",
      "    step: 100; loss: 23.264; l2dist: 3.222\n",
      "    step: 150; loss: 18.085; l2dist: 3.271\n",
      "    step: 200; loss: 14.379; l2dist: 3.211\n",
      "    step: 250; loss: 12.752; l2dist: 3.133\n",
      "    step: 300; loss: 11.985; l2dist: 3.054\n",
      "    step: 350; loss: 11.386; l2dist: 2.981\n",
      "    step: 400; loss: 11.147; l2dist: 2.953\n",
      "    step: 450; loss: 11.049; l2dist: 2.930\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.288; l2dist: 0.000\n",
      "    step: 50; loss: 26.077; l2dist: 3.146\n",
      "    step: 100; loss: 22.109; l2dist: 3.065\n",
      "    step: 150; loss: 18.177; l2dist: 3.114\n",
      "    step: 200; loss: 15.153; l2dist: 3.124\n",
      "    step: 250; loss: 12.849; l2dist: 3.080\n",
      "    step: 300; loss: 11.652; l2dist: 3.062\n",
      "    step: 350; loss: 11.006; l2dist: 2.998\n",
      "    step: 400; loss: 10.557; l2dist: 2.966\n",
      "    step: 450; loss: 10.279; l2dist: 2.937\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 57.589; l2dist: 0.000\n",
      "    step: 50; loss: 25.002; l2dist: 3.062\n",
      "    step: 100; loss: 21.248; l2dist: 3.020\n",
      "    step: 150; loss: 17.706; l2dist: 3.067\n",
      "    step: 200; loss: 15.198; l2dist: 3.065\n",
      "    step: 250; loss: 13.014; l2dist: 3.042\n",
      "    step: 300; loss: 12.168; l2dist: 3.024\n",
      "    step: 350; loss: 11.445; l2dist: 2.960\n",
      "    step: 400; loss: 11.195; l2dist: 2.946\n",
      "    step: 450; loss: 10.861; l2dist: 2.901\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.312; l2dist: 0.000\n",
      "    step: 50; loss: 24.449; l2dist: 3.032\n",
      "    step: 100; loss: 21.079; l2dist: 2.981\n",
      "    step: 150; loss: 17.773; l2dist: 3.036\n",
      "    step: 200; loss: 15.237; l2dist: 3.048\n",
      "    step: 250; loss: 12.670; l2dist: 3.062\n",
      "    step: 300; loss: 11.712; l2dist: 3.043\n",
      "    step: 350; loss: 11.026; l2dist: 2.988\n",
      "    step: 400; loss: 10.582; l2dist: 2.950\n",
      "    step: 450; loss: 10.594; l2dist: 2.954\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.470; l2dist: 0.000\n",
      "    step: 50; loss: 24.329; l2dist: 3.008\n",
      "    step: 100; loss: 21.135; l2dist: 2.973\n",
      "    step: 150; loss: 17.890; l2dist: 3.017\n",
      "    step: 200; loss: 15.417; l2dist: 3.046\n",
      "    step: 250; loss: 12.970; l2dist: 3.058\n",
      "    step: 300; loss: 11.618; l2dist: 3.040\n",
      "    step: 350; loss: 11.129; l2dist: 3.018\n",
      "    step: 400; loss: 10.660; l2dist: 2.960\n",
      "    step: 450; loss: 10.433; l2dist: 2.946\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.329; l2dist: 0.000\n",
      "    step: 50; loss: 24.545; l2dist: 3.015\n",
      "    step: 100; loss: 21.377; l2dist: 2.958\n",
      "    step: 150; loss: 17.992; l2dist: 3.044\n",
      "    step: 200; loss: 15.436; l2dist: 3.069\n",
      "    step: 250; loss: 13.060; l2dist: 3.055\n",
      "    step: 300; loss: 12.217; l2dist: 3.015\n",
      "    step: 350; loss: 11.537; l2dist: 2.970\n",
      "    step: 400; loss: 10.845; l2dist: 2.981\n",
      "    step: 450; loss: 10.434; l2dist: 2.949\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.569; l2dist: 0.000\n",
      "    step: 50; loss: 13.750; l2dist: 1.862\n",
      "    step: 100; loss: 13.357; l2dist: 1.890\n",
      "    step: 150; loss: 12.894; l2dist: 1.956\n",
      "    step: 200; loss: 12.491; l2dist: 2.011\n",
      "    step: 250; loss: 12.187; l2dist: 2.049\n",
      "    step: 300; loss: 11.931; l2dist: 2.072\n",
      "    step: 350; loss: 11.824; l2dist: 2.072\n",
      "    step: 400; loss: 11.755; l2dist: 2.071\n",
      "    step: 450; loss: 11.697; l2dist: 2.079\n",
      "binary step: 0; number of successful adv: 22/100\n",
      "    step: 0; loss: 153.801; l2dist: 0.000\n",
      "    step: 50; loss: 43.818; l2dist: 4.523\n",
      "    step: 100; loss: 23.773; l2dist: 3.994\n",
      "    step: 150; loss: 16.417; l2dist: 3.611\n",
      "    step: 200; loss: 14.381; l2dist: 3.404\n",
      "    step: 250; loss: 13.351; l2dist: 3.281\n",
      "    step: 300; loss: 12.373; l2dist: 3.191\n",
      "    step: 350; loss: 12.019; l2dist: 3.158\n",
      "    step: 400; loss: 11.532; l2dist: 3.093\n",
      "    step: 450; loss: 11.409; l2dist: 3.072\n",
      "binary step: 1; number of successful adv: 89/100\n",
      "    step: 0; loss: 211.953; l2dist: 0.000\n",
      "    step: 50; loss: 42.521; l2dist: 4.561\n",
      "    step: 100; loss: 23.588; l2dist: 4.170\n",
      "    step: 150; loss: 17.199; l2dist: 3.733\n",
      "    step: 200; loss: 14.395; l2dist: 3.447\n",
      "    step: 250; loss: 13.022; l2dist: 3.288\n",
      "    step: 300; loss: 12.477; l2dist: 3.216\n",
      "    step: 350; loss: 12.076; l2dist: 3.162\n",
      "    step: 400; loss: 11.884; l2dist: 3.129\n",
      "    step: 450; loss: 11.751; l2dist: 3.130\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 128.838; l2dist: 0.000\n",
      "    step: 50; loss: 33.435; l2dist: 4.020\n",
      "    step: 100; loss: 21.742; l2dist: 3.778\n",
      "    step: 150; loss: 15.399; l2dist: 3.509\n",
      "    step: 200; loss: 13.378; l2dist: 3.303\n",
      "    step: 250; loss: 12.526; l2dist: 3.197\n",
      "    step: 300; loss: 11.840; l2dist: 3.114\n",
      "    step: 350; loss: 11.710; l2dist: 3.091\n",
      "    step: 400; loss: 11.142; l2dist: 3.029\n",
      "    step: 450; loss: 11.162; l2dist: 3.030\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.002; l2dist: 0.000\n",
      "    step: 50; loss: 28.235; l2dist: 3.618\n",
      "    step: 100; loss: 20.137; l2dist: 3.454\n",
      "    step: 150; loss: 14.763; l2dist: 3.335\n",
      "    step: 200; loss: 12.696; l2dist: 3.174\n",
      "    step: 250; loss: 11.894; l2dist: 3.095\n",
      "    step: 300; loss: 11.253; l2dist: 3.016\n",
      "    step: 350; loss: 10.821; l2dist: 2.978\n",
      "    step: 400; loss: 10.860; l2dist: 2.985\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.559; l2dist: 0.000\n",
      "    step: 50; loss: 26.041; l2dist: 3.409\n",
      "    step: 100; loss: 19.969; l2dist: 3.247\n",
      "    step: 150; loss: 15.192; l2dist: 3.243\n",
      "    step: 200; loss: 12.763; l2dist: 3.118\n",
      "    step: 250; loss: 11.849; l2dist: 3.046\n",
      "    step: 300; loss: 11.284; l2dist: 2.995\n",
      "    step: 350; loss: 11.169; l2dist: 2.979\n",
      "    step: 400; loss: 10.983; l2dist: 2.970\n",
      "    step: 450; loss: 10.803; l2dist: 2.937\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.742; l2dist: 0.000\n",
      "    step: 50; loss: 25.311; l2dist: 3.279\n",
      "    step: 100; loss: 20.115; l2dist: 3.162\n",
      "    step: 150; loss: 15.734; l2dist: 3.163\n",
      "    step: 200; loss: 13.042; l2dist: 3.118\n",
      "    step: 250; loss: 11.767; l2dist: 3.031\n",
      "    step: 300; loss: 11.171; l2dist: 2.981\n",
      "    step: 350; loss: 10.866; l2dist: 2.959\n",
      "    step: 400; loss: 10.691; l2dist: 2.951\n",
      "    step: 450; loss: 10.587; l2dist: 2.929\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.098; l2dist: 0.000\n",
      "    step: 50; loss: 24.775; l2dist: 3.244\n",
      "    step: 100; loss: 19.675; l2dist: 3.130\n",
      "    step: 150; loss: 15.659; l2dist: 3.150\n",
      "    step: 200; loss: 13.174; l2dist: 3.096\n",
      "    step: 250; loss: 11.977; l2dist: 3.058\n",
      "    step: 300; loss: 11.252; l2dist: 2.996\n",
      "    step: 350; loss: 10.864; l2dist: 2.963\n",
      "    step: 400; loss: 10.554; l2dist: 2.927\n",
      "    step: 450; loss: 10.424; l2dist: 2.922\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.019; l2dist: 0.000\n",
      "    step: 50; loss: 24.485; l2dist: 3.235\n",
      "    step: 100; loss: 19.604; l2dist: 3.131\n",
      "    step: 150; loss: 15.737; l2dist: 3.141\n",
      "    step: 200; loss: 13.237; l2dist: 3.108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 250; loss: 11.751; l2dist: 3.048\n",
      "    step: 300; loss: 11.255; l2dist: 2.996\n",
      "    step: 350; loss: 10.892; l2dist: 2.967\n",
      "    step: 400; loss: 10.601; l2dist: 2.937\n",
      "    step: 450; loss: 10.445; l2dist: 2.909\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.365; l2dist: 0.000\n",
      "    step: 50; loss: 24.821; l2dist: 3.252\n",
      "    step: 100; loss: 19.742; l2dist: 3.149\n",
      "    step: 150; loss: 15.747; l2dist: 3.155\n",
      "    step: 200; loss: 13.120; l2dist: 3.106\n",
      "    step: 250; loss: 11.856; l2dist: 3.045\n",
      "    step: 300; loss: 11.300; l2dist: 2.989\n",
      "    step: 350; loss: 10.811; l2dist: 2.951\n",
      "    step: 400; loss: 10.623; l2dist: 2.929\n",
      "    step: 450; loss: 10.528; l2dist: 2.919\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.772; l2dist: 0.000\n",
      "    step: 50; loss: 15.172; l2dist: 2.005\n",
      "    step: 100; loss: 14.454; l2dist: 2.033\n",
      "    step: 150; loss: 13.603; l2dist: 2.135\n",
      "    step: 200; loss: 12.946; l2dist: 2.184\n",
      "    step: 250; loss: 12.583; l2dist: 2.231\n",
      "    step: 300; loss: 12.304; l2dist: 2.264\n",
      "    step: 350; loss: 12.092; l2dist: 2.280\n",
      "    step: 400; loss: 11.943; l2dist: 2.299\n",
      "    step: 450; loss: 11.795; l2dist: 2.295\n",
      "binary step: 0; number of successful adv: 34/100\n",
      "    step: 0; loss: 137.637; l2dist: 0.000\n",
      "    step: 50; loss: 40.419; l2dist: 4.579\n",
      "    step: 100; loss: 23.069; l2dist: 4.154\n",
      "    step: 150; loss: 17.176; l2dist: 3.747\n",
      "    step: 200; loss: 14.968; l2dist: 3.520\n",
      "    step: 250; loss: 13.918; l2dist: 3.419\n",
      "    step: 300; loss: 13.119; l2dist: 3.338\n",
      "    step: 350; loss: 12.679; l2dist: 3.286\n",
      "    step: 400; loss: 12.377; l2dist: 3.246\n",
      "    step: 450; loss: 12.227; l2dist: 3.222\n",
      "binary step: 1; number of successful adv: 96/100\n",
      "    step: 0; loss: 142.167; l2dist: 0.000\n",
      "    step: 50; loss: 35.882; l2dist: 4.354\n",
      "    step: 100; loss: 22.640; l2dist: 3.979\n",
      "    step: 150; loss: 16.796; l2dist: 3.655\n",
      "    step: 200; loss: 14.588; l2dist: 3.464\n",
      "    step: 250; loss: 13.443; l2dist: 3.354\n",
      "    step: 300; loss: 12.803; l2dist: 3.280\n",
      "    step: 350; loss: 12.435; l2dist: 3.241\n",
      "    step: 400; loss: 12.144; l2dist: 3.204\n",
      "    step: 450; loss: 12.002; l2dist: 3.186\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 93.801; l2dist: 0.000\n",
      "    step: 50; loss: 29.838; l2dist: 3.833\n",
      "    step: 100; loss: 21.181; l2dist: 3.629\n",
      "    step: 150; loss: 16.249; l2dist: 3.476\n",
      "    step: 200; loss: 14.137; l2dist: 3.360\n",
      "    step: 250; loss: 13.086; l2dist: 3.291\n",
      "    step: 300; loss: 12.517; l2dist: 3.233\n",
      "    step: 350; loss: 12.071; l2dist: 3.186\n",
      "    step: 400; loss: 11.905; l2dist: 3.176\n",
      "    step: 450; loss: 11.760; l2dist: 3.154\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.780; l2dist: 0.000\n",
      "    step: 50; loss: 25.999; l2dist: 3.402\n",
      "    step: 100; loss: 20.728; l2dist: 3.303\n",
      "    step: 150; loss: 16.324; l2dist: 3.320\n",
      "    step: 200; loss: 14.054; l2dist: 3.258\n",
      "    step: 250; loss: 12.872; l2dist: 3.209\n",
      "    step: 300; loss: 12.263; l2dist: 3.172\n",
      "    step: 350; loss: 11.910; l2dist: 3.129\n",
      "    step: 400; loss: 11.668; l2dist: 3.120\n",
      "    step: 450; loss: 11.593; l2dist: 3.117\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.541; l2dist: 0.000\n",
      "    step: 50; loss: 24.198; l2dist: 3.210\n",
      "    step: 100; loss: 20.099; l2dist: 3.143\n",
      "    step: 150; loss: 16.759; l2dist: 3.198\n",
      "    step: 200; loss: 14.612; l2dist: 3.185\n",
      "    step: 250; loss: 13.312; l2dist: 3.151\n",
      "    step: 300; loss: 12.522; l2dist: 3.171\n",
      "    step: 350; loss: 12.039; l2dist: 3.135\n",
      "    step: 400; loss: 11.868; l2dist: 3.119\n",
      "    step: 450; loss: 11.684; l2dist: 3.104\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.618; l2dist: 0.000\n",
      "    step: 50; loss: 23.368; l2dist: 3.131\n",
      "    step: 100; loss: 19.867; l2dist: 3.054\n",
      "    step: 150; loss: 16.829; l2dist: 3.090\n",
      "    step: 200; loss: 14.807; l2dist: 3.117\n",
      "    step: 250; loss: 13.536; l2dist: 3.114\n",
      "    step: 300; loss: 12.822; l2dist: 3.078\n",
      "    step: 350; loss: 12.540; l2dist: 3.076\n",
      "    step: 400; loss: 12.279; l2dist: 3.048\n",
      "    step: 450; loss: 12.173; l2dist: 3.054\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.493; l2dist: 0.000\n",
      "    step: 50; loss: 23.310; l2dist: 3.128\n",
      "    step: 100; loss: 19.953; l2dist: 3.072\n",
      "    step: 150; loss: 16.881; l2dist: 3.118\n",
      "    step: 200; loss: 14.904; l2dist: 3.127\n",
      "    step: 250; loss: 13.578; l2dist: 3.130\n",
      "    step: 300; loss: 12.862; l2dist: 3.108\n",
      "    step: 350; loss: 12.471; l2dist: 3.099\n",
      "    step: 400; loss: 12.240; l2dist: 3.078\n",
      "    step: 450; loss: 12.082; l2dist: 3.072\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.397; l2dist: 0.000\n",
      "    step: 50; loss: 23.155; l2dist: 3.118\n",
      "    step: 100; loss: 19.762; l2dist: 3.081\n",
      "    step: 150; loss: 16.790; l2dist: 3.109\n",
      "    step: 200; loss: 14.872; l2dist: 3.119\n",
      "    step: 250; loss: 13.654; l2dist: 3.138\n",
      "    step: 300; loss: 12.588; l2dist: 3.143\n",
      "    step: 350; loss: 12.255; l2dist: 3.131\n",
      "    step: 400; loss: 11.936; l2dist: 3.096\n",
      "    step: 450; loss: 11.625; l2dist: 3.078\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.798; l2dist: 0.000\n",
      "    step: 50; loss: 23.243; l2dist: 3.134\n",
      "    step: 100; loss: 19.867; l2dist: 3.088\n",
      "    step: 150; loss: 16.838; l2dist: 3.110\n",
      "    step: 200; loss: 14.942; l2dist: 3.120\n",
      "    step: 250; loss: 13.819; l2dist: 3.127\n",
      "    step: 300; loss: 12.853; l2dist: 3.120\n",
      "    step: 350; loss: 12.498; l2dist: 3.112\n",
      "    step: 400; loss: 12.250; l2dist: 3.088\n",
      "    step: 450; loss: 12.066; l2dist: 3.074\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.188; l2dist: 0.000\n",
      "    step: 50; loss: 12.561; l2dist: 1.880\n",
      "    step: 100; loss: 12.226; l2dist: 1.906\n",
      "    step: 150; loss: 11.661; l2dist: 1.984\n",
      "    step: 200; loss: 11.283; l2dist: 2.040\n",
      "    step: 250; loss: 10.994; l2dist: 2.069\n",
      "    step: 300; loss: 10.755; l2dist: 2.088\n",
      "    step: 350; loss: 10.567; l2dist: 2.098\n",
      "    step: 400; loss: 10.463; l2dist: 2.094\n",
      "    step: 450; loss: 10.410; l2dist: 2.100\n",
      "binary step: 0; number of successful adv: 26/100\n",
      "    step: 0; loss: 128.996; l2dist: 0.000\n",
      "    step: 50; loss: 39.269; l2dist: 4.219\n",
      "    step: 100; loss: 20.077; l2dist: 3.865\n",
      "    step: 150; loss: 14.403; l2dist: 3.449\n",
      "    step: 200; loss: 12.560; l2dist: 3.246\n",
      "    step: 250; loss: 11.579; l2dist: 3.134\n",
      "    step: 300; loss: 10.985; l2dist: 3.069\n",
      "    step: 350; loss: 10.581; l2dist: 3.024\n",
      "    step: 400; loss: 10.287; l2dist: 2.986\n",
      "    step: 450; loss: 10.154; l2dist: 2.972\n",
      "binary step: 1; number of successful adv: 92/100\n",
      "    step: 0; loss: 172.873; l2dist: 0.000\n",
      "    step: 50; loss: 37.254; l2dist: 4.156\n",
      "    step: 100; loss: 21.247; l2dist: 3.909\n",
      "    step: 150; loss: 14.846; l2dist: 3.514\n",
      "    step: 200; loss: 12.756; l2dist: 3.296\n",
      "    step: 250; loss: 11.676; l2dist: 3.178\n",
      "    step: 300; loss: 11.131; l2dist: 3.110\n",
      "    step: 350; loss: 10.682; l2dist: 3.050\n",
      "    step: 400; loss: 10.568; l2dist: 3.036\n",
      "    step: 450; loss: 10.379; l2dist: 3.014\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.271; l2dist: 0.000\n",
      "    step: 50; loss: 29.790; l2dist: 3.768\n",
      "    step: 100; loss: 19.828; l2dist: 3.598\n",
      "    step: 150; loss: 13.818; l2dist: 3.382\n",
      "    step: 200; loss: 11.981; l2dist: 3.204\n",
      "    step: 250; loss: 11.130; l2dist: 3.111\n",
      "    step: 300; loss: 10.624; l2dist: 3.048\n",
      "    step: 350; loss: 10.323; l2dist: 3.013\n",
      "    step: 400; loss: 10.155; l2dist: 2.990\n",
      "    step: 450; loss: 9.926; l2dist: 2.962\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 82.584; l2dist: 0.000\n",
      "    step: 50; loss: 26.221; l2dist: 3.509\n",
      "    step: 100; loss: 19.287; l2dist: 3.408\n",
      "    step: 150; loss: 13.851; l2dist: 3.348\n",
      "    step: 200; loss: 11.938; l2dist: 3.190\n",
      "    step: 250; loss: 11.059; l2dist: 3.095\n",
      "    step: 300; loss: 10.505; l2dist: 3.028\n",
      "    step: 350; loss: 10.178; l2dist: 2.989\n",
      "    step: 400; loss: 9.864; l2dist: 2.943\n",
      "    step: 450; loss: 9.704; l2dist: 2.930\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.606; l2dist: 0.000\n",
      "    step: 50; loss: 24.372; l2dist: 3.351\n",
      "    step: 100; loss: 18.733; l2dist: 3.282\n",
      "    step: 150; loss: 14.244; l2dist: 3.273\n",
      "    step: 200; loss: 11.950; l2dist: 3.171\n",
      "    step: 250; loss: 11.037; l2dist: 3.079\n",
      "    step: 300; loss: 10.550; l2dist: 3.011\n",
      "    step: 350; loss: 10.134; l2dist: 2.972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 9.809; l2dist: 2.940\n",
      "    step: 450; loss: 9.656; l2dist: 2.924\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.215; l2dist: 0.000\n",
      "    step: 50; loss: 23.255; l2dist: 3.260\n",
      "    step: 100; loss: 18.412; l2dist: 3.187\n",
      "    step: 150; loss: 14.321; l2dist: 3.199\n",
      "    step: 200; loss: 12.016; l2dist: 3.142\n",
      "    step: 250; loss: 10.967; l2dist: 3.056\n",
      "    step: 300; loss: 10.633; l2dist: 3.017\n",
      "    step: 350; loss: 10.209; l2dist: 2.980\n",
      "    step: 400; loss: 9.885; l2dist: 2.935\n",
      "    step: 450; loss: 9.751; l2dist: 2.934\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.315; l2dist: 0.000\n",
      "    step: 50; loss: 22.646; l2dist: 3.219\n",
      "    step: 100; loss: 18.174; l2dist: 3.142\n",
      "    step: 150; loss: 14.535; l2dist: 3.139\n",
      "    step: 200; loss: 12.475; l2dist: 3.092\n",
      "    step: 250; loss: 11.358; l2dist: 3.012\n",
      "    step: 300; loss: 10.865; l2dist: 2.962\n",
      "    step: 350; loss: 10.162; l2dist: 2.960\n",
      "    step: 400; loss: 9.793; l2dist: 2.912\n",
      "    step: 450; loss: 9.469; l2dist: 2.879\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.182; l2dist: 0.000\n",
      "    step: 50; loss: 22.550; l2dist: 3.194\n",
      "    step: 100; loss: 18.134; l2dist: 3.131\n",
      "    step: 150; loss: 14.471; l2dist: 3.136\n",
      "    step: 200; loss: 12.544; l2dist: 3.070\n",
      "    step: 250; loss: 11.374; l2dist: 3.009\n",
      "    step: 300; loss: 10.719; l2dist: 2.938\n",
      "    step: 350; loss: 10.302; l2dist: 2.905\n",
      "    step: 400; loss: 10.152; l2dist: 2.880\n",
      "    step: 450; loss: 10.105; l2dist: 2.870\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.111; l2dist: 0.000\n",
      "    step: 50; loss: 22.683; l2dist: 3.213\n",
      "    step: 100; loss: 18.031; l2dist: 3.154\n",
      "    step: 150; loss: 14.445; l2dist: 3.141\n",
      "    step: 200; loss: 12.291; l2dist: 3.116\n",
      "    step: 250; loss: 10.935; l2dist: 3.045\n",
      "    step: 300; loss: 10.336; l2dist: 2.990\n",
      "    step: 350; loss: 9.828; l2dist: 2.936\n",
      "    step: 400; loss: 9.607; l2dist: 2.899\n",
      "    step: 450; loss: 9.428; l2dist: 2.889\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.950; l2dist: 0.000\n",
      "    step: 50; loss: 16.791; l2dist: 1.916\n",
      "    step: 100; loss: 16.349; l2dist: 1.968\n",
      "    step: 150; loss: 15.708; l2dist: 2.065\n",
      "    step: 200; loss: 15.150; l2dist: 2.121\n",
      "    step: 250; loss: 14.803; l2dist: 2.138\n",
      "    step: 300; loss: 14.555; l2dist: 2.145\n",
      "    step: 350; loss: 14.349; l2dist: 2.168\n",
      "    step: 400; loss: 14.237; l2dist: 2.176\n",
      "    step: 450; loss: 14.156; l2dist: 2.183\n",
      "binary step: 0; number of successful adv: 22/100\n",
      "    step: 0; loss: 165.974; l2dist: 0.000\n",
      "    step: 50; loss: 49.848; l2dist: 5.113\n",
      "    step: 100; loss: 28.929; l2dist: 4.672\n",
      "    step: 150; loss: 20.617; l2dist: 4.129\n",
      "    step: 200; loss: 17.485; l2dist: 3.827\n",
      "    step: 250; loss: 15.970; l2dist: 3.683\n",
      "    step: 300; loss: 15.186; l2dist: 3.601\n",
      "    step: 350; loss: 14.577; l2dist: 3.542\n",
      "    step: 400; loss: 14.318; l2dist: 3.490\n",
      "    step: 450; loss: 13.956; l2dist: 3.457\n",
      "binary step: 1; number of successful adv: 92/100\n",
      "    step: 0; loss: 224.645; l2dist: 0.000\n",
      "    step: 50; loss: 48.536; l2dist: 5.347\n",
      "    step: 100; loss: 29.564; l2dist: 4.689\n",
      "    step: 150; loss: 21.810; l2dist: 4.251\n",
      "    step: 200; loss: 18.204; l2dist: 3.944\n",
      "    step: 250; loss: 16.619; l2dist: 3.781\n",
      "    step: 300; loss: 15.687; l2dist: 3.678\n",
      "    step: 350; loss: 14.996; l2dist: 3.608\n",
      "    step: 400; loss: 14.524; l2dist: 3.542\n",
      "    step: 450; loss: 14.613; l2dist: 3.562\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 140.281; l2dist: 0.000\n",
      "    step: 50; loss: 39.473; l2dist: 4.769\n",
      "    step: 100; loss: 26.384; l2dist: 4.263\n",
      "    step: 150; loss: 20.216; l2dist: 3.976\n",
      "    step: 200; loss: 16.824; l2dist: 3.773\n",
      "    step: 250; loss: 15.496; l2dist: 3.637\n",
      "    step: 300; loss: 14.671; l2dist: 3.553\n",
      "    step: 350; loss: 14.058; l2dist: 3.489\n",
      "    step: 400; loss: 13.935; l2dist: 3.463\n",
      "    step: 450; loss: 13.751; l2dist: 3.454\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 114.260; l2dist: 0.000\n",
      "    step: 50; loss: 35.201; l2dist: 4.377\n",
      "    step: 100; loss: 24.492; l2dist: 3.992\n",
      "    step: 150; loss: 19.676; l2dist: 3.818\n",
      "    step: 200; loss: 16.903; l2dist: 3.677\n",
      "    step: 250; loss: 15.163; l2dist: 3.568\n",
      "    step: 300; loss: 14.488; l2dist: 3.500\n",
      "    step: 350; loss: 13.864; l2dist: 3.431\n",
      "    step: 400; loss: 13.645; l2dist: 3.408\n",
      "    step: 450; loss: 13.348; l2dist: 3.384\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 94.362; l2dist: 0.000\n",
      "    step: 50; loss: 31.583; l2dist: 4.102\n",
      "    step: 100; loss: 22.831; l2dist: 3.803\n",
      "    step: 150; loss: 18.799; l2dist: 3.638\n",
      "    step: 200; loss: 16.344; l2dist: 3.528\n",
      "    step: 250; loss: 14.942; l2dist: 3.460\n",
      "    step: 300; loss: 14.126; l2dist: 3.405\n",
      "    step: 350; loss: 13.589; l2dist: 3.364\n",
      "    step: 400; loss: 13.331; l2dist: 3.336\n",
      "    step: 450; loss: 13.160; l2dist: 3.325\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.107; l2dist: 0.000\n",
      "    step: 50; loss: 30.467; l2dist: 3.996\n",
      "    step: 100; loss: 23.029; l2dist: 3.750\n",
      "    step: 150; loss: 18.768; l2dist: 3.617\n",
      "    step: 200; loss: 16.302; l2dist: 3.507\n",
      "    step: 250; loss: 14.843; l2dist: 3.439\n",
      "    step: 300; loss: 14.080; l2dist: 3.385\n",
      "    step: 350; loss: 13.637; l2dist: 3.355\n",
      "    step: 400; loss: 13.415; l2dist: 3.333\n",
      "    step: 450; loss: 13.149; l2dist: 3.307\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 84.830; l2dist: 0.000\n",
      "    step: 50; loss: 29.800; l2dist: 3.949\n",
      "    step: 100; loss: 22.893; l2dist: 3.705\n",
      "    step: 150; loss: 18.691; l2dist: 3.592\n",
      "    step: 200; loss: 16.232; l2dist: 3.490\n",
      "    step: 250; loss: 14.911; l2dist: 3.427\n",
      "    step: 300; loss: 13.926; l2dist: 3.379\n",
      "    step: 350; loss: 13.582; l2dist: 3.356\n",
      "    step: 400; loss: 13.264; l2dist: 3.326\n",
      "    step: 450; loss: 13.110; l2dist: 3.321\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 82.687; l2dist: 0.000\n",
      "    step: 50; loss: 29.414; l2dist: 3.924\n",
      "    step: 100; loss: 22.761; l2dist: 3.691\n",
      "    step: 150; loss: 18.789; l2dist: 3.580\n",
      "    step: 200; loss: 16.258; l2dist: 3.494\n",
      "    step: 250; loss: 14.872; l2dist: 3.435\n",
      "    step: 300; loss: 14.058; l2dist: 3.389\n",
      "    step: 350; loss: 13.579; l2dist: 3.356\n",
      "    step: 400; loss: 13.279; l2dist: 3.338\n",
      "    step: 450; loss: 13.139; l2dist: 3.315\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.344; l2dist: 0.000\n",
      "    step: 50; loss: 29.491; l2dist: 3.939\n",
      "    step: 100; loss: 22.834; l2dist: 3.712\n",
      "    step: 150; loss: 18.804; l2dist: 3.597\n",
      "    step: 200; loss: 16.324; l2dist: 3.508\n",
      "    step: 250; loss: 14.783; l2dist: 3.452\n",
      "    step: 300; loss: 13.977; l2dist: 3.394\n",
      "    step: 350; loss: 13.551; l2dist: 3.367\n",
      "    step: 400; loss: 13.292; l2dist: 3.345\n",
      "    step: 450; loss: 13.154; l2dist: 3.331\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.384; l2dist: 0.000\n",
      "    step: 50; loss: 16.559; l2dist: 2.099\n",
      "    step: 100; loss: 15.810; l2dist: 2.147\n",
      "    step: 150; loss: 15.107; l2dist: 2.227\n",
      "    step: 200; loss: 14.365; l2dist: 2.304\n",
      "    step: 250; loss: 13.702; l2dist: 2.320\n",
      "    step: 300; loss: 13.369; l2dist: 2.310\n",
      "    step: 350; loss: 13.191; l2dist: 2.317\n",
      "    step: 400; loss: 13.064; l2dist: 2.318\n",
      "    step: 450; loss: 12.957; l2dist: 2.322\n",
      "binary step: 0; number of successful adv: 28/100\n",
      "    step: 0; loss: 159.797; l2dist: 0.000\n",
      "    step: 50; loss: 47.467; l2dist: 4.828\n",
      "    step: 100; loss: 26.146; l2dist: 4.360\n",
      "    step: 150; loss: 18.996; l2dist: 3.905\n",
      "    step: 200; loss: 16.432; l2dist: 3.672\n",
      "    step: 250; loss: 15.189; l2dist: 3.552\n",
      "    step: 300; loss: 14.421; l2dist: 3.468\n",
      "    step: 350; loss: 13.984; l2dist: 3.423\n",
      "    step: 400; loss: 13.720; l2dist: 3.392\n",
      "    step: 450; loss: 13.375; l2dist: 3.356\n",
      "binary step: 1; number of successful adv: 88/100\n",
      "    step: 0; loss: 294.330; l2dist: 0.000\n",
      "    step: 50; loss: 52.232; l2dist: 5.304\n",
      "    step: 100; loss: 30.167; l2dist: 4.715\n",
      "    step: 150; loss: 21.550; l2dist: 4.201\n",
      "    step: 200; loss: 18.155; l2dist: 3.888\n",
      "    step: 250; loss: 16.609; l2dist: 3.713\n",
      "    step: 300; loss: 15.560; l2dist: 3.607\n",
      "    step: 350; loss: 14.951; l2dist: 3.533\n",
      "    step: 400; loss: 14.530; l2dist: 3.483\n",
      "    step: 450; loss: 14.262; l2dist: 3.452\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 174.501; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 40.625; l2dist: 4.609\n",
      "    step: 100; loss: 26.575; l2dist: 4.225\n",
      "    step: 150; loss: 19.105; l2dist: 3.886\n",
      "    step: 200; loss: 16.352; l2dist: 3.669\n",
      "    step: 250; loss: 15.220; l2dist: 3.567\n",
      "    step: 300; loss: 14.196; l2dist: 3.458\n",
      "    step: 350; loss: 13.901; l2dist: 3.429\n",
      "    step: 400; loss: 13.305; l2dist: 3.365\n",
      "    step: 450; loss: 13.222; l2dist: 3.359\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 122.238; l2dist: 0.000\n",
      "    step: 50; loss: 34.265; l2dist: 4.125\n",
      "    step: 100; loss: 24.835; l2dist: 3.841\n",
      "    step: 150; loss: 18.677; l2dist: 3.703\n",
      "    step: 200; loss: 15.646; l2dist: 3.534\n",
      "    step: 250; loss: 14.191; l2dist: 3.424\n",
      "    step: 300; loss: 13.814; l2dist: 3.390\n",
      "    step: 350; loss: 13.399; l2dist: 3.350\n",
      "    step: 400; loss: 12.866; l2dist: 3.306\n",
      "    step: 450; loss: 12.574; l2dist: 3.275\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 99.401; l2dist: 0.000\n",
      "    step: 50; loss: 30.781; l2dist: 3.859\n",
      "    step: 100; loss: 23.420; l2dist: 3.633\n",
      "    step: 150; loss: 18.877; l2dist: 3.551\n",
      "    step: 200; loss: 15.792; l2dist: 3.483\n",
      "    step: 250; loss: 14.151; l2dist: 3.393\n",
      "    step: 300; loss: 13.400; l2dist: 3.335\n",
      "    step: 350; loss: 13.004; l2dist: 3.285\n",
      "    step: 400; loss: 12.700; l2dist: 3.262\n",
      "    step: 450; loss: 12.620; l2dist: 3.255\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.658; l2dist: 0.000\n",
      "    step: 50; loss: 29.349; l2dist: 3.738\n",
      "    step: 100; loss: 23.120; l2dist: 3.523\n",
      "    step: 150; loss: 18.637; l2dist: 3.492\n",
      "    step: 200; loss: 15.694; l2dist: 3.464\n",
      "    step: 250; loss: 14.158; l2dist: 3.382\n",
      "    step: 300; loss: 13.521; l2dist: 3.342\n",
      "    step: 350; loss: 12.959; l2dist: 3.291\n",
      "    step: 400; loss: 12.642; l2dist: 3.259\n",
      "    step: 450; loss: 12.434; l2dist: 3.245\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.194; l2dist: 0.000\n",
      "    step: 50; loss: 28.753; l2dist: 3.700\n",
      "    step: 100; loss: 22.714; l2dist: 3.499\n",
      "    step: 150; loss: 18.409; l2dist: 3.487\n",
      "    step: 200; loss: 15.735; l2dist: 3.460\n",
      "    step: 250; loss: 14.202; l2dist: 3.408\n",
      "    step: 300; loss: 13.275; l2dist: 3.321\n",
      "    step: 350; loss: 12.902; l2dist: 3.289\n",
      "    step: 400; loss: 12.704; l2dist: 3.276\n",
      "    step: 450; loss: 12.382; l2dist: 3.233\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.713; l2dist: 0.000\n",
      "    step: 50; loss: 28.510; l2dist: 3.690\n",
      "    step: 100; loss: 22.694; l2dist: 3.478\n",
      "    step: 150; loss: 18.433; l2dist: 3.476\n",
      "    step: 200; loss: 15.735; l2dist: 3.456\n",
      "    step: 250; loss: 14.196; l2dist: 3.388\n",
      "    step: 300; loss: 13.449; l2dist: 3.338\n",
      "    step: 350; loss: 12.910; l2dist: 3.286\n",
      "    step: 400; loss: 12.684; l2dist: 3.266\n",
      "    step: 450; loss: 12.512; l2dist: 3.248\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.367; l2dist: 0.000\n",
      "    step: 50; loss: 28.632; l2dist: 3.698\n",
      "    step: 100; loss: 22.836; l2dist: 3.481\n",
      "    step: 150; loss: 18.360; l2dist: 3.485\n",
      "    step: 200; loss: 15.707; l2dist: 3.472\n",
      "    step: 250; loss: 14.089; l2dist: 3.394\n",
      "    step: 300; loss: 13.203; l2dist: 3.319\n",
      "    step: 350; loss: 12.945; l2dist: 3.293\n",
      "    step: 400; loss: 12.615; l2dist: 3.254\n",
      "    step: 450; loss: 12.325; l2dist: 3.226\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.632; l2dist: 0.000\n",
      "    step: 50; loss: 15.242; l2dist: 1.897\n",
      "    step: 100; loss: 14.753; l2dist: 1.928\n",
      "    step: 150; loss: 13.970; l2dist: 2.059\n",
      "    step: 200; loss: 13.301; l2dist: 2.163\n",
      "    step: 250; loss: 12.780; l2dist: 2.213\n",
      "    step: 300; loss: 12.400; l2dist: 2.238\n",
      "    step: 350; loss: 12.083; l2dist: 2.250\n",
      "    step: 400; loss: 11.847; l2dist: 2.263\n",
      "    step: 450; loss: 11.735; l2dist: 2.275\n",
      "binary step: 0; number of successful adv: 29/100\n",
      "    step: 0; loss: 145.729; l2dist: 0.000\n",
      "    step: 50; loss: 46.857; l2dist: 4.664\n",
      "    step: 100; loss: 24.804; l2dist: 4.242\n",
      "    step: 150; loss: 17.068; l2dist: 3.764\n",
      "    step: 200; loss: 14.747; l2dist: 3.499\n",
      "    step: 250; loss: 13.367; l2dist: 3.355\n",
      "    step: 300; loss: 12.764; l2dist: 3.291\n",
      "    step: 350; loss: 12.222; l2dist: 3.222\n",
      "    step: 400; loss: 11.832; l2dist: 3.185\n",
      "    step: 450; loss: 11.663; l2dist: 3.169\n",
      "binary step: 1; number of successful adv: 87/100\n",
      "    step: 0; loss: 265.099; l2dist: 0.000\n",
      "    step: 50; loss: 51.630; l2dist: 4.934\n",
      "    step: 100; loss: 28.811; l2dist: 4.554\n",
      "    step: 150; loss: 20.215; l2dist: 3.978\n",
      "    step: 200; loss: 16.018; l2dist: 3.640\n",
      "    step: 250; loss: 14.446; l2dist: 3.486\n",
      "    step: 300; loss: 13.208; l2dist: 3.346\n",
      "    step: 350; loss: 12.910; l2dist: 3.322\n",
      "    step: 400; loss: 12.734; l2dist: 3.286\n",
      "    step: 450; loss: 12.375; l2dist: 3.247\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 156.453; l2dist: 0.000\n",
      "    step: 50; loss: 38.719; l2dist: 4.348\n",
      "    step: 100; loss: 24.063; l2dist: 4.018\n",
      "    step: 150; loss: 17.355; l2dist: 3.668\n",
      "    step: 200; loss: 14.115; l2dist: 3.423\n",
      "    step: 250; loss: 13.016; l2dist: 3.320\n",
      "    step: 300; loss: 12.420; l2dist: 3.244\n",
      "    step: 350; loss: 11.910; l2dist: 3.184\n",
      "    step: 400; loss: 11.830; l2dist: 3.178\n",
      "    step: 450; loss: 11.504; l2dist: 3.142\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.038; l2dist: 0.000\n",
      "    step: 50; loss: 32.025; l2dist: 3.938\n",
      "    step: 100; loss: 22.170; l2dist: 3.653\n",
      "    step: 150; loss: 16.563; l2dist: 3.497\n",
      "    step: 200; loss: 13.891; l2dist: 3.365\n",
      "    step: 250; loss: 12.666; l2dist: 3.255\n",
      "    step: 300; loss: 12.185; l2dist: 3.207\n",
      "    step: 350; loss: 11.665; l2dist: 3.147\n",
      "    step: 400; loss: 11.391; l2dist: 3.110\n",
      "    step: 450; loss: 11.241; l2dist: 3.101\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 95.000; l2dist: 0.000\n",
      "    step: 50; loss: 29.091; l2dist: 3.684\n",
      "    step: 100; loss: 21.104; l2dist: 3.478\n",
      "    step: 150; loss: 16.436; l2dist: 3.398\n",
      "    step: 200; loss: 13.918; l2dist: 3.297\n",
      "    step: 250; loss: 12.651; l2dist: 3.207\n",
      "    step: 300; loss: 11.870; l2dist: 3.139\n",
      "    step: 350; loss: 11.526; l2dist: 3.107\n",
      "    step: 400; loss: 11.303; l2dist: 3.086\n",
      "    step: 450; loss: 11.206; l2dist: 3.073\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.229; l2dist: 0.000\n",
      "    step: 50; loss: 27.564; l2dist: 3.631\n",
      "    step: 100; loss: 20.448; l2dist: 3.441\n",
      "    step: 150; loss: 15.872; l2dist: 3.359\n",
      "    step: 200; loss: 13.674; l2dist: 3.260\n",
      "    step: 250; loss: 12.663; l2dist: 3.187\n",
      "    step: 300; loss: 11.825; l2dist: 3.131\n",
      "    step: 350; loss: 11.408; l2dist: 3.078\n",
      "    step: 400; loss: 11.265; l2dist: 3.075\n",
      "    step: 450; loss: 11.117; l2dist: 3.065\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.932; l2dist: 0.000\n",
      "    step: 50; loss: 27.188; l2dist: 3.598\n",
      "    step: 100; loss: 20.255; l2dist: 3.426\n",
      "    step: 150; loss: 15.871; l2dist: 3.357\n",
      "    step: 200; loss: 13.766; l2dist: 3.271\n",
      "    step: 250; loss: 12.650; l2dist: 3.191\n",
      "    step: 300; loss: 11.800; l2dist: 3.119\n",
      "    step: 350; loss: 11.488; l2dist: 3.098\n",
      "    step: 400; loss: 11.167; l2dist: 3.065\n",
      "    step: 450; loss: 11.019; l2dist: 3.059\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 85.451; l2dist: 0.000\n",
      "    step: 50; loss: 26.919; l2dist: 3.577\n",
      "    step: 100; loss: 20.125; l2dist: 3.410\n",
      "    step: 150; loss: 15.932; l2dist: 3.336\n",
      "    step: 200; loss: 13.739; l2dist: 3.255\n",
      "    step: 250; loss: 12.518; l2dist: 3.166\n",
      "    step: 300; loss: 11.861; l2dist: 3.122\n",
      "    step: 350; loss: 11.454; l2dist: 3.094\n",
      "    step: 400; loss: 11.116; l2dist: 3.049\n",
      "    step: 450; loss: 11.000; l2dist: 3.058\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.851; l2dist: 0.000\n",
      "    step: 50; loss: 27.244; l2dist: 3.600\n",
      "    step: 100; loss: 20.339; l2dist: 3.425\n",
      "    step: 150; loss: 15.952; l2dist: 3.364\n",
      "    step: 200; loss: 13.628; l2dist: 3.254\n",
      "    step: 250; loss: 12.493; l2dist: 3.170\n",
      "    step: 300; loss: 11.720; l2dist: 3.115\n",
      "    step: 350; loss: 11.433; l2dist: 3.088\n",
      "    step: 400; loss: 11.077; l2dist: 3.056\n",
      "    step: 450; loss: 11.124; l2dist: 3.069\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.960; l2dist: 0.000\n",
      "    step: 50; loss: 12.730; l2dist: 1.743\n",
      "    step: 100; loss: 12.389; l2dist: 1.780\n",
      "    step: 150; loss: 11.802; l2dist: 1.858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 200; loss: 11.460; l2dist: 1.916\n",
      "    step: 250; loss: 11.163; l2dist: 1.954\n",
      "    step: 300; loss: 10.954; l2dist: 1.960\n",
      "    step: 350; loss: 10.808; l2dist: 1.968\n",
      "    step: 400; loss: 10.623; l2dist: 1.977\n",
      "    step: 450; loss: 10.532; l2dist: 1.978\n",
      "binary step: 0; number of successful adv: 24/100\n",
      "    step: 0; loss: 129.804; l2dist: 0.000\n",
      "    step: 50; loss: 36.418; l2dist: 4.450\n",
      "    step: 100; loss: 21.014; l2dist: 3.967\n",
      "    step: 150; loss: 15.359; l2dist: 3.607\n",
      "    step: 200; loss: 13.547; l2dist: 3.395\n",
      "    step: 250; loss: 12.512; l2dist: 3.281\n",
      "    step: 300; loss: 11.866; l2dist: 3.201\n",
      "    step: 350; loss: 11.686; l2dist: 3.181\n",
      "    step: 400; loss: 11.345; l2dist: 3.144\n",
      "    step: 450; loss: 11.390; l2dist: 3.145\n",
      "binary step: 1; number of successful adv: 89/100\n",
      "    step: 0; loss: 231.516; l2dist: 0.000\n",
      "    step: 50; loss: 37.663; l2dist: 4.668\n",
      "    step: 100; loss: 24.054; l2dist: 4.192\n",
      "    step: 150; loss: 17.045; l2dist: 3.770\n",
      "    step: 200; loss: 14.751; l2dist: 3.543\n",
      "    step: 250; loss: 13.567; l2dist: 3.406\n",
      "    step: 300; loss: 12.699; l2dist: 3.297\n",
      "    step: 350; loss: 12.485; l2dist: 3.265\n",
      "    step: 400; loss: 12.121; l2dist: 3.227\n",
      "    step: 450; loss: 11.629; l2dist: 3.173\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 267.341; l2dist: 0.000\n",
      "    step: 50; loss: 36.494; l2dist: 4.659\n",
      "    step: 100; loss: 23.164; l2dist: 4.140\n",
      "    step: 150; loss: 17.113; l2dist: 3.747\n",
      "    step: 200; loss: 14.200; l2dist: 3.456\n",
      "    step: 250; loss: 12.941; l2dist: 3.313\n",
      "    step: 300; loss: 12.175; l2dist: 3.219\n",
      "    step: 350; loss: 11.510; l2dist: 3.145\n",
      "    step: 400; loss: 11.473; l2dist: 3.128\n",
      "    step: 450; loss: 11.077; l2dist: 3.087\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 162.114; l2dist: 0.000\n",
      "    step: 50; loss: 29.562; l2dist: 4.156\n",
      "    step: 100; loss: 20.105; l2dist: 3.647\n",
      "    step: 150; loss: 15.007; l2dist: 3.397\n",
      "    step: 200; loss: 12.922; l2dist: 3.248\n",
      "    step: 250; loss: 12.090; l2dist: 3.175\n",
      "    step: 300; loss: 11.708; l2dist: 3.126\n",
      "    step: 350; loss: 11.085; l2dist: 3.066\n",
      "    step: 400; loss: 10.982; l2dist: 3.065\n",
      "    step: 450; loss: 10.694; l2dist: 3.030\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 120.402; l2dist: 0.000\n",
      "    step: 50; loss: 25.926; l2dist: 3.871\n",
      "    step: 100; loss: 18.193; l2dist: 3.422\n",
      "    step: 150; loss: 14.251; l2dist: 3.260\n",
      "    step: 200; loss: 12.612; l2dist: 3.168\n",
      "    step: 250; loss: 11.712; l2dist: 3.104\n",
      "    step: 300; loss: 11.349; l2dist: 3.082\n",
      "    step: 350; loss: 10.963; l2dist: 3.039\n",
      "    step: 400; loss: 10.776; l2dist: 3.024\n",
      "    step: 450; loss: 10.531; l2dist: 3.000\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 97.300; l2dist: 0.000\n",
      "    step: 50; loss: 24.004; l2dist: 3.698\n",
      "    step: 100; loss: 17.471; l2dist: 3.316\n",
      "    step: 150; loss: 14.330; l2dist: 3.198\n",
      "    step: 200; loss: 12.449; l2dist: 3.135\n",
      "    step: 250; loss: 11.623; l2dist: 3.072\n",
      "    step: 300; loss: 10.947; l2dist: 3.028\n",
      "    step: 350; loss: 10.766; l2dist: 3.007\n",
      "    step: 400; loss: 10.550; l2dist: 2.983\n",
      "    step: 450; loss: 10.281; l2dist: 2.957\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 87.348; l2dist: 0.000\n",
      "    step: 50; loss: 23.292; l2dist: 3.598\n",
      "    step: 100; loss: 17.296; l2dist: 3.269\n",
      "    step: 150; loss: 14.298; l2dist: 3.171\n",
      "    step: 200; loss: 12.631; l2dist: 3.129\n",
      "    step: 250; loss: 11.710; l2dist: 3.071\n",
      "    step: 300; loss: 11.217; l2dist: 3.026\n",
      "    step: 350; loss: 10.918; l2dist: 3.006\n",
      "    step: 400; loss: 10.857; l2dist: 2.996\n",
      "    step: 450; loss: 10.704; l2dist: 2.992\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 82.874; l2dist: 0.000\n",
      "    step: 50; loss: 22.666; l2dist: 3.564\n",
      "    step: 100; loss: 17.099; l2dist: 3.247\n",
      "    step: 150; loss: 14.351; l2dist: 3.160\n",
      "    step: 200; loss: 12.611; l2dist: 3.128\n",
      "    step: 250; loss: 11.721; l2dist: 3.078\n",
      "    step: 300; loss: 11.179; l2dist: 3.035\n",
      "    step: 350; loss: 10.805; l2dist: 2.998\n",
      "    step: 400; loss: 10.611; l2dist: 2.982\n",
      "    step: 450; loss: 10.468; l2dist: 2.967\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 84.198; l2dist: 0.000\n",
      "    step: 50; loss: 22.861; l2dist: 3.562\n",
      "    step: 100; loss: 17.061; l2dist: 3.248\n",
      "    step: 150; loss: 14.307; l2dist: 3.159\n",
      "    step: 200; loss: 12.607; l2dist: 3.129\n",
      "    step: 250; loss: 11.664; l2dist: 3.072\n",
      "    step: 300; loss: 11.161; l2dist: 3.048\n",
      "    step: 350; loss: 10.912; l2dist: 3.012\n",
      "    step: 400; loss: 10.611; l2dist: 2.993\n",
      "    step: 450; loss: 10.545; l2dist: 2.981\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.782; l2dist: 0.000\n",
      "    step: 50; loss: 15.350; l2dist: 2.013\n",
      "    step: 100; loss: 14.799; l2dist: 2.064\n",
      "    step: 150; loss: 14.111; l2dist: 2.148\n",
      "    step: 200; loss: 13.489; l2dist: 2.239\n",
      "    step: 250; loss: 12.914; l2dist: 2.254\n",
      "    step: 300; loss: 12.654; l2dist: 2.262\n",
      "    step: 350; loss: 12.443; l2dist: 2.273\n",
      "    step: 400; loss: 12.292; l2dist: 2.274\n",
      "    step: 450; loss: 12.259; l2dist: 2.279\n",
      "binary step: 0; number of successful adv: 31/100\n",
      "    step: 0; loss: 148.728; l2dist: 0.000\n",
      "    step: 50; loss: 46.994; l2dist: 4.519\n",
      "    step: 100; loss: 24.688; l2dist: 4.054\n",
      "    step: 150; loss: 17.225; l2dist: 3.711\n",
      "    step: 200; loss: 15.015; l2dist: 3.484\n",
      "    step: 250; loss: 13.837; l2dist: 3.356\n",
      "    step: 300; loss: 13.110; l2dist: 3.267\n",
      "    step: 350; loss: 12.524; l2dist: 3.205\n",
      "    step: 400; loss: 12.139; l2dist: 3.168\n",
      "    step: 450; loss: 11.971; l2dist: 3.154\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 225.197; l2dist: 0.000\n",
      "    step: 50; loss: 50.375; l2dist: 4.765\n",
      "    step: 100; loss: 26.430; l2dist: 4.261\n",
      "    step: 150; loss: 19.265; l2dist: 3.846\n",
      "    step: 200; loss: 15.813; l2dist: 3.592\n",
      "    step: 250; loss: 14.354; l2dist: 3.443\n",
      "    step: 300; loss: 13.733; l2dist: 3.370\n",
      "    step: 350; loss: 12.979; l2dist: 3.281\n",
      "    step: 400; loss: 12.563; l2dist: 3.222\n",
      "    step: 450; loss: 12.371; l2dist: 3.213\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 136.907; l2dist: 0.000\n",
      "    step: 50; loss: 38.845; l2dist: 4.192\n",
      "    step: 100; loss: 23.042; l2dist: 3.861\n",
      "    step: 150; loss: 17.161; l2dist: 3.581\n",
      "    step: 200; loss: 14.527; l2dist: 3.387\n",
      "    step: 250; loss: 13.357; l2dist: 3.306\n",
      "    step: 300; loss: 12.589; l2dist: 3.226\n",
      "    step: 350; loss: 12.345; l2dist: 3.189\n",
      "    step: 400; loss: 11.751; l2dist: 3.133\n",
      "    step: 450; loss: 11.789; l2dist: 3.135\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 105.891; l2dist: 0.000\n",
      "    step: 50; loss: 33.604; l2dist: 3.803\n",
      "    step: 100; loss: 21.384; l2dist: 3.600\n",
      "    step: 150; loss: 16.677; l2dist: 3.463\n",
      "    step: 200; loss: 14.168; l2dist: 3.322\n",
      "    step: 250; loss: 13.170; l2dist: 3.247\n",
      "    step: 300; loss: 12.547; l2dist: 3.184\n",
      "    step: 350; loss: 11.963; l2dist: 3.125\n",
      "    step: 400; loss: 11.660; l2dist: 3.098\n",
      "    step: 450; loss: 11.412; l2dist: 3.072\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 94.186; l2dist: 0.000\n",
      "    step: 50; loss: 30.960; l2dist: 3.645\n",
      "    step: 100; loss: 21.011; l2dist: 3.511\n",
      "    step: 150; loss: 16.449; l2dist: 3.389\n",
      "    step: 200; loss: 13.966; l2dist: 3.258\n",
      "    step: 250; loss: 12.835; l2dist: 3.196\n",
      "    step: 300; loss: 12.352; l2dist: 3.168\n",
      "    step: 350; loss: 11.859; l2dist: 3.110\n",
      "    step: 400; loss: 11.486; l2dist: 3.062\n",
      "    step: 450; loss: 11.339; l2dist: 3.056\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.206; l2dist: 0.000\n",
      "    step: 50; loss: 29.770; l2dist: 3.563\n",
      "    step: 100; loss: 20.707; l2dist: 3.424\n",
      "    step: 150; loss: 16.442; l2dist: 3.343\n",
      "    step: 200; loss: 14.085; l2dist: 3.231\n",
      "    step: 250; loss: 12.801; l2dist: 3.157\n",
      "    step: 300; loss: 12.100; l2dist: 3.128\n",
      "    step: 350; loss: 11.804; l2dist: 3.097\n",
      "    step: 400; loss: 11.557; l2dist: 3.066\n",
      "    step: 450; loss: 11.360; l2dist: 3.049\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.581; l2dist: 0.000\n",
      "    step: 50; loss: 29.056; l2dist: 3.531\n",
      "    step: 100; loss: 20.454; l2dist: 3.399\n",
      "    step: 150; loss: 16.400; l2dist: 3.322\n",
      "    step: 200; loss: 14.191; l2dist: 3.231\n",
      "    step: 250; loss: 12.813; l2dist: 3.184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 12.398; l2dist: 3.146\n",
      "    step: 350; loss: 11.870; l2dist: 3.102\n",
      "    step: 400; loss: 11.403; l2dist: 3.067\n",
      "    step: 450; loss: 11.209; l2dist: 3.040\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 85.887; l2dist: 0.000\n",
      "    step: 50; loss: 28.820; l2dist: 3.524\n",
      "    step: 100; loss: 20.326; l2dist: 3.392\n",
      "    step: 150; loss: 16.555; l2dist: 3.301\n",
      "    step: 200; loss: 14.181; l2dist: 3.228\n",
      "    step: 250; loss: 12.710; l2dist: 3.159\n",
      "    step: 300; loss: 12.174; l2dist: 3.130\n",
      "    step: 350; loss: 11.792; l2dist: 3.106\n",
      "    step: 400; loss: 11.484; l2dist: 3.062\n",
      "    step: 450; loss: 11.114; l2dist: 3.022\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.951; l2dist: 0.000\n",
      "    step: 50; loss: 28.856; l2dist: 3.544\n",
      "    step: 100; loss: 20.210; l2dist: 3.409\n",
      "    step: 150; loss: 16.381; l2dist: 3.319\n",
      "    step: 200; loss: 14.176; l2dist: 3.235\n",
      "    step: 250; loss: 12.721; l2dist: 3.182\n",
      "    step: 300; loss: 12.161; l2dist: 3.151\n",
      "    step: 350; loss: 11.754; l2dist: 3.108\n",
      "    step: 400; loss: 11.411; l2dist: 3.044\n",
      "    step: 450; loss: 11.035; l2dist: 3.012\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.506; l2dist: 0.000\n",
      "    step: 50; loss: 14.290; l2dist: 1.937\n",
      "    step: 100; loss: 13.949; l2dist: 1.951\n",
      "    step: 150; loss: 13.451; l2dist: 2.030\n",
      "    step: 200; loss: 12.915; l2dist: 2.094\n",
      "    step: 250; loss: 12.525; l2dist: 2.145\n",
      "    step: 300; loss: 12.238; l2dist: 2.160\n",
      "    step: 350; loss: 11.975; l2dist: 2.172\n",
      "    step: 400; loss: 11.842; l2dist: 2.177\n",
      "    step: 450; loss: 11.752; l2dist: 2.174\n",
      "binary step: 0; number of successful adv: 30/100\n",
      "    step: 0; loss: 141.365; l2dist: 0.000\n",
      "    step: 50; loss: 43.250; l2dist: 4.522\n",
      "    step: 100; loss: 26.242; l2dist: 4.101\n",
      "    step: 150; loss: 18.062; l2dist: 3.693\n",
      "    step: 200; loss: 14.826; l2dist: 3.499\n",
      "    step: 250; loss: 13.566; l2dist: 3.375\n",
      "    step: 300; loss: 12.742; l2dist: 3.279\n",
      "    step: 350; loss: 12.375; l2dist: 3.236\n",
      "    step: 400; loss: 12.053; l2dist: 3.207\n",
      "    step: 450; loss: 11.852; l2dist: 3.178\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 232.873; l2dist: 0.000\n",
      "    step: 50; loss: 44.520; l2dist: 4.619\n",
      "    step: 100; loss: 27.768; l2dist: 4.258\n",
      "    step: 150; loss: 19.918; l2dist: 3.895\n",
      "    step: 200; loss: 16.424; l2dist: 3.656\n",
      "    step: 250; loss: 14.502; l2dist: 3.493\n",
      "    step: 300; loss: 13.809; l2dist: 3.416\n",
      "    step: 350; loss: 13.062; l2dist: 3.330\n",
      "    step: 400; loss: 12.421; l2dist: 3.273\n",
      "    step: 450; loss: 12.284; l2dist: 3.242\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 140.948; l2dist: 0.000\n",
      "    step: 50; loss: 34.323; l2dist: 4.130\n",
      "    step: 100; loss: 24.294; l2dist: 3.843\n",
      "    step: 150; loss: 17.812; l2dist: 3.645\n",
      "    step: 200; loss: 14.974; l2dist: 3.473\n",
      "    step: 250; loss: 13.475; l2dist: 3.385\n",
      "    step: 300; loss: 12.671; l2dist: 3.283\n",
      "    step: 350; loss: 12.011; l2dist: 3.216\n",
      "    step: 400; loss: 11.650; l2dist: 3.189\n",
      "    step: 450; loss: 11.553; l2dist: 3.167\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 102.381; l2dist: 0.000\n",
      "    step: 50; loss: 28.968; l2dist: 3.742\n",
      "    step: 100; loss: 22.406; l2dist: 3.507\n",
      "    step: 150; loss: 17.111; l2dist: 3.433\n",
      "    step: 200; loss: 14.182; l2dist: 3.357\n",
      "    step: 250; loss: 13.103; l2dist: 3.281\n",
      "    step: 300; loss: 11.998; l2dist: 3.210\n",
      "    step: 350; loss: 11.618; l2dist: 3.166\n",
      "    step: 400; loss: 11.428; l2dist: 3.136\n",
      "    step: 450; loss: 11.211; l2dist: 3.115\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.897; l2dist: 0.000\n",
      "    step: 50; loss: 26.679; l2dist: 3.563\n",
      "    step: 100; loss: 21.446; l2dist: 3.385\n",
      "    step: 150; loss: 17.229; l2dist: 3.340\n",
      "    step: 200; loss: 14.180; l2dist: 3.302\n",
      "    step: 250; loss: 12.843; l2dist: 3.261\n",
      "    step: 300; loss: 12.047; l2dist: 3.204\n",
      "    step: 350; loss: 11.720; l2dist: 3.163\n",
      "    step: 400; loss: 11.421; l2dist: 3.140\n",
      "    step: 450; loss: 11.097; l2dist: 3.101\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 81.756; l2dist: 0.000\n",
      "    step: 50; loss: 25.382; l2dist: 3.464\n",
      "    step: 100; loss: 20.874; l2dist: 3.311\n",
      "    step: 150; loss: 17.052; l2dist: 3.310\n",
      "    step: 200; loss: 14.371; l2dist: 3.278\n",
      "    step: 250; loss: 12.857; l2dist: 3.230\n",
      "    step: 300; loss: 12.208; l2dist: 3.214\n",
      "    step: 350; loss: 11.702; l2dist: 3.146\n",
      "    step: 400; loss: 11.429; l2dist: 3.128\n",
      "    step: 450; loss: 11.171; l2dist: 3.097\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 80.903; l2dist: 0.000\n",
      "    step: 50; loss: 25.066; l2dist: 3.456\n",
      "    step: 100; loss: 20.411; l2dist: 3.307\n",
      "    step: 150; loss: 16.888; l2dist: 3.275\n",
      "    step: 200; loss: 14.263; l2dist: 3.260\n",
      "    step: 250; loss: 12.840; l2dist: 3.234\n",
      "    step: 300; loss: 11.978; l2dist: 3.184\n",
      "    step: 350; loss: 11.542; l2dist: 3.145\n",
      "    step: 400; loss: 11.338; l2dist: 3.126\n",
      "    step: 450; loss: 10.972; l2dist: 3.082\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.584; l2dist: 0.000\n",
      "    step: 50; loss: 24.755; l2dist: 3.436\n",
      "    step: 100; loss: 20.446; l2dist: 3.273\n",
      "    step: 150; loss: 16.788; l2dist: 3.260\n",
      "    step: 200; loss: 14.241; l2dist: 3.238\n",
      "    step: 250; loss: 12.794; l2dist: 3.202\n",
      "    step: 300; loss: 12.037; l2dist: 3.178\n",
      "    step: 350; loss: 11.588; l2dist: 3.127\n",
      "    step: 400; loss: 11.389; l2dist: 3.104\n",
      "    step: 450; loss: 11.230; l2dist: 3.072\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 80.700; l2dist: 0.000\n",
      "    step: 50; loss: 25.007; l2dist: 3.448\n",
      "    step: 100; loss: 20.469; l2dist: 3.305\n",
      "    step: 150; loss: 16.752; l2dist: 3.277\n",
      "    step: 200; loss: 14.193; l2dist: 3.252\n",
      "    step: 250; loss: 12.805; l2dist: 3.210\n",
      "    step: 300; loss: 12.037; l2dist: 3.189\n",
      "    step: 350; loss: 11.483; l2dist: 3.136\n",
      "    step: 400; loss: 11.262; l2dist: 3.111\n",
      "    step: 450; loss: 10.975; l2dist: 3.079\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.513; l2dist: 0.000\n",
      "    step: 50; loss: 14.677; l2dist: 1.890\n",
      "    step: 100; loss: 14.294; l2dist: 1.923\n",
      "    step: 150; loss: 13.599; l2dist: 2.058\n",
      "    step: 200; loss: 12.673; l2dist: 2.175\n",
      "    step: 250; loss: 12.023; l2dist: 2.193\n",
      "    step: 300; loss: 11.637; l2dist: 2.208\n",
      "    step: 350; loss: 11.317; l2dist: 2.232\n",
      "    step: 400; loss: 11.060; l2dist: 2.232\n",
      "    step: 450; loss: 10.928; l2dist: 2.225\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 126.307; l2dist: 0.000\n",
      "    step: 50; loss: 37.540; l2dist: 4.343\n",
      "    step: 100; loss: 24.457; l2dist: 3.869\n",
      "    step: 150; loss: 16.438; l2dist: 3.533\n",
      "    step: 200; loss: 14.055; l2dist: 3.416\n",
      "    step: 250; loss: 12.873; l2dist: 3.273\n",
      "    step: 300; loss: 12.219; l2dist: 3.212\n",
      "    step: 350; loss: 12.154; l2dist: 3.198\n",
      "    step: 400; loss: 11.681; l2dist: 3.149\n",
      "    step: 450; loss: 11.364; l2dist: 3.104\n",
      "binary step: 1; number of successful adv: 87/100\n",
      "    step: 0; loss: 205.209; l2dist: 0.000\n",
      "    step: 50; loss: 38.832; l2dist: 4.340\n",
      "    step: 100; loss: 25.204; l2dist: 4.076\n",
      "    step: 150; loss: 17.950; l2dist: 3.711\n",
      "    step: 200; loss: 14.870; l2dist: 3.482\n",
      "    step: 250; loss: 13.751; l2dist: 3.386\n",
      "    step: 300; loss: 12.941; l2dist: 3.273\n",
      "    step: 350; loss: 12.234; l2dist: 3.198\n",
      "    step: 400; loss: 12.208; l2dist: 3.194\n",
      "    step: 450; loss: 11.999; l2dist: 3.176\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 204.005; l2dist: 0.000\n",
      "    step: 50; loss: 31.363; l2dist: 4.030\n",
      "    step: 100; loss: 22.465; l2dist: 3.733\n",
      "    step: 150; loss: 17.340; l2dist: 3.532\n",
      "    step: 200; loss: 14.446; l2dist: 3.348\n",
      "    step: 250; loss: 12.871; l2dist: 3.252\n",
      "    step: 300; loss: 12.565; l2dist: 3.241\n",
      "    step: 350; loss: 12.080; l2dist: 3.193\n",
      "    step: 400; loss: 11.596; l2dist: 3.135\n",
      "    step: 450; loss: 11.712; l2dist: 3.159\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 134.830; l2dist: 0.000\n",
      "    step: 50; loss: 27.002; l2dist: 3.690\n",
      "    step: 100; loss: 20.334; l2dist: 3.428\n",
      "    step: 150; loss: 15.979; l2dist: 3.340\n",
      "    step: 200; loss: 13.544; l2dist: 3.231\n",
      "    step: 250; loss: 12.243; l2dist: 3.154\n",
      "    step: 300; loss: 11.592; l2dist: 3.118\n",
      "    step: 350; loss: 11.684; l2dist: 3.135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 100.747; l2dist: 0.000\n",
      "    step: 50; loss: 24.422; l2dist: 3.436\n",
      "    step: 100; loss: 19.352; l2dist: 3.219\n",
      "    step: 150; loss: 15.501; l2dist: 3.204\n",
      "    step: 200; loss: 13.459; l2dist: 3.135\n",
      "    step: 250; loss: 12.279; l2dist: 3.120\n",
      "    step: 300; loss: 11.608; l2dist: 3.083\n",
      "    step: 350; loss: 11.488; l2dist: 3.079\n",
      "    step: 400; loss: 10.908; l2dist: 3.033\n",
      "    step: 450; loss: 10.899; l2dist: 3.024\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.673; l2dist: 0.000\n",
      "    step: 50; loss: 23.617; l2dist: 3.298\n",
      "    step: 100; loss: 19.064; l2dist: 3.145\n",
      "    step: 150; loss: 15.662; l2dist: 3.126\n",
      "    step: 200; loss: 13.605; l2dist: 3.086\n",
      "    step: 250; loss: 12.475; l2dist: 3.072\n",
      "    step: 300; loss: 11.743; l2dist: 3.055\n",
      "    step: 350; loss: 11.599; l2dist: 3.056\n",
      "    step: 400; loss: 11.267; l2dist: 3.026\n",
      "    step: 450; loss: 10.918; l2dist: 2.994\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.623; l2dist: 0.000\n",
      "    step: 50; loss: 22.925; l2dist: 3.246\n",
      "    step: 100; loss: 18.538; l2dist: 3.118\n",
      "    step: 150; loss: 15.316; l2dist: 3.120\n",
      "    step: 200; loss: 13.271; l2dist: 3.095\n",
      "    step: 250; loss: 12.124; l2dist: 3.065\n",
      "    step: 300; loss: 11.437; l2dist: 3.043\n",
      "    step: 350; loss: 11.134; l2dist: 3.010\n",
      "    step: 400; loss: 11.084; l2dist: 3.012\n",
      "    step: 450; loss: 10.858; l2dist: 3.001\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.838; l2dist: 0.000\n",
      "    step: 50; loss: 22.696; l2dist: 3.230\n",
      "    step: 100; loss: 18.596; l2dist: 3.105\n",
      "    step: 150; loss: 15.133; l2dist: 3.115\n",
      "    step: 200; loss: 13.176; l2dist: 3.090\n",
      "    step: 250; loss: 12.041; l2dist: 3.059\n",
      "    step: 300; loss: 11.398; l2dist: 3.049\n",
      "    step: 350; loss: 11.111; l2dist: 3.026\n",
      "    step: 400; loss: 10.963; l2dist: 3.005\n",
      "    step: 450; loss: 10.768; l2dist: 2.983\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 80.424; l2dist: 0.000\n",
      "    step: 50; loss: 22.806; l2dist: 3.250\n",
      "    step: 100; loss: 18.583; l2dist: 3.100\n",
      "    step: 150; loss: 15.240; l2dist: 3.130\n",
      "    step: 200; loss: 13.215; l2dist: 3.084\n",
      "    step: 250; loss: 12.132; l2dist: 3.055\n",
      "    step: 300; loss: 11.437; l2dist: 3.043\n",
      "    step: 350; loss: 11.053; l2dist: 3.010\n",
      "    step: 400; loss: 10.838; l2dist: 2.991\n",
      "    step: 450; loss: 10.785; l2dist: 2.993\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.917; l2dist: 0.000\n",
      "    step: 50; loss: 15.432; l2dist: 1.878\n",
      "    step: 100; loss: 14.957; l2dist: 1.906\n",
      "    step: 150; loss: 14.394; l2dist: 1.994\n",
      "    step: 200; loss: 13.957; l2dist: 2.071\n",
      "    step: 250; loss: 13.590; l2dist: 2.108\n",
      "    step: 300; loss: 13.228; l2dist: 2.158\n",
      "    step: 350; loss: 12.994; l2dist: 2.168\n",
      "    step: 400; loss: 12.806; l2dist: 2.177\n",
      "    step: 450; loss: 12.737; l2dist: 2.172\n",
      "binary step: 0; number of successful adv: 22/100\n",
      "    step: 0; loss: 161.186; l2dist: 0.000\n",
      "    step: 50; loss: 44.067; l2dist: 4.761\n",
      "    step: 100; loss: 25.815; l2dist: 4.320\n",
      "    step: 150; loss: 17.946; l2dist: 3.868\n",
      "    step: 200; loss: 15.494; l2dist: 3.602\n",
      "    step: 250; loss: 14.260; l2dist: 3.473\n",
      "    step: 300; loss: 13.593; l2dist: 3.395\n",
      "    step: 350; loss: 13.199; l2dist: 3.345\n",
      "    step: 400; loss: 12.812; l2dist: 3.313\n",
      "    step: 450; loss: 12.689; l2dist: 3.286\n",
      "binary step: 1; number of successful adv: 96/100\n",
      "    step: 0; loss: 132.321; l2dist: 0.000\n",
      "    step: 50; loss: 36.738; l2dist: 4.349\n",
      "    step: 100; loss: 24.367; l2dist: 4.073\n",
      "    step: 150; loss: 17.458; l2dist: 3.723\n",
      "    step: 200; loss: 15.082; l2dist: 3.522\n",
      "    step: 250; loss: 13.646; l2dist: 3.409\n",
      "    step: 300; loss: 13.146; l2dist: 3.357\n",
      "    step: 350; loss: 12.604; l2dist: 3.303\n",
      "    step: 400; loss: 12.493; l2dist: 3.291\n",
      "    step: 450; loss: 12.364; l2dist: 3.267\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 84.486; l2dist: 0.000\n",
      "    step: 50; loss: 29.695; l2dist: 3.800\n",
      "    step: 100; loss: 22.462; l2dist: 3.702\n",
      "    step: 150; loss: 17.242; l2dist: 3.562\n",
      "    step: 200; loss: 14.536; l2dist: 3.418\n",
      "    step: 250; loss: 13.312; l2dist: 3.342\n",
      "    step: 300; loss: 12.864; l2dist: 3.315\n",
      "    step: 350; loss: 12.441; l2dist: 3.260\n",
      "    step: 400; loss: 12.183; l2dist: 3.240\n",
      "    step: 450; loss: 12.063; l2dist: 3.228\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.449; l2dist: 0.000\n",
      "    step: 50; loss: 26.062; l2dist: 3.497\n",
      "    step: 100; loss: 21.191; l2dist: 3.436\n",
      "    step: 150; loss: 17.111; l2dist: 3.444\n",
      "    step: 200; loss: 14.790; l2dist: 3.332\n",
      "    step: 250; loss: 13.784; l2dist: 3.261\n",
      "    step: 300; loss: 12.774; l2dist: 3.241\n",
      "    step: 350; loss: 12.421; l2dist: 3.214\n",
      "    step: 400; loss: 12.159; l2dist: 3.198\n",
      "    step: 450; loss: 12.057; l2dist: 3.181\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.620; l2dist: 0.000\n",
      "    step: 50; loss: 24.783; l2dist: 3.375\n",
      "    step: 100; loss: 20.736; l2dist: 3.311\n",
      "    step: 150; loss: 17.162; l2dist: 3.382\n",
      "    step: 200; loss: 14.880; l2dist: 3.308\n",
      "    step: 250; loss: 13.315; l2dist: 3.258\n",
      "    step: 300; loss: 12.575; l2dist: 3.216\n",
      "    step: 350; loss: 12.139; l2dist: 3.196\n",
      "    step: 400; loss: 11.885; l2dist: 3.174\n",
      "    step: 450; loss: 11.753; l2dist: 3.156\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.327; l2dist: 0.000\n",
      "    step: 50; loss: 24.425; l2dist: 3.328\n",
      "    step: 100; loss: 20.499; l2dist: 3.260\n",
      "    step: 150; loss: 17.318; l2dist: 3.318\n",
      "    step: 200; loss: 15.270; l2dist: 3.286\n",
      "    step: 250; loss: 13.918; l2dist: 3.270\n",
      "    step: 300; loss: 12.850; l2dist: 3.225\n",
      "    step: 350; loss: 12.382; l2dist: 3.206\n",
      "    step: 400; loss: 12.214; l2dist: 3.193\n",
      "    step: 450; loss: 11.989; l2dist: 3.171\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.729; l2dist: 0.000\n",
      "    step: 50; loss: 24.219; l2dist: 3.297\n",
      "    step: 100; loss: 20.449; l2dist: 3.244\n",
      "    step: 150; loss: 17.349; l2dist: 3.306\n",
      "    step: 200; loss: 15.377; l2dist: 3.283\n",
      "    step: 250; loss: 14.051; l2dist: 3.252\n",
      "    step: 300; loss: 12.992; l2dist: 3.224\n",
      "    step: 350; loss: 12.379; l2dist: 3.193\n",
      "    step: 400; loss: 12.124; l2dist: 3.170\n",
      "    step: 450; loss: 11.974; l2dist: 3.158\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.391; l2dist: 0.000\n",
      "    step: 50; loss: 24.111; l2dist: 3.285\n",
      "    step: 100; loss: 20.353; l2dist: 3.244\n",
      "    step: 150; loss: 17.292; l2dist: 3.300\n",
      "    step: 200; loss: 15.434; l2dist: 3.281\n",
      "    step: 250; loss: 14.094; l2dist: 3.239\n",
      "    step: 300; loss: 12.923; l2dist: 3.215\n",
      "    step: 350; loss: 12.370; l2dist: 3.196\n",
      "    step: 400; loss: 12.035; l2dist: 3.171\n",
      "    step: 450; loss: 11.735; l2dist: 3.148\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.147; l2dist: 0.000\n",
      "    step: 50; loss: 24.266; l2dist: 3.298\n",
      "    step: 100; loss: 20.562; l2dist: 3.257\n",
      "    step: 150; loss: 17.382; l2dist: 3.310\n",
      "    step: 200; loss: 15.412; l2dist: 3.289\n",
      "    step: 250; loss: 14.122; l2dist: 3.249\n",
      "    step: 300; loss: 12.969; l2dist: 3.217\n",
      "    step: 350; loss: 12.452; l2dist: 3.200\n",
      "    step: 400; loss: 12.137; l2dist: 3.187\n",
      "    step: 450; loss: 12.017; l2dist: 3.185\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.075; l2dist: 0.000\n",
      "    step: 50; loss: 17.557; l2dist: 1.986\n",
      "    step: 100; loss: 17.069; l2dist: 2.047\n",
      "    step: 150; loss: 16.341; l2dist: 2.179\n",
      "    step: 200; loss: 15.658; l2dist: 2.283\n",
      "    step: 250; loss: 15.096; l2dist: 2.337\n",
      "    step: 300; loss: 14.714; l2dist: 2.360\n",
      "    step: 350; loss: 14.515; l2dist: 2.371\n",
      "    step: 400; loss: 14.396; l2dist: 2.380\n",
      "    step: 450; loss: 14.343; l2dist: 2.378\n",
      "binary step: 0; number of successful adv: 20/100\n",
      "    step: 0; loss: 176.292; l2dist: 0.000\n",
      "    step: 50; loss: 51.136; l2dist: 5.172\n",
      "    step: 100; loss: 28.282; l2dist: 4.690\n",
      "    step: 150; loss: 20.503; l2dist: 4.166\n",
      "    step: 200; loss: 18.030; l2dist: 3.940\n",
      "    step: 250; loss: 16.536; l2dist: 3.776\n",
      "    step: 300; loss: 15.608; l2dist: 3.691\n",
      "    step: 350; loss: 15.169; l2dist: 3.641\n",
      "    step: 400; loss: 14.911; l2dist: 3.613\n",
      "    step: 450; loss: 14.619; l2dist: 3.581\n",
      "binary step: 1; number of successful adv: 85/100\n",
      "    step: 0; loss: 356.598; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 67.530; l2dist: 5.547\n",
      "    step: 100; loss: 33.443; l2dist: 5.043\n",
      "    step: 150; loss: 24.217; l2dist: 4.495\n",
      "    step: 200; loss: 20.004; l2dist: 4.128\n",
      "    step: 250; loss: 18.018; l2dist: 3.937\n",
      "    step: 300; loss: 17.324; l2dist: 3.859\n",
      "    step: 350; loss: 16.369; l2dist: 3.777\n",
      "    step: 400; loss: 15.896; l2dist: 3.717\n",
      "    step: 450; loss: 15.718; l2dist: 3.696\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 208.901; l2dist: 0.000\n",
      "    step: 50; loss: 50.665; l2dist: 4.956\n",
      "    step: 100; loss: 28.619; l2dist: 4.549\n",
      "    step: 150; loss: 20.543; l2dist: 4.152\n",
      "    step: 200; loss: 17.821; l2dist: 3.919\n",
      "    step: 250; loss: 16.575; l2dist: 3.785\n",
      "    step: 300; loss: 15.933; l2dist: 3.726\n",
      "    step: 350; loss: 15.290; l2dist: 3.652\n",
      "    step: 400; loss: 14.968; l2dist: 3.614\n",
      "    step: 450; loss: 14.712; l2dist: 3.592\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 159.137; l2dist: 0.000\n",
      "    step: 50; loss: 42.807; l2dist: 4.598\n",
      "    step: 100; loss: 26.457; l2dist: 4.238\n",
      "    step: 150; loss: 19.430; l2dist: 3.965\n",
      "    step: 200; loss: 17.201; l2dist: 3.828\n",
      "    step: 250; loss: 15.877; l2dist: 3.695\n",
      "    step: 300; loss: 15.325; l2dist: 3.643\n",
      "    step: 350; loss: 14.894; l2dist: 3.605\n",
      "    step: 400; loss: 14.688; l2dist: 3.574\n",
      "    step: 450; loss: 14.474; l2dist: 3.557\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 134.380; l2dist: 0.000\n",
      "    step: 50; loss: 37.790; l2dist: 4.290\n",
      "    step: 100; loss: 25.075; l2dist: 4.017\n",
      "    step: 150; loss: 19.111; l2dist: 3.846\n",
      "    step: 200; loss: 16.978; l2dist: 3.754\n",
      "    step: 250; loss: 15.791; l2dist: 3.679\n",
      "    step: 300; loss: 14.888; l2dist: 3.596\n",
      "    step: 350; loss: 14.527; l2dist: 3.561\n",
      "    step: 400; loss: 14.353; l2dist: 3.543\n",
      "    step: 450; loss: 14.291; l2dist: 3.537\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 122.402; l2dist: 0.000\n",
      "    step: 50; loss: 35.667; l2dist: 4.153\n",
      "    step: 100; loss: 24.657; l2dist: 3.938\n",
      "    step: 150; loss: 18.847; l2dist: 3.819\n",
      "    step: 200; loss: 16.719; l2dist: 3.735\n",
      "    step: 250; loss: 15.578; l2dist: 3.643\n",
      "    step: 300; loss: 14.998; l2dist: 3.599\n",
      "    step: 350; loss: 14.680; l2dist: 3.550\n",
      "    step: 400; loss: 14.438; l2dist: 3.538\n",
      "    step: 450; loss: 14.229; l2dist: 3.523\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 117.126; l2dist: 0.000\n",
      "    step: 50; loss: 34.437; l2dist: 4.074\n",
      "    step: 100; loss: 24.439; l2dist: 3.878\n",
      "    step: 150; loss: 18.877; l2dist: 3.792\n",
      "    step: 200; loss: 16.463; l2dist: 3.702\n",
      "    step: 250; loss: 15.319; l2dist: 3.620\n",
      "    step: 300; loss: 14.824; l2dist: 3.579\n",
      "    step: 350; loss: 14.299; l2dist: 3.528\n",
      "    step: 400; loss: 13.993; l2dist: 3.502\n",
      "    step: 450; loss: 13.945; l2dist: 3.497\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 114.915; l2dist: 0.000\n",
      "    step: 50; loss: 34.035; l2dist: 4.030\n",
      "    step: 100; loss: 24.375; l2dist: 3.858\n",
      "    step: 150; loss: 18.848; l2dist: 3.783\n",
      "    step: 200; loss: 16.479; l2dist: 3.710\n",
      "    step: 250; loss: 15.453; l2dist: 3.626\n",
      "    step: 300; loss: 14.817; l2dist: 3.562\n",
      "    step: 350; loss: 14.467; l2dist: 3.533\n",
      "    step: 400; loss: 14.223; l2dist: 3.510\n",
      "    step: 450; loss: 14.041; l2dist: 3.496\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.981; l2dist: 0.000\n",
      "    step: 50; loss: 34.212; l2dist: 4.042\n",
      "    step: 100; loss: 24.458; l2dist: 3.865\n",
      "    step: 150; loss: 18.830; l2dist: 3.792\n",
      "    step: 200; loss: 16.309; l2dist: 3.707\n",
      "    step: 250; loss: 15.466; l2dist: 3.641\n",
      "    step: 300; loss: 14.697; l2dist: 3.561\n",
      "    step: 350; loss: 14.460; l2dist: 3.538\n",
      "    step: 400; loss: 14.165; l2dist: 3.510\n",
      "    step: 450; loss: 13.988; l2dist: 3.499\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.492; l2dist: 0.000\n",
      "    step: 50; loss: 16.913; l2dist: 1.958\n",
      "    step: 100; loss: 16.449; l2dist: 2.003\n",
      "    step: 150; loss: 15.613; l2dist: 2.156\n",
      "    step: 200; loss: 14.948; l2dist: 2.217\n",
      "    step: 250; loss: 14.454; l2dist: 2.266\n",
      "    step: 300; loss: 14.040; l2dist: 2.290\n",
      "    step: 350; loss: 13.756; l2dist: 2.288\n",
      "    step: 400; loss: 13.547; l2dist: 2.299\n",
      "    step: 450; loss: 13.410; l2dist: 2.300\n",
      "binary step: 0; number of successful adv: 19/100\n",
      "    step: 0; loss: 167.987; l2dist: 0.000\n",
      "    step: 50; loss: 53.328; l2dist: 5.089\n",
      "    step: 100; loss: 28.086; l2dist: 4.591\n",
      "    step: 150; loss: 19.800; l2dist: 4.097\n",
      "    step: 200; loss: 16.816; l2dist: 3.794\n",
      "    step: 250; loss: 15.514; l2dist: 3.670\n",
      "    step: 300; loss: 14.710; l2dist: 3.570\n",
      "    step: 350; loss: 14.231; l2dist: 3.523\n",
      "    step: 400; loss: 13.778; l2dist: 3.479\n",
      "    step: 450; loss: 13.630; l2dist: 3.456\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 227.496; l2dist: 0.000\n",
      "    step: 50; loss: 50.780; l2dist: 4.982\n",
      "    step: 100; loss: 29.474; l2dist: 4.700\n",
      "    step: 150; loss: 20.855; l2dist: 4.197\n",
      "    step: 200; loss: 17.725; l2dist: 3.924\n",
      "    step: 250; loss: 16.059; l2dist: 3.759\n",
      "    step: 300; loss: 15.085; l2dist: 3.656\n",
      "    step: 350; loss: 14.551; l2dist: 3.588\n",
      "    step: 400; loss: 14.273; l2dist: 3.554\n",
      "    step: 450; loss: 14.010; l2dist: 3.532\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 146.494; l2dist: 0.000\n",
      "    step: 50; loss: 41.439; l2dist: 4.444\n",
      "    step: 100; loss: 27.660; l2dist: 4.229\n",
      "    step: 150; loss: 19.559; l2dist: 4.019\n",
      "    step: 200; loss: 16.359; l2dist: 3.763\n",
      "    step: 250; loss: 14.902; l2dist: 3.618\n",
      "    step: 300; loss: 14.179; l2dist: 3.545\n",
      "    step: 350; loss: 13.786; l2dist: 3.493\n",
      "    step: 400; loss: 13.496; l2dist: 3.456\n",
      "    step: 450; loss: 13.316; l2dist: 3.445\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 114.492; l2dist: 0.000\n",
      "    step: 50; loss: 36.553; l2dist: 4.122\n",
      "    step: 100; loss: 26.190; l2dist: 3.987\n",
      "    step: 150; loss: 19.400; l2dist: 3.906\n",
      "    step: 200; loss: 16.073; l2dist: 3.730\n",
      "    step: 250; loss: 14.747; l2dist: 3.613\n",
      "    step: 300; loss: 13.943; l2dist: 3.528\n",
      "    step: 350; loss: 13.627; l2dist: 3.485\n",
      "    step: 400; loss: 13.205; l2dist: 3.450\n",
      "    step: 450; loss: 13.001; l2dist: 3.423\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 99.350; l2dist: 0.000\n",
      "    step: 50; loss: 33.931; l2dist: 3.974\n",
      "    step: 100; loss: 25.296; l2dist: 3.835\n",
      "    step: 150; loss: 19.469; l2dist: 3.796\n",
      "    step: 200; loss: 15.910; l2dist: 3.699\n",
      "    step: 250; loss: 14.777; l2dist: 3.605\n",
      "    step: 300; loss: 13.850; l2dist: 3.507\n",
      "    step: 350; loss: 13.448; l2dist: 3.463\n",
      "    step: 400; loss: 13.186; l2dist: 3.433\n",
      "    step: 450; loss: 13.061; l2dist: 3.421\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.290; l2dist: 0.000\n",
      "    step: 50; loss: 32.985; l2dist: 3.857\n",
      "    step: 100; loss: 25.339; l2dist: 3.765\n",
      "    step: 150; loss: 19.691; l2dist: 3.739\n",
      "    step: 200; loss: 16.148; l2dist: 3.663\n",
      "    step: 250; loss: 14.552; l2dist: 3.563\n",
      "    step: 300; loss: 13.826; l2dist: 3.495\n",
      "    step: 350; loss: 13.326; l2dist: 3.444\n",
      "    step: 400; loss: 13.028; l2dist: 3.417\n",
      "    step: 450; loss: 12.913; l2dist: 3.408\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.368; l2dist: 0.000\n",
      "    step: 50; loss: 32.468; l2dist: 3.844\n",
      "    step: 100; loss: 25.141; l2dist: 3.758\n",
      "    step: 150; loss: 19.360; l2dist: 3.734\n",
      "    step: 200; loss: 16.017; l2dist: 3.648\n",
      "    step: 250; loss: 14.649; l2dist: 3.554\n",
      "    step: 300; loss: 14.005; l2dist: 3.498\n",
      "    step: 350; loss: 13.410; l2dist: 3.438\n",
      "    step: 400; loss: 13.137; l2dist: 3.414\n",
      "    step: 450; loss: 12.962; l2dist: 3.400\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 87.569; l2dist: 0.000\n",
      "    step: 50; loss: 32.127; l2dist: 3.817\n",
      "    step: 100; loss: 24.987; l2dist: 3.747\n",
      "    step: 150; loss: 19.445; l2dist: 3.707\n",
      "    step: 200; loss: 16.134; l2dist: 3.642\n",
      "    step: 250; loss: 14.622; l2dist: 3.551\n",
      "    step: 300; loss: 14.035; l2dist: 3.492\n",
      "    step: 350; loss: 13.514; l2dist: 3.458\n",
      "    step: 400; loss: 13.263; l2dist: 3.434\n",
      "    step: 450; loss: 12.900; l2dist: 3.395\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.322; l2dist: 0.000\n",
      "    step: 50; loss: 32.277; l2dist: 3.823\n",
      "    step: 100; loss: 24.990; l2dist: 3.753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 19.262; l2dist: 3.733\n",
      "    step: 200; loss: 16.067; l2dist: 3.652\n",
      "    step: 250; loss: 14.663; l2dist: 3.565\n",
      "    step: 300; loss: 13.958; l2dist: 3.494\n",
      "    step: 350; loss: 13.543; l2dist: 3.454\n",
      "    step: 400; loss: 13.182; l2dist: 3.421\n",
      "    step: 450; loss: 13.052; l2dist: 3.399\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.275; l2dist: 0.000\n",
      "    step: 50; loss: 15.573; l2dist: 1.867\n",
      "    step: 100; loss: 15.203; l2dist: 1.887\n",
      "    step: 150; loss: 14.600; l2dist: 1.991\n",
      "    step: 200; loss: 13.948; l2dist: 2.055\n",
      "    step: 250; loss: 13.523; l2dist: 2.078\n",
      "    step: 300; loss: 13.368; l2dist: 2.091\n",
      "    step: 350; loss: 13.245; l2dist: 2.112\n",
      "    step: 400; loss: 13.171; l2dist: 2.126\n",
      "    step: 450; loss: 13.118; l2dist: 2.144\n",
      "binary step: 0; number of successful adv: 26/100\n",
      "    step: 0; loss: 152.482; l2dist: 0.000\n",
      "    step: 50; loss: 46.536; l2dist: 4.732\n",
      "    step: 100; loss: 28.470; l2dist: 4.300\n",
      "    step: 150; loss: 18.650; l2dist: 3.868\n",
      "    step: 200; loss: 15.898; l2dist: 3.622\n",
      "    step: 250; loss: 14.627; l2dist: 3.502\n",
      "    step: 300; loss: 13.812; l2dist: 3.401\n",
      "    step: 350; loss: 13.319; l2dist: 3.355\n",
      "    step: 400; loss: 12.903; l2dist: 3.309\n",
      "    step: 450; loss: 12.795; l2dist: 3.292\n",
      "binary step: 1; number of successful adv: 87/100\n",
      "    step: 0; loss: 241.930; l2dist: 0.000\n",
      "    step: 50; loss: 48.805; l2dist: 4.805\n",
      "    step: 100; loss: 30.049; l2dist: 4.546\n",
      "    step: 150; loss: 21.063; l2dist: 4.098\n",
      "    step: 200; loss: 17.092; l2dist: 3.799\n",
      "    step: 250; loss: 15.172; l2dist: 3.604\n",
      "    step: 300; loss: 13.904; l2dist: 3.489\n",
      "    step: 350; loss: 13.371; l2dist: 3.420\n",
      "    step: 400; loss: 12.958; l2dist: 3.361\n",
      "    step: 450; loss: 13.191; l2dist: 3.382\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 146.442; l2dist: 0.000\n",
      "    step: 50; loss: 37.543; l2dist: 4.263\n",
      "    step: 100; loss: 26.365; l2dist: 4.055\n",
      "    step: 150; loss: 18.887; l2dist: 3.805\n",
      "    step: 200; loss: 15.504; l2dist: 3.604\n",
      "    step: 250; loss: 14.140; l2dist: 3.461\n",
      "    step: 300; loss: 13.519; l2dist: 3.410\n",
      "    step: 350; loss: 13.003; l2dist: 3.340\n",
      "    step: 400; loss: 12.446; l2dist: 3.298\n",
      "    step: 450; loss: 12.490; l2dist: 3.284\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 105.003; l2dist: 0.000\n",
      "    step: 50; loss: 31.383; l2dist: 3.954\n",
      "    step: 100; loss: 23.595; l2dist: 3.760\n",
      "    step: 150; loss: 17.806; l2dist: 3.610\n",
      "    step: 200; loss: 14.660; l2dist: 3.459\n",
      "    step: 250; loss: 13.623; l2dist: 3.371\n",
      "    step: 300; loss: 12.729; l2dist: 3.287\n",
      "    step: 350; loss: 12.211; l2dist: 3.237\n",
      "    step: 400; loss: 11.929; l2dist: 3.210\n",
      "    step: 450; loss: 11.899; l2dist: 3.200\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 87.109; l2dist: 0.000\n",
      "    step: 50; loss: 28.531; l2dist: 3.743\n",
      "    step: 100; loss: 22.385; l2dist: 3.589\n",
      "    step: 150; loss: 17.204; l2dist: 3.494\n",
      "    step: 200; loss: 14.641; l2dist: 3.361\n",
      "    step: 250; loss: 13.369; l2dist: 3.289\n",
      "    step: 300; loss: 12.786; l2dist: 3.254\n",
      "    step: 350; loss: 12.382; l2dist: 3.243\n",
      "    step: 400; loss: 11.871; l2dist: 3.191\n",
      "    step: 450; loss: 11.548; l2dist: 3.156\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 80.624; l2dist: 0.000\n",
      "    step: 50; loss: 27.358; l2dist: 3.646\n",
      "    step: 100; loss: 22.103; l2dist: 3.499\n",
      "    step: 150; loss: 17.302; l2dist: 3.443\n",
      "    step: 200; loss: 14.689; l2dist: 3.328\n",
      "    step: 250; loss: 13.280; l2dist: 3.261\n",
      "    step: 300; loss: 12.877; l2dist: 3.249\n",
      "    step: 350; loss: 12.368; l2dist: 3.225\n",
      "    step: 400; loss: 12.184; l2dist: 3.208\n",
      "    step: 450; loss: 11.842; l2dist: 3.179\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.671; l2dist: 0.000\n",
      "    step: 50; loss: 27.188; l2dist: 3.607\n",
      "    step: 100; loss: 22.067; l2dist: 3.477\n",
      "    step: 150; loss: 17.267; l2dist: 3.428\n",
      "    step: 200; loss: 14.729; l2dist: 3.332\n",
      "    step: 250; loss: 13.200; l2dist: 3.273\n",
      "    step: 300; loss: 12.678; l2dist: 3.245\n",
      "    step: 350; loss: 12.114; l2dist: 3.192\n",
      "    step: 400; loss: 11.922; l2dist: 3.181\n",
      "    step: 450; loss: 11.565; l2dist: 3.152\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 77.763; l2dist: 0.000\n",
      "    step: 50; loss: 27.012; l2dist: 3.605\n",
      "    step: 100; loss: 22.082; l2dist: 3.490\n",
      "    step: 150; loss: 17.201; l2dist: 3.434\n",
      "    step: 200; loss: 14.488; l2dist: 3.333\n",
      "    step: 250; loss: 13.113; l2dist: 3.258\n",
      "    step: 300; loss: 12.720; l2dist: 3.235\n",
      "    step: 350; loss: 12.429; l2dist: 3.212\n",
      "    step: 400; loss: 11.908; l2dist: 3.184\n",
      "    step: 450; loss: 11.622; l2dist: 3.153\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.551; l2dist: 0.000\n",
      "    step: 50; loss: 27.113; l2dist: 3.620\n",
      "    step: 100; loss: 22.021; l2dist: 3.503\n",
      "    step: 150; loss: 17.304; l2dist: 3.437\n",
      "    step: 200; loss: 14.655; l2dist: 3.334\n",
      "    step: 250; loss: 13.157; l2dist: 3.271\n",
      "    step: 300; loss: 12.458; l2dist: 3.223\n",
      "    step: 350; loss: 12.081; l2dist: 3.208\n",
      "    step: 400; loss: 11.803; l2dist: 3.182\n",
      "    step: 450; loss: 11.668; l2dist: 3.168\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.039; l2dist: 0.000\n",
      "    step: 50; loss: 16.365; l2dist: 2.091\n",
      "    step: 100; loss: 15.833; l2dist: 2.138\n",
      "    step: 150; loss: 15.020; l2dist: 2.269\n",
      "    step: 200; loss: 14.177; l2dist: 2.369\n",
      "    step: 250; loss: 13.602; l2dist: 2.394\n",
      "    step: 300; loss: 13.272; l2dist: 2.405\n",
      "    step: 350; loss: 13.033; l2dist: 2.404\n",
      "    step: 400; loss: 12.876; l2dist: 2.406\n",
      "    step: 450; loss: 12.787; l2dist: 2.421\n",
      "binary step: 0; number of successful adv: 25/100\n",
      "    step: 0; loss: 159.291; l2dist: 0.000\n",
      "    step: 50; loss: 46.006; l2dist: 4.882\n",
      "    step: 100; loss: 24.207; l2dist: 4.304\n",
      "    step: 150; loss: 18.427; l2dist: 3.865\n",
      "    step: 200; loss: 15.927; l2dist: 3.618\n",
      "    step: 250; loss: 14.735; l2dist: 3.488\n",
      "    step: 300; loss: 13.719; l2dist: 3.399\n",
      "    step: 350; loss: 13.512; l2dist: 3.367\n",
      "    step: 400; loss: 13.013; l2dist: 3.322\n",
      "    step: 450; loss: 12.720; l2dist: 3.293\n",
      "binary step: 1; number of successful adv: 89/100\n",
      "    step: 0; loss: 297.343; l2dist: 0.000\n",
      "    step: 50; loss: 51.034; l2dist: 4.974\n",
      "    step: 100; loss: 27.986; l2dist: 4.589\n",
      "    step: 150; loss: 20.696; l2dist: 4.067\n",
      "    step: 200; loss: 17.528; l2dist: 3.799\n",
      "    step: 250; loss: 15.893; l2dist: 3.641\n",
      "    step: 300; loss: 14.900; l2dist: 3.543\n",
      "    step: 350; loss: 14.282; l2dist: 3.470\n",
      "    step: 400; loss: 13.772; l2dist: 3.422\n",
      "    step: 450; loss: 13.366; l2dist: 3.368\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 176.818; l2dist: 0.000\n",
      "    step: 50; loss: 38.854; l2dist: 4.461\n",
      "    step: 100; loss: 24.350; l2dist: 4.135\n",
      "    step: 150; loss: 18.372; l2dist: 3.834\n",
      "    step: 200; loss: 15.939; l2dist: 3.636\n",
      "    step: 250; loss: 14.609; l2dist: 3.500\n",
      "    step: 300; loss: 13.767; l2dist: 3.426\n",
      "    step: 350; loss: 13.500; l2dist: 3.384\n",
      "    step: 400; loss: 13.061; l2dist: 3.347\n",
      "    step: 450; loss: 12.768; l2dist: 3.322\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 122.891; l2dist: 0.000\n",
      "    step: 50; loss: 32.731; l2dist: 4.005\n",
      "    step: 100; loss: 23.087; l2dist: 3.837\n",
      "    step: 150; loss: 17.956; l2dist: 3.691\n",
      "    step: 200; loss: 15.444; l2dist: 3.555\n",
      "    step: 250; loss: 14.264; l2dist: 3.445\n",
      "    step: 300; loss: 13.464; l2dist: 3.362\n",
      "    step: 350; loss: 12.916; l2dist: 3.311\n",
      "    step: 400; loss: 12.803; l2dist: 3.308\n",
      "    step: 450; loss: 12.466; l2dist: 3.278\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 99.856; l2dist: 0.000\n",
      "    step: 50; loss: 29.668; l2dist: 3.761\n",
      "    step: 100; loss: 22.271; l2dist: 3.654\n",
      "    step: 150; loss: 17.946; l2dist: 3.551\n",
      "    step: 200; loss: 15.141; l2dist: 3.459\n",
      "    step: 250; loss: 13.823; l2dist: 3.388\n",
      "    step: 300; loss: 13.004; l2dist: 3.319\n",
      "    step: 350; loss: 12.686; l2dist: 3.285\n",
      "    step: 400; loss: 12.308; l2dist: 3.253\n",
      "    step: 450; loss: 12.063; l2dist: 3.224\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 93.673; l2dist: 0.000\n",
      "    step: 50; loss: 28.746; l2dist: 3.698\n",
      "    step: 100; loss: 21.896; l2dist: 3.571\n",
      "    step: 150; loss: 17.858; l2dist: 3.503\n",
      "    step: 200; loss: 15.120; l2dist: 3.420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 250; loss: 13.651; l2dist: 3.334\n",
      "    step: 300; loss: 13.066; l2dist: 3.293\n",
      "    step: 350; loss: 12.598; l2dist: 3.253\n",
      "    step: 400; loss: 12.325; l2dist: 3.227\n",
      "    step: 450; loss: 12.199; l2dist: 3.206\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.399; l2dist: 0.000\n",
      "    step: 50; loss: 28.358; l2dist: 3.669\n",
      "    step: 100; loss: 21.860; l2dist: 3.554\n",
      "    step: 150; loss: 17.559; l2dist: 3.527\n",
      "    step: 200; loss: 14.964; l2dist: 3.445\n",
      "    step: 250; loss: 13.572; l2dist: 3.341\n",
      "    step: 300; loss: 12.851; l2dist: 3.282\n",
      "    step: 350; loss: 12.625; l2dist: 3.255\n",
      "    step: 400; loss: 12.241; l2dist: 3.224\n",
      "    step: 450; loss: 11.980; l2dist: 3.197\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 87.593; l2dist: 0.000\n",
      "    step: 50; loss: 28.009; l2dist: 3.637\n",
      "    step: 100; loss: 21.626; l2dist: 3.525\n",
      "    step: 150; loss: 17.481; l2dist: 3.500\n",
      "    step: 200; loss: 14.984; l2dist: 3.422\n",
      "    step: 250; loss: 13.540; l2dist: 3.319\n",
      "    step: 300; loss: 12.830; l2dist: 3.266\n",
      "    step: 350; loss: 12.425; l2dist: 3.232\n",
      "    step: 400; loss: 12.195; l2dist: 3.207\n",
      "    step: 450; loss: 11.907; l2dist: 3.178\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.466; l2dist: 0.000\n",
      "    step: 50; loss: 28.264; l2dist: 3.667\n",
      "    step: 100; loss: 21.592; l2dist: 3.551\n",
      "    step: 150; loss: 17.342; l2dist: 3.512\n",
      "    step: 200; loss: 14.809; l2dist: 3.437\n",
      "    step: 250; loss: 13.536; l2dist: 3.336\n",
      "    step: 300; loss: 12.848; l2dist: 3.276\n",
      "    step: 350; loss: 12.334; l2dist: 3.230\n",
      "    step: 400; loss: 11.994; l2dist: 3.197\n",
      "    step: 450; loss: 11.903; l2dist: 3.192\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.014; l2dist: 0.000\n",
      "    step: 50; loss: 13.774; l2dist: 1.988\n",
      "    step: 100; loss: 13.435; l2dist: 2.000\n",
      "    step: 150; loss: 12.674; l2dist: 2.107\n",
      "    step: 200; loss: 12.025; l2dist: 2.181\n",
      "    step: 250; loss: 11.640; l2dist: 2.204\n",
      "    step: 300; loss: 11.410; l2dist: 2.213\n",
      "    step: 350; loss: 11.252; l2dist: 2.221\n",
      "    step: 400; loss: 11.118; l2dist: 2.223\n",
      "    step: 450; loss: 11.053; l2dist: 2.225\n",
      "binary step: 0; number of successful adv: 35/100\n",
      "    step: 0; loss: 134.212; l2dist: 0.000\n",
      "    step: 50; loss: 40.763; l2dist: 4.048\n",
      "    step: 100; loss: 20.160; l2dist: 3.814\n",
      "    step: 150; loss: 14.444; l2dist: 3.421\n",
      "    step: 200; loss: 12.470; l2dist: 3.202\n",
      "    step: 250; loss: 11.355; l2dist: 3.056\n",
      "    step: 300; loss: 10.767; l2dist: 2.979\n",
      "    step: 350; loss: 10.467; l2dist: 2.948\n",
      "    step: 400; loss: 10.206; l2dist: 2.912\n",
      "    step: 450; loss: 9.976; l2dist: 2.898\n",
      "binary step: 1; number of successful adv: 96/100\n",
      "    step: 0; loss: 139.884; l2dist: 0.000\n",
      "    step: 50; loss: 33.194; l2dist: 3.901\n",
      "    step: 100; loss: 20.263; l2dist: 3.695\n",
      "    step: 150; loss: 15.313; l2dist: 3.374\n",
      "    step: 200; loss: 12.668; l2dist: 3.198\n",
      "    step: 250; loss: 11.461; l2dist: 3.069\n",
      "    step: 300; loss: 10.793; l2dist: 3.001\n",
      "    step: 350; loss: 10.778; l2dist: 3.002\n",
      "    step: 400; loss: 10.197; l2dist: 2.925\n",
      "    step: 450; loss: 10.172; l2dist: 2.919\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 94.244; l2dist: 0.000\n",
      "    step: 50; loss: 27.888; l2dist: 3.535\n",
      "    step: 100; loss: 19.418; l2dist: 3.439\n",
      "    step: 150; loss: 14.007; l2dist: 3.240\n",
      "    step: 200; loss: 11.868; l2dist: 3.097\n",
      "    step: 250; loss: 11.000; l2dist: 3.015\n",
      "    step: 300; loss: 10.507; l2dist: 2.961\n",
      "    step: 350; loss: 10.064; l2dist: 2.906\n",
      "    step: 400; loss: 9.827; l2dist: 2.878\n",
      "    step: 450; loss: 9.709; l2dist: 2.864\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.772; l2dist: 0.000\n",
      "    step: 50; loss: 24.626; l2dist: 3.208\n",
      "    step: 100; loss: 19.238; l2dist: 3.138\n",
      "    step: 150; loss: 15.286; l2dist: 3.084\n",
      "    step: 200; loss: 12.462; l2dist: 3.017\n",
      "    step: 250; loss: 11.071; l2dist: 2.971\n",
      "    step: 300; loss: 10.253; l2dist: 2.879\n",
      "    step: 350; loss: 10.078; l2dist: 2.846\n",
      "    step: 400; loss: 9.777; l2dist: 2.821\n",
      "    step: 450; loss: 9.726; l2dist: 2.810\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.686; l2dist: 0.000\n",
      "    step: 50; loss: 22.945; l2dist: 3.032\n",
      "    step: 100; loss: 18.679; l2dist: 2.974\n",
      "    step: 150; loss: 15.569; l2dist: 2.984\n",
      "    step: 200; loss: 13.077; l2dist: 2.991\n",
      "    step: 250; loss: 11.509; l2dist: 2.936\n",
      "    step: 300; loss: 10.772; l2dist: 2.884\n",
      "    step: 350; loss: 10.402; l2dist: 2.826\n",
      "    step: 400; loss: 10.176; l2dist: 2.805\n",
      "    step: 450; loss: 10.054; l2dist: 2.772\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.648; l2dist: 0.000\n",
      "    step: 50; loss: 22.198; l2dist: 2.942\n",
      "    step: 100; loss: 18.498; l2dist: 2.892\n",
      "    step: 150; loss: 15.673; l2dist: 2.903\n",
      "    step: 200; loss: 13.568; l2dist: 2.918\n",
      "    step: 250; loss: 12.168; l2dist: 2.887\n",
      "    step: 300; loss: 11.262; l2dist: 2.849\n",
      "    step: 350; loss: 10.799; l2dist: 2.814\n",
      "    step: 400; loss: 10.638; l2dist: 2.777\n",
      "    step: 450; loss: 10.422; l2dist: 2.759\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.171; l2dist: 0.000\n",
      "    step: 50; loss: 21.835; l2dist: 2.901\n",
      "    step: 100; loss: 18.322; l2dist: 2.855\n",
      "    step: 150; loss: 15.755; l2dist: 2.863\n",
      "    step: 200; loss: 13.771; l2dist: 2.879\n",
      "    step: 250; loss: 12.193; l2dist: 2.877\n",
      "    step: 300; loss: 11.105; l2dist: 2.837\n",
      "    step: 350; loss: 10.428; l2dist: 2.826\n",
      "    step: 400; loss: 10.205; l2dist: 2.800\n",
      "    step: 450; loss: 10.023; l2dist: 2.777\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.921; l2dist: 0.000\n",
      "    step: 50; loss: 21.726; l2dist: 2.890\n",
      "    step: 100; loss: 18.340; l2dist: 2.852\n",
      "    step: 150; loss: 15.777; l2dist: 2.858\n",
      "    step: 200; loss: 13.848; l2dist: 2.895\n",
      "    step: 250; loss: 12.185; l2dist: 2.898\n",
      "    step: 300; loss: 11.061; l2dist: 2.845\n",
      "    step: 350; loss: 10.665; l2dist: 2.807\n",
      "    step: 400; loss: 10.454; l2dist: 2.778\n",
      "    step: 450; loss: 10.115; l2dist: 2.783\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.437; l2dist: 0.000\n",
      "    step: 50; loss: 21.818; l2dist: 2.904\n",
      "    step: 100; loss: 18.372; l2dist: 2.859\n",
      "    step: 150; loss: 15.846; l2dist: 2.868\n",
      "    step: 200; loss: 13.889; l2dist: 2.910\n",
      "    step: 250; loss: 12.114; l2dist: 2.885\n",
      "    step: 300; loss: 11.396; l2dist: 2.828\n",
      "    step: 350; loss: 11.107; l2dist: 2.795\n",
      "    step: 400; loss: 10.915; l2dist: 2.771\n",
      "    step: 450; loss: 10.460; l2dist: 2.782\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.619; l2dist: 0.000\n",
      "    step: 50; loss: 13.944; l2dist: 1.757\n",
      "    step: 100; loss: 13.732; l2dist: 1.774\n",
      "    step: 150; loss: 13.158; l2dist: 1.916\n",
      "    step: 200; loss: 12.576; l2dist: 1.995\n",
      "    step: 250; loss: 12.077; l2dist: 2.054\n",
      "    step: 300; loss: 11.684; l2dist: 2.091\n",
      "    step: 350; loss: 11.438; l2dist: 2.088\n",
      "    step: 400; loss: 11.286; l2dist: 2.089\n",
      "    step: 450; loss: 11.221; l2dist: 2.088\n",
      "binary step: 0; number of successful adv: 28/100\n",
      "    step: 0; loss: 130.606; l2dist: 0.000\n",
      "    step: 50; loss: 43.706; l2dist: 4.411\n",
      "    step: 100; loss: 23.900; l2dist: 4.094\n",
      "    step: 150; loss: 15.990; l2dist: 3.634\n",
      "    step: 200; loss: 13.794; l2dist: 3.401\n",
      "    step: 250; loss: 12.790; l2dist: 3.286\n",
      "    step: 300; loss: 12.204; l2dist: 3.214\n",
      "    step: 350; loss: 11.841; l2dist: 3.175\n",
      "    step: 400; loss: 11.414; l2dist: 3.131\n",
      "    step: 450; loss: 11.309; l2dist: 3.107\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 213.330; l2dist: 0.000\n",
      "    step: 50; loss: 45.798; l2dist: 4.642\n",
      "    step: 100; loss: 25.967; l2dist: 4.311\n",
      "    step: 150; loss: 17.455; l2dist: 3.806\n",
      "    step: 200; loss: 14.856; l2dist: 3.552\n",
      "    step: 250; loss: 13.501; l2dist: 3.403\n",
      "    step: 300; loss: 12.800; l2dist: 3.321\n",
      "    step: 350; loss: 12.244; l2dist: 3.258\n",
      "    step: 400; loss: 12.046; l2dist: 3.246\n",
      "    step: 450; loss: 11.686; l2dist: 3.200\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 252.718; l2dist: 0.000\n",
      "    step: 50; loss: 42.745; l2dist: 4.679\n",
      "    step: 100; loss: 24.029; l2dist: 4.312\n",
      "    step: 150; loss: 18.381; l2dist: 3.902\n",
      "    step: 200; loss: 15.283; l2dist: 3.596\n",
      "    step: 250; loss: 13.458; l2dist: 3.398\n",
      "    step: 300; loss: 12.558; l2dist: 3.297\n",
      "    step: 350; loss: 11.998; l2dist: 3.233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 11.843; l2dist: 3.193\n",
      "    step: 450; loss: 11.556; l2dist: 3.176\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 154.541; l2dist: 0.000\n",
      "    step: 50; loss: 34.334; l2dist: 4.232\n",
      "    step: 100; loss: 21.370; l2dist: 3.913\n",
      "    step: 150; loss: 15.967; l2dist: 3.611\n",
      "    step: 200; loss: 13.783; l2dist: 3.414\n",
      "    step: 250; loss: 12.425; l2dist: 3.273\n",
      "    step: 300; loss: 11.921; l2dist: 3.231\n",
      "    step: 350; loss: 11.533; l2dist: 3.182\n",
      "    step: 400; loss: 11.123; l2dist: 3.144\n",
      "    step: 450; loss: 11.130; l2dist: 3.139\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.772; l2dist: 0.000\n",
      "    step: 50; loss: 30.324; l2dist: 3.925\n",
      "    step: 100; loss: 20.058; l2dist: 3.649\n",
      "    step: 150; loss: 15.604; l2dist: 3.496\n",
      "    step: 200; loss: 13.311; l2dist: 3.347\n",
      "    step: 250; loss: 12.326; l2dist: 3.247\n",
      "    step: 300; loss: 11.880; l2dist: 3.195\n",
      "    step: 350; loss: 11.483; l2dist: 3.157\n",
      "    step: 400; loss: 11.347; l2dist: 3.141\n",
      "    step: 450; loss: 11.223; l2dist: 3.128\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.428; l2dist: 0.000\n",
      "    step: 50; loss: 28.581; l2dist: 3.691\n",
      "    step: 100; loss: 19.877; l2dist: 3.493\n",
      "    step: 150; loss: 15.555; l2dist: 3.434\n",
      "    step: 200; loss: 13.064; l2dist: 3.305\n",
      "    step: 250; loss: 12.020; l2dist: 3.203\n",
      "    step: 300; loss: 11.547; l2dist: 3.158\n",
      "    step: 350; loss: 11.407; l2dist: 3.145\n",
      "    step: 400; loss: 11.090; l2dist: 3.110\n",
      "    step: 450; loss: 10.854; l2dist: 3.096\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 80.448; l2dist: 0.000\n",
      "    step: 50; loss: 27.414; l2dist: 3.564\n",
      "    step: 100; loss: 20.445; l2dist: 3.368\n",
      "    step: 150; loss: 15.843; l2dist: 3.371\n",
      "    step: 200; loss: 13.231; l2dist: 3.249\n",
      "    step: 250; loss: 11.990; l2dist: 3.164\n",
      "    step: 300; loss: 11.600; l2dist: 3.129\n",
      "    step: 350; loss: 11.243; l2dist: 3.090\n",
      "    step: 400; loss: 11.017; l2dist: 3.070\n",
      "    step: 450; loss: 10.891; l2dist: 3.063\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.616; l2dist: 0.000\n",
      "    step: 50; loss: 26.960; l2dist: 3.509\n",
      "    step: 100; loss: 20.090; l2dist: 3.347\n",
      "    step: 150; loss: 15.565; l2dist: 3.361\n",
      "    step: 200; loss: 12.986; l2dist: 3.261\n",
      "    step: 250; loss: 11.883; l2dist: 3.161\n",
      "    step: 300; loss: 11.433; l2dist: 3.133\n",
      "    step: 350; loss: 11.145; l2dist: 3.097\n",
      "    step: 400; loss: 10.759; l2dist: 3.065\n",
      "    step: 450; loss: 10.691; l2dist: 3.061\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.110; l2dist: 0.000\n",
      "    step: 50; loss: 27.202; l2dist: 3.531\n",
      "    step: 100; loss: 20.290; l2dist: 3.369\n",
      "    step: 150; loss: 15.771; l2dist: 3.379\n",
      "    step: 200; loss: 13.171; l2dist: 3.261\n",
      "    step: 250; loss: 12.015; l2dist: 3.160\n",
      "    step: 300; loss: 11.521; l2dist: 3.118\n",
      "    step: 350; loss: 11.291; l2dist: 3.113\n",
      "    step: 400; loss: 11.007; l2dist: 3.077\n",
      "    step: 450; loss: 10.770; l2dist: 3.058\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.077; l2dist: 0.000\n",
      "    step: 50; loss: 15.478; l2dist: 2.077\n",
      "    step: 100; loss: 15.042; l2dist: 2.114\n",
      "    step: 150; loss: 14.272; l2dist: 2.225\n",
      "    step: 200; loss: 13.470; l2dist: 2.310\n",
      "    step: 250; loss: 12.937; l2dist: 2.325\n",
      "    step: 300; loss: 12.557; l2dist: 2.348\n",
      "    step: 350; loss: 12.314; l2dist: 2.348\n",
      "    step: 400; loss: 12.170; l2dist: 2.356\n",
      "    step: 450; loss: 12.110; l2dist: 2.349\n",
      "binary step: 0; number of successful adv: 36/100\n",
      "    step: 0; loss: 145.724; l2dist: 0.000\n",
      "    step: 50; loss: 43.552; l2dist: 4.524\n",
      "    step: 100; loss: 25.351; l2dist: 4.139\n",
      "    step: 150; loss: 18.027; l2dist: 3.735\n",
      "    step: 200; loss: 15.331; l2dist: 3.523\n",
      "    step: 250; loss: 14.155; l2dist: 3.393\n",
      "    step: 300; loss: 13.603; l2dist: 3.331\n",
      "    step: 350; loss: 13.013; l2dist: 3.270\n",
      "    step: 400; loss: 12.825; l2dist: 3.249\n",
      "    step: 450; loss: 12.556; l2dist: 3.218\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 277.556; l2dist: 0.000\n",
      "    step: 50; loss: 52.102; l2dist: 5.077\n",
      "    step: 100; loss: 29.181; l2dist: 4.635\n",
      "    step: 150; loss: 20.687; l2dist: 4.088\n",
      "    step: 200; loss: 17.420; l2dist: 3.763\n",
      "    step: 250; loss: 15.675; l2dist: 3.578\n",
      "    step: 300; loss: 14.641; l2dist: 3.453\n",
      "    step: 350; loss: 14.262; l2dist: 3.413\n",
      "    step: 400; loss: 13.529; l2dist: 3.325\n",
      "    step: 450; loss: 13.337; l2dist: 3.315\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 409.735; l2dist: 0.000\n",
      "    step: 50; loss: 58.822; l2dist: 5.435\n",
      "    step: 100; loss: 30.545; l2dist: 4.890\n",
      "    step: 150; loss: 22.545; l2dist: 4.300\n",
      "    step: 200; loss: 18.880; l2dist: 3.944\n",
      "    step: 250; loss: 16.552; l2dist: 3.698\n",
      "    step: 300; loss: 15.389; l2dist: 3.564\n",
      "    step: 350; loss: 14.734; l2dist: 3.489\n",
      "    step: 400; loss: 14.237; l2dist: 3.428\n",
      "    step: 450; loss: 14.057; l2dist: 3.406\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 251.896; l2dist: 0.000\n",
      "    step: 50; loss: 45.493; l2dist: 4.924\n",
      "    step: 100; loss: 25.764; l2dist: 4.308\n",
      "    step: 150; loss: 19.257; l2dist: 3.885\n",
      "    step: 200; loss: 16.569; l2dist: 3.653\n",
      "    step: 250; loss: 15.181; l2dist: 3.527\n",
      "    step: 300; loss: 14.096; l2dist: 3.410\n",
      "    step: 350; loss: 13.778; l2dist: 3.389\n",
      "    step: 400; loss: 13.443; l2dist: 3.343\n",
      "    step: 450; loss: 13.159; l2dist: 3.312\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 173.326; l2dist: 0.000\n",
      "    step: 50; loss: 37.203; l2dist: 4.445\n",
      "    step: 100; loss: 23.775; l2dist: 3.863\n",
      "    step: 150; loss: 17.277; l2dist: 3.616\n",
      "    step: 200; loss: 15.229; l2dist: 3.472\n",
      "    step: 250; loss: 14.054; l2dist: 3.368\n",
      "    step: 300; loss: 13.501; l2dist: 3.328\n",
      "    step: 350; loss: 13.012; l2dist: 3.281\n",
      "    step: 400; loss: 12.593; l2dist: 3.233\n",
      "    step: 450; loss: 12.349; l2dist: 3.210\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 136.180; l2dist: 0.000\n",
      "    step: 50; loss: 32.655; l2dist: 4.190\n",
      "    step: 100; loss: 22.150; l2dist: 3.673\n",
      "    step: 150; loss: 16.673; l2dist: 3.522\n",
      "    step: 200; loss: 14.765; l2dist: 3.397\n",
      "    step: 250; loss: 13.778; l2dist: 3.311\n",
      "    step: 300; loss: 13.202; l2dist: 3.261\n",
      "    step: 350; loss: 12.909; l2dist: 3.222\n",
      "    step: 400; loss: 12.491; l2dist: 3.201\n",
      "    step: 450; loss: 12.374; l2dist: 3.185\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 119.783; l2dist: 0.000\n",
      "    step: 50; loss: 31.118; l2dist: 4.019\n",
      "    step: 100; loss: 22.157; l2dist: 3.603\n",
      "    step: 150; loss: 17.040; l2dist: 3.475\n",
      "    step: 200; loss: 14.887; l2dist: 3.387\n",
      "    step: 250; loss: 13.711; l2dist: 3.315\n",
      "    step: 300; loss: 13.056; l2dist: 3.257\n",
      "    step: 350; loss: 12.679; l2dist: 3.228\n",
      "    step: 400; loss: 12.348; l2dist: 3.201\n",
      "    step: 450; loss: 12.022; l2dist: 3.159\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 112.629; l2dist: 0.000\n",
      "    step: 50; loss: 30.178; l2dist: 3.963\n",
      "    step: 100; loss: 21.882; l2dist: 3.574\n",
      "    step: 150; loss: 16.924; l2dist: 3.465\n",
      "    step: 200; loss: 14.901; l2dist: 3.358\n",
      "    step: 250; loss: 13.829; l2dist: 3.300\n",
      "    step: 300; loss: 13.132; l2dist: 3.250\n",
      "    step: 350; loss: 12.743; l2dist: 3.210\n",
      "    step: 400; loss: 12.381; l2dist: 3.172\n",
      "    step: 450; loss: 12.215; l2dist: 3.148\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.954; l2dist: 0.000\n",
      "    step: 50; loss: 30.456; l2dist: 3.984\n",
      "    step: 100; loss: 22.082; l2dist: 3.576\n",
      "    step: 150; loss: 16.978; l2dist: 3.491\n",
      "    step: 200; loss: 14.791; l2dist: 3.375\n",
      "    step: 250; loss: 13.810; l2dist: 3.309\n",
      "    step: 300; loss: 13.085; l2dist: 3.258\n",
      "    step: 350; loss: 12.561; l2dist: 3.215\n",
      "    step: 400; loss: 12.356; l2dist: 3.199\n",
      "    step: 450; loss: 12.133; l2dist: 3.177\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.223; l2dist: 0.000\n",
      "    step: 50; loss: 12.837; l2dist: 1.729\n",
      "    step: 100; loss: 12.528; l2dist: 1.756\n",
      "    step: 150; loss: 11.936; l2dist: 1.848\n",
      "    step: 200; loss: 11.554; l2dist: 1.878\n",
      "    step: 250; loss: 11.304; l2dist: 1.906\n",
      "    step: 300; loss: 11.079; l2dist: 1.917\n",
      "    step: 350; loss: 10.969; l2dist: 1.922\n",
      "    step: 400; loss: 10.893; l2dist: 1.928\n",
      "    step: 450; loss: 10.849; l2dist: 1.932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 0; number of successful adv: 26/100\n",
      "    step: 0; loss: 130.980; l2dist: 0.000\n",
      "    step: 50; loss: 43.530; l2dist: 4.244\n",
      "    step: 100; loss: 26.082; l2dist: 3.975\n",
      "    step: 150; loss: 16.180; l2dist: 3.655\n",
      "    step: 200; loss: 13.813; l2dist: 3.398\n",
      "    step: 250; loss: 12.445; l2dist: 3.249\n",
      "    step: 300; loss: 11.803; l2dist: 3.163\n",
      "    step: 350; loss: 11.244; l2dist: 3.097\n",
      "    step: 400; loss: 11.079; l2dist: 3.067\n",
      "    step: 450; loss: 10.804; l2dist: 3.039\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 201.799; l2dist: 0.000\n",
      "    step: 50; loss: 45.586; l2dist: 4.312\n",
      "    step: 100; loss: 25.863; l2dist: 4.202\n",
      "    step: 150; loss: 18.121; l2dist: 3.809\n",
      "    step: 200; loss: 14.895; l2dist: 3.531\n",
      "    step: 250; loss: 13.376; l2dist: 3.376\n",
      "    step: 300; loss: 12.283; l2dist: 3.255\n",
      "    step: 350; loss: 11.865; l2dist: 3.188\n",
      "    step: 400; loss: 11.469; l2dist: 3.151\n",
      "    step: 450; loss: 11.258; l2dist: 3.119\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 119.646; l2dist: 0.000\n",
      "    step: 50; loss: 33.522; l2dist: 3.849\n",
      "    step: 100; loss: 23.387; l2dist: 3.663\n",
      "    step: 150; loss: 16.217; l2dist: 3.534\n",
      "    step: 200; loss: 13.475; l2dist: 3.339\n",
      "    step: 250; loss: 12.213; l2dist: 3.226\n",
      "    step: 300; loss: 11.832; l2dist: 3.177\n",
      "    step: 350; loss: 11.199; l2dist: 3.110\n",
      "    step: 400; loss: 10.804; l2dist: 3.066\n",
      "    step: 450; loss: 10.665; l2dist: 3.061\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.014; l2dist: 0.000\n",
      "    step: 50; loss: 28.991; l2dist: 3.574\n",
      "    step: 100; loss: 21.367; l2dist: 3.379\n",
      "    step: 150; loss: 15.645; l2dist: 3.409\n",
      "    step: 200; loss: 13.062; l2dist: 3.283\n",
      "    step: 250; loss: 11.929; l2dist: 3.193\n",
      "    step: 300; loss: 11.295; l2dist: 3.119\n",
      "    step: 350; loss: 10.956; l2dist: 3.073\n",
      "    step: 400; loss: 10.675; l2dist: 3.043\n",
      "    step: 450; loss: 10.267; l2dist: 2.985\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.826; l2dist: 0.000\n",
      "    step: 50; loss: 27.110; l2dist: 3.424\n",
      "    step: 100; loss: 20.444; l2dist: 3.242\n",
      "    step: 150; loss: 15.756; l2dist: 3.295\n",
      "    step: 200; loss: 13.255; l2dist: 3.215\n",
      "    step: 250; loss: 11.857; l2dist: 3.146\n",
      "    step: 300; loss: 11.268; l2dist: 3.100\n",
      "    step: 350; loss: 10.928; l2dist: 3.064\n",
      "    step: 400; loss: 10.618; l2dist: 3.031\n",
      "    step: 450; loss: 10.304; l2dist: 2.999\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.111; l2dist: 0.000\n",
      "    step: 50; loss: 26.168; l2dist: 3.332\n",
      "    step: 100; loss: 19.589; l2dist: 3.197\n",
      "    step: 150; loss: 15.642; l2dist: 3.227\n",
      "    step: 200; loss: 13.331; l2dist: 3.165\n",
      "    step: 250; loss: 12.011; l2dist: 3.124\n",
      "    step: 300; loss: 11.423; l2dist: 3.091\n",
      "    step: 350; loss: 10.795; l2dist: 3.026\n",
      "    step: 400; loss: 10.499; l2dist: 2.994\n",
      "    step: 450; loss: 10.303; l2dist: 2.986\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 72.257; l2dist: 0.000\n",
      "    step: 50; loss: 25.496; l2dist: 3.315\n",
      "    step: 100; loss: 19.328; l2dist: 3.182\n",
      "    step: 150; loss: 15.514; l2dist: 3.211\n",
      "    step: 200; loss: 13.261; l2dist: 3.165\n",
      "    step: 250; loss: 11.912; l2dist: 3.126\n",
      "    step: 300; loss: 11.196; l2dist: 3.068\n",
      "    step: 350; loss: 10.805; l2dist: 3.026\n",
      "    step: 400; loss: 10.444; l2dist: 2.993\n",
      "    step: 450; loss: 10.288; l2dist: 2.976\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.681; l2dist: 0.000\n",
      "    step: 50; loss: 25.360; l2dist: 3.306\n",
      "    step: 100; loss: 19.589; l2dist: 3.152\n",
      "    step: 150; loss: 15.897; l2dist: 3.174\n",
      "    step: 200; loss: 13.822; l2dist: 3.119\n",
      "    step: 250; loss: 12.505; l2dist: 3.086\n",
      "    step: 300; loss: 11.461; l2dist: 3.076\n",
      "    step: 350; loss: 11.028; l2dist: 3.044\n",
      "    step: 400; loss: 10.586; l2dist: 2.998\n",
      "    step: 450; loss: 10.366; l2dist: 2.975\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 72.665; l2dist: 0.000\n",
      "    step: 50; loss: 25.646; l2dist: 3.306\n",
      "    step: 100; loss: 19.692; l2dist: 3.167\n",
      "    step: 150; loss: 15.869; l2dist: 3.198\n",
      "    step: 200; loss: 13.811; l2dist: 3.139\n",
      "    step: 250; loss: 12.536; l2dist: 3.099\n",
      "    step: 300; loss: 11.474; l2dist: 3.073\n",
      "    step: 350; loss: 10.944; l2dist: 3.046\n",
      "    step: 400; loss: 10.635; l2dist: 3.018\n",
      "    step: 450; loss: 10.482; l2dist: 2.994\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.063; l2dist: 0.000\n",
      "    step: 50; loss: 14.300; l2dist: 1.867\n",
      "    step: 100; loss: 13.939; l2dist: 1.883\n",
      "    step: 150; loss: 13.330; l2dist: 1.995\n",
      "    step: 200; loss: 12.885; l2dist: 2.068\n",
      "    step: 250; loss: 12.498; l2dist: 2.132\n",
      "    step: 300; loss: 12.195; l2dist: 2.155\n",
      "    step: 350; loss: 12.001; l2dist: 2.160\n",
      "    step: 400; loss: 11.890; l2dist: 2.161\n",
      "    step: 450; loss: 11.843; l2dist: 2.163\n",
      "binary step: 0; number of successful adv: 28/100\n",
      "    step: 0; loss: 142.324; l2dist: 0.000\n",
      "    step: 50; loss: 46.054; l2dist: 4.383\n",
      "    step: 100; loss: 25.115; l2dist: 4.033\n",
      "    step: 150; loss: 16.674; l2dist: 3.639\n",
      "    step: 200; loss: 14.568; l2dist: 3.431\n",
      "    step: 250; loss: 13.510; l2dist: 3.317\n",
      "    step: 300; loss: 12.859; l2dist: 3.255\n",
      "    step: 350; loss: 12.478; l2dist: 3.211\n",
      "    step: 400; loss: 12.136; l2dist: 3.178\n",
      "    step: 450; loss: 12.005; l2dist: 3.156\n",
      "binary step: 1; number of successful adv: 89/100\n",
      "    step: 0; loss: 246.601; l2dist: 0.000\n",
      "    step: 50; loss: 48.636; l2dist: 4.664\n",
      "    step: 100; loss: 27.592; l2dist: 4.299\n",
      "    step: 150; loss: 19.357; l2dist: 3.846\n",
      "    step: 200; loss: 15.519; l2dist: 3.586\n",
      "    step: 250; loss: 13.889; l2dist: 3.424\n",
      "    step: 300; loss: 13.281; l2dist: 3.342\n",
      "    step: 350; loss: 12.684; l2dist: 3.268\n",
      "    step: 400; loss: 12.146; l2dist: 3.212\n",
      "    step: 450; loss: 11.958; l2dist: 3.178\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 147.322; l2dist: 0.000\n",
      "    step: 50; loss: 36.867; l2dist: 4.067\n",
      "    step: 100; loss: 23.726; l2dist: 3.801\n",
      "    step: 150; loss: 16.881; l2dist: 3.582\n",
      "    step: 200; loss: 14.039; l2dist: 3.405\n",
      "    step: 250; loss: 12.954; l2dist: 3.294\n",
      "    step: 300; loss: 12.146; l2dist: 3.199\n",
      "    step: 350; loss: 11.931; l2dist: 3.177\n",
      "    step: 400; loss: 11.524; l2dist: 3.130\n",
      "    step: 450; loss: 11.434; l2dist: 3.121\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 106.148; l2dist: 0.000\n",
      "    step: 50; loss: 31.370; l2dist: 3.719\n",
      "    step: 100; loss: 22.221; l2dist: 3.531\n",
      "    step: 150; loss: 16.576; l2dist: 3.461\n",
      "    step: 200; loss: 13.693; l2dist: 3.341\n",
      "    step: 250; loss: 12.593; l2dist: 3.233\n",
      "    step: 300; loss: 11.927; l2dist: 3.164\n",
      "    step: 350; loss: 11.534; l2dist: 3.122\n",
      "    step: 400; loss: 11.330; l2dist: 3.098\n",
      "    step: 450; loss: 11.181; l2dist: 3.079\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.421; l2dist: 0.000\n",
      "    step: 50; loss: 28.487; l2dist: 3.531\n",
      "    step: 100; loss: 21.845; l2dist: 3.409\n",
      "    step: 150; loss: 16.456; l2dist: 3.400\n",
      "    step: 200; loss: 13.575; l2dist: 3.314\n",
      "    step: 250; loss: 12.479; l2dist: 3.226\n",
      "    step: 300; loss: 11.759; l2dist: 3.146\n",
      "    step: 350; loss: 11.496; l2dist: 3.131\n",
      "    step: 400; loss: 11.023; l2dist: 3.066\n",
      "    step: 450; loss: 10.946; l2dist: 3.055\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.898; l2dist: 0.000\n",
      "    step: 50; loss: 26.595; l2dist: 3.410\n",
      "    step: 100; loss: 20.767; l2dist: 3.322\n",
      "    step: 150; loss: 16.501; l2dist: 3.321\n",
      "    step: 200; loss: 13.568; l2dist: 3.279\n",
      "    step: 250; loss: 12.333; l2dist: 3.187\n",
      "    step: 300; loss: 11.699; l2dist: 3.128\n",
      "    step: 350; loss: 11.351; l2dist: 3.092\n",
      "    step: 400; loss: 11.082; l2dist: 3.053\n",
      "    step: 450; loss: 10.995; l2dist: 3.052\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.388; l2dist: 0.000\n",
      "    step: 50; loss: 25.853; l2dist: 3.383\n",
      "    step: 100; loss: 20.342; l2dist: 3.311\n",
      "    step: 150; loss: 16.221; l2dist: 3.297\n",
      "    step: 200; loss: 13.465; l2dist: 3.262\n",
      "    step: 250; loss: 12.330; l2dist: 3.183\n",
      "    step: 300; loss: 11.653; l2dist: 3.120\n",
      "    step: 350; loss: 11.369; l2dist: 3.095\n",
      "    step: 400; loss: 11.180; l2dist: 3.077\n",
      "    step: 450; loss: 10.997; l2dist: 3.066\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.197; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 25.846; l2dist: 3.362\n",
      "    step: 100; loss: 20.488; l2dist: 3.301\n",
      "    step: 150; loss: 16.481; l2dist: 3.289\n",
      "    step: 200; loss: 13.521; l2dist: 3.269\n",
      "    step: 250; loss: 12.424; l2dist: 3.202\n",
      "    step: 300; loss: 11.792; l2dist: 3.152\n",
      "    step: 350; loss: 11.487; l2dist: 3.105\n",
      "    step: 400; loss: 11.267; l2dist: 3.094\n",
      "    step: 450; loss: 11.079; l2dist: 3.077\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.024; l2dist: 0.000\n",
      "    step: 50; loss: 26.039; l2dist: 3.378\n",
      "    step: 100; loss: 20.556; l2dist: 3.302\n",
      "    step: 150; loss: 16.651; l2dist: 3.304\n",
      "    step: 200; loss: 13.624; l2dist: 3.287\n",
      "    step: 250; loss: 12.330; l2dist: 3.203\n",
      "    step: 300; loss: 11.847; l2dist: 3.158\n",
      "    step: 350; loss: 11.434; l2dist: 3.118\n",
      "    step: 400; loss: 11.208; l2dist: 3.101\n",
      "    step: 450; loss: 11.077; l2dist: 3.086\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.322; l2dist: 0.000\n",
      "    step: 50; loss: 13.117; l2dist: 1.772\n",
      "    step: 100; loss: 12.760; l2dist: 1.794\n",
      "    step: 150; loss: 12.173; l2dist: 1.913\n",
      "    step: 200; loss: 11.738; l2dist: 1.980\n",
      "    step: 250; loss: 11.434; l2dist: 2.018\n",
      "    step: 300; loss: 11.196; l2dist: 2.031\n",
      "    step: 350; loss: 11.084; l2dist: 2.038\n",
      "    step: 400; loss: 11.018; l2dist: 2.046\n",
      "    step: 450; loss: 10.961; l2dist: 2.052\n",
      "binary step: 0; number of successful adv: 22/100\n",
      "    step: 0; loss: 142.620; l2dist: 0.000\n",
      "    step: 50; loss: 40.895; l2dist: 4.543\n",
      "    step: 100; loss: 21.532; l2dist: 4.089\n",
      "    step: 150; loss: 15.465; l2dist: 3.586\n",
      "    step: 200; loss: 13.246; l2dist: 3.356\n",
      "    step: 250; loss: 12.164; l2dist: 3.222\n",
      "    step: 300; loss: 11.674; l2dist: 3.171\n",
      "    step: 350; loss: 11.302; l2dist: 3.121\n",
      "    step: 400; loss: 11.051; l2dist: 3.101\n",
      "    step: 450; loss: 10.961; l2dist: 3.088\n",
      "binary step: 1; number of successful adv: 89/100\n",
      "    step: 0; loss: 250.679; l2dist: 0.000\n",
      "    step: 50; loss: 46.286; l2dist: 4.803\n",
      "    step: 100; loss: 23.806; l2dist: 4.272\n",
      "    step: 150; loss: 17.263; l2dist: 3.788\n",
      "    step: 200; loss: 14.783; l2dist: 3.515\n",
      "    step: 250; loss: 13.529; l2dist: 3.377\n",
      "    step: 300; loss: 12.595; l2dist: 3.271\n",
      "    step: 350; loss: 12.039; l2dist: 3.203\n",
      "    step: 400; loss: 11.732; l2dist: 3.168\n",
      "    step: 450; loss: 11.513; l2dist: 3.146\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 151.815; l2dist: 0.000\n",
      "    step: 50; loss: 35.231; l2dist: 4.270\n",
      "    step: 100; loss: 20.848; l2dist: 3.915\n",
      "    step: 150; loss: 15.348; l2dist: 3.567\n",
      "    step: 200; loss: 13.286; l2dist: 3.353\n",
      "    step: 250; loss: 12.342; l2dist: 3.250\n",
      "    step: 300; loss: 11.718; l2dist: 3.174\n",
      "    step: 350; loss: 11.415; l2dist: 3.132\n",
      "    step: 400; loss: 11.130; l2dist: 3.110\n",
      "    step: 450; loss: 11.032; l2dist: 3.097\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.354; l2dist: 0.000\n",
      "    step: 50; loss: 29.687; l2dist: 3.939\n",
      "    step: 100; loss: 19.786; l2dist: 3.697\n",
      "    step: 150; loss: 14.659; l2dist: 3.472\n",
      "    step: 200; loss: 12.703; l2dist: 3.290\n",
      "    step: 250; loss: 11.796; l2dist: 3.175\n",
      "    step: 300; loss: 11.262; l2dist: 3.123\n",
      "    step: 350; loss: 11.131; l2dist: 3.100\n",
      "    step: 400; loss: 10.953; l2dist: 3.078\n",
      "    step: 450; loss: 10.665; l2dist: 3.050\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.682; l2dist: 0.000\n",
      "    step: 50; loss: 26.822; l2dist: 3.741\n",
      "    step: 100; loss: 18.853; l2dist: 3.548\n",
      "    step: 150; loss: 14.529; l2dist: 3.393\n",
      "    step: 200; loss: 12.543; l2dist: 3.245\n",
      "    step: 250; loss: 11.681; l2dist: 3.155\n",
      "    step: 300; loss: 11.183; l2dist: 3.104\n",
      "    step: 350; loss: 10.925; l2dist: 3.068\n",
      "    step: 400; loss: 10.801; l2dist: 3.059\n",
      "    step: 450; loss: 10.618; l2dist: 3.036\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 80.008; l2dist: 0.000\n",
      "    step: 50; loss: 25.886; l2dist: 3.584\n",
      "    step: 100; loss: 18.916; l2dist: 3.427\n",
      "    step: 150; loss: 14.698; l2dist: 3.318\n",
      "    step: 200; loss: 12.534; l2dist: 3.201\n",
      "    step: 250; loss: 11.595; l2dist: 3.117\n",
      "    step: 300; loss: 11.238; l2dist: 3.084\n",
      "    step: 350; loss: 10.887; l2dist: 3.049\n",
      "    step: 400; loss: 10.797; l2dist: 3.024\n",
      "    step: 450; loss: 10.608; l2dist: 3.013\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 77.356; l2dist: 0.000\n",
      "    step: 50; loss: 25.519; l2dist: 3.549\n",
      "    step: 100; loss: 18.544; l2dist: 3.417\n",
      "    step: 150; loss: 14.455; l2dist: 3.308\n",
      "    step: 200; loss: 12.340; l2dist: 3.197\n",
      "    step: 250; loss: 11.519; l2dist: 3.113\n",
      "    step: 300; loss: 11.144; l2dist: 3.081\n",
      "    step: 350; loss: 10.932; l2dist: 3.061\n",
      "    step: 400; loss: 10.696; l2dist: 3.030\n",
      "    step: 450; loss: 10.630; l2dist: 3.026\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.494; l2dist: 0.000\n",
      "    step: 50; loss: 25.329; l2dist: 3.520\n",
      "    step: 100; loss: 18.581; l2dist: 3.391\n",
      "    step: 150; loss: 14.457; l2dist: 3.287\n",
      "    step: 200; loss: 12.528; l2dist: 3.201\n",
      "    step: 250; loss: 11.565; l2dist: 3.115\n",
      "    step: 300; loss: 11.225; l2dist: 3.079\n",
      "    step: 350; loss: 10.989; l2dist: 3.055\n",
      "    step: 400; loss: 10.785; l2dist: 3.033\n",
      "    step: 450; loss: 10.703; l2dist: 3.026\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.196; l2dist: 0.000\n",
      "    step: 50; loss: 25.354; l2dist: 3.538\n",
      "    step: 100; loss: 18.486; l2dist: 3.411\n",
      "    step: 150; loss: 14.451; l2dist: 3.308\n",
      "    step: 200; loss: 12.375; l2dist: 3.197\n",
      "    step: 250; loss: 11.483; l2dist: 3.112\n",
      "    step: 300; loss: 11.084; l2dist: 3.080\n",
      "    step: 350; loss: 10.760; l2dist: 3.033\n",
      "    step: 400; loss: 10.706; l2dist: 3.038\n",
      "    step: 450; loss: 10.554; l2dist: 3.018\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.917; l2dist: 0.000\n",
      "    step: 50; loss: 15.466; l2dist: 2.045\n",
      "    step: 100; loss: 15.086; l2dist: 2.060\n",
      "    step: 150; loss: 14.541; l2dist: 2.153\n",
      "    step: 200; loss: 13.989; l2dist: 2.226\n",
      "    step: 250; loss: 13.487; l2dist: 2.280\n",
      "    step: 300; loss: 13.041; l2dist: 2.318\n",
      "    step: 350; loss: 12.698; l2dist: 2.323\n",
      "    step: 400; loss: 12.536; l2dist: 2.321\n",
      "    step: 450; loss: 12.463; l2dist: 2.320\n",
      "binary step: 0; number of successful adv: 28/100\n",
      "    step: 0; loss: 151.848; l2dist: 0.000\n",
      "    step: 50; loss: 44.377; l2dist: 4.673\n",
      "    step: 100; loss: 26.533; l2dist: 4.172\n",
      "    step: 150; loss: 16.680; l2dist: 3.735\n",
      "    step: 200; loss: 14.554; l2dist: 3.509\n",
      "    step: 250; loss: 13.504; l2dist: 3.392\n",
      "    step: 300; loss: 12.816; l2dist: 3.307\n",
      "    step: 350; loss: 12.444; l2dist: 3.256\n",
      "    step: 400; loss: 12.226; l2dist: 3.246\n",
      "    step: 450; loss: 12.117; l2dist: 3.219\n",
      "binary step: 1; number of successful adv: 94/100\n",
      "    step: 0; loss: 178.089; l2dist: 0.000\n",
      "    step: 50; loss: 39.954; l2dist: 4.544\n",
      "    step: 100; loss: 23.880; l2dist: 4.230\n",
      "    step: 150; loss: 17.619; l2dist: 3.812\n",
      "    step: 200; loss: 15.048; l2dist: 3.580\n",
      "    step: 250; loss: 13.700; l2dist: 3.436\n",
      "    step: 300; loss: 12.947; l2dist: 3.358\n",
      "    step: 350; loss: 12.362; l2dist: 3.282\n",
      "    step: 400; loss: 12.278; l2dist: 3.270\n",
      "    step: 450; loss: 11.927; l2dist: 3.228\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 112.827; l2dist: 0.000\n",
      "    step: 50; loss: 32.477; l2dist: 4.032\n",
      "    step: 100; loss: 22.913; l2dist: 3.875\n",
      "    step: 150; loss: 16.265; l2dist: 3.668\n",
      "    step: 200; loss: 13.846; l2dist: 3.455\n",
      "    step: 250; loss: 12.922; l2dist: 3.358\n",
      "    step: 300; loss: 12.298; l2dist: 3.288\n",
      "    step: 350; loss: 11.938; l2dist: 3.242\n",
      "    step: 400; loss: 11.742; l2dist: 3.218\n",
      "    step: 450; loss: 11.518; l2dist: 3.205\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.020; l2dist: 0.000\n",
      "    step: 50; loss: 28.570; l2dist: 3.710\n",
      "    step: 100; loss: 22.319; l2dist: 3.593\n",
      "    step: 150; loss: 16.704; l2dist: 3.549\n",
      "    step: 200; loss: 14.369; l2dist: 3.409\n",
      "    step: 250; loss: 12.757; l2dist: 3.307\n",
      "    step: 300; loss: 12.057; l2dist: 3.243\n",
      "    step: 350; loss: 11.753; l2dist: 3.196\n",
      "    step: 400; loss: 11.522; l2dist: 3.177\n",
      "    step: 450; loss: 11.461; l2dist: 3.163\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.338; l2dist: 0.000\n",
      "    step: 50; loss: 26.247; l2dist: 3.471\n",
      "    step: 100; loss: 21.522; l2dist: 3.396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 17.052; l2dist: 3.395\n",
      "    step: 200; loss: 14.720; l2dist: 3.337\n",
      "    step: 250; loss: 13.538; l2dist: 3.240\n",
      "    step: 300; loss: 12.788; l2dist: 3.215\n",
      "    step: 350; loss: 12.102; l2dist: 3.162\n",
      "    step: 400; loss: 11.531; l2dist: 3.163\n",
      "    step: 450; loss: 11.278; l2dist: 3.126\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.466; l2dist: 0.000\n",
      "    step: 50; loss: 25.544; l2dist: 3.403\n",
      "    step: 100; loss: 21.214; l2dist: 3.347\n",
      "    step: 150; loss: 17.171; l2dist: 3.343\n",
      "    step: 200; loss: 14.765; l2dist: 3.343\n",
      "    step: 250; loss: 13.334; l2dist: 3.266\n",
      "    step: 300; loss: 12.660; l2dist: 3.203\n",
      "    step: 350; loss: 12.213; l2dist: 3.161\n",
      "    step: 400; loss: 11.923; l2dist: 3.130\n",
      "    step: 450; loss: 11.747; l2dist: 3.111\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.254; l2dist: 0.000\n",
      "    step: 50; loss: 25.160; l2dist: 3.368\n",
      "    step: 100; loss: 21.290; l2dist: 3.307\n",
      "    step: 150; loss: 17.457; l2dist: 3.312\n",
      "    step: 200; loss: 15.085; l2dist: 3.294\n",
      "    step: 250; loss: 13.975; l2dist: 3.222\n",
      "    step: 300; loss: 13.201; l2dist: 3.159\n",
      "    step: 350; loss: 12.573; l2dist: 3.153\n",
      "    step: 400; loss: 12.318; l2dist: 3.119\n",
      "    step: 450; loss: 12.121; l2dist: 3.097\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.821; l2dist: 0.000\n",
      "    step: 50; loss: 24.921; l2dist: 3.337\n",
      "    step: 100; loss: 21.295; l2dist: 3.293\n",
      "    step: 150; loss: 17.562; l2dist: 3.310\n",
      "    step: 200; loss: 15.370; l2dist: 3.270\n",
      "    step: 250; loss: 13.994; l2dist: 3.227\n",
      "    step: 300; loss: 13.269; l2dist: 3.174\n",
      "    step: 350; loss: 12.893; l2dist: 3.138\n",
      "    step: 400; loss: 12.601; l2dist: 3.102\n",
      "    step: 450; loss: 12.576; l2dist: 3.113\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.364; l2dist: 0.000\n",
      "    step: 50; loss: 25.055; l2dist: 3.347\n",
      "    step: 100; loss: 21.309; l2dist: 3.302\n",
      "    step: 150; loss: 17.550; l2dist: 3.321\n",
      "    step: 200; loss: 15.100; l2dist: 3.298\n",
      "    step: 250; loss: 13.903; l2dist: 3.218\n",
      "    step: 300; loss: 13.320; l2dist: 3.168\n",
      "    step: 350; loss: 12.843; l2dist: 3.118\n",
      "    step: 400; loss: 12.238; l2dist: 3.110\n",
      "    step: 450; loss: 12.093; l2dist: 3.095\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.552; l2dist: 0.000\n",
      "    step: 50; loss: 14.312; l2dist: 1.887\n",
      "    step: 100; loss: 13.925; l2dist: 1.908\n",
      "    step: 150; loss: 13.205; l2dist: 2.030\n",
      "    step: 200; loss: 12.602; l2dist: 2.092\n",
      "    step: 250; loss: 12.244; l2dist: 2.119\n",
      "    step: 300; loss: 12.011; l2dist: 2.143\n",
      "    step: 350; loss: 11.850; l2dist: 2.149\n",
      "    step: 400; loss: 11.764; l2dist: 2.150\n",
      "    step: 450; loss: 11.611; l2dist: 2.160\n",
      "binary step: 0; number of successful adv: 24/100\n",
      "    step: 0; loss: 139.744; l2dist: 0.000\n",
      "    step: 50; loss: 42.420; l2dist: 4.462\n",
      "    step: 100; loss: 24.402; l2dist: 3.939\n",
      "    step: 150; loss: 15.966; l2dist: 3.588\n",
      "    step: 200; loss: 13.917; l2dist: 3.374\n",
      "    step: 250; loss: 12.900; l2dist: 3.258\n",
      "    step: 300; loss: 12.261; l2dist: 3.191\n",
      "    step: 350; loss: 11.843; l2dist: 3.148\n",
      "    step: 400; loss: 11.620; l2dist: 3.114\n",
      "    step: 450; loss: 11.523; l2dist: 3.098\n",
      "binary step: 1; number of successful adv: 92/100\n",
      "    step: 0; loss: 171.899; l2dist: 0.000\n",
      "    step: 50; loss: 37.680; l2dist: 4.408\n",
      "    step: 100; loss: 22.910; l2dist: 4.095\n",
      "    step: 150; loss: 16.627; l2dist: 3.643\n",
      "    step: 200; loss: 14.061; l2dist: 3.423\n",
      "    step: 250; loss: 13.001; l2dist: 3.305\n",
      "    step: 300; loss: 12.479; l2dist: 3.241\n",
      "    step: 350; loss: 12.198; l2dist: 3.211\n",
      "    step: 400; loss: 12.061; l2dist: 3.190\n",
      "    step: 450; loss: 11.988; l2dist: 3.177\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.492; l2dist: 0.000\n",
      "    step: 50; loss: 31.356; l2dist: 3.948\n",
      "    step: 100; loss: 21.942; l2dist: 3.744\n",
      "    step: 150; loss: 15.443; l2dist: 3.490\n",
      "    step: 200; loss: 13.462; l2dist: 3.336\n",
      "    step: 250; loss: 12.456; l2dist: 3.231\n",
      "    step: 300; loss: 12.000; l2dist: 3.171\n",
      "    step: 350; loss: 11.550; l2dist: 3.131\n",
      "    step: 400; loss: 11.538; l2dist: 3.125\n",
      "    step: 450; loss: 11.405; l2dist: 3.113\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 87.814; l2dist: 0.000\n",
      "    step: 50; loss: 27.384; l2dist: 3.671\n",
      "    step: 100; loss: 20.552; l2dist: 3.497\n",
      "    step: 150; loss: 15.186; l2dist: 3.372\n",
      "    step: 200; loss: 12.942; l2dist: 3.234\n",
      "    step: 250; loss: 12.101; l2dist: 3.181\n",
      "    step: 300; loss: 11.746; l2dist: 3.140\n",
      "    step: 350; loss: 11.287; l2dist: 3.103\n",
      "    step: 400; loss: 11.162; l2dist: 3.097\n",
      "    step: 450; loss: 11.033; l2dist: 3.076\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.744; l2dist: 0.000\n",
      "    step: 50; loss: 26.287; l2dist: 3.543\n",
      "    step: 100; loss: 20.415; l2dist: 3.426\n",
      "    step: 150; loss: 15.244; l2dist: 3.340\n",
      "    step: 200; loss: 13.021; l2dist: 3.233\n",
      "    step: 250; loss: 11.996; l2dist: 3.174\n",
      "    step: 300; loss: 11.396; l2dist: 3.121\n",
      "    step: 350; loss: 11.066; l2dist: 3.075\n",
      "    step: 400; loss: 10.912; l2dist: 3.064\n",
      "    step: 450; loss: 10.624; l2dist: 3.027\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.716; l2dist: 0.000\n",
      "    step: 50; loss: 25.388; l2dist: 3.489\n",
      "    step: 100; loss: 20.392; l2dist: 3.337\n",
      "    step: 150; loss: 15.175; l2dist: 3.322\n",
      "    step: 200; loss: 12.895; l2dist: 3.194\n",
      "    step: 250; loss: 11.816; l2dist: 3.137\n",
      "    step: 300; loss: 11.315; l2dist: 3.100\n",
      "    step: 350; loss: 11.070; l2dist: 3.068\n",
      "    step: 400; loss: 10.953; l2dist: 3.061\n",
      "    step: 450; loss: 10.663; l2dist: 3.028\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.200; l2dist: 0.000\n",
      "    step: 50; loss: 25.055; l2dist: 3.458\n",
      "    step: 100; loss: 20.359; l2dist: 3.289\n",
      "    step: 150; loss: 15.600; l2dist: 3.318\n",
      "    step: 200; loss: 13.111; l2dist: 3.211\n",
      "    step: 250; loss: 11.908; l2dist: 3.144\n",
      "    step: 300; loss: 11.456; l2dist: 3.113\n",
      "    step: 350; loss: 11.058; l2dist: 3.072\n",
      "    step: 400; loss: 10.867; l2dist: 3.048\n",
      "    step: 450; loss: 10.721; l2dist: 3.036\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 72.197; l2dist: 0.000\n",
      "    step: 50; loss: 24.796; l2dist: 3.442\n",
      "    step: 100; loss: 20.149; l2dist: 3.276\n",
      "    step: 150; loss: 15.291; l2dist: 3.297\n",
      "    step: 200; loss: 13.024; l2dist: 3.191\n",
      "    step: 250; loss: 12.042; l2dist: 3.163\n",
      "    step: 300; loss: 11.380; l2dist: 3.100\n",
      "    step: 350; loss: 11.057; l2dist: 3.070\n",
      "    step: 400; loss: 10.720; l2dist: 3.039\n",
      "    step: 450; loss: 10.698; l2dist: 3.033\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 72.782; l2dist: 0.000\n",
      "    step: 50; loss: 24.898; l2dist: 3.453\n",
      "    step: 100; loss: 20.171; l2dist: 3.281\n",
      "    step: 150; loss: 15.279; l2dist: 3.301\n",
      "    step: 200; loss: 12.913; l2dist: 3.186\n",
      "    step: 250; loss: 11.909; l2dist: 3.142\n",
      "    step: 300; loss: 11.379; l2dist: 3.097\n",
      "    step: 350; loss: 10.848; l2dist: 3.059\n",
      "    step: 400; loss: 10.703; l2dist: 3.032\n",
      "    step: 450; loss: 10.626; l2dist: 3.026\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.965; l2dist: 0.000\n",
      "    step: 50; loss: 12.670; l2dist: 1.833\n",
      "    step: 100; loss: 12.460; l2dist: 1.837\n",
      "    step: 150; loss: 12.012; l2dist: 1.928\n",
      "    step: 200; loss: 11.445; l2dist: 2.025\n",
      "    step: 250; loss: 10.991; l2dist: 2.073\n",
      "    step: 300; loss: 10.673; l2dist: 2.091\n",
      "    step: 350; loss: 10.485; l2dist: 2.100\n",
      "    step: 400; loss: 10.395; l2dist: 2.105\n",
      "    step: 450; loss: 10.305; l2dist: 2.100\n",
      "binary step: 0; number of successful adv: 22/100\n",
      "    step: 0; loss: 140.631; l2dist: 0.000\n",
      "    step: 50; loss: 37.288; l2dist: 4.499\n",
      "    step: 100; loss: 18.607; l2dist: 3.900\n",
      "    step: 150; loss: 14.150; l2dist: 3.466\n",
      "    step: 200; loss: 12.339; l2dist: 3.258\n",
      "    step: 250; loss: 11.331; l2dist: 3.135\n",
      "    step: 300; loss: 10.990; l2dist: 3.077\n",
      "    step: 350; loss: 10.579; l2dist: 3.048\n",
      "    step: 400; loss: 10.159; l2dist: 2.989\n",
      "    step: 450; loss: 10.034; l2dist: 2.974\n",
      "binary step: 1; number of successful adv: 86/100\n",
      "    step: 0; loss: 324.282; l2dist: 0.000\n",
      "    step: 50; loss: 44.289; l2dist: 4.980\n",
      "    step: 100; loss: 22.462; l2dist: 4.335\n",
      "    step: 150; loss: 16.883; l2dist: 3.807\n",
      "    step: 200; loss: 14.309; l2dist: 3.500\n",
      "    step: 250; loss: 13.004; l2dist: 3.336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 12.197; l2dist: 3.253\n",
      "    step: 350; loss: 11.715; l2dist: 3.193\n",
      "    step: 400; loss: 11.154; l2dist: 3.113\n",
      "    step: 450; loss: 10.944; l2dist: 3.111\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 372.188; l2dist: 0.000\n",
      "    step: 50; loss: 44.448; l2dist: 5.092\n",
      "    step: 100; loss: 22.512; l2dist: 4.399\n",
      "    step: 150; loss: 17.038; l2dist: 3.850\n",
      "    step: 200; loss: 14.011; l2dist: 3.505\n",
      "    step: 250; loss: 12.355; l2dist: 3.302\n",
      "    step: 300; loss: 11.367; l2dist: 3.173\n",
      "    step: 350; loss: 10.847; l2dist: 3.109\n",
      "    step: 400; loss: 10.413; l2dist: 3.052\n",
      "    step: 450; loss: 10.440; l2dist: 3.053\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 223.543; l2dist: 0.000\n",
      "    step: 50; loss: 34.598; l2dist: 4.585\n",
      "    step: 100; loss: 18.977; l2dist: 3.948\n",
      "    step: 150; loss: 14.159; l2dist: 3.503\n",
      "    step: 200; loss: 12.090; l2dist: 3.255\n",
      "    step: 250; loss: 11.145; l2dist: 3.143\n",
      "    step: 300; loss: 10.530; l2dist: 3.056\n",
      "    step: 350; loss: 10.161; l2dist: 3.009\n",
      "    step: 400; loss: 9.910; l2dist: 2.969\n",
      "    step: 450; loss: 9.847; l2dist: 2.968\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 149.270; l2dist: 0.000\n",
      "    step: 50; loss: 28.460; l2dist: 4.140\n",
      "    step: 100; loss: 16.685; l2dist: 3.606\n",
      "    step: 150; loss: 12.858; l2dist: 3.301\n",
      "    step: 200; loss: 11.319; l2dist: 3.139\n",
      "    step: 250; loss: 10.565; l2dist: 3.035\n",
      "    step: 300; loss: 10.086; l2dist: 2.991\n",
      "    step: 350; loss: 9.833; l2dist: 2.959\n",
      "    step: 400; loss: 9.691; l2dist: 2.955\n",
      "    step: 450; loss: 9.552; l2dist: 2.930\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 116.445; l2dist: 0.000\n",
      "    step: 50; loss: 25.333; l2dist: 3.873\n",
      "    step: 100; loss: 15.787; l2dist: 3.433\n",
      "    step: 150; loss: 12.410; l2dist: 3.222\n",
      "    step: 200; loss: 11.063; l2dist: 3.091\n",
      "    step: 250; loss: 10.312; l2dist: 3.003\n",
      "    step: 300; loss: 9.797; l2dist: 2.940\n",
      "    step: 350; loss: 9.646; l2dist: 2.922\n",
      "    step: 400; loss: 9.537; l2dist: 2.911\n",
      "    step: 450; loss: 9.434; l2dist: 2.894\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 100.880; l2dist: 0.000\n",
      "    step: 50; loss: 23.902; l2dist: 3.737\n",
      "    step: 100; loss: 15.603; l2dist: 3.368\n",
      "    step: 150; loss: 12.173; l2dist: 3.178\n",
      "    step: 200; loss: 10.831; l2dist: 3.061\n",
      "    step: 250; loss: 10.218; l2dist: 2.981\n",
      "    step: 300; loss: 9.870; l2dist: 2.952\n",
      "    step: 350; loss: 9.600; l2dist: 2.915\n",
      "    step: 400; loss: 9.451; l2dist: 2.894\n",
      "    step: 450; loss: 9.301; l2dist: 2.873\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.059; l2dist: 0.000\n",
      "    step: 50; loss: 23.110; l2dist: 3.656\n",
      "    step: 100; loss: 15.416; l2dist: 3.322\n",
      "    step: 150; loss: 12.122; l2dist: 3.146\n",
      "    step: 200; loss: 10.858; l2dist: 3.036\n",
      "    step: 250; loss: 10.191; l2dist: 2.967\n",
      "    step: 300; loss: 9.950; l2dist: 2.935\n",
      "    step: 350; loss: 9.687; l2dist: 2.919\n",
      "    step: 400; loss: 9.523; l2dist: 2.899\n",
      "    step: 450; loss: 9.355; l2dist: 2.878\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.530; l2dist: 0.000\n",
      "    step: 50; loss: 23.136; l2dist: 3.670\n",
      "    step: 100; loss: 15.407; l2dist: 3.319\n",
      "    step: 150; loss: 12.140; l2dist: 3.157\n",
      "    step: 200; loss: 10.974; l2dist: 3.046\n",
      "    step: 250; loss: 10.159; l2dist: 2.976\n",
      "    step: 300; loss: 9.783; l2dist: 2.931\n",
      "    step: 350; loss: 9.645; l2dist: 2.905\n",
      "    step: 400; loss: 9.392; l2dist: 2.886\n",
      "    step: 450; loss: 9.324; l2dist: 2.867\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.384; l2dist: 0.000\n",
      "    step: 50; loss: 12.514; l2dist: 1.931\n",
      "    step: 100; loss: 12.263; l2dist: 1.948\n",
      "    step: 150; loss: 11.698; l2dist: 2.037\n",
      "    step: 200; loss: 11.194; l2dist: 2.120\n",
      "    step: 250; loss: 10.731; l2dist: 2.185\n",
      "    step: 300; loss: 10.265; l2dist: 2.197\n",
      "    step: 350; loss: 10.067; l2dist: 2.205\n",
      "    step: 400; loss: 9.934; l2dist: 2.210\n",
      "    step: 450; loss: 9.893; l2dist: 2.209\n",
      "binary step: 0; number of successful adv: 36/100\n",
      "    step: 0; loss: 113.113; l2dist: 0.000\n",
      "    step: 50; loss: 34.402; l2dist: 4.075\n",
      "    step: 100; loss: 19.023; l2dist: 3.721\n",
      "    step: 150; loss: 13.473; l2dist: 3.297\n",
      "    step: 200; loss: 11.585; l2dist: 3.082\n",
      "    step: 250; loss: 10.638; l2dist: 2.969\n",
      "    step: 300; loss: 10.086; l2dist: 2.896\n",
      "    step: 350; loss: 9.875; l2dist: 2.857\n",
      "    step: 400; loss: 9.576; l2dist: 2.832\n",
      "    step: 450; loss: 9.421; l2dist: 2.803\n",
      "binary step: 1; number of successful adv: 97/100\n",
      "    step: 0; loss: 89.376; l2dist: 0.000\n",
      "    step: 50; loss: 27.327; l2dist: 3.680\n",
      "    step: 100; loss: 18.798; l2dist: 3.500\n",
      "    step: 150; loss: 13.391; l2dist: 3.231\n",
      "    step: 200; loss: 11.155; l2dist: 3.026\n",
      "    step: 250; loss: 10.260; l2dist: 2.930\n",
      "    step: 300; loss: 9.800; l2dist: 2.870\n",
      "    step: 350; loss: 9.580; l2dist: 2.841\n",
      "    step: 400; loss: 9.368; l2dist: 2.805\n",
      "    step: 450; loss: 9.155; l2dist: 2.779\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.944; l2dist: 0.000\n",
      "    step: 50; loss: 23.435; l2dist: 3.357\n",
      "    step: 100; loss: 17.971; l2dist: 3.271\n",
      "    step: 150; loss: 13.256; l2dist: 3.136\n",
      "    step: 200; loss: 11.004; l2dist: 2.986\n",
      "    step: 250; loss: 10.021; l2dist: 2.872\n",
      "    step: 300; loss: 9.540; l2dist: 2.830\n",
      "    step: 350; loss: 9.350; l2dist: 2.785\n",
      "    step: 400; loss: 9.093; l2dist: 2.763\n",
      "    step: 450; loss: 8.933; l2dist: 2.745\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.077; l2dist: 0.000\n",
      "    step: 50; loss: 21.234; l2dist: 3.141\n",
      "    step: 100; loss: 17.158; l2dist: 3.091\n",
      "    step: 150; loss: 13.455; l2dist: 3.042\n",
      "    step: 200; loss: 11.088; l2dist: 2.951\n",
      "    step: 250; loss: 10.021; l2dist: 2.870\n",
      "    step: 300; loss: 9.659; l2dist: 2.812\n",
      "    step: 350; loss: 9.375; l2dist: 2.786\n",
      "    step: 400; loss: 9.151; l2dist: 2.760\n",
      "    step: 450; loss: 9.094; l2dist: 2.756\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.563; l2dist: 0.000\n",
      "    step: 50; loss: 20.276; l2dist: 3.042\n",
      "    step: 100; loss: 16.776; l2dist: 2.985\n",
      "    step: 150; loss: 13.101; l2dist: 2.984\n",
      "    step: 200; loss: 11.112; l2dist: 2.904\n",
      "    step: 250; loss: 10.038; l2dist: 2.821\n",
      "    step: 300; loss: 9.496; l2dist: 2.776\n",
      "    step: 350; loss: 9.174; l2dist: 2.751\n",
      "    step: 400; loss: 9.010; l2dist: 2.731\n",
      "    step: 450; loss: 8.869; l2dist: 2.717\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.873; l2dist: 0.000\n",
      "    step: 50; loss: 20.081; l2dist: 3.035\n",
      "    step: 100; loss: 16.522; l2dist: 2.970\n",
      "    step: 150; loss: 13.084; l2dist: 2.961\n",
      "    step: 200; loss: 11.159; l2dist: 2.902\n",
      "    step: 250; loss: 10.086; l2dist: 2.828\n",
      "    step: 300; loss: 9.588; l2dist: 2.771\n",
      "    step: 350; loss: 9.200; l2dist: 2.749\n",
      "    step: 400; loss: 9.029; l2dist: 2.746\n",
      "    step: 450; loss: 8.969; l2dist: 2.730\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.803; l2dist: 0.000\n",
      "    step: 50; loss: 19.991; l2dist: 3.030\n",
      "    step: 100; loss: 16.578; l2dist: 2.959\n",
      "    step: 150; loss: 13.263; l2dist: 2.949\n",
      "    step: 200; loss: 11.215; l2dist: 2.903\n",
      "    step: 250; loss: 10.098; l2dist: 2.830\n",
      "    step: 300; loss: 9.567; l2dist: 2.777\n",
      "    step: 350; loss: 9.295; l2dist: 2.747\n",
      "    step: 400; loss: 9.158; l2dist: 2.750\n",
      "    step: 450; loss: 9.025; l2dist: 2.731\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.595; l2dist: 0.000\n",
      "    step: 50; loss: 19.886; l2dist: 3.023\n",
      "    step: 100; loss: 16.502; l2dist: 2.941\n",
      "    step: 150; loss: 13.223; l2dist: 2.925\n",
      "    step: 200; loss: 11.312; l2dist: 2.880\n",
      "    step: 250; loss: 10.232; l2dist: 2.808\n",
      "    step: 300; loss: 9.657; l2dist: 2.779\n",
      "    step: 350; loss: 9.318; l2dist: 2.750\n",
      "    step: 400; loss: 9.169; l2dist: 2.738\n",
      "    step: 450; loss: 9.090; l2dist: 2.735\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.937; l2dist: 0.000\n",
      "    step: 50; loss: 19.944; l2dist: 3.027\n",
      "    step: 100; loss: 16.556; l2dist: 2.952\n",
      "    step: 150; loss: 13.296; l2dist: 2.931\n",
      "    step: 200; loss: 11.343; l2dist: 2.896\n",
      "    step: 250; loss: 10.092; l2dist: 2.824\n",
      "    step: 300; loss: 9.572; l2dist: 2.783\n",
      "    step: 350; loss: 9.253; l2dist: 2.755\n",
      "    step: 400; loss: 9.050; l2dist: 2.749\n",
      "    step: 450; loss: 8.908; l2dist: 2.723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.949; l2dist: 0.000\n",
      "    step: 50; loss: 13.222; l2dist: 1.846\n",
      "    step: 100; loss: 12.900; l2dist: 1.878\n",
      "    step: 150; loss: 12.476; l2dist: 1.938\n",
      "    step: 200; loss: 12.035; l2dist: 2.004\n",
      "    step: 250; loss: 11.719; l2dist: 2.024\n",
      "    step: 300; loss: 11.540; l2dist: 2.047\n",
      "    step: 350; loss: 11.428; l2dist: 2.056\n",
      "    step: 400; loss: 11.189; l2dist: 2.086\n",
      "    step: 450; loss: 11.084; l2dist: 2.082\n",
      "binary step: 0; number of successful adv: 29/100\n",
      "    step: 0; loss: 134.547; l2dist: 0.000\n",
      "    step: 50; loss: 37.242; l2dist: 4.336\n",
      "    step: 100; loss: 21.976; l2dist: 3.969\n",
      "    step: 150; loss: 15.558; l2dist: 3.520\n",
      "    step: 200; loss: 13.187; l2dist: 3.287\n",
      "    step: 250; loss: 12.004; l2dist: 3.132\n",
      "    step: 300; loss: 11.414; l2dist: 3.069\n",
      "    step: 350; loss: 10.949; l2dist: 3.004\n",
      "    step: 400; loss: 10.683; l2dist: 2.971\n",
      "    step: 450; loss: 10.530; l2dist: 2.959\n",
      "binary step: 1; number of successful adv: 87/100\n",
      "    step: 0; loss: 233.989; l2dist: 0.000\n",
      "    step: 50; loss: 45.564; l2dist: 4.710\n",
      "    step: 100; loss: 25.810; l2dist: 4.195\n",
      "    step: 150; loss: 18.067; l2dist: 3.775\n",
      "    step: 200; loss: 14.772; l2dist: 3.483\n",
      "    step: 250; loss: 13.650; l2dist: 3.347\n",
      "    step: 300; loss: 13.121; l2dist: 3.279\n",
      "    step: 350; loss: 12.266; l2dist: 3.180\n",
      "    step: 400; loss: 11.729; l2dist: 3.098\n",
      "    step: 450; loss: 11.260; l2dist: 3.052\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 217.961; l2dist: 0.000\n",
      "    step: 50; loss: 36.961; l2dist: 4.377\n",
      "    step: 100; loss: 23.686; l2dist: 3.963\n",
      "    step: 150; loss: 16.765; l2dist: 3.622\n",
      "    step: 200; loss: 13.880; l2dist: 3.381\n",
      "    step: 250; loss: 12.476; l2dist: 3.191\n",
      "    step: 300; loss: 11.914; l2dist: 3.131\n",
      "    step: 350; loss: 11.315; l2dist: 3.062\n",
      "    step: 400; loss: 11.182; l2dist: 3.031\n",
      "    step: 450; loss: 10.848; l2dist: 3.007\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 135.390; l2dist: 0.000\n",
      "    step: 50; loss: 30.492; l2dist: 3.922\n",
      "    step: 100; loss: 20.634; l2dist: 3.594\n",
      "    step: 150; loss: 15.550; l2dist: 3.388\n",
      "    step: 200; loss: 12.893; l2dist: 3.215\n",
      "    step: 250; loss: 11.883; l2dist: 3.109\n",
      "    step: 300; loss: 11.458; l2dist: 3.059\n",
      "    step: 350; loss: 11.024; l2dist: 3.004\n",
      "    step: 400; loss: 10.905; l2dist: 2.989\n",
      "    step: 450; loss: 10.612; l2dist: 2.964\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 100.675; l2dist: 0.000\n",
      "    step: 50; loss: 27.384; l2dist: 3.641\n",
      "    step: 100; loss: 19.440; l2dist: 3.431\n",
      "    step: 150; loss: 14.955; l2dist: 3.281\n",
      "    step: 200; loss: 12.679; l2dist: 3.161\n",
      "    step: 250; loss: 11.659; l2dist: 3.067\n",
      "    step: 300; loss: 11.444; l2dist: 3.053\n",
      "    step: 350; loss: 10.720; l2dist: 2.975\n",
      "    step: 400; loss: 10.477; l2dist: 2.948\n",
      "    step: 450; loss: 10.223; l2dist: 2.923\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 84.227; l2dist: 0.000\n",
      "    step: 50; loss: 25.513; l2dist: 3.484\n",
      "    step: 100; loss: 19.800; l2dist: 3.291\n",
      "    step: 150; loss: 14.928; l2dist: 3.229\n",
      "    step: 200; loss: 12.739; l2dist: 3.129\n",
      "    step: 250; loss: 11.807; l2dist: 3.082\n",
      "    step: 300; loss: 11.139; l2dist: 3.016\n",
      "    step: 350; loss: 10.656; l2dist: 2.964\n",
      "    step: 400; loss: 10.287; l2dist: 2.921\n",
      "    step: 450; loss: 10.010; l2dist: 2.889\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.969; l2dist: 0.000\n",
      "    step: 50; loss: 24.696; l2dist: 3.416\n",
      "    step: 100; loss: 19.382; l2dist: 3.254\n",
      "    step: 150; loss: 14.830; l2dist: 3.192\n",
      "    step: 200; loss: 12.755; l2dist: 3.092\n",
      "    step: 250; loss: 11.641; l2dist: 3.040\n",
      "    step: 300; loss: 11.033; l2dist: 2.977\n",
      "    step: 350; loss: 10.681; l2dist: 2.957\n",
      "    step: 400; loss: 10.408; l2dist: 2.916\n",
      "    step: 450; loss: 10.271; l2dist: 2.911\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.483; l2dist: 0.000\n",
      "    step: 50; loss: 24.350; l2dist: 3.376\n",
      "    step: 100; loss: 19.215; l2dist: 3.228\n",
      "    step: 150; loss: 14.807; l2dist: 3.170\n",
      "    step: 200; loss: 12.617; l2dist: 3.083\n",
      "    step: 250; loss: 11.555; l2dist: 3.021\n",
      "    step: 300; loss: 10.902; l2dist: 2.962\n",
      "    step: 350; loss: 10.531; l2dist: 2.937\n",
      "    step: 400; loss: 10.166; l2dist: 2.900\n",
      "    step: 450; loss: 10.066; l2dist: 2.876\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.212; l2dist: 0.000\n",
      "    step: 50; loss: 24.453; l2dist: 3.388\n",
      "    step: 100; loss: 19.126; l2dist: 3.252\n",
      "    step: 150; loss: 14.879; l2dist: 3.194\n",
      "    step: 200; loss: 12.598; l2dist: 3.078\n",
      "    step: 250; loss: 11.495; l2dist: 3.024\n",
      "    step: 300; loss: 10.788; l2dist: 2.964\n",
      "    step: 350; loss: 10.445; l2dist: 2.931\n",
      "    step: 400; loss: 10.181; l2dist: 2.899\n",
      "    step: 450; loss: 10.030; l2dist: 2.883\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.507; l2dist: 0.000\n",
      "    step: 50; loss: 15.743; l2dist: 1.986\n",
      "    step: 100; loss: 15.075; l2dist: 2.022\n",
      "    step: 150; loss: 14.499; l2dist: 2.104\n",
      "    step: 200; loss: 13.849; l2dist: 2.191\n",
      "    step: 250; loss: 13.202; l2dist: 2.250\n",
      "    step: 300; loss: 12.746; l2dist: 2.275\n",
      "    step: 350; loss: 12.419; l2dist: 2.285\n",
      "    step: 400; loss: 12.226; l2dist: 2.275\n",
      "    step: 450; loss: 12.111; l2dist: 2.270\n",
      "binary step: 0; number of successful adv: 29/100\n",
      "    step: 0; loss: 153.730; l2dist: 0.000\n",
      "    step: 50; loss: 45.446; l2dist: 4.574\n",
      "    step: 100; loss: 27.003; l2dist: 4.214\n",
      "    step: 150; loss: 17.771; l2dist: 3.793\n",
      "    step: 200; loss: 15.331; l2dist: 3.545\n",
      "    step: 250; loss: 13.958; l2dist: 3.401\n",
      "    step: 300; loss: 13.140; l2dist: 3.320\n",
      "    step: 350; loss: 12.641; l2dist: 3.258\n",
      "    step: 400; loss: 12.374; l2dist: 3.226\n",
      "    step: 450; loss: 12.165; l2dist: 3.189\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 213.951; l2dist: 0.000\n",
      "    step: 50; loss: 51.220; l2dist: 4.843\n",
      "    step: 100; loss: 27.256; l2dist: 4.420\n",
      "    step: 150; loss: 19.159; l2dist: 3.968\n",
      "    step: 200; loss: 16.037; l2dist: 3.659\n",
      "    step: 250; loss: 14.433; l2dist: 3.487\n",
      "    step: 300; loss: 13.522; l2dist: 3.377\n",
      "    step: 350; loss: 13.293; l2dist: 3.333\n",
      "    step: 400; loss: 12.680; l2dist: 3.273\n",
      "    step: 450; loss: 12.484; l2dist: 3.240\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 129.514; l2dist: 0.000\n",
      "    step: 50; loss: 40.046; l2dist: 4.194\n",
      "    step: 100; loss: 24.761; l2dist: 3.957\n",
      "    step: 150; loss: 17.886; l2dist: 3.767\n",
      "    step: 200; loss: 15.100; l2dist: 3.538\n",
      "    step: 250; loss: 13.752; l2dist: 3.392\n",
      "    step: 300; loss: 13.009; l2dist: 3.300\n",
      "    step: 350; loss: 12.540; l2dist: 3.246\n",
      "    step: 400; loss: 12.345; l2dist: 3.218\n",
      "    step: 450; loss: 11.973; l2dist: 3.177\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.202; l2dist: 0.000\n",
      "    step: 50; loss: 32.484; l2dist: 3.733\n",
      "    step: 100; loss: 23.724; l2dist: 3.579\n",
      "    step: 150; loss: 17.321; l2dist: 3.565\n",
      "    step: 200; loss: 14.554; l2dist: 3.435\n",
      "    step: 250; loss: 13.282; l2dist: 3.321\n",
      "    step: 300; loss: 12.597; l2dist: 3.253\n",
      "    step: 350; loss: 12.100; l2dist: 3.203\n",
      "    step: 400; loss: 11.707; l2dist: 3.158\n",
      "    step: 450; loss: 11.452; l2dist: 3.133\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.827; l2dist: 0.000\n",
      "    step: 50; loss: 28.774; l2dist: 3.470\n",
      "    step: 100; loss: 22.766; l2dist: 3.386\n",
      "    step: 150; loss: 17.535; l2dist: 3.425\n",
      "    step: 200; loss: 14.677; l2dist: 3.330\n",
      "    step: 250; loss: 13.268; l2dist: 3.228\n",
      "    step: 300; loss: 12.537; l2dist: 3.203\n",
      "    step: 350; loss: 11.961; l2dist: 3.164\n",
      "    step: 400; loss: 11.720; l2dist: 3.136\n",
      "    step: 450; loss: 11.438; l2dist: 3.095\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.902; l2dist: 0.000\n",
      "    step: 50; loss: 27.340; l2dist: 3.388\n",
      "    step: 100; loss: 22.191; l2dist: 3.336\n",
      "    step: 150; loss: 17.739; l2dist: 3.354\n",
      "    step: 200; loss: 14.683; l2dist: 3.337\n",
      "    step: 250; loss: 13.470; l2dist: 3.244\n",
      "    step: 300; loss: 12.316; l2dist: 3.169\n",
      "    step: 350; loss: 11.904; l2dist: 3.130\n",
      "    step: 400; loss: 11.670; l2dist: 3.108\n",
      "    step: 450; loss: 11.409; l2dist: 3.087\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.228; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 26.375; l2dist: 3.335\n",
      "    step: 100; loss: 21.892; l2dist: 3.284\n",
      "    step: 150; loss: 17.800; l2dist: 3.314\n",
      "    step: 200; loss: 14.667; l2dist: 3.331\n",
      "    step: 250; loss: 13.204; l2dist: 3.249\n",
      "    step: 300; loss: 12.350; l2dist: 3.182\n",
      "    step: 350; loss: 11.941; l2dist: 3.142\n",
      "    step: 400; loss: 11.583; l2dist: 3.111\n",
      "    step: 450; loss: 11.377; l2dist: 3.085\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.657; l2dist: 0.000\n",
      "    step: 50; loss: 26.063; l2dist: 3.313\n",
      "    step: 100; loss: 21.783; l2dist: 3.275\n",
      "    step: 150; loss: 17.760; l2dist: 3.290\n",
      "    step: 200; loss: 14.785; l2dist: 3.290\n",
      "    step: 250; loss: 13.299; l2dist: 3.208\n",
      "    step: 300; loss: 12.568; l2dist: 3.151\n",
      "    step: 350; loss: 11.946; l2dist: 3.137\n",
      "    step: 400; loss: 11.607; l2dist: 3.102\n",
      "    step: 450; loss: 11.366; l2dist: 3.077\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.528; l2dist: 0.000\n",
      "    step: 50; loss: 26.246; l2dist: 3.334\n",
      "    step: 100; loss: 21.860; l2dist: 3.294\n",
      "    step: 150; loss: 17.839; l2dist: 3.306\n",
      "    step: 200; loss: 14.898; l2dist: 3.304\n",
      "    step: 250; loss: 13.402; l2dist: 3.230\n",
      "    step: 300; loss: 12.616; l2dist: 3.200\n",
      "    step: 350; loss: 11.788; l2dist: 3.145\n",
      "    step: 400; loss: 11.410; l2dist: 3.107\n",
      "    step: 450; loss: 11.300; l2dist: 3.089\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.828; l2dist: 0.000\n",
      "    step: 50; loss: 16.397; l2dist: 2.072\n",
      "    step: 100; loss: 15.658; l2dist: 2.130\n",
      "    step: 150; loss: 14.703; l2dist: 2.277\n",
      "    step: 200; loss: 13.781; l2dist: 2.360\n",
      "    step: 250; loss: 13.289; l2dist: 2.379\n",
      "    step: 300; loss: 12.915; l2dist: 2.395\n",
      "    step: 350; loss: 12.692; l2dist: 2.402\n",
      "    step: 400; loss: 12.566; l2dist: 2.399\n",
      "    step: 450; loss: 12.490; l2dist: 2.405\n",
      "binary step: 0; number of successful adv: 26/100\n",
      "    step: 0; loss: 168.212; l2dist: 0.000\n",
      "    step: 50; loss: 43.702; l2dist: 4.962\n",
      "    step: 100; loss: 24.501; l2dist: 4.251\n",
      "    step: 150; loss: 18.329; l2dist: 3.795\n",
      "    step: 200; loss: 15.866; l2dist: 3.567\n",
      "    step: 250; loss: 14.660; l2dist: 3.445\n",
      "    step: 300; loss: 13.951; l2dist: 3.368\n",
      "    step: 350; loss: 13.609; l2dist: 3.340\n",
      "    step: 400; loss: 13.358; l2dist: 3.315\n",
      "    step: 450; loss: 13.243; l2dist: 3.300\n",
      "binary step: 1; number of successful adv: 95/100\n",
      "    step: 0; loss: 180.313; l2dist: 0.000\n",
      "    step: 50; loss: 42.592; l2dist: 4.892\n",
      "    step: 100; loss: 23.935; l2dist: 4.187\n",
      "    step: 150; loss: 17.837; l2dist: 3.793\n",
      "    step: 200; loss: 15.769; l2dist: 3.601\n",
      "    step: 250; loss: 14.689; l2dist: 3.494\n",
      "    step: 300; loss: 13.985; l2dist: 3.404\n",
      "    step: 350; loss: 13.506; l2dist: 3.360\n",
      "    step: 400; loss: 13.253; l2dist: 3.332\n",
      "    step: 450; loss: 13.195; l2dist: 3.324\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 314.830; l2dist: 0.000\n",
      "    step: 50; loss: 42.564; l2dist: 4.947\n",
      "    step: 100; loss: 25.084; l2dist: 4.308\n",
      "    step: 150; loss: 18.613; l2dist: 3.831\n",
      "    step: 200; loss: 15.997; l2dist: 3.571\n",
      "    step: 250; loss: 14.696; l2dist: 3.437\n",
      "    step: 300; loss: 13.886; l2dist: 3.370\n",
      "    step: 350; loss: 13.433; l2dist: 3.328\n",
      "    step: 400; loss: 13.153; l2dist: 3.303\n",
      "    step: 450; loss: 12.868; l2dist: 3.281\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 196.060; l2dist: 0.000\n",
      "    step: 50; loss: 35.527; l2dist: 4.394\n",
      "    step: 100; loss: 22.580; l2dist: 3.927\n",
      "    step: 150; loss: 17.531; l2dist: 3.661\n",
      "    step: 200; loss: 15.168; l2dist: 3.488\n",
      "    step: 250; loss: 14.052; l2dist: 3.395\n",
      "    step: 300; loss: 13.398; l2dist: 3.342\n",
      "    step: 350; loss: 13.056; l2dist: 3.313\n",
      "    step: 400; loss: 13.061; l2dist: 3.310\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 137.598; l2dist: 0.000\n",
      "    step: 50; loss: 30.951; l2dist: 3.994\n",
      "    step: 100; loss: 21.436; l2dist: 3.694\n",
      "    step: 150; loss: 17.019; l2dist: 3.522\n",
      "    step: 200; loss: 15.034; l2dist: 3.419\n",
      "    step: 250; loss: 13.907; l2dist: 3.333\n",
      "    step: 300; loss: 13.333; l2dist: 3.301\n",
      "    step: 350; loss: 12.981; l2dist: 3.277\n",
      "    step: 400; loss: 12.798; l2dist: 3.254\n",
      "    step: 450; loss: 12.644; l2dist: 3.241\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 112.172; l2dist: 0.000\n",
      "    step: 50; loss: 29.026; l2dist: 3.817\n",
      "    step: 100; loss: 21.133; l2dist: 3.637\n",
      "    step: 150; loss: 16.896; l2dist: 3.503\n",
      "    step: 200; loss: 14.940; l2dist: 3.400\n",
      "    step: 250; loss: 13.777; l2dist: 3.330\n",
      "    step: 300; loss: 13.205; l2dist: 3.284\n",
      "    step: 350; loss: 12.835; l2dist: 3.253\n",
      "    step: 400; loss: 12.568; l2dist: 3.235\n",
      "    step: 450; loss: 12.424; l2dist: 3.215\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 99.791; l2dist: 0.000\n",
      "    step: 50; loss: 28.249; l2dist: 3.734\n",
      "    step: 100; loss: 21.102; l2dist: 3.591\n",
      "    step: 150; loss: 16.789; l2dist: 3.494\n",
      "    step: 200; loss: 14.728; l2dist: 3.406\n",
      "    step: 250; loss: 13.537; l2dist: 3.323\n",
      "    step: 300; loss: 12.966; l2dist: 3.286\n",
      "    step: 350; loss: 12.620; l2dist: 3.248\n",
      "    step: 400; loss: 12.459; l2dist: 3.227\n",
      "    step: 450; loss: 12.291; l2dist: 3.210\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.788; l2dist: 0.000\n",
      "    step: 50; loss: 27.853; l2dist: 3.691\n",
      "    step: 100; loss: 21.022; l2dist: 3.581\n",
      "    step: 150; loss: 16.894; l2dist: 3.498\n",
      "    step: 200; loss: 14.841; l2dist: 3.411\n",
      "    step: 250; loss: 13.703; l2dist: 3.347\n",
      "    step: 300; loss: 13.173; l2dist: 3.310\n",
      "    step: 350; loss: 12.847; l2dist: 3.277\n",
      "    step: 400; loss: 12.604; l2dist: 3.260\n",
      "    step: 450; loss: 12.504; l2dist: 3.249\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 93.535; l2dist: 0.000\n",
      "    step: 50; loss: 27.948; l2dist: 3.707\n",
      "    step: 100; loss: 21.042; l2dist: 3.591\n",
      "    step: 150; loss: 16.918; l2dist: 3.501\n",
      "    step: 200; loss: 14.887; l2dist: 3.399\n",
      "    step: 250; loss: 13.828; l2dist: 3.338\n",
      "    step: 300; loss: 13.296; l2dist: 3.297\n",
      "    step: 350; loss: 12.992; l2dist: 3.272\n",
      "    step: 400; loss: 12.818; l2dist: 3.252\n",
      "    step: 450; loss: 12.656; l2dist: 3.236\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.124; l2dist: 0.000\n",
      "    step: 50; loss: 15.691; l2dist: 2.179\n",
      "    step: 100; loss: 15.027; l2dist: 2.229\n",
      "    step: 150; loss: 14.064; l2dist: 2.360\n",
      "    step: 200; loss: 13.287; l2dist: 2.424\n",
      "    step: 250; loss: 12.881; l2dist: 2.451\n",
      "    step: 300; loss: 12.654; l2dist: 2.468\n",
      "    step: 350; loss: 12.466; l2dist: 2.478\n",
      "    step: 400; loss: 12.288; l2dist: 2.488\n",
      "    step: 450; loss: 12.187; l2dist: 2.489\n",
      "binary step: 0; number of successful adv: 23/100\n",
      "    step: 0; loss: 176.225; l2dist: 0.000\n",
      "    step: 50; loss: 41.564; l2dist: 5.012\n",
      "    step: 100; loss: 23.474; l2dist: 4.341\n",
      "    step: 150; loss: 17.606; l2dist: 3.894\n",
      "    step: 200; loss: 15.508; l2dist: 3.671\n",
      "    step: 250; loss: 14.206; l2dist: 3.542\n",
      "    step: 300; loss: 13.650; l2dist: 3.463\n",
      "    step: 350; loss: 13.338; l2dist: 3.445\n",
      "    step: 400; loss: 13.083; l2dist: 3.410\n",
      "    step: 450; loss: 12.951; l2dist: 3.401\n",
      "binary step: 1; number of successful adv: 95/100\n",
      "    step: 0; loss: 179.681; l2dist: 0.000\n",
      "    step: 50; loss: 37.281; l2dist: 4.703\n",
      "    step: 100; loss: 22.853; l2dist: 4.185\n",
      "    step: 150; loss: 17.258; l2dist: 3.826\n",
      "    step: 200; loss: 15.108; l2dist: 3.630\n",
      "    step: 250; loss: 14.134; l2dist: 3.525\n",
      "    step: 300; loss: 13.673; l2dist: 3.471\n",
      "    step: 350; loss: 13.239; l2dist: 3.422\n",
      "    step: 400; loss: 12.954; l2dist: 3.386\n",
      "    step: 450; loss: 12.856; l2dist: 3.380\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.726; l2dist: 0.000\n",
      "    step: 50; loss: 30.567; l2dist: 4.195\n",
      "    step: 100; loss: 21.254; l2dist: 3.908\n",
      "    step: 150; loss: 16.328; l2dist: 3.665\n",
      "    step: 200; loss: 14.546; l2dist: 3.553\n",
      "    step: 250; loss: 13.508; l2dist: 3.452\n",
      "    step: 300; loss: 13.020; l2dist: 3.403\n",
      "    step: 350; loss: 12.745; l2dist: 3.368\n",
      "    step: 400; loss: 12.604; l2dist: 3.356\n",
      "    step: 450; loss: 12.321; l2dist: 3.326\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.496; l2dist: 0.000\n",
      "    step: 50; loss: 27.809; l2dist: 3.935\n",
      "    step: 100; loss: 20.536; l2dist: 3.740\n",
      "    step: 150; loss: 16.117; l2dist: 3.627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 200; loss: 14.366; l2dist: 3.510\n",
      "    step: 250; loss: 13.316; l2dist: 3.423\n",
      "    step: 300; loss: 12.869; l2dist: 3.377\n",
      "    step: 350; loss: 12.590; l2dist: 3.357\n",
      "    step: 400; loss: 12.461; l2dist: 3.329\n",
      "    step: 450; loss: 12.292; l2dist: 3.317\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.189; l2dist: 0.000\n",
      "    step: 50; loss: 26.468; l2dist: 3.810\n",
      "    step: 100; loss: 20.024; l2dist: 3.659\n",
      "    step: 150; loss: 15.989; l2dist: 3.572\n",
      "    step: 200; loss: 14.160; l2dist: 3.478\n",
      "    step: 250; loss: 13.275; l2dist: 3.388\n",
      "    step: 300; loss: 12.835; l2dist: 3.347\n",
      "    step: 350; loss: 12.554; l2dist: 3.342\n",
      "    step: 400; loss: 12.328; l2dist: 3.323\n",
      "    step: 450; loss: 12.247; l2dist: 3.318\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.137; l2dist: 0.000\n",
      "    step: 50; loss: 26.005; l2dist: 3.746\n",
      "    step: 100; loss: 20.108; l2dist: 3.610\n",
      "    step: 150; loss: 15.923; l2dist: 3.550\n",
      "    step: 200; loss: 13.949; l2dist: 3.461\n",
      "    step: 250; loss: 13.239; l2dist: 3.405\n",
      "    step: 300; loss: 12.614; l2dist: 3.348\n",
      "    step: 350; loss: 12.340; l2dist: 3.323\n",
      "    step: 400; loss: 12.164; l2dist: 3.303\n",
      "    step: 450; loss: 12.089; l2dist: 3.292\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.437; l2dist: 0.000\n",
      "    step: 50; loss: 25.699; l2dist: 3.708\n",
      "    step: 100; loss: 20.058; l2dist: 3.589\n",
      "    step: 150; loss: 15.963; l2dist: 3.522\n",
      "    step: 200; loss: 14.049; l2dist: 3.447\n",
      "    step: 250; loss: 13.234; l2dist: 3.389\n",
      "    step: 300; loss: 12.674; l2dist: 3.354\n",
      "    step: 350; loss: 12.345; l2dist: 3.311\n",
      "    step: 400; loss: 12.181; l2dist: 3.299\n",
      "    step: 450; loss: 12.062; l2dist: 3.288\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.551; l2dist: 0.000\n",
      "    step: 50; loss: 25.607; l2dist: 3.692\n",
      "    step: 100; loss: 19.995; l2dist: 3.567\n",
      "    step: 150; loss: 15.971; l2dist: 3.506\n",
      "    step: 200; loss: 14.017; l2dist: 3.444\n",
      "    step: 250; loss: 13.312; l2dist: 3.387\n",
      "    step: 300; loss: 12.604; l2dist: 3.343\n",
      "    step: 350; loss: 12.353; l2dist: 3.310\n",
      "    step: 400; loss: 12.181; l2dist: 3.305\n",
      "    step: 450; loss: 12.083; l2dist: 3.292\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.927; l2dist: 0.000\n",
      "    step: 50; loss: 25.651; l2dist: 3.715\n",
      "    step: 100; loss: 20.035; l2dist: 3.592\n",
      "    step: 150; loss: 15.981; l2dist: 3.532\n",
      "    step: 200; loss: 14.101; l2dist: 3.467\n",
      "    step: 250; loss: 13.297; l2dist: 3.384\n",
      "    step: 300; loss: 12.639; l2dist: 3.362\n",
      "    step: 350; loss: 12.393; l2dist: 3.334\n",
      "    step: 400; loss: 12.162; l2dist: 3.307\n",
      "    step: 450; loss: 12.022; l2dist: 3.286\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.223; l2dist: 0.000\n",
      "    step: 50; loss: 16.909; l2dist: 2.080\n",
      "    step: 100; loss: 16.434; l2dist: 2.120\n",
      "    step: 150; loss: 15.774; l2dist: 2.217\n",
      "    step: 200; loss: 15.278; l2dist: 2.307\n",
      "    step: 250; loss: 14.923; l2dist: 2.373\n",
      "    step: 300; loss: 14.624; l2dist: 2.404\n",
      "    step: 350; loss: 14.462; l2dist: 2.414\n",
      "    step: 400; loss: 14.314; l2dist: 2.427\n",
      "    step: 450; loss: 14.237; l2dist: 2.430\n",
      "binary step: 0; number of successful adv: 20/100\n",
      "    step: 0; loss: 190.401; l2dist: 0.000\n",
      "    step: 50; loss: 48.693; l2dist: 5.479\n",
      "    step: 100; loss: 26.929; l2dist: 4.761\n",
      "    step: 150; loss: 20.418; l2dist: 4.221\n",
      "    step: 200; loss: 17.893; l2dist: 3.970\n",
      "    step: 250; loss: 16.315; l2dist: 3.801\n",
      "    step: 300; loss: 15.473; l2dist: 3.709\n",
      "    step: 350; loss: 14.999; l2dist: 3.655\n",
      "    step: 400; loss: 14.688; l2dist: 3.620\n",
      "    step: 450; loss: 14.301; l2dist: 3.577\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 289.692; l2dist: 0.000\n",
      "    step: 50; loss: 52.748; l2dist: 5.561\n",
      "    step: 100; loss: 29.881; l2dist: 4.890\n",
      "    step: 150; loss: 22.437; l2dist: 4.380\n",
      "    step: 200; loss: 19.179; l2dist: 4.103\n",
      "    step: 250; loss: 17.356; l2dist: 3.916\n",
      "    step: 300; loss: 16.156; l2dist: 3.802\n",
      "    step: 350; loss: 15.559; l2dist: 3.732\n",
      "    step: 400; loss: 15.088; l2dist: 3.678\n",
      "    step: 450; loss: 14.735; l2dist: 3.641\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 349.046; l2dist: 0.000\n",
      "    step: 50; loss: 46.883; l2dist: 5.310\n",
      "    step: 100; loss: 28.654; l2dist: 4.745\n",
      "    step: 150; loss: 21.473; l2dist: 4.264\n",
      "    step: 200; loss: 17.924; l2dist: 3.979\n",
      "    step: 250; loss: 16.295; l2dist: 3.811\n",
      "    step: 300; loss: 15.362; l2dist: 3.711\n",
      "    step: 350; loss: 14.788; l2dist: 3.639\n",
      "    step: 400; loss: 14.582; l2dist: 3.616\n",
      "    step: 450; loss: 14.587; l2dist: 3.633\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 215.282; l2dist: 0.000\n",
      "    step: 50; loss: 38.351; l2dist: 4.760\n",
      "    step: 100; loss: 25.212; l2dist: 4.292\n",
      "    step: 150; loss: 19.403; l2dist: 3.989\n",
      "    step: 200; loss: 16.626; l2dist: 3.798\n",
      "    step: 250; loss: 15.540; l2dist: 3.695\n",
      "    step: 300; loss: 15.026; l2dist: 3.636\n",
      "    step: 350; loss: 14.641; l2dist: 3.608\n",
      "    step: 400; loss: 14.460; l2dist: 3.589\n",
      "    step: 450; loss: 14.178; l2dist: 3.567\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 151.793; l2dist: 0.000\n",
      "    step: 50; loss: 33.598; l2dist: 4.376\n",
      "    step: 100; loss: 23.618; l2dist: 3.991\n",
      "    step: 150; loss: 19.071; l2dist: 3.803\n",
      "    step: 200; loss: 16.466; l2dist: 3.715\n",
      "    step: 250; loss: 15.457; l2dist: 3.633\n",
      "    step: 300; loss: 14.658; l2dist: 3.575\n",
      "    step: 350; loss: 14.532; l2dist: 3.569\n",
      "    step: 400; loss: 14.207; l2dist: 3.540\n",
      "    step: 450; loss: 13.903; l2dist: 3.504\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.883; l2dist: 0.000\n",
      "    step: 50; loss: 31.616; l2dist: 4.179\n",
      "    step: 100; loss: 23.102; l2dist: 3.867\n",
      "    step: 150; loss: 18.883; l2dist: 3.734\n",
      "    step: 200; loss: 16.541; l2dist: 3.655\n",
      "    step: 250; loss: 15.256; l2dist: 3.613\n",
      "    step: 300; loss: 14.666; l2dist: 3.569\n",
      "    step: 350; loss: 14.464; l2dist: 3.559\n",
      "    step: 400; loss: 14.221; l2dist: 3.533\n",
      "    step: 450; loss: 13.898; l2dist: 3.500\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 108.482; l2dist: 0.000\n",
      "    step: 50; loss: 30.246; l2dist: 4.052\n",
      "    step: 100; loss: 22.761; l2dist: 3.786\n",
      "    step: 150; loss: 18.792; l2dist: 3.686\n",
      "    step: 200; loss: 16.457; l2dist: 3.630\n",
      "    step: 250; loss: 15.259; l2dist: 3.596\n",
      "    step: 300; loss: 14.694; l2dist: 3.563\n",
      "    step: 350; loss: 14.372; l2dist: 3.538\n",
      "    step: 400; loss: 14.135; l2dist: 3.513\n",
      "    step: 450; loss: 13.729; l2dist: 3.483\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 102.386; l2dist: 0.000\n",
      "    step: 50; loss: 29.757; l2dist: 3.999\n",
      "    step: 100; loss: 22.715; l2dist: 3.749\n",
      "    step: 150; loss: 18.803; l2dist: 3.671\n",
      "    step: 200; loss: 16.378; l2dist: 3.633\n",
      "    step: 250; loss: 15.197; l2dist: 3.581\n",
      "    step: 300; loss: 14.687; l2dist: 3.540\n",
      "    step: 350; loss: 14.378; l2dist: 3.521\n",
      "    step: 400; loss: 13.908; l2dist: 3.488\n",
      "    step: 450; loss: 13.844; l2dist: 3.478\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 103.190; l2dist: 0.000\n",
      "    step: 50; loss: 29.977; l2dist: 4.016\n",
      "    step: 100; loss: 22.995; l2dist: 3.763\n",
      "    step: 150; loss: 18.932; l2dist: 3.684\n",
      "    step: 200; loss: 16.450; l2dist: 3.635\n",
      "    step: 250; loss: 15.430; l2dist: 3.604\n",
      "    step: 300; loss: 14.686; l2dist: 3.555\n",
      "    step: 350; loss: 14.227; l2dist: 3.523\n",
      "    step: 400; loss: 13.937; l2dist: 3.503\n",
      "    step: 450; loss: 13.710; l2dist: 3.472\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.549; l2dist: 0.000\n",
      "    step: 50; loss: 15.957; l2dist: 2.025\n",
      "    step: 100; loss: 15.455; l2dist: 2.073\n",
      "    step: 150; loss: 14.535; l2dist: 2.217\n",
      "    step: 200; loss: 13.796; l2dist: 2.300\n",
      "    step: 250; loss: 13.324; l2dist: 2.343\n",
      "    step: 300; loss: 12.897; l2dist: 2.358\n",
      "    step: 350; loss: 12.655; l2dist: 2.365\n",
      "    step: 400; loss: 12.523; l2dist: 2.357\n",
      "    step: 450; loss: 12.391; l2dist: 2.354\n",
      "binary step: 0; number of successful adv: 27/100\n",
      "    step: 0; loss: 147.545; l2dist: 0.000\n",
      "    step: 50; loss: 39.315; l2dist: 4.743\n",
      "    step: 100; loss: 21.729; l2dist: 4.223\n",
      "    step: 150; loss: 16.764; l2dist: 3.790\n",
      "    step: 200; loss: 14.888; l2dist: 3.590\n",
      "    step: 250; loss: 13.833; l2dist: 3.470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 13.105; l2dist: 3.387\n",
      "    step: 350; loss: 12.631; l2dist: 3.336\n",
      "    step: 400; loss: 12.443; l2dist: 3.316\n",
      "    step: 450; loss: 12.354; l2dist: 3.303\n",
      "binary step: 1; number of successful adv: 90/100\n",
      "    step: 0; loss: 250.427; l2dist: 0.000\n",
      "    step: 50; loss: 44.019; l2dist: 5.091\n",
      "    step: 100; loss: 25.484; l2dist: 4.430\n",
      "    step: 150; loss: 18.515; l2dist: 3.979\n",
      "    step: 200; loss: 16.089; l2dist: 3.756\n",
      "    step: 250; loss: 14.987; l2dist: 3.627\n",
      "    step: 300; loss: 14.141; l2dist: 3.528\n",
      "    step: 350; loss: 13.469; l2dist: 3.428\n",
      "    step: 400; loss: 13.250; l2dist: 3.421\n",
      "    step: 450; loss: 13.371; l2dist: 3.436\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 150.740; l2dist: 0.000\n",
      "    step: 50; loss: 34.970; l2dist: 4.459\n",
      "    step: 100; loss: 21.953; l2dist: 4.005\n",
      "    step: 150; loss: 16.560; l2dist: 3.750\n",
      "    step: 200; loss: 14.931; l2dist: 3.608\n",
      "    step: 250; loss: 13.737; l2dist: 3.470\n",
      "    step: 300; loss: 13.104; l2dist: 3.401\n",
      "    step: 350; loss: 12.616; l2dist: 3.347\n",
      "    step: 400; loss: 12.422; l2dist: 3.327\n",
      "    step: 450; loss: 12.221; l2dist: 3.297\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 103.335; l2dist: 0.000\n",
      "    step: 50; loss: 30.425; l2dist: 4.010\n",
      "    step: 100; loss: 20.943; l2dist: 3.777\n",
      "    step: 150; loss: 16.120; l2dist: 3.640\n",
      "    step: 200; loss: 14.364; l2dist: 3.501\n",
      "    step: 250; loss: 13.539; l2dist: 3.426\n",
      "    step: 300; loss: 12.905; l2dist: 3.357\n",
      "    step: 350; loss: 12.494; l2dist: 3.311\n",
      "    step: 400; loss: 12.336; l2dist: 3.282\n",
      "    step: 450; loss: 12.076; l2dist: 3.265\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 84.211; l2dist: 0.000\n",
      "    step: 50; loss: 27.713; l2dist: 3.766\n",
      "    step: 100; loss: 20.508; l2dist: 3.587\n",
      "    step: 150; loss: 16.661; l2dist: 3.525\n",
      "    step: 200; loss: 14.101; l2dist: 3.455\n",
      "    step: 250; loss: 13.269; l2dist: 3.381\n",
      "    step: 300; loss: 12.672; l2dist: 3.324\n",
      "    step: 350; loss: 12.427; l2dist: 3.289\n",
      "    step: 400; loss: 12.194; l2dist: 3.275\n",
      "    step: 450; loss: 11.911; l2dist: 3.245\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.521; l2dist: 0.000\n",
      "    step: 50; loss: 26.271; l2dist: 3.656\n",
      "    step: 100; loss: 20.194; l2dist: 3.528\n",
      "    step: 150; loss: 16.595; l2dist: 3.485\n",
      "    step: 200; loss: 14.299; l2dist: 3.416\n",
      "    step: 250; loss: 13.442; l2dist: 3.384\n",
      "    step: 300; loss: 12.669; l2dist: 3.298\n",
      "    step: 350; loss: 12.390; l2dist: 3.267\n",
      "    step: 400; loss: 12.041; l2dist: 3.234\n",
      "    step: 450; loss: 11.979; l2dist: 3.226\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.019; l2dist: 0.000\n",
      "    step: 50; loss: 25.714; l2dist: 3.585\n",
      "    step: 100; loss: 20.065; l2dist: 3.478\n",
      "    step: 150; loss: 16.547; l2dist: 3.463\n",
      "    step: 200; loss: 14.125; l2dist: 3.409\n",
      "    step: 250; loss: 13.212; l2dist: 3.353\n",
      "    step: 300; loss: 12.566; l2dist: 3.282\n",
      "    step: 350; loss: 12.279; l2dist: 3.252\n",
      "    step: 400; loss: 12.047; l2dist: 3.225\n",
      "    step: 450; loss: 11.910; l2dist: 3.204\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.595; l2dist: 0.000\n",
      "    step: 50; loss: 25.352; l2dist: 3.554\n",
      "    step: 100; loss: 20.054; l2dist: 3.472\n",
      "    step: 150; loss: 16.544; l2dist: 3.450\n",
      "    step: 200; loss: 14.247; l2dist: 3.408\n",
      "    step: 250; loss: 13.235; l2dist: 3.357\n",
      "    step: 300; loss: 12.622; l2dist: 3.284\n",
      "    step: 350; loss: 12.226; l2dist: 3.248\n",
      "    step: 400; loss: 11.962; l2dist: 3.212\n",
      "    step: 450; loss: 11.771; l2dist: 3.183\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.552; l2dist: 0.000\n",
      "    step: 50; loss: 25.501; l2dist: 3.563\n",
      "    step: 100; loss: 20.085; l2dist: 3.479\n",
      "    step: 150; loss: 16.552; l2dist: 3.461\n",
      "    step: 200; loss: 14.124; l2dist: 3.414\n",
      "    step: 250; loss: 13.125; l2dist: 3.343\n",
      "    step: 300; loss: 12.599; l2dist: 3.280\n",
      "    step: 350; loss: 12.150; l2dist: 3.244\n",
      "    step: 400; loss: 12.006; l2dist: 3.213\n",
      "    step: 450; loss: 11.783; l2dist: 3.187\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.641; l2dist: 0.000\n",
      "    step: 50; loss: 15.082; l2dist: 2.037\n",
      "    step: 100; loss: 14.595; l2dist: 2.051\n",
      "    step: 150; loss: 13.853; l2dist: 2.157\n",
      "    step: 200; loss: 13.239; l2dist: 2.228\n",
      "    step: 250; loss: 12.808; l2dist: 2.279\n",
      "    step: 300; loss: 12.449; l2dist: 2.322\n",
      "    step: 350; loss: 12.149; l2dist: 2.328\n",
      "    step: 400; loss: 11.941; l2dist: 2.344\n",
      "    step: 450; loss: 11.733; l2dist: 2.353\n",
      "binary step: 0; number of successful adv: 28/100\n",
      "    step: 0; loss: 146.510; l2dist: 0.000\n",
      "    step: 50; loss: 38.058; l2dist: 4.796\n",
      "    step: 100; loss: 21.842; l2dist: 4.219\n",
      "    step: 150; loss: 16.314; l2dist: 3.712\n",
      "    step: 200; loss: 14.255; l2dist: 3.495\n",
      "    step: 250; loss: 13.461; l2dist: 3.396\n",
      "    step: 300; loss: 12.899; l2dist: 3.344\n",
      "    step: 350; loss: 12.645; l2dist: 3.315\n",
      "    step: 400; loss: 12.443; l2dist: 3.286\n",
      "    step: 450; loss: 12.290; l2dist: 3.268\n",
      "binary step: 1; number of successful adv: 85/100\n",
      "    step: 0; loss: 380.532; l2dist: 0.000\n",
      "    step: 50; loss: 52.284; l2dist: 5.336\n",
      "    step: 100; loss: 28.192; l2dist: 4.744\n",
      "    step: 150; loss: 20.851; l2dist: 4.193\n",
      "    step: 200; loss: 17.455; l2dist: 3.858\n",
      "    step: 250; loss: 15.798; l2dist: 3.682\n",
      "    step: 300; loss: 15.059; l2dist: 3.591\n",
      "    step: 350; loss: 14.307; l2dist: 3.514\n",
      "    step: 400; loss: 13.746; l2dist: 3.472\n",
      "    step: 450; loss: 13.138; l2dist: 3.404\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 222.095; l2dist: 0.000\n",
      "    step: 50; loss: 40.152; l2dist: 4.788\n",
      "    step: 100; loss: 24.076; l2dist: 4.238\n",
      "    step: 150; loss: 17.773; l2dist: 3.856\n",
      "    step: 200; loss: 15.470; l2dist: 3.640\n",
      "    step: 250; loss: 14.274; l2dist: 3.515\n",
      "    step: 300; loss: 13.685; l2dist: 3.441\n",
      "    step: 350; loss: 13.101; l2dist: 3.389\n",
      "    step: 400; loss: 12.566; l2dist: 3.335\n",
      "    step: 450; loss: 12.293; l2dist: 3.305\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 147.047; l2dist: 0.000\n",
      "    step: 50; loss: 32.784; l2dist: 4.318\n",
      "    step: 100; loss: 21.875; l2dist: 3.930\n",
      "    step: 150; loss: 16.695; l2dist: 3.678\n",
      "    step: 200; loss: 14.587; l2dist: 3.529\n",
      "    step: 250; loss: 13.726; l2dist: 3.455\n",
      "    step: 300; loss: 12.855; l2dist: 3.365\n",
      "    step: 350; loss: 12.567; l2dist: 3.334\n",
      "    step: 400; loss: 12.137; l2dist: 3.285\n",
      "    step: 450; loss: 12.038; l2dist: 3.271\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.872; l2dist: 0.000\n",
      "    step: 50; loss: 29.352; l2dist: 4.043\n",
      "    step: 100; loss: 20.914; l2dist: 3.756\n",
      "    step: 150; loss: 16.327; l2dist: 3.591\n",
      "    step: 200; loss: 14.000; l2dist: 3.432\n",
      "    step: 250; loss: 13.366; l2dist: 3.383\n",
      "    step: 300; loss: 12.644; l2dist: 3.307\n",
      "    step: 350; loss: 12.173; l2dist: 3.264\n",
      "    step: 400; loss: 11.847; l2dist: 3.228\n",
      "    step: 450; loss: 11.875; l2dist: 3.228\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 96.690; l2dist: 0.000\n",
      "    step: 50; loss: 27.345; l2dist: 3.888\n",
      "    step: 100; loss: 20.050; l2dist: 3.664\n",
      "    step: 150; loss: 15.834; l2dist: 3.539\n",
      "    step: 200; loss: 13.682; l2dist: 3.398\n",
      "    step: 250; loss: 12.936; l2dist: 3.336\n",
      "    step: 300; loss: 12.432; l2dist: 3.292\n",
      "    step: 350; loss: 11.881; l2dist: 3.244\n",
      "    step: 400; loss: 11.752; l2dist: 3.224\n",
      "    step: 450; loss: 11.536; l2dist: 3.208\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.768; l2dist: 0.000\n",
      "    step: 50; loss: 26.609; l2dist: 3.817\n",
      "    step: 100; loss: 19.920; l2dist: 3.618\n",
      "    step: 150; loss: 15.867; l2dist: 3.519\n",
      "    step: 200; loss: 13.749; l2dist: 3.389\n",
      "    step: 250; loss: 12.834; l2dist: 3.310\n",
      "    step: 300; loss: 12.289; l2dist: 3.260\n",
      "    step: 350; loss: 12.140; l2dist: 3.250\n",
      "    step: 400; loss: 11.777; l2dist: 3.220\n",
      "    step: 450; loss: 11.672; l2dist: 3.193\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 84.946; l2dist: 0.000\n",
      "    step: 50; loss: 26.137; l2dist: 3.774\n",
      "    step: 100; loss: 19.772; l2dist: 3.600\n",
      "    step: 150; loss: 15.786; l2dist: 3.500\n",
      "    step: 200; loss: 13.601; l2dist: 3.378\n",
      "    step: 250; loss: 12.770; l2dist: 3.304\n",
      "    step: 300; loss: 12.453; l2dist: 3.272\n",
      "    step: 350; loss: 12.013; l2dist: 3.236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 11.809; l2dist: 3.208\n",
      "    step: 450; loss: 11.552; l2dist: 3.188\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.772; l2dist: 0.000\n",
      "    step: 50; loss: 26.320; l2dist: 3.799\n",
      "    step: 100; loss: 19.995; l2dist: 3.613\n",
      "    step: 150; loss: 15.898; l2dist: 3.521\n",
      "    step: 200; loss: 13.801; l2dist: 3.420\n",
      "    step: 250; loss: 12.826; l2dist: 3.328\n",
      "    step: 300; loss: 12.370; l2dist: 3.299\n",
      "    step: 350; loss: 12.087; l2dist: 3.268\n",
      "    step: 400; loss: 11.809; l2dist: 3.238\n",
      "    step: 450; loss: 11.651; l2dist: 3.223\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.104; l2dist: 0.000\n",
      "    step: 50; loss: 14.302; l2dist: 1.876\n",
      "    step: 100; loss: 13.982; l2dist: 1.893\n",
      "    step: 150; loss: 13.527; l2dist: 1.993\n",
      "    step: 200; loss: 12.997; l2dist: 2.107\n",
      "    step: 250; loss: 12.396; l2dist: 2.175\n",
      "    step: 300; loss: 11.947; l2dist: 2.208\n",
      "    step: 350; loss: 11.574; l2dist: 2.233\n",
      "    step: 400; loss: 11.231; l2dist: 2.251\n",
      "    step: 450; loss: 11.113; l2dist: 2.243\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 132.792; l2dist: 0.000\n",
      "    step: 50; loss: 36.509; l2dist: 4.511\n",
      "    step: 100; loss: 22.670; l2dist: 4.059\n",
      "    step: 150; loss: 16.136; l2dist: 3.627\n",
      "    step: 200; loss: 13.805; l2dist: 3.378\n",
      "    step: 250; loss: 12.827; l2dist: 3.289\n",
      "    step: 300; loss: 12.242; l2dist: 3.204\n",
      "    step: 350; loss: 11.995; l2dist: 3.184\n",
      "    step: 400; loss: 11.722; l2dist: 3.149\n",
      "    step: 450; loss: 11.590; l2dist: 3.137\n",
      "binary step: 1; number of successful adv: 89/100\n",
      "    step: 0; loss: 212.460; l2dist: 0.000\n",
      "    step: 50; loss: 41.366; l2dist: 4.561\n",
      "    step: 100; loss: 23.978; l2dist: 4.228\n",
      "    step: 150; loss: 17.212; l2dist: 3.776\n",
      "    step: 200; loss: 14.418; l2dist: 3.491\n",
      "    step: 250; loss: 13.366; l2dist: 3.372\n",
      "    step: 300; loss: 12.610; l2dist: 3.285\n",
      "    step: 350; loss: 12.460; l2dist: 3.270\n",
      "    step: 400; loss: 12.080; l2dist: 3.215\n",
      "    step: 450; loss: 11.682; l2dist: 3.175\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 131.552; l2dist: 0.000\n",
      "    step: 50; loss: 33.237; l2dist: 4.063\n",
      "    step: 100; loss: 21.616; l2dist: 3.853\n",
      "    step: 150; loss: 15.976; l2dist: 3.580\n",
      "    step: 200; loss: 13.830; l2dist: 3.419\n",
      "    step: 250; loss: 12.618; l2dist: 3.287\n",
      "    step: 300; loss: 12.088; l2dist: 3.220\n",
      "    step: 350; loss: 11.773; l2dist: 3.192\n",
      "    step: 400; loss: 11.549; l2dist: 3.152\n",
      "    step: 450; loss: 11.419; l2dist: 3.142\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.693; l2dist: 0.000\n",
      "    step: 50; loss: 27.658; l2dist: 3.680\n",
      "    step: 100; loss: 20.356; l2dist: 3.513\n",
      "    step: 150; loss: 15.580; l2dist: 3.409\n",
      "    step: 200; loss: 13.327; l2dist: 3.282\n",
      "    step: 250; loss: 12.357; l2dist: 3.218\n",
      "    step: 300; loss: 11.922; l2dist: 3.173\n",
      "    step: 350; loss: 11.660; l2dist: 3.143\n",
      "    step: 400; loss: 11.264; l2dist: 3.097\n",
      "    step: 450; loss: 11.232; l2dist: 3.102\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.514; l2dist: 0.000\n",
      "    step: 50; loss: 25.356; l2dist: 3.470\n",
      "    step: 100; loss: 19.489; l2dist: 3.342\n",
      "    step: 150; loss: 15.609; l2dist: 3.318\n",
      "    step: 200; loss: 13.602; l2dist: 3.226\n",
      "    step: 250; loss: 12.381; l2dist: 3.156\n",
      "    step: 300; loss: 11.751; l2dist: 3.120\n",
      "    step: 350; loss: 11.432; l2dist: 3.094\n",
      "    step: 400; loss: 11.273; l2dist: 3.070\n",
      "    step: 450; loss: 11.105; l2dist: 3.059\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.755; l2dist: 0.000\n",
      "    step: 50; loss: 24.555; l2dist: 3.367\n",
      "    step: 100; loss: 19.617; l2dist: 3.275\n",
      "    step: 150; loss: 15.606; l2dist: 3.268\n",
      "    step: 200; loss: 13.630; l2dist: 3.195\n",
      "    step: 250; loss: 12.371; l2dist: 3.144\n",
      "    step: 300; loss: 11.653; l2dist: 3.092\n",
      "    step: 350; loss: 11.311; l2dist: 3.068\n",
      "    step: 400; loss: 11.037; l2dist: 3.032\n",
      "    step: 450; loss: 10.994; l2dist: 3.022\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.819; l2dist: 0.000\n",
      "    step: 50; loss: 23.987; l2dist: 3.324\n",
      "    step: 100; loss: 19.402; l2dist: 3.240\n",
      "    step: 150; loss: 15.598; l2dist: 3.247\n",
      "    step: 200; loss: 13.571; l2dist: 3.189\n",
      "    step: 250; loss: 12.330; l2dist: 3.141\n",
      "    step: 300; loss: 11.765; l2dist: 3.099\n",
      "    step: 350; loss: 11.336; l2dist: 3.079\n",
      "    step: 400; loss: 11.107; l2dist: 3.055\n",
      "    step: 450; loss: 11.010; l2dist: 3.052\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.752; l2dist: 0.000\n",
      "    step: 50; loss: 23.697; l2dist: 3.297\n",
      "    step: 100; loss: 19.397; l2dist: 3.222\n",
      "    step: 150; loss: 15.662; l2dist: 3.242\n",
      "    step: 200; loss: 13.703; l2dist: 3.172\n",
      "    step: 250; loss: 12.385; l2dist: 3.129\n",
      "    step: 300; loss: 11.812; l2dist: 3.103\n",
      "    step: 350; loss: 11.404; l2dist: 3.089\n",
      "    step: 400; loss: 11.159; l2dist: 3.074\n",
      "    step: 450; loss: 11.054; l2dist: 3.077\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.559; l2dist: 0.000\n",
      "    step: 50; loss: 23.808; l2dist: 3.315\n",
      "    step: 100; loss: 19.432; l2dist: 3.236\n",
      "    step: 150; loss: 15.660; l2dist: 3.248\n",
      "    step: 200; loss: 13.733; l2dist: 3.197\n",
      "    step: 250; loss: 12.452; l2dist: 3.150\n",
      "    step: 300; loss: 11.891; l2dist: 3.121\n",
      "    step: 350; loss: 11.450; l2dist: 3.101\n",
      "    step: 400; loss: 11.160; l2dist: 3.075\n",
      "    step: 450; loss: 10.949; l2dist: 3.059\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.541; l2dist: 0.000\n",
      "    step: 50; loss: 15.943; l2dist: 1.876\n",
      "    step: 100; loss: 15.637; l2dist: 1.910\n",
      "    step: 150; loss: 15.063; l2dist: 2.038\n",
      "    step: 200; loss: 14.487; l2dist: 2.143\n",
      "    step: 250; loss: 14.121; l2dist: 2.191\n",
      "    step: 300; loss: 13.789; l2dist: 2.217\n",
      "    step: 350; loss: 13.535; l2dist: 2.230\n",
      "    step: 400; loss: 13.370; l2dist: 2.249\n",
      "    step: 450; loss: 13.243; l2dist: 2.258\n",
      "binary step: 0; number of successful adv: 26/100\n",
      "    step: 0; loss: 156.399; l2dist: 0.000\n",
      "    step: 50; loss: 48.939; l2dist: 4.868\n",
      "    step: 100; loss: 25.689; l2dist: 4.471\n",
      "    step: 150; loss: 18.683; l2dist: 3.941\n",
      "    step: 200; loss: 16.048; l2dist: 3.661\n",
      "    step: 250; loss: 14.571; l2dist: 3.517\n",
      "    step: 300; loss: 13.848; l2dist: 3.448\n",
      "    step: 350; loss: 13.362; l2dist: 3.398\n",
      "    step: 400; loss: 13.156; l2dist: 3.378\n",
      "    step: 450; loss: 12.873; l2dist: 3.344\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 222.800; l2dist: 0.000\n",
      "    step: 50; loss: 49.065; l2dist: 4.896\n",
      "    step: 100; loss: 28.056; l2dist: 4.492\n",
      "    step: 150; loss: 20.148; l2dist: 4.030\n",
      "    step: 200; loss: 16.982; l2dist: 3.768\n",
      "    step: 250; loss: 15.353; l2dist: 3.623\n",
      "    step: 300; loss: 14.333; l2dist: 3.510\n",
      "    step: 350; loss: 13.922; l2dist: 3.468\n",
      "    step: 400; loss: 13.503; l2dist: 3.427\n",
      "    step: 450; loss: 13.373; l2dist: 3.414\n",
      "binary step: 2; number of successful adv: 98/100\n",
      "    step: 0; loss: 361.827; l2dist: 0.000\n",
      "    step: 50; loss: 44.437; l2dist: 4.720\n",
      "    step: 100; loss: 27.564; l2dist: 4.370\n",
      "    step: 150; loss: 21.287; l2dist: 4.023\n",
      "    step: 200; loss: 17.553; l2dist: 3.751\n",
      "    step: 250; loss: 15.537; l2dist: 3.579\n",
      "    step: 300; loss: 14.252; l2dist: 3.472\n",
      "    step: 350; loss: 13.507; l2dist: 3.411\n",
      "    step: 400; loss: 13.039; l2dist: 3.370\n",
      "    step: 450; loss: 12.864; l2dist: 3.348\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 217.478; l2dist: 0.000\n",
      "    step: 50; loss: 36.351; l2dist: 4.299\n",
      "    step: 100; loss: 24.333; l2dist: 3.948\n",
      "    step: 150; loss: 19.027; l2dist: 3.727\n",
      "    step: 200; loss: 16.050; l2dist: 3.547\n",
      "    step: 250; loss: 14.285; l2dist: 3.414\n",
      "    step: 300; loss: 13.227; l2dist: 3.342\n",
      "    step: 350; loss: 12.758; l2dist: 3.307\n",
      "    step: 400; loss: 12.386; l2dist: 3.273\n",
      "    step: 450; loss: 12.146; l2dist: 3.263\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 148.980; l2dist: 0.000\n",
      "    step: 50; loss: 32.483; l2dist: 4.024\n",
      "    step: 100; loss: 22.524; l2dist: 3.703\n",
      "    step: 150; loss: 18.330; l2dist: 3.546\n",
      "    step: 200; loss: 15.536; l2dist: 3.454\n",
      "    step: 250; loss: 13.890; l2dist: 3.350\n",
      "    step: 300; loss: 13.099; l2dist: 3.303\n",
      "    step: 350; loss: 12.599; l2dist: 3.270\n",
      "    step: 400; loss: 12.373; l2dist: 3.275\n",
      "    step: 450; loss: 12.577; l2dist: 3.305\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 112.626; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 29.882; l2dist: 3.793\n",
      "    step: 100; loss: 22.007; l2dist: 3.557\n",
      "    step: 150; loss: 17.724; l2dist: 3.469\n",
      "    step: 200; loss: 15.308; l2dist: 3.386\n",
      "    step: 250; loss: 13.832; l2dist: 3.304\n",
      "    step: 300; loss: 12.981; l2dist: 3.259\n",
      "    step: 350; loss: 12.724; l2dist: 3.247\n",
      "    step: 400; loss: 12.497; l2dist: 3.231\n",
      "    step: 450; loss: 12.211; l2dist: 3.217\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.587; l2dist: 0.000\n",
      "    step: 50; loss: 29.828; l2dist: 3.860\n",
      "    step: 100; loss: 21.478; l2dist: 3.583\n",
      "    step: 150; loss: 17.690; l2dist: 3.437\n",
      "    step: 200; loss: 15.385; l2dist: 3.387\n",
      "    step: 250; loss: 13.890; l2dist: 3.318\n",
      "    step: 300; loss: 13.135; l2dist: 3.274\n",
      "    step: 350; loss: 12.658; l2dist: 3.241\n",
      "    step: 400; loss: 12.441; l2dist: 3.234\n",
      "    step: 450; loss: 12.270; l2dist: 3.225\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 129.949; l2dist: 0.000\n",
      "    step: 50; loss: 29.783; l2dist: 3.850\n",
      "    step: 100; loss: 21.477; l2dist: 3.541\n",
      "    step: 150; loss: 17.869; l2dist: 3.434\n",
      "    step: 200; loss: 15.544; l2dist: 3.371\n",
      "    step: 250; loss: 13.979; l2dist: 3.300\n",
      "    step: 300; loss: 13.161; l2dist: 3.270\n",
      "    step: 350; loss: 12.624; l2dist: 3.245\n",
      "    step: 400; loss: 12.396; l2dist: 3.240\n",
      "    step: 450; loss: 12.150; l2dist: 3.224\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 134.537; l2dist: 0.000\n",
      "    step: 50; loss: 30.179; l2dist: 3.878\n",
      "    step: 100; loss: 21.658; l2dist: 3.575\n",
      "    step: 150; loss: 17.871; l2dist: 3.444\n",
      "    step: 200; loss: 15.566; l2dist: 3.383\n",
      "    step: 250; loss: 13.900; l2dist: 3.306\n",
      "    step: 300; loss: 13.109; l2dist: 3.270\n",
      "    step: 350; loss: 12.610; l2dist: 3.240\n",
      "    step: 400; loss: 12.289; l2dist: 3.221\n",
      "    step: 450; loss: 12.126; l2dist: 3.203\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.200; l2dist: 0.000\n",
      "    step: 50; loss: 16.648; l2dist: 2.015\n",
      "    step: 100; loss: 16.157; l2dist: 2.028\n",
      "    step: 150; loss: 15.481; l2dist: 2.153\n",
      "    step: 200; loss: 14.800; l2dist: 2.244\n",
      "    step: 250; loss: 14.356; l2dist: 2.291\n",
      "    step: 300; loss: 13.894; l2dist: 2.321\n",
      "    step: 350; loss: 13.553; l2dist: 2.354\n",
      "    step: 400; loss: 13.305; l2dist: 2.376\n",
      "    step: 450; loss: 13.157; l2dist: 2.376\n",
      "binary step: 0; number of successful adv: 27/100\n",
      "    step: 0; loss: 168.360; l2dist: 0.000\n",
      "    step: 50; loss: 48.507; l2dist: 4.934\n",
      "    step: 100; loss: 27.215; l2dist: 4.431\n",
      "    step: 150; loss: 19.536; l2dist: 4.004\n",
      "    step: 200; loss: 16.270; l2dist: 3.698\n",
      "    step: 250; loss: 14.685; l2dist: 3.548\n",
      "    step: 300; loss: 13.982; l2dist: 3.449\n",
      "    step: 350; loss: 13.381; l2dist: 3.390\n",
      "    step: 400; loss: 13.249; l2dist: 3.368\n",
      "    step: 450; loss: 12.927; l2dist: 3.330\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 261.109; l2dist: 0.000\n",
      "    step: 50; loss: 53.767; l2dist: 5.210\n",
      "    step: 100; loss: 28.518; l2dist: 4.635\n",
      "    step: 150; loss: 20.896; l2dist: 4.093\n",
      "    step: 200; loss: 17.452; l2dist: 3.793\n",
      "    step: 250; loss: 15.565; l2dist: 3.633\n",
      "    step: 300; loss: 14.935; l2dist: 3.560\n",
      "    step: 350; loss: 13.993; l2dist: 3.467\n",
      "    step: 400; loss: 13.950; l2dist: 3.453\n",
      "    step: 450; loss: 13.332; l2dist: 3.388\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 160.472; l2dist: 0.000\n",
      "    step: 50; loss: 41.917; l2dist: 4.663\n",
      "    step: 100; loss: 25.916; l2dist: 4.201\n",
      "    step: 150; loss: 18.907; l2dist: 3.857\n",
      "    step: 200; loss: 15.755; l2dist: 3.633\n",
      "    step: 250; loss: 14.459; l2dist: 3.495\n",
      "    step: 300; loss: 13.612; l2dist: 3.421\n",
      "    step: 350; loss: 13.167; l2dist: 3.371\n",
      "    step: 400; loss: 13.087; l2dist: 3.351\n",
      "    step: 450; loss: 12.999; l2dist: 3.348\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 106.980; l2dist: 0.000\n",
      "    step: 50; loss: 34.447; l2dist: 4.174\n",
      "    step: 100; loss: 23.870; l2dist: 3.834\n",
      "    step: 150; loss: 18.166; l2dist: 3.682\n",
      "    step: 200; loss: 15.324; l2dist: 3.545\n",
      "    step: 250; loss: 13.921; l2dist: 3.425\n",
      "    step: 300; loss: 13.154; l2dist: 3.348\n",
      "    step: 350; loss: 12.695; l2dist: 3.296\n",
      "    step: 400; loss: 12.593; l2dist: 3.294\n",
      "    step: 450; loss: 12.329; l2dist: 3.260\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.575; l2dist: 0.000\n",
      "    step: 50; loss: 31.985; l2dist: 3.916\n",
      "    step: 100; loss: 23.284; l2dist: 3.710\n",
      "    step: 150; loss: 18.242; l2dist: 3.617\n",
      "    step: 200; loss: 15.438; l2dist: 3.500\n",
      "    step: 250; loss: 13.908; l2dist: 3.404\n",
      "    step: 300; loss: 13.021; l2dist: 3.315\n",
      "    step: 350; loss: 12.739; l2dist: 3.304\n",
      "    step: 400; loss: 12.478; l2dist: 3.267\n",
      "    step: 450; loss: 12.305; l2dist: 3.248\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.670; l2dist: 0.000\n",
      "    step: 50; loss: 30.742; l2dist: 3.812\n",
      "    step: 100; loss: 22.871; l2dist: 3.624\n",
      "    step: 150; loss: 18.613; l2dist: 3.555\n",
      "    step: 200; loss: 15.698; l2dist: 3.473\n",
      "    step: 250; loss: 13.990; l2dist: 3.391\n",
      "    step: 300; loss: 13.321; l2dist: 3.344\n",
      "    step: 350; loss: 12.924; l2dist: 3.328\n",
      "    step: 400; loss: 12.492; l2dist: 3.275\n",
      "    step: 450; loss: 12.541; l2dist: 3.288\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 81.277; l2dist: 0.000\n",
      "    step: 50; loss: 29.757; l2dist: 3.737\n",
      "    step: 100; loss: 22.654; l2dist: 3.587\n",
      "    step: 150; loss: 18.586; l2dist: 3.528\n",
      "    step: 200; loss: 15.667; l2dist: 3.455\n",
      "    step: 250; loss: 14.007; l2dist: 3.373\n",
      "    step: 300; loss: 13.220; l2dist: 3.306\n",
      "    step: 350; loss: 12.840; l2dist: 3.297\n",
      "    step: 400; loss: 12.599; l2dist: 3.278\n",
      "    step: 450; loss: 12.394; l2dist: 3.259\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.140; l2dist: 0.000\n",
      "    step: 50; loss: 29.365; l2dist: 3.709\n",
      "    step: 100; loss: 22.452; l2dist: 3.583\n",
      "    step: 150; loss: 18.383; l2dist: 3.522\n",
      "    step: 200; loss: 15.527; l2dist: 3.450\n",
      "    step: 250; loss: 13.944; l2dist: 3.368\n",
      "    step: 300; loss: 13.174; l2dist: 3.323\n",
      "    step: 350; loss: 12.810; l2dist: 3.303\n",
      "    step: 400; loss: 12.469; l2dist: 3.266\n",
      "    step: 450; loss: 12.296; l2dist: 3.248\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.954; l2dist: 0.000\n",
      "    step: 50; loss: 29.518; l2dist: 3.727\n",
      "    step: 100; loss: 22.666; l2dist: 3.592\n",
      "    step: 150; loss: 18.487; l2dist: 3.526\n",
      "    step: 200; loss: 15.588; l2dist: 3.460\n",
      "    step: 250; loss: 13.966; l2dist: 3.381\n",
      "    step: 300; loss: 13.385; l2dist: 3.334\n",
      "    step: 350; loss: 13.020; l2dist: 3.311\n",
      "    step: 400; loss: 12.590; l2dist: 3.280\n",
      "    step: 450; loss: 12.301; l2dist: 3.262\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.642; l2dist: 0.000\n",
      "    step: 50; loss: 13.703; l2dist: 1.819\n",
      "    step: 100; loss: 13.422; l2dist: 1.838\n",
      "    step: 150; loss: 13.061; l2dist: 1.905\n",
      "    step: 200; loss: 12.606; l2dist: 1.986\n",
      "    step: 250; loss: 12.099; l2dist: 2.056\n",
      "    step: 300; loss: 11.577; l2dist: 2.085\n",
      "    step: 350; loss: 11.188; l2dist: 2.099\n",
      "    step: 400; loss: 10.987; l2dist: 2.100\n",
      "    step: 450; loss: 10.877; l2dist: 2.113\n",
      "binary step: 0; number of successful adv: 28/100\n",
      "    step: 0; loss: 132.477; l2dist: 0.000\n",
      "    step: 50; loss: 39.336; l2dist: 4.444\n",
      "    step: 100; loss: 24.604; l2dist: 3.952\n",
      "    step: 150; loss: 16.866; l2dist: 3.659\n",
      "    step: 200; loss: 14.416; l2dist: 3.456\n",
      "    step: 250; loss: 13.213; l2dist: 3.326\n",
      "    step: 300; loss: 12.517; l2dist: 3.242\n",
      "    step: 350; loss: 12.230; l2dist: 3.204\n",
      "    step: 400; loss: 12.020; l2dist: 3.185\n",
      "    step: 450; loss: 11.806; l2dist: 3.162\n",
      "binary step: 1; number of successful adv: 88/100\n",
      "    step: 0; loss: 185.757; l2dist: 0.000\n",
      "    step: 50; loss: 40.687; l2dist: 4.332\n",
      "    step: 100; loss: 27.643; l2dist: 4.165\n",
      "    step: 150; loss: 19.099; l2dist: 3.840\n",
      "    step: 200; loss: 15.489; l2dist: 3.596\n",
      "    step: 250; loss: 13.837; l2dist: 3.406\n",
      "    step: 300; loss: 13.014; l2dist: 3.308\n",
      "    step: 350; loss: 12.614; l2dist: 3.264\n",
      "    step: 400; loss: 12.168; l2dist: 3.210\n",
      "    step: 450; loss: 12.140; l2dist: 3.200\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 116.801; l2dist: 0.000\n",
      "    step: 50; loss: 32.657; l2dist: 3.897\n",
      "    step: 100; loss: 24.738; l2dist: 3.719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 17.970; l2dist: 3.629\n",
      "    step: 200; loss: 14.613; l2dist: 3.465\n",
      "    step: 250; loss: 13.281; l2dist: 3.336\n",
      "    step: 300; loss: 12.394; l2dist: 3.235\n",
      "    step: 350; loss: 11.995; l2dist: 3.181\n",
      "    step: 400; loss: 11.739; l2dist: 3.153\n",
      "    step: 450; loss: 11.522; l2dist: 3.129\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.177; l2dist: 0.000\n",
      "    step: 50; loss: 28.170; l2dist: 3.586\n",
      "    step: 100; loss: 22.447; l2dist: 3.437\n",
      "    step: 150; loss: 17.683; l2dist: 3.415\n",
      "    step: 200; loss: 14.269; l2dist: 3.361\n",
      "    step: 250; loss: 12.914; l2dist: 3.236\n",
      "    step: 300; loss: 12.179; l2dist: 3.175\n",
      "    step: 350; loss: 11.509; l2dist: 3.108\n",
      "    step: 400; loss: 11.271; l2dist: 3.070\n",
      "    step: 450; loss: 11.162; l2dist: 3.048\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.648; l2dist: 0.000\n",
      "    step: 50; loss: 26.684; l2dist: 3.440\n",
      "    step: 100; loss: 21.853; l2dist: 3.320\n",
      "    step: 150; loss: 17.252; l2dist: 3.360\n",
      "    step: 200; loss: 14.372; l2dist: 3.325\n",
      "    step: 250; loss: 12.928; l2dist: 3.219\n",
      "    step: 300; loss: 12.120; l2dist: 3.149\n",
      "    step: 350; loss: 11.923; l2dist: 3.128\n",
      "    step: 400; loss: 11.465; l2dist: 3.093\n",
      "    step: 450; loss: 11.145; l2dist: 3.054\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 80.019; l2dist: 0.000\n",
      "    step: 50; loss: 26.050; l2dist: 3.374\n",
      "    step: 100; loss: 21.754; l2dist: 3.252\n",
      "    step: 150; loss: 17.612; l2dist: 3.312\n",
      "    step: 200; loss: 14.570; l2dist: 3.315\n",
      "    step: 250; loss: 12.928; l2dist: 3.197\n",
      "    step: 300; loss: 12.125; l2dist: 3.144\n",
      "    step: 350; loss: 11.690; l2dist: 3.108\n",
      "    step: 400; loss: 11.271; l2dist: 3.052\n",
      "    step: 450; loss: 11.016; l2dist: 3.035\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.794; l2dist: 0.000\n",
      "    step: 50; loss: 25.591; l2dist: 3.345\n",
      "    step: 100; loss: 21.509; l2dist: 3.218\n",
      "    step: 150; loss: 17.667; l2dist: 3.295\n",
      "    step: 200; loss: 14.694; l2dist: 3.313\n",
      "    step: 250; loss: 13.144; l2dist: 3.232\n",
      "    step: 300; loss: 12.210; l2dist: 3.157\n",
      "    step: 350; loss: 11.887; l2dist: 3.130\n",
      "    step: 400; loss: 11.492; l2dist: 3.091\n",
      "    step: 450; loss: 11.139; l2dist: 3.051\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.423; l2dist: 0.000\n",
      "    step: 50; loss: 25.551; l2dist: 3.346\n",
      "    step: 100; loss: 21.472; l2dist: 3.228\n",
      "    step: 150; loss: 17.577; l2dist: 3.294\n",
      "    step: 200; loss: 14.754; l2dist: 3.317\n",
      "    step: 250; loss: 13.242; l2dist: 3.230\n",
      "    step: 300; loss: 12.259; l2dist: 3.165\n",
      "    step: 350; loss: 11.883; l2dist: 3.141\n",
      "    step: 400; loss: 11.415; l2dist: 3.085\n",
      "    step: 450; loss: 11.197; l2dist: 3.065\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.755; l2dist: 0.000\n",
      "    step: 50; loss: 25.754; l2dist: 3.366\n",
      "    step: 100; loss: 21.588; l2dist: 3.240\n",
      "    step: 150; loss: 17.678; l2dist: 3.309\n",
      "    step: 200; loss: 14.692; l2dist: 3.318\n",
      "    step: 250; loss: 13.098; l2dist: 3.222\n",
      "    step: 300; loss: 12.314; l2dist: 3.173\n",
      "    step: 350; loss: 11.748; l2dist: 3.129\n",
      "    step: 400; loss: 11.381; l2dist: 3.098\n",
      "    step: 450; loss: 11.104; l2dist: 3.053\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.804; l2dist: 0.000\n",
      "    step: 50; loss: 16.340; l2dist: 1.902\n",
      "    step: 100; loss: 15.685; l2dist: 1.941\n",
      "    step: 150; loss: 15.058; l2dist: 2.040\n",
      "    step: 200; loss: 14.573; l2dist: 2.121\n",
      "    step: 250; loss: 14.151; l2dist: 2.167\n",
      "    step: 300; loss: 13.734; l2dist: 2.225\n",
      "    step: 350; loss: 13.436; l2dist: 2.259\n",
      "    step: 400; loss: 13.250; l2dist: 2.275\n",
      "    step: 450; loss: 13.205; l2dist: 2.275\n",
      "binary step: 0; number of successful adv: 16/100\n",
      "    step: 0; loss: 166.588; l2dist: 0.000\n",
      "    step: 50; loss: 46.964; l2dist: 5.216\n",
      "    step: 100; loss: 25.037; l2dist: 4.606\n",
      "    step: 150; loss: 19.351; l2dist: 4.116\n",
      "    step: 200; loss: 16.755; l2dist: 3.850\n",
      "    step: 250; loss: 15.449; l2dist: 3.720\n",
      "    step: 300; loss: 14.417; l2dist: 3.606\n",
      "    step: 350; loss: 14.008; l2dist: 3.565\n",
      "    step: 400; loss: 13.934; l2dist: 3.547\n",
      "    step: 450; loss: 13.548; l2dist: 3.507\n",
      "binary step: 1; number of successful adv: 83/100\n",
      "    step: 0; loss: 308.670; l2dist: 0.000\n",
      "    step: 50; loss: 54.231; l2dist: 5.390\n",
      "    step: 100; loss: 30.900; l2dist: 4.962\n",
      "    step: 150; loss: 22.212; l2dist: 4.413\n",
      "    step: 200; loss: 18.444; l2dist: 4.053\n",
      "    step: 250; loss: 16.388; l2dist: 3.840\n",
      "    step: 300; loss: 15.373; l2dist: 3.721\n",
      "    step: 350; loss: 14.838; l2dist: 3.678\n",
      "    step: 400; loss: 14.541; l2dist: 3.640\n",
      "    step: 450; loss: 14.166; l2dist: 3.591\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 184.374; l2dist: 0.000\n",
      "    step: 50; loss: 42.383; l2dist: 4.835\n",
      "    step: 100; loss: 26.581; l2dist: 4.478\n",
      "    step: 150; loss: 19.459; l2dist: 4.098\n",
      "    step: 200; loss: 16.631; l2dist: 3.838\n",
      "    step: 250; loss: 15.061; l2dist: 3.685\n",
      "    step: 300; loss: 14.415; l2dist: 3.628\n",
      "    step: 350; loss: 14.028; l2dist: 3.553\n",
      "    step: 400; loss: 13.721; l2dist: 3.514\n",
      "    step: 450; loss: 13.406; l2dist: 3.484\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 130.524; l2dist: 0.000\n",
      "    step: 50; loss: 35.406; l2dist: 4.421\n",
      "    step: 100; loss: 24.480; l2dist: 4.130\n",
      "    step: 150; loss: 18.599; l2dist: 3.898\n",
      "    step: 200; loss: 15.640; l2dist: 3.717\n",
      "    step: 250; loss: 14.597; l2dist: 3.614\n",
      "    step: 300; loss: 13.899; l2dist: 3.541\n",
      "    step: 350; loss: 13.506; l2dist: 3.505\n",
      "    step: 400; loss: 13.361; l2dist: 3.484\n",
      "    step: 450; loss: 13.120; l2dist: 3.458\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 107.989; l2dist: 0.000\n",
      "    step: 50; loss: 31.985; l2dist: 4.144\n",
      "    step: 100; loss: 23.486; l2dist: 3.938\n",
      "    step: 150; loss: 18.321; l2dist: 3.799\n",
      "    step: 200; loss: 15.618; l2dist: 3.648\n",
      "    step: 250; loss: 14.392; l2dist: 3.551\n",
      "    step: 300; loss: 13.960; l2dist: 3.495\n",
      "    step: 350; loss: 13.533; l2dist: 3.462\n",
      "    step: 400; loss: 13.405; l2dist: 3.444\n",
      "    step: 450; loss: 13.201; l2dist: 3.422\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 97.824; l2dist: 0.000\n",
      "    step: 50; loss: 30.407; l2dist: 4.028\n",
      "    step: 100; loss: 22.896; l2dist: 3.882\n",
      "    step: 150; loss: 18.002; l2dist: 3.768\n",
      "    step: 200; loss: 15.293; l2dist: 3.632\n",
      "    step: 250; loss: 14.297; l2dist: 3.545\n",
      "    step: 300; loss: 13.556; l2dist: 3.476\n",
      "    step: 350; loss: 13.270; l2dist: 3.448\n",
      "    step: 400; loss: 12.953; l2dist: 3.421\n",
      "    step: 450; loss: 12.822; l2dist: 3.396\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.525; l2dist: 0.000\n",
      "    step: 50; loss: 29.554; l2dist: 3.979\n",
      "    step: 100; loss: 22.820; l2dist: 3.835\n",
      "    step: 150; loss: 18.017; l2dist: 3.745\n",
      "    step: 200; loss: 15.372; l2dist: 3.623\n",
      "    step: 250; loss: 14.324; l2dist: 3.548\n",
      "    step: 300; loss: 13.580; l2dist: 3.472\n",
      "    step: 350; loss: 13.308; l2dist: 3.442\n",
      "    step: 400; loss: 13.018; l2dist: 3.411\n",
      "    step: 450; loss: 12.921; l2dist: 3.394\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.889; l2dist: 0.000\n",
      "    step: 50; loss: 29.162; l2dist: 3.960\n",
      "    step: 100; loss: 22.729; l2dist: 3.829\n",
      "    step: 150; loss: 17.974; l2dist: 3.738\n",
      "    step: 200; loss: 15.367; l2dist: 3.619\n",
      "    step: 250; loss: 14.276; l2dist: 3.538\n",
      "    step: 300; loss: 13.607; l2dist: 3.469\n",
      "    step: 350; loss: 13.300; l2dist: 3.434\n",
      "    step: 400; loss: 12.975; l2dist: 3.407\n",
      "    step: 450; loss: 12.945; l2dist: 3.393\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.098; l2dist: 0.000\n",
      "    step: 50; loss: 29.478; l2dist: 3.976\n",
      "    step: 100; loss: 22.922; l2dist: 3.840\n",
      "    step: 150; loss: 17.935; l2dist: 3.753\n",
      "    step: 200; loss: 15.319; l2dist: 3.634\n",
      "    step: 250; loss: 14.259; l2dist: 3.546\n",
      "    step: 300; loss: 13.665; l2dist: 3.488\n",
      "    step: 350; loss: 13.444; l2dist: 3.475\n",
      "    step: 400; loss: 13.124; l2dist: 3.422\n",
      "    step: 450; loss: 12.938; l2dist: 3.416\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.376; l2dist: 0.000\n",
      "    step: 50; loss: 17.941; l2dist: 2.148\n",
      "    step: 100; loss: 17.369; l2dist: 2.168\n",
      "    step: 150; loss: 16.615; l2dist: 2.263\n",
      "    step: 200; loss: 15.972; l2dist: 2.329\n",
      "    step: 250; loss: 15.411; l2dist: 2.363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 15.066; l2dist: 2.372\n",
      "    step: 350; loss: 14.850; l2dist: 2.381\n",
      "    step: 400; loss: 14.631; l2dist: 2.404\n",
      "    step: 450; loss: 14.555; l2dist: 2.407\n",
      "binary step: 0; number of successful adv: 22/100\n",
      "    step: 0; loss: 169.023; l2dist: 0.000\n",
      "    step: 50; loss: 52.255; l2dist: 5.242\n",
      "    step: 100; loss: 29.365; l2dist: 4.852\n",
      "    step: 150; loss: 21.313; l2dist: 4.282\n",
      "    step: 200; loss: 18.291; l2dist: 3.973\n",
      "    step: 250; loss: 16.681; l2dist: 3.808\n",
      "    step: 300; loss: 15.669; l2dist: 3.697\n",
      "    step: 350; loss: 15.150; l2dist: 3.644\n",
      "    step: 400; loss: 14.787; l2dist: 3.606\n",
      "    step: 450; loss: 14.616; l2dist: 3.585\n",
      "binary step: 1; number of successful adv: 85/100\n",
      "    step: 0; loss: 301.561; l2dist: 0.000\n",
      "    step: 50; loss: 60.766; l2dist: 5.567\n",
      "    step: 100; loss: 33.967; l2dist: 5.156\n",
      "    step: 150; loss: 23.758; l2dist: 4.594\n",
      "    step: 200; loss: 19.816; l2dist: 4.232\n",
      "    step: 250; loss: 17.793; l2dist: 4.015\n",
      "    step: 300; loss: 16.486; l2dist: 3.871\n",
      "    step: 350; loss: 15.939; l2dist: 3.811\n",
      "    step: 400; loss: 15.275; l2dist: 3.732\n",
      "    step: 450; loss: 15.120; l2dist: 3.692\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 178.263; l2dist: 0.000\n",
      "    step: 50; loss: 46.202; l2dist: 4.916\n",
      "    step: 100; loss: 28.993; l2dist: 4.558\n",
      "    step: 150; loss: 21.507; l2dist: 4.263\n",
      "    step: 200; loss: 18.109; l2dist: 4.016\n",
      "    step: 250; loss: 16.545; l2dist: 3.841\n",
      "    step: 300; loss: 15.406; l2dist: 3.734\n",
      "    step: 350; loss: 14.981; l2dist: 3.670\n",
      "    step: 400; loss: 14.496; l2dist: 3.632\n",
      "    step: 450; loss: 14.187; l2dist: 3.594\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 126.846; l2dist: 0.000\n",
      "    step: 50; loss: 37.921; l2dist: 4.412\n",
      "    step: 100; loss: 26.254; l2dist: 4.143\n",
      "    step: 150; loss: 20.449; l2dist: 4.007\n",
      "    step: 200; loss: 17.518; l2dist: 3.850\n",
      "    step: 250; loss: 16.037; l2dist: 3.758\n",
      "    step: 300; loss: 15.213; l2dist: 3.678\n",
      "    step: 350; loss: 14.880; l2dist: 3.643\n",
      "    step: 400; loss: 14.319; l2dist: 3.572\n",
      "    step: 450; loss: 14.114; l2dist: 3.554\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 116.013; l2dist: 0.000\n",
      "    step: 50; loss: 34.982; l2dist: 4.230\n",
      "    step: 100; loss: 25.450; l2dist: 3.995\n",
      "    step: 150; loss: 20.046; l2dist: 3.921\n",
      "    step: 200; loss: 17.350; l2dist: 3.780\n",
      "    step: 250; loss: 16.098; l2dist: 3.690\n",
      "    step: 300; loss: 15.403; l2dist: 3.640\n",
      "    step: 350; loss: 14.803; l2dist: 3.600\n",
      "    step: 400; loss: 14.670; l2dist: 3.577\n",
      "    step: 450; loss: 14.323; l2dist: 3.543\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.943; l2dist: 0.000\n",
      "    step: 50; loss: 34.121; l2dist: 4.160\n",
      "    step: 100; loss: 25.364; l2dist: 3.964\n",
      "    step: 150; loss: 20.187; l2dist: 3.901\n",
      "    step: 200; loss: 17.448; l2dist: 3.770\n",
      "    step: 250; loss: 16.147; l2dist: 3.686\n",
      "    step: 300; loss: 15.460; l2dist: 3.641\n",
      "    step: 350; loss: 14.877; l2dist: 3.606\n",
      "    step: 400; loss: 14.661; l2dist: 3.580\n",
      "    step: 450; loss: 14.295; l2dist: 3.545\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.179; l2dist: 0.000\n",
      "    step: 50; loss: 33.107; l2dist: 4.135\n",
      "    step: 100; loss: 24.909; l2dist: 3.929\n",
      "    step: 150; loss: 19.994; l2dist: 3.877\n",
      "    step: 200; loss: 17.391; l2dist: 3.758\n",
      "    step: 250; loss: 15.793; l2dist: 3.660\n",
      "    step: 300; loss: 15.108; l2dist: 3.599\n",
      "    step: 350; loss: 14.567; l2dist: 3.555\n",
      "    step: 400; loss: 14.191; l2dist: 3.531\n",
      "    step: 450; loss: 14.124; l2dist: 3.521\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 108.752; l2dist: 0.000\n",
      "    step: 50; loss: 32.721; l2dist: 4.100\n",
      "    step: 100; loss: 24.918; l2dist: 3.910\n",
      "    step: 150; loss: 19.925; l2dist: 3.873\n",
      "    step: 200; loss: 17.296; l2dist: 3.751\n",
      "    step: 250; loss: 15.974; l2dist: 3.672\n",
      "    step: 300; loss: 15.304; l2dist: 3.622\n",
      "    step: 350; loss: 14.711; l2dist: 3.584\n",
      "    step: 400; loss: 14.388; l2dist: 3.550\n",
      "    step: 450; loss: 14.256; l2dist: 3.531\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.250; l2dist: 0.000\n",
      "    step: 50; loss: 32.910; l2dist: 4.123\n",
      "    step: 100; loss: 25.056; l2dist: 3.922\n",
      "    step: 150; loss: 19.993; l2dist: 3.884\n",
      "    step: 200; loss: 17.316; l2dist: 3.755\n",
      "    step: 250; loss: 15.915; l2dist: 3.666\n",
      "    step: 300; loss: 15.215; l2dist: 3.631\n",
      "    step: 350; loss: 14.496; l2dist: 3.575\n",
      "    step: 400; loss: 14.303; l2dist: 3.558\n",
      "    step: 450; loss: 13.970; l2dist: 3.518\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.613; l2dist: 0.000\n",
      "    step: 50; loss: 16.576; l2dist: 2.015\n",
      "    step: 100; loss: 16.135; l2dist: 2.035\n",
      "    step: 150; loss: 15.391; l2dist: 2.156\n",
      "    step: 200; loss: 14.702; l2dist: 2.260\n",
      "    step: 250; loss: 14.082; l2dist: 2.295\n",
      "    step: 300; loss: 13.793; l2dist: 2.308\n",
      "    step: 350; loss: 13.527; l2dist: 2.330\n",
      "    step: 400; loss: 13.363; l2dist: 2.328\n",
      "    step: 450; loss: 13.275; l2dist: 2.331\n",
      "binary step: 0; number of successful adv: 20/100\n",
      "    step: 0; loss: 172.691; l2dist: 0.000\n",
      "    step: 50; loss: 51.789; l2dist: 5.055\n",
      "    step: 100; loss: 26.965; l2dist: 4.576\n",
      "    step: 150; loss: 19.872; l2dist: 4.043\n",
      "    step: 200; loss: 16.833; l2dist: 3.793\n",
      "    step: 250; loss: 15.360; l2dist: 3.657\n",
      "    step: 300; loss: 14.467; l2dist: 3.546\n",
      "    step: 350; loss: 14.098; l2dist: 3.498\n",
      "    step: 400; loss: 13.652; l2dist: 3.456\n",
      "    step: 450; loss: 13.486; l2dist: 3.446\n",
      "binary step: 1; number of successful adv: 93/100\n",
      "    step: 0; loss: 194.294; l2dist: 0.000\n",
      "    step: 50; loss: 47.521; l2dist: 4.749\n",
      "    step: 100; loss: 28.306; l2dist: 4.521\n",
      "    step: 150; loss: 19.667; l2dist: 4.079\n",
      "    step: 200; loss: 16.492; l2dist: 3.789\n",
      "    step: 250; loss: 14.996; l2dist: 3.627\n",
      "    step: 300; loss: 14.255; l2dist: 3.545\n",
      "    step: 350; loss: 13.629; l2dist: 3.478\n",
      "    step: 400; loss: 13.350; l2dist: 3.441\n",
      "    step: 450; loss: 13.091; l2dist: 3.412\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.442; l2dist: 0.000\n",
      "    step: 50; loss: 37.766; l2dist: 4.234\n",
      "    step: 100; loss: 26.011; l2dist: 4.141\n",
      "    step: 150; loss: 18.633; l2dist: 3.909\n",
      "    step: 200; loss: 15.576; l2dist: 3.680\n",
      "    step: 250; loss: 14.382; l2dist: 3.560\n",
      "    step: 300; loss: 13.670; l2dist: 3.475\n",
      "    step: 350; loss: 13.285; l2dist: 3.431\n",
      "    step: 400; loss: 13.039; l2dist: 3.401\n",
      "    step: 450; loss: 12.832; l2dist: 3.372\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.573; l2dist: 0.000\n",
      "    step: 50; loss: 32.412; l2dist: 3.871\n",
      "    step: 100; loss: 24.638; l2dist: 3.818\n",
      "    step: 150; loss: 18.292; l2dist: 3.745\n",
      "    step: 200; loss: 15.466; l2dist: 3.612\n",
      "    step: 250; loss: 14.014; l2dist: 3.508\n",
      "    step: 300; loss: 13.367; l2dist: 3.432\n",
      "    step: 350; loss: 12.889; l2dist: 3.385\n",
      "    step: 400; loss: 12.498; l2dist: 3.340\n",
      "    step: 450; loss: 12.486; l2dist: 3.337\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.590; l2dist: 0.000\n",
      "    step: 50; loss: 30.223; l2dist: 3.701\n",
      "    step: 100; loss: 23.736; l2dist: 3.664\n",
      "    step: 150; loss: 18.090; l2dist: 3.668\n",
      "    step: 200; loss: 15.343; l2dist: 3.538\n",
      "    step: 250; loss: 13.890; l2dist: 3.449\n",
      "    step: 300; loss: 13.121; l2dist: 3.388\n",
      "    step: 350; loss: 12.854; l2dist: 3.351\n",
      "    step: 400; loss: 12.505; l2dist: 3.327\n",
      "    step: 450; loss: 12.247; l2dist: 3.301\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.358; l2dist: 0.000\n",
      "    step: 50; loss: 28.979; l2dist: 3.609\n",
      "    step: 100; loss: 23.019; l2dist: 3.604\n",
      "    step: 150; loss: 17.997; l2dist: 3.609\n",
      "    step: 200; loss: 15.125; l2dist: 3.495\n",
      "    step: 250; loss: 13.952; l2dist: 3.419\n",
      "    step: 300; loss: 13.079; l2dist: 3.363\n",
      "    step: 350; loss: 12.621; l2dist: 3.332\n",
      "    step: 400; loss: 12.369; l2dist: 3.302\n",
      "    step: 450; loss: 12.215; l2dist: 3.290\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.720; l2dist: 0.000\n",
      "    step: 50; loss: 28.487; l2dist: 3.582\n",
      "    step: 100; loss: 23.074; l2dist: 3.562\n",
      "    step: 150; loss: 18.068; l2dist: 3.606\n",
      "    step: 200; loss: 15.260; l2dist: 3.490\n",
      "    step: 250; loss: 13.912; l2dist: 3.416\n",
      "    step: 300; loss: 13.227; l2dist: 3.375\n",
      "    step: 350; loss: 12.832; l2dist: 3.345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 12.609; l2dist: 3.324\n",
      "    step: 450; loss: 12.329; l2dist: 3.290\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.651; l2dist: 0.000\n",
      "    step: 50; loss: 28.107; l2dist: 3.566\n",
      "    step: 100; loss: 22.775; l2dist: 3.551\n",
      "    step: 150; loss: 18.045; l2dist: 3.580\n",
      "    step: 200; loss: 15.207; l2dist: 3.500\n",
      "    step: 250; loss: 13.908; l2dist: 3.411\n",
      "    step: 300; loss: 13.142; l2dist: 3.374\n",
      "    step: 350; loss: 12.799; l2dist: 3.343\n",
      "    step: 400; loss: 12.574; l2dist: 3.312\n",
      "    step: 450; loss: 12.307; l2dist: 3.290\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.295; l2dist: 0.000\n",
      "    step: 50; loss: 28.366; l2dist: 3.576\n",
      "    step: 100; loss: 22.936; l2dist: 3.572\n",
      "    step: 150; loss: 18.152; l2dist: 3.589\n",
      "    step: 200; loss: 15.391; l2dist: 3.509\n",
      "    step: 250; loss: 13.926; l2dist: 3.420\n",
      "    step: 300; loss: 13.269; l2dist: 3.386\n",
      "    step: 350; loss: 12.839; l2dist: 3.341\n",
      "    step: 400; loss: 12.537; l2dist: 3.320\n",
      "    step: 450; loss: 12.299; l2dist: 3.302\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.648; l2dist: 0.000\n",
      "    step: 50; loss: 17.320; l2dist: 2.114\n",
      "    step: 100; loss: 16.734; l2dist: 2.146\n",
      "    step: 150; loss: 15.931; l2dist: 2.283\n",
      "    step: 200; loss: 15.278; l2dist: 2.372\n",
      "    step: 250; loss: 14.717; l2dist: 2.411\n",
      "    step: 300; loss: 14.322; l2dist: 2.437\n",
      "    step: 350; loss: 14.028; l2dist: 2.460\n",
      "    step: 400; loss: 13.802; l2dist: 2.486\n",
      "    step: 450; loss: 13.636; l2dist: 2.504\n",
      "binary step: 0; number of successful adv: 27/100\n",
      "    step: 0; loss: 177.857; l2dist: 0.000\n",
      "    step: 50; loss: 50.263; l2dist: 5.250\n",
      "    step: 100; loss: 27.094; l2dist: 4.689\n",
      "    step: 150; loss: 19.997; l2dist: 4.129\n",
      "    step: 200; loss: 16.985; l2dist: 3.831\n",
      "    step: 250; loss: 15.499; l2dist: 3.674\n",
      "    step: 300; loss: 14.609; l2dist: 3.579\n",
      "    step: 350; loss: 14.143; l2dist: 3.513\n",
      "    step: 400; loss: 13.718; l2dist: 3.469\n",
      "    step: 450; loss: 13.602; l2dist: 3.463\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 297.037; l2dist: 0.000\n",
      "    step: 50; loss: 49.898; l2dist: 5.334\n",
      "    step: 100; loss: 29.776; l2dist: 4.813\n",
      "    step: 150; loss: 21.833; l2dist: 4.223\n",
      "    step: 200; loss: 18.149; l2dist: 3.894\n",
      "    step: 250; loss: 16.093; l2dist: 3.704\n",
      "    step: 300; loss: 15.090; l2dist: 3.594\n",
      "    step: 350; loss: 14.635; l2dist: 3.543\n",
      "    step: 400; loss: 13.893; l2dist: 3.468\n",
      "    step: 450; loss: 13.912; l2dist: 3.454\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 409.191; l2dist: 0.000\n",
      "    step: 50; loss: 51.090; l2dist: 5.604\n",
      "    step: 100; loss: 30.179; l2dist: 4.931\n",
      "    step: 150; loss: 22.420; l2dist: 4.363\n",
      "    step: 200; loss: 18.429; l2dist: 3.994\n",
      "    step: 250; loss: 16.442; l2dist: 3.770\n",
      "    step: 300; loss: 15.321; l2dist: 3.650\n",
      "    step: 350; loss: 14.636; l2dist: 3.577\n",
      "    step: 400; loss: 14.787; l2dist: 3.587\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 248.618; l2dist: 0.000\n",
      "    step: 50; loss: 41.617; l2dist: 5.043\n",
      "    step: 100; loss: 26.214; l2dist: 4.409\n",
      "    step: 150; loss: 19.915; l2dist: 4.003\n",
      "    step: 200; loss: 16.636; l2dist: 3.747\n",
      "    step: 250; loss: 15.090; l2dist: 3.596\n",
      "    step: 300; loss: 14.332; l2dist: 3.521\n",
      "    step: 350; loss: 14.230; l2dist: 3.505\n",
      "    step: 400; loss: 13.399; l2dist: 3.414\n",
      "    step: 450; loss: 13.617; l2dist: 3.448\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 166.608; l2dist: 0.000\n",
      "    step: 50; loss: 34.748; l2dist: 4.534\n",
      "    step: 100; loss: 23.167; l2dist: 4.016\n",
      "    step: 150; loss: 18.259; l2dist: 3.764\n",
      "    step: 200; loss: 15.802; l2dist: 3.614\n",
      "    step: 250; loss: 14.706; l2dist: 3.532\n",
      "    step: 300; loss: 13.686; l2dist: 3.424\n",
      "    step: 350; loss: 13.407; l2dist: 3.405\n",
      "    step: 400; loss: 13.091; l2dist: 3.380\n",
      "    step: 450; loss: 13.003; l2dist: 3.368\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 129.687; l2dist: 0.000\n",
      "    step: 50; loss: 32.075; l2dist: 4.271\n",
      "    step: 100; loss: 22.766; l2dist: 3.855\n",
      "    step: 150; loss: 18.242; l2dist: 3.679\n",
      "    step: 200; loss: 16.088; l2dist: 3.557\n",
      "    step: 250; loss: 14.807; l2dist: 3.498\n",
      "    step: 300; loss: 13.996; l2dist: 3.428\n",
      "    step: 350; loss: 13.448; l2dist: 3.385\n",
      "    step: 400; loss: 13.048; l2dist: 3.363\n",
      "    step: 450; loss: 12.727; l2dist: 3.330\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.513; l2dist: 0.000\n",
      "    step: 50; loss: 30.734; l2dist: 4.077\n",
      "    step: 100; loss: 22.380; l2dist: 3.790\n",
      "    step: 150; loss: 18.016; l2dist: 3.623\n",
      "    step: 200; loss: 16.056; l2dist: 3.513\n",
      "    step: 250; loss: 14.822; l2dist: 3.447\n",
      "    step: 300; loss: 13.849; l2dist: 3.399\n",
      "    step: 350; loss: 13.465; l2dist: 3.379\n",
      "    step: 400; loss: 13.066; l2dist: 3.341\n",
      "    step: 450; loss: 12.911; l2dist: 3.325\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 114.618; l2dist: 0.000\n",
      "    step: 50; loss: 30.550; l2dist: 4.114\n",
      "    step: 100; loss: 22.284; l2dist: 3.783\n",
      "    step: 150; loss: 18.008; l2dist: 3.617\n",
      "    step: 200; loss: 16.087; l2dist: 3.530\n",
      "    step: 250; loss: 15.000; l2dist: 3.478\n",
      "    step: 300; loss: 14.136; l2dist: 3.418\n",
      "    step: 350; loss: 13.602; l2dist: 3.387\n",
      "    step: 400; loss: 13.217; l2dist: 3.351\n",
      "    step: 450; loss: 13.225; l2dist: 3.355\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 122.428; l2dist: 0.000\n",
      "    step: 50; loss: 31.056; l2dist: 4.189\n",
      "    step: 100; loss: 22.531; l2dist: 3.811\n",
      "    step: 150; loss: 18.017; l2dist: 3.634\n",
      "    step: 200; loss: 16.045; l2dist: 3.541\n",
      "    step: 250; loss: 14.931; l2dist: 3.485\n",
      "    step: 300; loss: 13.982; l2dist: 3.419\n",
      "    step: 350; loss: 13.380; l2dist: 3.378\n",
      "    step: 400; loss: 13.245; l2dist: 3.376\n",
      "    step: 450; loss: 13.080; l2dist: 3.352\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.299; l2dist: 0.000\n",
      "    step: 50; loss: 15.936; l2dist: 2.034\n",
      "    step: 100; loss: 15.410; l2dist: 2.071\n",
      "    step: 150; loss: 14.700; l2dist: 2.151\n",
      "    step: 200; loss: 14.154; l2dist: 2.225\n",
      "    step: 250; loss: 13.840; l2dist: 2.271\n",
      "    step: 300; loss: 13.569; l2dist: 2.307\n",
      "    step: 350; loss: 13.330; l2dist: 2.338\n",
      "    step: 400; loss: 13.161; l2dist: 2.346\n",
      "    step: 450; loss: 13.046; l2dist: 2.357\n",
      "binary step: 0; number of successful adv: 25/100\n",
      "    step: 0; loss: 157.124; l2dist: 0.000\n",
      "    step: 50; loss: 42.125; l2dist: 4.980\n",
      "    step: 100; loss: 24.515; l2dist: 4.353\n",
      "    step: 150; loss: 18.081; l2dist: 3.924\n",
      "    step: 200; loss: 15.794; l2dist: 3.686\n",
      "    step: 250; loss: 14.715; l2dist: 3.575\n",
      "    step: 300; loss: 14.033; l2dist: 3.497\n",
      "    step: 350; loss: 13.805; l2dist: 3.468\n",
      "    step: 400; loss: 13.450; l2dist: 3.430\n",
      "    step: 450; loss: 13.369; l2dist: 3.413\n",
      "binary step: 1; number of successful adv: 87/100\n",
      "    step: 0; loss: 290.858; l2dist: 0.000\n",
      "    step: 50; loss: 45.861; l2dist: 5.221\n",
      "    step: 100; loss: 28.531; l2dist: 4.655\n",
      "    step: 150; loss: 20.928; l2dist: 4.179\n",
      "    step: 200; loss: 17.423; l2dist: 3.907\n",
      "    step: 250; loss: 15.763; l2dist: 3.728\n",
      "    step: 300; loss: 15.041; l2dist: 3.632\n",
      "    step: 350; loss: 14.434; l2dist: 3.570\n",
      "    step: 400; loss: 14.358; l2dist: 3.552\n",
      "    step: 450; loss: 13.851; l2dist: 3.501\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 169.137; l2dist: 0.000\n",
      "    step: 50; loss: 35.476; l2dist: 4.564\n",
      "    step: 100; loss: 24.599; l2dist: 4.175\n",
      "    step: 150; loss: 18.667; l2dist: 3.873\n",
      "    step: 200; loss: 15.939; l2dist: 3.707\n",
      "    step: 250; loss: 14.839; l2dist: 3.585\n",
      "    step: 300; loss: 14.291; l2dist: 3.542\n",
      "    step: 350; loss: 13.565; l2dist: 3.457\n",
      "    step: 400; loss: 13.243; l2dist: 3.428\n",
      "    step: 450; loss: 13.175; l2dist: 3.418\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.545; l2dist: 0.000\n",
      "    step: 50; loss: 29.760; l2dist: 4.105\n",
      "    step: 100; loss: 22.654; l2dist: 3.848\n",
      "    step: 150; loss: 17.895; l2dist: 3.695\n",
      "    step: 200; loss: 15.373; l2dist: 3.572\n",
      "    step: 250; loss: 14.296; l2dist: 3.501\n",
      "    step: 300; loss: 13.715; l2dist: 3.456\n",
      "    step: 350; loss: 13.303; l2dist: 3.420\n",
      "    step: 400; loss: 13.082; l2dist: 3.393\n",
      "    step: 450; loss: 12.845; l2dist: 3.361\n",
      "binary step: 4; number of successful adv: 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 98.233; l2dist: 0.000\n",
      "    step: 50; loss: 27.531; l2dist: 3.908\n",
      "    step: 100; loss: 22.001; l2dist: 3.689\n",
      "    step: 150; loss: 17.819; l2dist: 3.579\n",
      "    step: 200; loss: 15.326; l2dist: 3.529\n",
      "    step: 250; loss: 14.300; l2dist: 3.446\n",
      "    step: 300; loss: 13.882; l2dist: 3.421\n",
      "    step: 350; loss: 13.394; l2dist: 3.365\n",
      "    step: 400; loss: 13.070; l2dist: 3.336\n",
      "    step: 450; loss: 12.925; l2dist: 3.327\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.082; l2dist: 0.000\n",
      "    step: 50; loss: 26.461; l2dist: 3.823\n",
      "    step: 100; loss: 21.557; l2dist: 3.617\n",
      "    step: 150; loss: 17.668; l2dist: 3.525\n",
      "    step: 200; loss: 15.518; l2dist: 3.486\n",
      "    step: 250; loss: 14.423; l2dist: 3.429\n",
      "    step: 300; loss: 13.632; l2dist: 3.382\n",
      "    step: 350; loss: 13.276; l2dist: 3.354\n",
      "    step: 400; loss: 13.032; l2dist: 3.342\n",
      "    step: 450; loss: 12.904; l2dist: 3.331\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.235; l2dist: 0.000\n",
      "    step: 50; loss: 25.962; l2dist: 3.763\n",
      "    step: 100; loss: 21.429; l2dist: 3.574\n",
      "    step: 150; loss: 17.676; l2dist: 3.491\n",
      "    step: 200; loss: 15.714; l2dist: 3.437\n",
      "    step: 250; loss: 14.404; l2dist: 3.404\n",
      "    step: 300; loss: 13.615; l2dist: 3.356\n",
      "    step: 350; loss: 13.247; l2dist: 3.323\n",
      "    step: 400; loss: 13.079; l2dist: 3.308\n",
      "    step: 450; loss: 12.882; l2dist: 3.300\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.859; l2dist: 0.000\n",
      "    step: 50; loss: 25.705; l2dist: 3.735\n",
      "    step: 100; loss: 21.281; l2dist: 3.565\n",
      "    step: 150; loss: 17.566; l2dist: 3.488\n",
      "    step: 200; loss: 15.434; l2dist: 3.448\n",
      "    step: 250; loss: 14.403; l2dist: 3.398\n",
      "    step: 300; loss: 13.826; l2dist: 3.356\n",
      "    step: 350; loss: 13.520; l2dist: 3.328\n",
      "    step: 400; loss: 13.197; l2dist: 3.314\n",
      "    step: 450; loss: 12.995; l2dist: 3.302\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 85.066; l2dist: 0.000\n",
      "    step: 50; loss: 25.918; l2dist: 3.761\n",
      "    step: 100; loss: 21.396; l2dist: 3.587\n",
      "    step: 150; loss: 17.671; l2dist: 3.509\n",
      "    step: 200; loss: 15.545; l2dist: 3.457\n",
      "    step: 250; loss: 14.384; l2dist: 3.414\n",
      "    step: 300; loss: 13.811; l2dist: 3.355\n",
      "    step: 350; loss: 13.499; l2dist: 3.339\n",
      "    step: 400; loss: 13.288; l2dist: 3.318\n",
      "    step: 450; loss: 13.007; l2dist: 3.309\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.052; l2dist: 0.000\n",
      "    step: 50; loss: 14.733; l2dist: 2.054\n",
      "    step: 100; loss: 14.328; l2dist: 2.056\n",
      "    step: 150; loss: 13.711; l2dist: 2.167\n",
      "    step: 200; loss: 13.126; l2dist: 2.238\n",
      "    step: 250; loss: 12.685; l2dist: 2.281\n",
      "    step: 300; loss: 12.222; l2dist: 2.304\n",
      "    step: 350; loss: 11.997; l2dist: 2.312\n",
      "    step: 400; loss: 11.801; l2dist: 2.307\n",
      "    step: 450; loss: 11.622; l2dist: 2.302\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 143.443; l2dist: 0.000\n",
      "    step: 50; loss: 35.761; l2dist: 4.660\n",
      "    step: 100; loss: 23.474; l2dist: 4.018\n",
      "    step: 150; loss: 16.739; l2dist: 3.586\n",
      "    step: 200; loss: 14.110; l2dist: 3.408\n",
      "    step: 250; loss: 13.184; l2dist: 3.307\n",
      "    step: 300; loss: 12.566; l2dist: 3.233\n",
      "    step: 350; loss: 12.241; l2dist: 3.190\n",
      "    step: 400; loss: 12.073; l2dist: 3.168\n",
      "    step: 450; loss: 11.984; l2dist: 3.163\n",
      "binary step: 1; number of successful adv: 92/100\n",
      "    step: 0; loss: 210.318; l2dist: 0.000\n",
      "    step: 50; loss: 39.626; l2dist: 4.709\n",
      "    step: 100; loss: 24.856; l2dist: 4.051\n",
      "    step: 150; loss: 17.511; l2dist: 3.694\n",
      "    step: 200; loss: 14.988; l2dist: 3.487\n",
      "    step: 250; loss: 13.655; l2dist: 3.359\n",
      "    step: 300; loss: 12.875; l2dist: 3.284\n",
      "    step: 350; loss: 12.523; l2dist: 3.239\n",
      "    step: 400; loss: 12.147; l2dist: 3.206\n",
      "    step: 450; loss: 11.998; l2dist: 3.186\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 129.918; l2dist: 0.000\n",
      "    step: 50; loss: 31.244; l2dist: 4.142\n",
      "    step: 100; loss: 21.904; l2dist: 3.705\n",
      "    step: 150; loss: 16.735; l2dist: 3.500\n",
      "    step: 200; loss: 14.097; l2dist: 3.363\n",
      "    step: 250; loss: 12.986; l2dist: 3.261\n",
      "    step: 300; loss: 12.253; l2dist: 3.191\n",
      "    step: 350; loss: 11.717; l2dist: 3.153\n",
      "    step: 400; loss: 11.528; l2dist: 3.144\n",
      "    step: 450; loss: 11.278; l2dist: 3.114\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 104.552; l2dist: 0.000\n",
      "    step: 50; loss: 28.249; l2dist: 3.916\n",
      "    step: 100; loss: 20.840; l2dist: 3.554\n",
      "    step: 150; loss: 17.018; l2dist: 3.407\n",
      "    step: 200; loss: 14.638; l2dist: 3.315\n",
      "    step: 250; loss: 13.016; l2dist: 3.251\n",
      "    step: 300; loss: 12.187; l2dist: 3.196\n",
      "    step: 350; loss: 11.674; l2dist: 3.156\n",
      "    step: 400; loss: 11.365; l2dist: 3.122\n",
      "    step: 450; loss: 11.185; l2dist: 3.104\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.288; l2dist: 0.000\n",
      "    step: 50; loss: 25.926; l2dist: 3.682\n",
      "    step: 100; loss: 19.986; l2dist: 3.371\n",
      "    step: 150; loss: 16.511; l2dist: 3.316\n",
      "    step: 200; loss: 14.469; l2dist: 3.232\n",
      "    step: 250; loss: 13.421; l2dist: 3.169\n",
      "    step: 300; loss: 12.732; l2dist: 3.108\n",
      "    step: 350; loss: 12.325; l2dist: 3.088\n",
      "    step: 400; loss: 12.126; l2dist: 3.068\n",
      "    step: 450; loss: 12.022; l2dist: 3.075\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.368; l2dist: 0.000\n",
      "    step: 50; loss: 25.010; l2dist: 3.628\n",
      "    step: 100; loss: 19.454; l2dist: 3.324\n",
      "    step: 150; loss: 16.254; l2dist: 3.276\n",
      "    step: 200; loss: 14.489; l2dist: 3.215\n",
      "    step: 250; loss: 13.294; l2dist: 3.161\n",
      "    step: 300; loss: 12.575; l2dist: 3.118\n",
      "    step: 350; loss: 12.157; l2dist: 3.089\n",
      "    step: 400; loss: 11.899; l2dist: 3.068\n",
      "    step: 450; loss: 11.792; l2dist: 3.063\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.472; l2dist: 0.000\n",
      "    step: 50; loss: 24.630; l2dist: 3.576\n",
      "    step: 100; loss: 19.143; l2dist: 3.295\n",
      "    step: 150; loss: 16.337; l2dist: 3.251\n",
      "    step: 200; loss: 14.603; l2dist: 3.218\n",
      "    step: 250; loss: 13.456; l2dist: 3.176\n",
      "    step: 300; loss: 12.643; l2dist: 3.131\n",
      "    step: 350; loss: 12.200; l2dist: 3.101\n",
      "    step: 400; loss: 11.881; l2dist: 3.088\n",
      "    step: 450; loss: 11.706; l2dist: 3.071\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.925; l2dist: 0.000\n",
      "    step: 50; loss: 24.448; l2dist: 3.562\n",
      "    step: 100; loss: 19.064; l2dist: 3.284\n",
      "    step: 150; loss: 16.492; l2dist: 3.242\n",
      "    step: 200; loss: 14.603; l2dist: 3.216\n",
      "    step: 250; loss: 13.440; l2dist: 3.172\n",
      "    step: 300; loss: 12.624; l2dist: 3.119\n",
      "    step: 350; loss: 12.130; l2dist: 3.100\n",
      "    step: 400; loss: 11.926; l2dist: 3.098\n",
      "    step: 450; loss: 11.735; l2dist: 3.079\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.502; l2dist: 0.000\n",
      "    step: 50; loss: 24.507; l2dist: 3.575\n",
      "    step: 100; loss: 19.108; l2dist: 3.293\n",
      "    step: 150; loss: 16.480; l2dist: 3.245\n",
      "    step: 200; loss: 14.647; l2dist: 3.229\n",
      "    step: 250; loss: 13.429; l2dist: 3.162\n",
      "    step: 300; loss: 12.605; l2dist: 3.125\n",
      "    step: 350; loss: 12.135; l2dist: 3.104\n",
      "    step: 400; loss: 11.876; l2dist: 3.094\n",
      "    step: 450; loss: 11.762; l2dist: 3.071\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.324; l2dist: 0.000\n",
      "    step: 50; loss: 15.586; l2dist: 1.896\n",
      "    step: 100; loss: 15.184; l2dist: 1.923\n",
      "    step: 150; loss: 14.491; l2dist: 2.040\n",
      "    step: 200; loss: 13.938; l2dist: 2.092\n",
      "    step: 250; loss: 13.545; l2dist: 2.118\n",
      "    step: 300; loss: 13.230; l2dist: 2.129\n",
      "    step: 350; loss: 13.064; l2dist: 2.135\n",
      "    step: 400; loss: 12.984; l2dist: 2.132\n",
      "    step: 450; loss: 12.962; l2dist: 2.134\n",
      "binary step: 0; number of successful adv: 24/100\n",
      "    step: 0; loss: 154.533; l2dist: 0.000\n",
      "    step: 50; loss: 50.704; l2dist: 4.702\n",
      "    step: 100; loss: 30.191; l2dist: 4.409\n",
      "    step: 150; loss: 20.080; l2dist: 3.969\n",
      "    step: 200; loss: 16.436; l2dist: 3.721\n",
      "    step: 250; loss: 14.754; l2dist: 3.550\n",
      "    step: 300; loss: 13.716; l2dist: 3.430\n",
      "    step: 350; loss: 13.194; l2dist: 3.369\n",
      "    step: 400; loss: 12.849; l2dist: 3.327\n",
      "    step: 450; loss: 12.491; l2dist: 3.284\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 208.251; l2dist: 0.000\n",
      "    step: 50; loss: 50.843; l2dist: 4.910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 30.606; l2dist: 4.557\n",
      "    step: 150; loss: 21.059; l2dist: 4.076\n",
      "    step: 200; loss: 16.984; l2dist: 3.770\n",
      "    step: 250; loss: 15.098; l2dist: 3.614\n",
      "    step: 300; loss: 14.474; l2dist: 3.527\n",
      "    step: 350; loss: 13.650; l2dist: 3.449\n",
      "    step: 400; loss: 13.237; l2dist: 3.399\n",
      "    step: 450; loss: 12.947; l2dist: 3.360\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.422; l2dist: 0.000\n",
      "    step: 50; loss: 38.337; l2dist: 4.309\n",
      "    step: 100; loss: 26.274; l2dist: 4.022\n",
      "    step: 150; loss: 19.390; l2dist: 3.810\n",
      "    step: 200; loss: 15.695; l2dist: 3.606\n",
      "    step: 250; loss: 14.394; l2dist: 3.499\n",
      "    step: 300; loss: 13.329; l2dist: 3.390\n",
      "    step: 350; loss: 12.797; l2dist: 3.332\n",
      "    step: 400; loss: 12.354; l2dist: 3.286\n",
      "    step: 450; loss: 12.246; l2dist: 3.263\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.235; l2dist: 0.000\n",
      "    step: 50; loss: 32.402; l2dist: 3.860\n",
      "    step: 100; loss: 24.220; l2dist: 3.651\n",
      "    step: 150; loss: 19.055; l2dist: 3.616\n",
      "    step: 200; loss: 15.839; l2dist: 3.519\n",
      "    step: 250; loss: 14.043; l2dist: 3.430\n",
      "    step: 300; loss: 13.095; l2dist: 3.341\n",
      "    step: 350; loss: 12.489; l2dist: 3.272\n",
      "    step: 400; loss: 12.095; l2dist: 3.238\n",
      "    step: 450; loss: 11.838; l2dist: 3.217\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.457; l2dist: 0.000\n",
      "    step: 50; loss: 29.557; l2dist: 3.663\n",
      "    step: 100; loss: 22.927; l2dist: 3.488\n",
      "    step: 150; loss: 18.589; l2dist: 3.510\n",
      "    step: 200; loss: 15.978; l2dist: 3.424\n",
      "    step: 250; loss: 13.833; l2dist: 3.360\n",
      "    step: 300; loss: 13.010; l2dist: 3.319\n",
      "    step: 350; loss: 12.413; l2dist: 3.258\n",
      "    step: 400; loss: 11.892; l2dist: 3.201\n",
      "    step: 450; loss: 11.847; l2dist: 3.193\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.313; l2dist: 0.000\n",
      "    step: 50; loss: 28.441; l2dist: 3.550\n",
      "    step: 100; loss: 22.314; l2dist: 3.428\n",
      "    step: 150; loss: 18.463; l2dist: 3.444\n",
      "    step: 200; loss: 15.758; l2dist: 3.369\n",
      "    step: 250; loss: 13.846; l2dist: 3.304\n",
      "    step: 300; loss: 12.745; l2dist: 3.244\n",
      "    step: 350; loss: 12.187; l2dist: 3.207\n",
      "    step: 400; loss: 11.819; l2dist: 3.170\n",
      "    step: 450; loss: 11.707; l2dist: 3.158\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.191; l2dist: 0.000\n",
      "    step: 50; loss: 27.359; l2dist: 3.489\n",
      "    step: 100; loss: 22.042; l2dist: 3.366\n",
      "    step: 150; loss: 18.492; l2dist: 3.401\n",
      "    step: 200; loss: 15.730; l2dist: 3.380\n",
      "    step: 250; loss: 14.001; l2dist: 3.306\n",
      "    step: 300; loss: 12.673; l2dist: 3.245\n",
      "    step: 350; loss: 12.286; l2dist: 3.211\n",
      "    step: 400; loss: 11.773; l2dist: 3.176\n",
      "    step: 450; loss: 11.516; l2dist: 3.129\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.115; l2dist: 0.000\n",
      "    step: 50; loss: 27.152; l2dist: 3.467\n",
      "    step: 100; loss: 21.955; l2dist: 3.361\n",
      "    step: 150; loss: 18.416; l2dist: 3.407\n",
      "    step: 200; loss: 15.615; l2dist: 3.381\n",
      "    step: 250; loss: 13.790; l2dist: 3.301\n",
      "    step: 300; loss: 12.584; l2dist: 3.228\n",
      "    step: 350; loss: 12.174; l2dist: 3.211\n",
      "    step: 400; loss: 11.697; l2dist: 3.165\n",
      "    step: 450; loss: 11.420; l2dist: 3.145\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.859; l2dist: 0.000\n",
      "    step: 50; loss: 27.273; l2dist: 3.485\n",
      "    step: 100; loss: 22.269; l2dist: 3.363\n",
      "    step: 150; loss: 18.507; l2dist: 3.417\n",
      "    step: 200; loss: 15.782; l2dist: 3.391\n",
      "    step: 250; loss: 13.846; l2dist: 3.316\n",
      "    step: 300; loss: 12.637; l2dist: 3.241\n",
      "    step: 350; loss: 12.168; l2dist: 3.210\n",
      "    step: 400; loss: 11.765; l2dist: 3.182\n",
      "    step: 450; loss: 11.443; l2dist: 3.138\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.645; l2dist: 0.000\n",
      "    step: 50; loss: 14.735; l2dist: 1.905\n",
      "    step: 100; loss: 14.198; l2dist: 1.936\n",
      "    step: 150; loss: 13.482; l2dist: 2.033\n",
      "    step: 200; loss: 12.859; l2dist: 2.115\n",
      "    step: 250; loss: 12.406; l2dist: 2.146\n",
      "    step: 300; loss: 12.168; l2dist: 2.157\n",
      "    step: 350; loss: 12.003; l2dist: 2.169\n",
      "    step: 400; loss: 11.915; l2dist: 2.170\n",
      "    step: 450; loss: 11.842; l2dist: 2.175\n",
      "binary step: 0; number of successful adv: 29/100\n",
      "    step: 0; loss: 135.809; l2dist: 0.000\n",
      "    step: 50; loss: 46.337; l2dist: 4.411\n",
      "    step: 100; loss: 29.592; l2dist: 4.053\n",
      "    step: 150; loss: 20.599; l2dist: 3.702\n",
      "    step: 200; loss: 16.032; l2dist: 3.541\n",
      "    step: 250; loss: 14.084; l2dist: 3.436\n",
      "    step: 300; loss: 13.161; l2dist: 3.316\n",
      "    step: 350; loss: 12.519; l2dist: 3.244\n",
      "    step: 400; loss: 12.254; l2dist: 3.214\n",
      "    step: 450; loss: 11.703; l2dist: 3.154\n",
      "binary step: 1; number of successful adv: 89/100\n",
      "    step: 0; loss: 221.990; l2dist: 0.000\n",
      "    step: 50; loss: 49.310; l2dist: 4.623\n",
      "    step: 100; loss: 29.377; l2dist: 4.253\n",
      "    step: 150; loss: 20.372; l2dist: 3.877\n",
      "    step: 200; loss: 16.414; l2dist: 3.606\n",
      "    step: 250; loss: 14.462; l2dist: 3.424\n",
      "    step: 300; loss: 13.539; l2dist: 3.317\n",
      "    step: 350; loss: 12.943; l2dist: 3.275\n",
      "    step: 400; loss: 12.587; l2dist: 3.227\n",
      "    step: 450; loss: 12.217; l2dist: 3.190\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 132.619; l2dist: 0.000\n",
      "    step: 50; loss: 37.084; l2dist: 3.930\n",
      "    step: 100; loss: 25.945; l2dist: 3.668\n",
      "    step: 150; loss: 18.855; l2dist: 3.578\n",
      "    step: 200; loss: 15.293; l2dist: 3.408\n",
      "    step: 250; loss: 13.670; l2dist: 3.305\n",
      "    step: 300; loss: 12.739; l2dist: 3.210\n",
      "    step: 350; loss: 12.149; l2dist: 3.153\n",
      "    step: 400; loss: 11.827; l2dist: 3.128\n",
      "    step: 450; loss: 11.546; l2dist: 3.094\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 95.445; l2dist: 0.000\n",
      "    step: 50; loss: 30.862; l2dist: 3.525\n",
      "    step: 100; loss: 23.726; l2dist: 3.375\n",
      "    step: 150; loss: 19.188; l2dist: 3.372\n",
      "    step: 200; loss: 15.122; l2dist: 3.345\n",
      "    step: 250; loss: 13.314; l2dist: 3.252\n",
      "    step: 300; loss: 12.397; l2dist: 3.181\n",
      "    step: 350; loss: 11.930; l2dist: 3.129\n",
      "    step: 400; loss: 11.681; l2dist: 3.113\n",
      "    step: 450; loss: 11.349; l2dist: 3.085\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 80.066; l2dist: 0.000\n",
      "    step: 50; loss: 27.938; l2dist: 3.349\n",
      "    step: 100; loss: 22.417; l2dist: 3.235\n",
      "    step: 150; loss: 18.397; l2dist: 3.261\n",
      "    step: 200; loss: 15.090; l2dist: 3.271\n",
      "    step: 250; loss: 13.289; l2dist: 3.202\n",
      "    step: 300; loss: 12.237; l2dist: 3.137\n",
      "    step: 350; loss: 11.774; l2dist: 3.095\n",
      "    step: 400; loss: 11.446; l2dist: 3.082\n",
      "    step: 450; loss: 11.234; l2dist: 3.055\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.867; l2dist: 0.000\n",
      "    step: 50; loss: 26.339; l2dist: 3.239\n",
      "    step: 100; loss: 21.510; l2dist: 3.177\n",
      "    step: 150; loss: 18.292; l2dist: 3.161\n",
      "    step: 200; loss: 15.367; l2dist: 3.195\n",
      "    step: 250; loss: 13.125; l2dist: 3.166\n",
      "    step: 300; loss: 12.103; l2dist: 3.122\n",
      "    step: 350; loss: 11.552; l2dist: 3.083\n",
      "    step: 400; loss: 11.195; l2dist: 3.051\n",
      "    step: 450; loss: 11.079; l2dist: 3.046\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.486; l2dist: 0.000\n",
      "    step: 50; loss: 25.456; l2dist: 3.184\n",
      "    step: 100; loss: 21.096; l2dist: 3.137\n",
      "    step: 150; loss: 18.100; l2dist: 3.141\n",
      "    step: 200; loss: 15.556; l2dist: 3.164\n",
      "    step: 250; loss: 13.188; l2dist: 3.166\n",
      "    step: 300; loss: 12.200; l2dist: 3.117\n",
      "    step: 350; loss: 11.573; l2dist: 3.071\n",
      "    step: 400; loss: 11.182; l2dist: 3.050\n",
      "    step: 450; loss: 11.042; l2dist: 3.037\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.778; l2dist: 0.000\n",
      "    step: 50; loss: 25.041; l2dist: 3.176\n",
      "    step: 100; loss: 20.943; l2dist: 3.127\n",
      "    step: 150; loss: 17.974; l2dist: 3.132\n",
      "    step: 200; loss: 15.638; l2dist: 3.149\n",
      "    step: 250; loss: 13.455; l2dist: 3.147\n",
      "    step: 300; loss: 12.361; l2dist: 3.099\n",
      "    step: 350; loss: 11.805; l2dist: 3.079\n",
      "    step: 400; loss: 11.412; l2dist: 3.056\n",
      "    step: 450; loss: 11.085; l2dist: 3.031\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.429; l2dist: 0.000\n",
      "    step: 50; loss: 25.213; l2dist: 3.183\n",
      "    step: 100; loss: 21.060; l2dist: 3.135\n",
      "    step: 150; loss: 17.994; l2dist: 3.146\n",
      "    step: 200; loss: 15.548; l2dist: 3.162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 250; loss: 13.478; l2dist: 3.164\n",
      "    step: 300; loss: 12.417; l2dist: 3.120\n",
      "    step: 350; loss: 11.818; l2dist: 3.087\n",
      "    step: 400; loss: 11.501; l2dist: 3.067\n",
      "    step: 450; loss: 11.182; l2dist: 3.042\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.209; l2dist: 0.000\n",
      "    step: 50; loss: 15.384; l2dist: 1.963\n",
      "    step: 100; loss: 14.938; l2dist: 1.979\n",
      "    step: 150; loss: 14.134; l2dist: 2.128\n",
      "    step: 200; loss: 13.475; l2dist: 2.222\n",
      "    step: 250; loss: 12.976; l2dist: 2.266\n",
      "    step: 300; loss: 12.583; l2dist: 2.293\n",
      "    step: 350; loss: 12.259; l2dist: 2.307\n",
      "    step: 400; loss: 12.081; l2dist: 2.308\n",
      "    step: 450; loss: 12.011; l2dist: 2.312\n",
      "binary step: 0; number of successful adv: 31/100\n",
      "    step: 0; loss: 138.210; l2dist: 0.000\n",
      "    step: 50; loss: 39.436; l2dist: 4.657\n",
      "    step: 100; loss: 21.942; l2dist: 4.085\n",
      "    step: 150; loss: 16.689; l2dist: 3.675\n",
      "    step: 200; loss: 14.515; l2dist: 3.449\n",
      "    step: 250; loss: 13.512; l2dist: 3.348\n",
      "    step: 300; loss: 12.828; l2dist: 3.263\n",
      "    step: 350; loss: 12.556; l2dist: 3.230\n",
      "    step: 400; loss: 12.306; l2dist: 3.202\n",
      "    step: 450; loss: 12.119; l2dist: 3.181\n",
      "binary step: 1; number of successful adv: 87/100\n",
      "    step: 0; loss: 287.539; l2dist: 0.000\n",
      "    step: 50; loss: 46.238; l2dist: 4.969\n",
      "    step: 100; loss: 26.667; l2dist: 4.497\n",
      "    step: 150; loss: 19.631; l2dist: 4.000\n",
      "    step: 200; loss: 16.346; l2dist: 3.679\n",
      "    step: 250; loss: 14.738; l2dist: 3.511\n",
      "    step: 300; loss: 13.901; l2dist: 3.422\n",
      "    step: 350; loss: 13.427; l2dist: 3.372\n",
      "    step: 400; loss: 12.887; l2dist: 3.306\n",
      "    step: 450; loss: 12.635; l2dist: 3.271\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 167.293; l2dist: 0.000\n",
      "    step: 50; loss: 35.317; l2dist: 4.341\n",
      "    step: 100; loss: 23.174; l2dist: 4.032\n",
      "    step: 150; loss: 17.301; l2dist: 3.696\n",
      "    step: 200; loss: 14.808; l2dist: 3.507\n",
      "    step: 250; loss: 13.667; l2dist: 3.395\n",
      "    step: 300; loss: 13.010; l2dist: 3.324\n",
      "    step: 350; loss: 12.336; l2dist: 3.253\n",
      "    step: 400; loss: 12.136; l2dist: 3.224\n",
      "    step: 450; loss: 11.989; l2dist: 3.208\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 114.897; l2dist: 0.000\n",
      "    step: 50; loss: 29.771; l2dist: 3.944\n",
      "    step: 100; loss: 21.652; l2dist: 3.712\n",
      "    step: 150; loss: 16.864; l2dist: 3.589\n",
      "    step: 200; loss: 14.255; l2dist: 3.437\n",
      "    step: 250; loss: 13.093; l2dist: 3.333\n",
      "    step: 300; loss: 12.549; l2dist: 3.279\n",
      "    step: 350; loss: 12.148; l2dist: 3.233\n",
      "    step: 400; loss: 11.874; l2dist: 3.198\n",
      "    step: 450; loss: 11.739; l2dist: 3.179\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.986; l2dist: 0.000\n",
      "    step: 50; loss: 26.873; l2dist: 3.716\n",
      "    step: 100; loss: 20.906; l2dist: 3.503\n",
      "    step: 150; loss: 16.455; l2dist: 3.452\n",
      "    step: 200; loss: 14.222; l2dist: 3.366\n",
      "    step: 250; loss: 12.977; l2dist: 3.296\n",
      "    step: 300; loss: 12.415; l2dist: 3.240\n",
      "    step: 350; loss: 12.042; l2dist: 3.200\n",
      "    step: 400; loss: 11.716; l2dist: 3.167\n",
      "    step: 450; loss: 11.591; l2dist: 3.157\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.484; l2dist: 0.000\n",
      "    step: 50; loss: 25.448; l2dist: 3.608\n",
      "    step: 100; loss: 20.332; l2dist: 3.432\n",
      "    step: 150; loss: 16.269; l2dist: 3.405\n",
      "    step: 200; loss: 13.907; l2dist: 3.333\n",
      "    step: 250; loss: 12.820; l2dist: 3.238\n",
      "    step: 300; loss: 12.226; l2dist: 3.191\n",
      "    step: 350; loss: 11.981; l2dist: 3.182\n",
      "    step: 400; loss: 11.742; l2dist: 3.143\n",
      "    step: 450; loss: 11.493; l2dist: 3.126\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.995; l2dist: 0.000\n",
      "    step: 50; loss: 24.959; l2dist: 3.565\n",
      "    step: 100; loss: 20.010; l2dist: 3.400\n",
      "    step: 150; loss: 16.183; l2dist: 3.379\n",
      "    step: 200; loss: 13.891; l2dist: 3.311\n",
      "    step: 250; loss: 12.689; l2dist: 3.246\n",
      "    step: 300; loss: 12.294; l2dist: 3.191\n",
      "    step: 350; loss: 11.878; l2dist: 3.159\n",
      "    step: 400; loss: 11.740; l2dist: 3.156\n",
      "    step: 450; loss: 11.653; l2dist: 3.148\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.160; l2dist: 0.000\n",
      "    step: 50; loss: 24.773; l2dist: 3.546\n",
      "    step: 100; loss: 19.902; l2dist: 3.386\n",
      "    step: 150; loss: 16.080; l2dist: 3.368\n",
      "    step: 200; loss: 13.865; l2dist: 3.303\n",
      "    step: 250; loss: 12.756; l2dist: 3.238\n",
      "    step: 300; loss: 12.216; l2dist: 3.191\n",
      "    step: 350; loss: 11.907; l2dist: 3.163\n",
      "    step: 400; loss: 11.779; l2dist: 3.144\n",
      "    step: 450; loss: 11.566; l2dist: 3.142\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 70.327; l2dist: 0.000\n",
      "    step: 50; loss: 24.849; l2dist: 3.573\n",
      "    step: 100; loss: 20.026; l2dist: 3.413\n",
      "    step: 150; loss: 16.165; l2dist: 3.381\n",
      "    step: 200; loss: 13.795; l2dist: 3.312\n",
      "    step: 250; loss: 12.574; l2dist: 3.250\n",
      "    step: 300; loss: 12.180; l2dist: 3.212\n",
      "    step: 350; loss: 11.782; l2dist: 3.168\n",
      "    step: 400; loss: 11.634; l2dist: 3.155\n",
      "    step: 450; loss: 11.420; l2dist: 3.129\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.675; l2dist: 0.000\n",
      "    step: 50; loss: 13.117; l2dist: 2.046\n",
      "    step: 100; loss: 12.646; l2dist: 2.058\n",
      "    step: 150; loss: 11.941; l2dist: 2.162\n",
      "    step: 200; loss: 11.348; l2dist: 2.240\n",
      "    step: 250; loss: 10.813; l2dist: 2.275\n",
      "    step: 300; loss: 10.406; l2dist: 2.289\n",
      "    step: 350; loss: 10.132; l2dist: 2.294\n",
      "    step: 400; loss: 10.039; l2dist: 2.292\n",
      "    step: 450; loss: 9.991; l2dist: 2.291\n",
      "binary step: 0; number of successful adv: 45/100\n",
      "    step: 0; loss: 117.919; l2dist: 0.000\n",
      "    step: 50; loss: 31.773; l2dist: 3.970\n",
      "    step: 100; loss: 18.131; l2dist: 3.531\n",
      "    step: 150; loss: 12.821; l2dist: 3.172\n",
      "    step: 200; loss: 11.032; l2dist: 2.973\n",
      "    step: 250; loss: 10.224; l2dist: 2.871\n",
      "    step: 300; loss: 9.773; l2dist: 2.819\n",
      "    step: 350; loss: 9.487; l2dist: 2.771\n",
      "    step: 400; loss: 9.262; l2dist: 2.745\n",
      "    step: 450; loss: 9.142; l2dist: 2.730\n",
      "binary step: 1; number of successful adv: 92/100\n",
      "    step: 0; loss: 206.262; l2dist: 0.000\n",
      "    step: 50; loss: 36.164; l2dist: 4.310\n",
      "    step: 100; loss: 19.862; l2dist: 3.788\n",
      "    step: 150; loss: 14.057; l2dist: 3.345\n",
      "    step: 200; loss: 11.912; l2dist: 3.091\n",
      "    step: 250; loss: 11.025; l2dist: 2.987\n",
      "    step: 300; loss: 10.464; l2dist: 2.909\n",
      "    step: 350; loss: 10.202; l2dist: 2.879\n",
      "    step: 400; loss: 9.845; l2dist: 2.832\n",
      "    step: 450; loss: 9.725; l2dist: 2.822\n",
      "binary step: 2; number of successful adv: 99/100\n",
      "    step: 0; loss: 229.171; l2dist: 0.000\n",
      "    step: 50; loss: 29.984; l2dist: 4.021\n",
      "    step: 100; loss: 17.974; l2dist: 3.586\n",
      "    step: 150; loss: 13.573; l2dist: 3.233\n",
      "    step: 200; loss: 11.389; l2dist: 3.015\n",
      "    step: 250; loss: 10.378; l2dist: 2.901\n",
      "    step: 300; loss: 9.822; l2dist: 2.825\n",
      "    step: 350; loss: 9.554; l2dist: 2.797\n",
      "    step: 400; loss: 9.338; l2dist: 2.762\n",
      "    step: 450; loss: 9.177; l2dist: 2.751\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 138.134; l2dist: 0.000\n",
      "    step: 50; loss: 24.084; l2dist: 3.593\n",
      "    step: 100; loss: 16.433; l2dist: 3.242\n",
      "    step: 150; loss: 12.510; l2dist: 3.048\n",
      "    step: 200; loss: 10.540; l2dist: 2.902\n",
      "    step: 250; loss: 9.935; l2dist: 2.834\n",
      "    step: 300; loss: 9.461; l2dist: 2.771\n",
      "    step: 350; loss: 9.226; l2dist: 2.757\n",
      "    step: 400; loss: 8.973; l2dist: 2.724\n",
      "    step: 450; loss: 8.877; l2dist: 2.714\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 99.573; l2dist: 0.000\n",
      "    step: 50; loss: 22.106; l2dist: 3.380\n",
      "    step: 100; loss: 15.538; l2dist: 3.097\n",
      "    step: 150; loss: 12.122; l2dist: 2.953\n",
      "    step: 200; loss: 10.323; l2dist: 2.856\n",
      "    step: 250; loss: 9.734; l2dist: 2.784\n",
      "    step: 300; loss: 9.337; l2dist: 2.755\n",
      "    step: 350; loss: 9.010; l2dist: 2.731\n",
      "    step: 400; loss: 9.052; l2dist: 2.720\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 80.599; l2dist: 0.000\n",
      "    step: 50; loss: 20.549; l2dist: 3.263\n",
      "    step: 100; loss: 15.027; l2dist: 3.028\n",
      "    step: 150; loss: 12.018; l2dist: 2.947\n",
      "    step: 200; loss: 10.403; l2dist: 2.859\n",
      "    step: 250; loss: 9.571; l2dist: 2.784\n",
      "    step: 300; loss: 9.288; l2dist: 2.748\n",
      "    step: 350; loss: 9.063; l2dist: 2.721\n",
      "    step: 400; loss: 8.879; l2dist: 2.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 450; loss: 8.755; l2dist: 2.697\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.697; l2dist: 0.000\n",
      "    step: 50; loss: 19.839; l2dist: 3.187\n",
      "    step: 100; loss: 14.885; l2dist: 2.991\n",
      "    step: 150; loss: 12.313; l2dist: 2.898\n",
      "    step: 200; loss: 10.505; l2dist: 2.840\n",
      "    step: 250; loss: 9.676; l2dist: 2.785\n",
      "    step: 300; loss: 9.136; l2dist: 2.730\n",
      "    step: 350; loss: 8.905; l2dist: 2.709\n",
      "    step: 400; loss: 8.705; l2dist: 2.692\n",
      "    step: 450; loss: 8.626; l2dist: 2.681\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.260; l2dist: 0.000\n",
      "    step: 50; loss: 19.577; l2dist: 3.152\n",
      "    step: 100; loss: 14.862; l2dist: 2.974\n",
      "    step: 150; loss: 12.275; l2dist: 2.889\n",
      "    step: 200; loss: 10.873; l2dist: 2.823\n",
      "    step: 250; loss: 9.903; l2dist: 2.803\n",
      "    step: 300; loss: 9.278; l2dist: 2.739\n",
      "    step: 350; loss: 8.959; l2dist: 2.717\n",
      "    step: 400; loss: 8.763; l2dist: 2.704\n",
      "    step: 450; loss: 8.617; l2dist: 2.685\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.558; l2dist: 0.000\n",
      "    step: 50; loss: 19.664; l2dist: 3.163\n",
      "    step: 100; loss: 14.849; l2dist: 2.989\n",
      "    step: 150; loss: 12.289; l2dist: 2.896\n",
      "    step: 200; loss: 10.803; l2dist: 2.821\n",
      "    step: 250; loss: 9.724; l2dist: 2.797\n",
      "    step: 300; loss: 9.285; l2dist: 2.744\n",
      "    step: 350; loss: 8.973; l2dist: 2.696\n",
      "    step: 400; loss: 8.723; l2dist: 2.695\n",
      "    step: 450; loss: 8.626; l2dist: 2.687\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.546; l2dist: 0.000\n",
      "    step: 50; loss: 11.561; l2dist: 1.796\n",
      "    step: 100; loss: 11.276; l2dist: 1.801\n",
      "    step: 150; loss: 10.917; l2dist: 1.837\n",
      "    step: 200; loss: 10.612; l2dist: 1.865\n",
      "    step: 250; loss: 10.482; l2dist: 1.879\n",
      "    step: 300; loss: 10.349; l2dist: 1.904\n",
      "    step: 350; loss: 10.276; l2dist: 1.908\n",
      "    step: 400; loss: 10.243; l2dist: 1.909\n",
      "    step: 450; loss: 10.176; l2dist: 1.913\n",
      "binary step: 0; number of successful adv: 35/100\n",
      "    step: 0; loss: 104.835; l2dist: 0.000\n",
      "    step: 50; loss: 31.227; l2dist: 3.818\n",
      "    step: 100; loss: 16.538; l2dist: 3.367\n",
      "    step: 150; loss: 12.101; l2dist: 3.055\n",
      "    step: 200; loss: 10.754; l2dist: 2.910\n",
      "    step: 250; loss: 10.113; l2dist: 2.852\n",
      "    step: 300; loss: 9.667; l2dist: 2.798\n",
      "    step: 350; loss: 9.395; l2dist: 2.755\n",
      "    step: 400; loss: 9.221; l2dist: 2.733\n",
      "    step: 450; loss: 9.243; l2dist: 2.735\n",
      "binary step: 1; number of successful adv: 88/100\n",
      "    step: 0; loss: 222.906; l2dist: 0.000\n",
      "    step: 50; loss: 36.827; l2dist: 4.157\n",
      "    step: 100; loss: 20.366; l2dist: 3.742\n",
      "    step: 150; loss: 15.010; l2dist: 3.332\n",
      "    step: 200; loss: 12.550; l2dist: 3.112\n",
      "    step: 250; loss: 11.283; l2dist: 2.983\n",
      "    step: 300; loss: 10.495; l2dist: 2.882\n",
      "    step: 350; loss: 10.298; l2dist: 2.866\n",
      "    step: 400; loss: 9.866; l2dist: 2.816\n",
      "    step: 450; loss: 9.814; l2dist: 2.811\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 132.478; l2dist: 0.000\n",
      "    step: 50; loss: 29.042; l2dist: 3.667\n",
      "    step: 100; loss: 18.504; l2dist: 3.401\n",
      "    step: 150; loss: 13.105; l2dist: 3.164\n",
      "    step: 200; loss: 11.003; l2dist: 2.973\n",
      "    step: 250; loss: 10.244; l2dist: 2.877\n",
      "    step: 300; loss: 9.782; l2dist: 2.836\n",
      "    step: 350; loss: 9.303; l2dist: 2.774\n",
      "    step: 400; loss: 9.206; l2dist: 2.753\n",
      "    step: 450; loss: 9.204; l2dist: 2.752\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 93.756; l2dist: 0.000\n",
      "    step: 50; loss: 24.588; l2dist: 3.336\n",
      "    step: 100; loss: 16.691; l2dist: 3.174\n",
      "    step: 150; loss: 12.046; l2dist: 3.026\n",
      "    step: 200; loss: 10.424; l2dist: 2.901\n",
      "    step: 250; loss: 9.720; l2dist: 2.818\n",
      "    step: 300; loss: 9.292; l2dist: 2.776\n",
      "    step: 350; loss: 8.820; l2dist: 2.709\n",
      "    step: 400; loss: 8.650; l2dist: 2.691\n",
      "    step: 450; loss: 8.669; l2dist: 2.688\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 77.431; l2dist: 0.000\n",
      "    step: 50; loss: 22.307; l2dist: 3.169\n",
      "    step: 100; loss: 15.928; l2dist: 3.082\n",
      "    step: 150; loss: 12.048; l2dist: 2.982\n",
      "    step: 200; loss: 10.257; l2dist: 2.874\n",
      "    step: 250; loss: 9.471; l2dist: 2.780\n",
      "    step: 300; loss: 9.105; l2dist: 2.736\n",
      "    step: 350; loss: 8.846; l2dist: 2.705\n",
      "    step: 400; loss: 8.702; l2dist: 2.683\n",
      "    step: 450; loss: 8.654; l2dist: 2.677\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.656; l2dist: 0.000\n",
      "    step: 50; loss: 20.995; l2dist: 3.056\n",
      "    step: 100; loss: 15.434; l2dist: 3.028\n",
      "    step: 150; loss: 11.632; l2dist: 2.973\n",
      "    step: 200; loss: 10.152; l2dist: 2.859\n",
      "    step: 250; loss: 9.461; l2dist: 2.786\n",
      "    step: 300; loss: 9.087; l2dist: 2.735\n",
      "    step: 350; loss: 8.882; l2dist: 2.714\n",
      "    step: 400; loss: 8.777; l2dist: 2.706\n",
      "    step: 450; loss: 8.625; l2dist: 2.685\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.617; l2dist: 0.000\n",
      "    step: 50; loss: 20.703; l2dist: 2.988\n",
      "    step: 100; loss: 16.151; l2dist: 2.962\n",
      "    step: 150; loss: 11.661; l2dist: 2.955\n",
      "    step: 200; loss: 10.172; l2dist: 2.852\n",
      "    step: 250; loss: 9.390; l2dist: 2.771\n",
      "    step: 300; loss: 9.012; l2dist: 2.724\n",
      "    step: 350; loss: 8.945; l2dist: 2.710\n",
      "    step: 400; loss: 8.847; l2dist: 2.701\n",
      "    step: 450; loss: 8.615; l2dist: 2.680\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.073; l2dist: 0.000\n",
      "    step: 50; loss: 20.165; l2dist: 2.990\n",
      "    step: 100; loss: 15.925; l2dist: 2.960\n",
      "    step: 150; loss: 11.665; l2dist: 2.955\n",
      "    step: 200; loss: 10.271; l2dist: 2.866\n",
      "    step: 250; loss: 9.571; l2dist: 2.790\n",
      "    step: 300; loss: 9.124; l2dist: 2.743\n",
      "    step: 350; loss: 8.962; l2dist: 2.728\n",
      "    step: 400; loss: 8.746; l2dist: 2.697\n",
      "    step: 450; loss: 8.592; l2dist: 2.683\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.393; l2dist: 0.000\n",
      "    step: 50; loss: 20.312; l2dist: 2.994\n",
      "    step: 100; loss: 15.745; l2dist: 2.978\n",
      "    step: 150; loss: 11.654; l2dist: 2.952\n",
      "    step: 200; loss: 10.321; l2dist: 2.875\n",
      "    step: 250; loss: 9.500; l2dist: 2.779\n",
      "    step: 300; loss: 9.130; l2dist: 2.736\n",
      "    step: 350; loss: 8.931; l2dist: 2.713\n",
      "    step: 400; loss: 8.743; l2dist: 2.693\n",
      "    step: 450; loss: 8.711; l2dist: 2.696\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.454; l2dist: 0.000\n",
      "    step: 50; loss: 10.509; l2dist: 1.671\n",
      "    step: 100; loss: 10.253; l2dist: 1.667\n",
      "    step: 150; loss: 9.884; l2dist: 1.715\n",
      "    step: 200; loss: 9.599; l2dist: 1.733\n",
      "    step: 250; loss: 9.384; l2dist: 1.752\n",
      "    step: 300; loss: 9.284; l2dist: 1.767\n",
      "    step: 350; loss: 9.205; l2dist: 1.768\n",
      "    step: 400; loss: 9.167; l2dist: 1.771\n",
      "    step: 450; loss: 9.148; l2dist: 1.773\n",
      "binary step: 0; number of successful adv: 36/100\n",
      "    step: 0; loss: 94.750; l2dist: 0.000\n",
      "    step: 50; loss: 31.940; l2dist: 3.568\n",
      "    step: 100; loss: 19.013; l2dist: 3.351\n",
      "    step: 150; loss: 12.687; l2dist: 3.070\n",
      "    step: 200; loss: 10.789; l2dist: 2.882\n",
      "    step: 250; loss: 9.988; l2dist: 2.782\n",
      "    step: 300; loss: 9.580; l2dist: 2.723\n",
      "    step: 350; loss: 9.207; l2dist: 2.690\n",
      "    step: 400; loss: 8.984; l2dist: 2.657\n",
      "    step: 450; loss: 8.891; l2dist: 2.645\n",
      "binary step: 1; number of successful adv: 91/100\n",
      "    step: 0; loss: 163.222; l2dist: 0.000\n",
      "    step: 50; loss: 30.228; l2dist: 3.761\n",
      "    step: 100; loss: 20.929; l2dist: 3.476\n",
      "    step: 150; loss: 13.943; l2dist: 3.137\n",
      "    step: 200; loss: 10.969; l2dist: 2.891\n",
      "    step: 250; loss: 9.937; l2dist: 2.765\n",
      "    step: 300; loss: 9.447; l2dist: 2.710\n",
      "    step: 350; loss: 9.259; l2dist: 2.677\n",
      "    step: 400; loss: 8.806; l2dist: 2.634\n",
      "    step: 450; loss: 8.814; l2dist: 2.628\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 99.062; l2dist: 0.000\n",
      "    step: 50; loss: 23.780; l2dist: 3.303\n",
      "    step: 100; loss: 18.530; l2dist: 3.059\n",
      "    step: 150; loss: 12.678; l2dist: 2.959\n",
      "    step: 200; loss: 10.250; l2dist: 2.804\n",
      "    step: 250; loss: 9.313; l2dist: 2.688\n",
      "    step: 300; loss: 8.905; l2dist: 2.634\n",
      "    step: 350; loss: 8.627; l2dist: 2.602\n",
      "    step: 400; loss: 8.312; l2dist: 2.572\n",
      "    step: 450; loss: 8.380; l2dist: 2.576\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 77.421; l2dist: 0.000\n",
      "    step: 50; loss: 21.884; l2dist: 3.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 17.988; l2dist: 2.888\n",
      "    step: 150; loss: 13.527; l2dist: 2.875\n",
      "    step: 200; loss: 10.369; l2dist: 2.802\n",
      "    step: 250; loss: 9.479; l2dist: 2.700\n",
      "    step: 300; loss: 8.878; l2dist: 2.638\n",
      "    step: 350; loss: 8.559; l2dist: 2.588\n",
      "    step: 400; loss: 8.364; l2dist: 2.569\n",
      "    step: 450; loss: 8.313; l2dist: 2.562\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 66.084; l2dist: 0.000\n",
      "    step: 50; loss: 20.313; l2dist: 2.955\n",
      "    step: 100; loss: 17.136; l2dist: 2.813\n",
      "    step: 150; loss: 13.549; l2dist: 2.822\n",
      "    step: 200; loss: 10.440; l2dist: 2.789\n",
      "    step: 250; loss: 9.466; l2dist: 2.693\n",
      "    step: 300; loss: 8.867; l2dist: 2.627\n",
      "    step: 350; loss: 8.793; l2dist: 2.617\n",
      "    step: 400; loss: 8.431; l2dist: 2.570\n",
      "    step: 450; loss: 8.351; l2dist: 2.571\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.366; l2dist: 0.000\n",
      "    step: 50; loss: 19.586; l2dist: 2.913\n",
      "    step: 100; loss: 16.513; l2dist: 2.779\n",
      "    step: 150; loss: 12.893; l2dist: 2.799\n",
      "    step: 200; loss: 10.228; l2dist: 2.751\n",
      "    step: 250; loss: 9.314; l2dist: 2.680\n",
      "    step: 300; loss: 8.637; l2dist: 2.611\n",
      "    step: 350; loss: 8.516; l2dist: 2.594\n",
      "    step: 400; loss: 8.287; l2dist: 2.564\n",
      "    step: 450; loss: 8.216; l2dist: 2.550\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.147; l2dist: 0.000\n",
      "    step: 50; loss: 19.192; l2dist: 2.875\n",
      "    step: 100; loss: 16.128; l2dist: 2.764\n",
      "    step: 150; loss: 12.869; l2dist: 2.772\n",
      "    step: 200; loss: 10.155; l2dist: 2.757\n",
      "    step: 250; loss: 9.291; l2dist: 2.679\n",
      "    step: 300; loss: 8.684; l2dist: 2.614\n",
      "    step: 350; loss: 8.431; l2dist: 2.580\n",
      "    step: 400; loss: 8.327; l2dist: 2.569\n",
      "    step: 450; loss: 8.203; l2dist: 2.543\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.540; l2dist: 0.000\n",
      "    step: 50; loss: 18.933; l2dist: 2.859\n",
      "    step: 100; loss: 16.114; l2dist: 2.734\n",
      "    step: 150; loss: 13.013; l2dist: 2.768\n",
      "    step: 200; loss: 10.152; l2dist: 2.761\n",
      "    step: 250; loss: 9.316; l2dist: 2.684\n",
      "    step: 300; loss: 8.658; l2dist: 2.601\n",
      "    step: 350; loss: 8.451; l2dist: 2.579\n",
      "    step: 400; loss: 8.260; l2dist: 2.549\n",
      "    step: 450; loss: 8.308; l2dist: 2.554\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.475; l2dist: 0.000\n",
      "    step: 50; loss: 19.041; l2dist: 2.861\n",
      "    step: 100; loss: 16.236; l2dist: 2.743\n",
      "    step: 150; loss: 13.157; l2dist: 2.764\n",
      "    step: 200; loss: 10.116; l2dist: 2.749\n",
      "    step: 250; loss: 9.241; l2dist: 2.680\n",
      "    step: 300; loss: 8.757; l2dist: 2.617\n",
      "    step: 350; loss: 8.369; l2dist: 2.574\n",
      "    step: 400; loss: 8.259; l2dist: 2.553\n",
      "    step: 450; loss: 8.078; l2dist: 2.540\n",
      "binary step: 9; number of successful adv: 100/100\n"
     ]
    }
   ],
   "source": [
    "attack = CWL2Attack()\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            net, x[begin:end], y[begin:end], targeted=False,\n",
    "            binary_search_steps=10, max_iterations=500,\n",
    "            confidence=0, learning_rate=1e-1,\n",
    "            initial_const=1e1, abort_early=True)\n",
    "    return x_adv\n",
    "\n",
    "x_adv = attack_batch(x_test[ind].cuda(), y_test[ind].cuda(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010559662090813093\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = net(x_adv.cuda()).cpu()\n",
    "    ind_adv = np.where(y_pred.argmax(1) != y_test[ind])[0]\n",
    "    print((y_pred.argmax(1) == y_test[ind]).numpy().sum() / y_pred.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 0.8918 & 0.7575 & 0.4764\n"
     ]
    }
   ],
   "source": [
    "pert = (x_adv.detach().cpu() - x_test[ind]).view(x_adv.size(0), -1).norm(2, 1)\n",
    "d1 = (len(ind) - (pert[ind_adv] < 1).sum().numpy()) / y_test.size(0)\n",
    "d2 = (len(ind) - (pert[ind_adv] < 2).sum().numpy()) / y_test.size(0)\n",
    "d3 = (len(ind) - (pert[ind_adv] < 3).sum().numpy()) / y_test.size(0)\n",
    "print('& %.4f & %.4f & %.4f' % (d1, d2, d3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9060)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert[ind_adv].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred = dknn.credibility(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 947., 1119.,  809.,  953.,  545.,  869.,    0.,    0.,    0.,\n",
       "        4758.]),\n",
       " array([0.0265 , 0.12385, 0.2212 , 0.31855, 0.4159 , 0.51325, 0.6106 ,\n",
       "        0.70795, 0.8053 , 0.90265, 1.     ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.argmax(y_pred, 1) == y_test.numpy()\n",
    "num_correct_by_cred = np.zeros((10, ))\n",
    "num_cred = np.zeros((10, ))\n",
    "for i in np.arange(10):\n",
    "    ind = (cred > i * 0.1) & (cred <= i* 0.1 + 0.1)\n",
    "    num_cred[i] = np.sum(ind)\n",
    "    num_correct_by_cred[i] = np.sum(correct[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(np.arange(10) * 0.1, num_cred, width=0.05)\n",
    "ax.bar(np.arange(10) * 0.1 + 0.05, num_correct_by_cred, width=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.52764613, 0.83124478, 0.94539249, 0.98878343, 0.99498495,\n",
       "              nan,        nan,        nan,        nan, 0.99922103])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct_by_cred / num_cred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17333333333333334"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dknn.A.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = dknn.get_neighbors(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72602546\n",
      "0.6874378\n",
      "0.7105881\n",
      "0.94844836\n"
     ]
    }
   ],
   "source": [
    "for (D, I) in nn:\n",
    "    print(D[-1].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGD Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_np = x_test.cpu().numpy()\n",
    "y_test_np = y_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foolbox.models.PyTorchModel(net, bounds=(0, 1), num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = foolbox.criteria.Misclassification()\n",
    "distance = foolbox.distances.Linfinity\n",
    "\n",
    "attack = foolbox.attacks.RandomPGD(\n",
    "    model, criterion=criterion, distance=distance)\n",
    "\n",
    "def attack_wrap(x, y):\n",
    "    return attack(x, y, binary_search=False, epsilon=0.3, \n",
    "                  stepsize=0.01, iterations=300, \n",
    "                  random_start=True, return_early=True)\n",
    "\n",
    "x_adv = np.zeros_like(x_test_np)\n",
    "for i, (x, y) in enumerate(zip(x_test_np, y_test_np)):\n",
    "    x_adv[i] = attack_wrap(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_adv = torch.tensor(x_adv).to(device)\n",
    "    y_pred = net(x_adv).detach().cpu().numpy()\n",
    "np.mean(np.argmax(y_pred, 1) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of successful adv: 820/10000\n"
     ]
    }
   ],
   "source": [
    "from lib.pgd_attack import PGDAttack\n",
    "\n",
    "attack = PGDAttack()\n",
    "x_adv = attack(net, x_test.cuda(), y_test.to(device),\n",
    "               targeted=False, epsilon=0.01, max_epsilon=0.3,\n",
    "               max_iterations=300, num_restart=10, rand_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9180"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = net(x_adv)\n",
    "(y_pred.argmax(1).cpu() == y_test).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dknn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5736a610097b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dknn' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = dknn.classify(x_adv.cpu())\n",
    "(y_pred.argmax(1) == y_test.numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9.63e+03, 2.96e+02, 5.70e+01, 7.00e+00, 5.00e+00, 1.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 4.00e+00]),\n",
       " array([0.00383333, 0.10345   , 0.20306667, 0.30268333, 0.4023    ,\n",
       "        0.50191667, 0.60153333, 0.70115   , 0.80076667, 0.90038333,\n",
       "        1.        ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cred = dknn.credibility(y_pred)\n",
    "plt.hist(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9091"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DkNN Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attack = DKNNAttack()\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            dknn, x[begin:end], y[begin:end],\n",
    "            guide_layer=layers[0], m=75, binary_search_steps=1,\n",
    "            max_iterations=500, learning_rate=1e-1, \n",
    "            initial_const=1e3, abort_early=True, max_linf=0.4)\n",
    "    return x_adv\n",
    "\n",
    "num = 10\n",
    "x_adv = attack_batch(x_test[:num].cuda(), y_test[:num], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(78., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 78/100\n",
      "tensor(81., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 83/100\n",
      "tensor(1., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 83/100\n",
      "tensor(1., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 83/100\n",
      "tensor(1., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 83/100\n",
      "tensor(1., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 83/100\n",
      "tensor(1., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 83/100\n",
      "tensor(1., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 83/100\n",
      "tensor(1., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 83/100\n",
      "tensor(1., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 83/100\n"
     ]
    }
   ],
   "source": [
    "attack = DKNNL2Attack()\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            dknn, x[begin:end], y[begin:end],\n",
    "            guide_layer=layers[0], m=100, binary_search_steps=10,\n",
    "            max_iterations=1000, learning_rate=1e-1, guide=1,\n",
    "            initial_const=1e5, abort_early=True, random_start=False)\n",
    "    return x_adv\n",
    "\n",
    "num = 100\n",
    "x_adv = attack_batch(x_test[:num].cuda(), y_test[:num], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dknn.classify(x_adv)\n",
    "(y_pred.argmax(1) == y_test[:num].numpy()).sum() / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([33., 18.,  9., 10.,  0.,  0.,  0.,  0.,  0., 30.]),\n",
       " array([0.0385 , 0.13465, 0.2308 , 0.32695, 0.4231 , 0.51925, 0.6154 ,\n",
       "        0.71155, 0.8077 , 0.90385, 1.     ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cred = dknn.credibility(y_pred)\n",
    "plt.hist(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    plt.imshow(x_adv[i].cpu().detach().numpy().squeeze(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.752215"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_clean = dknn.classify(x_test[:num])\n",
    "ind = (y_clean.argmax(1) == y_test[:num].numpy()) & (y_pred.argmax(1) != y_test[:num].numpy())\n",
    "dist = np.sqrt(np.sum((x_adv.cpu().detach().numpy() - x_test.numpy()[:num])**2, (1, 2, 3)))\n",
    "np.mean(dist[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(x_adv, open('x_adv_dknn1_dist11.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ours: [2.1830862, 4.239626, 2.6917257, 3.8731990, 1.4814271, 2.63134,\n",
    "#        2.0538194, 1.270368, 2.9351923, 2.4314828]\n",
    "# bd2:  [2.9166877, 2.957195, 2.9477980, 2.8653080, 1.2530060, 2.33518, \n",
    "#        1.6536615, 1.168170, 1.6185884, 2.3803349]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CW L2 Attack\n",
    "\n",
    "without DkNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 121.171; l2dist: 0.000\n",
      "    step: 50; loss: 14.259; l2dist: 3.370\n",
      "    step: 100; loss: 5.832; l2dist: 2.329\n",
      "    step: 150; loss: 3.887; l2dist: 1.881\n",
      "    step: 200; loss: 3.105; l2dist: 1.666\n",
      "    step: 250; loss: 2.774; l2dist: 1.574\n",
      "    step: 300; loss: 2.581; l2dist: 1.513\n",
      "    step: 350; loss: 2.525; l2dist: 1.488\n",
      "    step: 400; loss: 2.486; l2dist: 1.473\n",
      "    step: 450; loss: 2.449; l2dist: 1.461\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 75.129; l2dist: 0.000\n",
      "    step: 50; loss: 10.528; l2dist: 2.859\n",
      "    step: 100; loss: 4.196; l2dist: 1.949\n",
      "    step: 150; loss: 3.019; l2dist: 1.639\n",
      "    step: 200; loss: 2.641; l2dist: 1.526\n",
      "    step: 250; loss: 2.478; l2dist: 1.469\n",
      "    step: 300; loss: 2.393; l2dist: 1.444\n",
      "    step: 350; loss: 2.319; l2dist: 1.425\n",
      "    step: 400; loss: 2.340; l2dist: 1.432\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.686; l2dist: 0.000\n",
      "    step: 50; loss: 8.584; l2dist: 2.540\n",
      "    step: 100; loss: 3.611; l2dist: 1.794\n",
      "    step: 150; loss: 2.719; l2dist: 1.548\n",
      "    step: 200; loss: 2.470; l2dist: 1.472\n",
      "    step: 250; loss: 2.363; l2dist: 1.433\n",
      "    step: 300; loss: 2.287; l2dist: 1.416\n",
      "    step: 350; loss: 2.289; l2dist: 1.408\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.072; l2dist: 0.000\n",
      "    step: 50; loss: 7.524; l2dist: 2.317\n",
      "    step: 100; loss: 3.475; l2dist: 1.732\n",
      "    step: 150; loss: 2.640; l2dist: 1.503\n",
      "    step: 200; loss: 2.406; l2dist: 1.442\n",
      "    step: 250; loss: 2.313; l2dist: 1.414\n",
      "    step: 300; loss: 2.268; l2dist: 1.404\n",
      "    step: 350; loss: 2.253; l2dist: 1.404\n",
      "    step: 400; loss: 2.235; l2dist: 1.394\n",
      "    step: 450; loss: 2.222; l2dist: 1.388\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.327; l2dist: 0.000\n",
      "    step: 50; loss: 6.720; l2dist: 2.073\n",
      "    step: 100; loss: 3.511; l2dist: 1.676\n",
      "    step: 150; loss: 2.627; l2dist: 1.487\n",
      "    step: 200; loss: 2.417; l2dist: 1.428\n",
      "    step: 250; loss: 2.314; l2dist: 1.404\n",
      "    step: 300; loss: 2.291; l2dist: 1.396\n",
      "    step: 350; loss: 2.278; l2dist: 1.384\n",
      "    step: 400; loss: 2.252; l2dist: 1.385\n",
      "    step: 450; loss: 2.229; l2dist: 1.384\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.244; l2dist: 0.000\n",
      "    step: 50; loss: 6.437; l2dist: 2.006\n",
      "    step: 100; loss: 3.453; l2dist: 1.617\n",
      "    step: 150; loss: 2.603; l2dist: 1.451\n",
      "    step: 200; loss: 2.391; l2dist: 1.407\n",
      "    step: 250; loss: 2.305; l2dist: 1.383\n",
      "    step: 300; loss: 2.247; l2dist: 1.370\n",
      "    step: 350; loss: 2.225; l2dist: 1.368\n",
      "    step: 400; loss: 2.229; l2dist: 1.366\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.416; l2dist: 0.000\n",
      "    step: 50; loss: 6.415; l2dist: 1.988\n",
      "    step: 100; loss: 3.436; l2dist: 1.607\n",
      "    step: 150; loss: 2.577; l2dist: 1.431\n",
      "    step: 200; loss: 2.369; l2dist: 1.386\n",
      "    step: 250; loss: 2.280; l2dist: 1.368\n",
      "    step: 300; loss: 2.246; l2dist: 1.355\n",
      "    step: 350; loss: 2.223; l2dist: 1.353\n",
      "    step: 400; loss: 2.212; l2dist: 1.349\n",
      "    step: 450; loss: 2.210; l2dist: 1.346\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.906; l2dist: 0.000\n",
      "    step: 50; loss: 6.411; l2dist: 1.981\n",
      "    step: 100; loss: 3.455; l2dist: 1.610\n",
      "    step: 150; loss: 2.578; l2dist: 1.442\n",
      "    step: 200; loss: 2.368; l2dist: 1.393\n",
      "    step: 250; loss: 2.292; l2dist: 1.373\n",
      "    step: 300; loss: 2.253; l2dist: 1.359\n",
      "    step: 350; loss: 2.240; l2dist: 1.354\n",
      "    step: 400; loss: 2.213; l2dist: 1.360\n",
      "    step: 450; loss: 2.213; l2dist: 1.351\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.751; l2dist: 0.000\n",
      "    step: 50; loss: 6.410; l2dist: 1.980\n",
      "    step: 100; loss: 3.449; l2dist: 1.608\n",
      "    step: 150; loss: 2.574; l2dist: 1.442\n",
      "    step: 200; loss: 2.367; l2dist: 1.402\n",
      "    step: 250; loss: 2.290; l2dist: 1.383\n",
      "    step: 300; loss: 2.247; l2dist: 1.364\n",
      "    step: 350; loss: 2.233; l2dist: 1.365\n",
      "    step: 400; loss: 2.218; l2dist: 1.365\n",
      "    step: 450; loss: 2.207; l2dist: 1.356\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.893; l2dist: 0.000\n",
      "    step: 50; loss: 6.454; l2dist: 1.990\n",
      "    step: 100; loss: 3.469; l2dist: 1.623\n",
      "    step: 150; loss: 2.591; l2dist: 1.448\n",
      "    step: 200; loss: 2.366; l2dist: 1.401\n",
      "    step: 250; loss: 2.282; l2dist: 1.385\n",
      "    step: 300; loss: 2.247; l2dist: 1.369\n",
      "    step: 350; loss: 2.240; l2dist: 1.365\n",
      "    step: 400; loss: 2.238; l2dist: 1.361\n",
      "    step: 450; loss: 2.210; l2dist: 1.364\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 128.587; l2dist: 0.000\n",
      "    step: 50; loss: 15.144; l2dist: 3.476\n",
      "    step: 100; loss: 6.006; l2dist: 2.370\n",
      "    step: 150; loss: 4.000; l2dist: 1.916\n",
      "    step: 200; loss: 3.227; l2dist: 1.710\n",
      "    step: 250; loss: 2.866; l2dist: 1.611\n",
      "    step: 300; loss: 2.705; l2dist: 1.551\n",
      "    step: 350; loss: 2.644; l2dist: 1.540\n",
      "    step: 400; loss: 2.548; l2dist: 1.513\n",
      "    step: 450; loss: 2.526; l2dist: 1.508\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 115.593; l2dist: 0.000\n",
      "    step: 50; loss: 12.955; l2dist: 3.190\n",
      "    step: 100; loss: 5.268; l2dist: 2.167\n",
      "    step: 150; loss: 3.650; l2dist: 1.783\n",
      "    step: 200; loss: 3.030; l2dist: 1.629\n",
      "    step: 250; loss: 2.705; l2dist: 1.550\n",
      "    step: 300; loss: 2.592; l2dist: 1.506\n",
      "    step: 350; loss: 2.495; l2dist: 1.491\n",
      "    step: 400; loss: 2.385; l2dist: 1.463\n",
      "    step: 450; loss: 2.353; l2dist: 1.462\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.682; l2dist: 0.000\n",
      "    step: 50; loss: 10.310; l2dist: 2.798\n",
      "    step: 100; loss: 4.111; l2dist: 1.912\n",
      "    step: 150; loss: 3.010; l2dist: 1.631\n",
      "    step: 200; loss: 2.623; l2dist: 1.536\n",
      "    step: 250; loss: 2.485; l2dist: 1.481\n",
      "    step: 300; loss: 2.336; l2dist: 1.448\n",
      "    step: 350; loss: 2.313; l2dist: 1.442\n",
      "    step: 400; loss: 2.278; l2dist: 1.424\n",
      "    step: 450; loss: 2.257; l2dist: 1.421\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.262; l2dist: 0.000\n",
      "    step: 50; loss: 8.787; l2dist: 2.519\n",
      "    step: 100; loss: 3.680; l2dist: 1.812\n",
      "    step: 150; loss: 2.724; l2dist: 1.567\n",
      "    step: 200; loss: 2.439; l2dist: 1.482\n",
      "    step: 250; loss: 2.346; l2dist: 1.446\n",
      "    step: 300; loss: 2.265; l2dist: 1.423\n",
      "    step: 350; loss: 2.250; l2dist: 1.416\n",
      "    step: 400; loss: 2.290; l2dist: 1.431\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.017; l2dist: 0.000\n",
      "    step: 50; loss: 7.824; l2dist: 2.299\n",
      "    step: 100; loss: 3.496; l2dist: 1.715\n",
      "    step: 150; loss: 2.634; l2dist: 1.512\n",
      "    step: 200; loss: 2.393; l2dist: 1.448\n",
      "    step: 250; loss: 2.302; l2dist: 1.420\n",
      "    step: 300; loss: 2.269; l2dist: 1.410\n",
      "    step: 350; loss: 2.240; l2dist: 1.403\n",
      "    step: 400; loss: 2.229; l2dist: 1.392\n",
      "    step: 450; loss: 2.221; l2dist: 1.395\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.538; l2dist: 0.000\n",
      "    step: 50; loss: 7.378; l2dist: 2.198\n",
      "    step: 100; loss: 3.336; l2dist: 1.637\n",
      "    step: 150; loss: 2.556; l2dist: 1.452\n",
      "    step: 200; loss: 2.327; l2dist: 1.398\n",
      "    step: 250; loss: 2.266; l2dist: 1.377\n",
      "    step: 300; loss: 2.206; l2dist: 1.372\n",
      "    step: 350; loss: 2.179; l2dist: 1.361\n",
      "    step: 400; loss: 2.156; l2dist: 1.358\n",
      "    step: 450; loss: 2.161; l2dist: 1.363\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.276; l2dist: 0.000\n",
      "    step: 50; loss: 7.214; l2dist: 2.164\n",
      "    step: 100; loss: 3.293; l2dist: 1.615\n",
      "    step: 150; loss: 2.536; l2dist: 1.443\n",
      "    step: 200; loss: 2.317; l2dist: 1.385\n",
      "    step: 250; loss: 2.234; l2dist: 1.362\n",
      "    step: 300; loss: 2.197; l2dist: 1.355\n",
      "    step: 350; loss: 2.168; l2dist: 1.353\n",
      "    step: 400; loss: 2.160; l2dist: 1.346\n",
      "    step: 450; loss: 2.150; l2dist: 1.348\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.289; l2dist: 0.000\n",
      "    step: 50; loss: 7.137; l2dist: 2.151\n",
      "    step: 100; loss: 3.302; l2dist: 1.628\n",
      "    step: 150; loss: 2.536; l2dist: 1.455\n",
      "    step: 200; loss: 2.313; l2dist: 1.398\n",
      "    step: 250; loss: 2.234; l2dist: 1.374\n",
      "    step: 300; loss: 2.210; l2dist: 1.365\n",
      "    step: 350; loss: 2.181; l2dist: 1.361\n",
      "    step: 400; loss: 2.155; l2dist: 1.353\n",
      "    step: 450; loss: 2.147; l2dist: 1.354\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.820; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 7.134; l2dist: 2.149\n",
      "    step: 100; loss: 3.299; l2dist: 1.621\n",
      "    step: 150; loss: 2.539; l2dist: 1.448\n",
      "    step: 200; loss: 2.314; l2dist: 1.398\n",
      "    step: 250; loss: 2.240; l2dist: 1.374\n",
      "    step: 300; loss: 2.193; l2dist: 1.367\n",
      "    step: 350; loss: 2.179; l2dist: 1.357\n",
      "    step: 400; loss: 2.167; l2dist: 1.357\n",
      "    step: 450; loss: 2.161; l2dist: 1.359\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.044; l2dist: 0.000\n",
      "    step: 50; loss: 7.187; l2dist: 2.160\n",
      "    step: 100; loss: 3.313; l2dist: 1.630\n",
      "    step: 150; loss: 2.546; l2dist: 1.454\n",
      "    step: 200; loss: 2.330; l2dist: 1.401\n",
      "    step: 250; loss: 2.244; l2dist: 1.386\n",
      "    step: 300; loss: 2.217; l2dist: 1.376\n",
      "    step: 350; loss: 2.177; l2dist: 1.369\n",
      "    step: 400; loss: 2.167; l2dist: 1.370\n",
      "    step: 450; loss: 2.162; l2dist: 1.366\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.501; l2dist: 0.000\n",
      "    step: 50; loss: 13.032; l2dist: 3.298\n",
      "    step: 100; loss: 5.212; l2dist: 2.202\n",
      "    step: 150; loss: 3.483; l2dist: 1.773\n",
      "    step: 200; loss: 2.797; l2dist: 1.580\n",
      "    step: 250; loss: 2.561; l2dist: 1.504\n",
      "    step: 300; loss: 2.363; l2dist: 1.437\n",
      "    step: 350; loss: 2.303; l2dist: 1.419\n",
      "    step: 400; loss: 2.223; l2dist: 1.393\n",
      "    step: 450; loss: 2.219; l2dist: 1.387\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 138.063; l2dist: 0.000\n",
      "    step: 50; loss: 12.945; l2dist: 3.250\n",
      "    step: 100; loss: 5.648; l2dist: 2.225\n",
      "    step: 150; loss: 3.782; l2dist: 1.796\n",
      "    step: 200; loss: 2.994; l2dist: 1.593\n",
      "    step: 250; loss: 2.676; l2dist: 1.500\n",
      "    step: 300; loss: 2.450; l2dist: 1.435\n",
      "    step: 350; loss: 2.348; l2dist: 1.413\n",
      "    step: 400; loss: 2.241; l2dist: 1.392\n",
      "    step: 450; loss: 2.208; l2dist: 1.382\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.534; l2dist: 0.000\n",
      "    step: 50; loss: 9.589; l2dist: 2.744\n",
      "    step: 100; loss: 4.016; l2dist: 1.864\n",
      "    step: 150; loss: 2.822; l2dist: 1.553\n",
      "    step: 200; loss: 2.442; l2dist: 1.442\n",
      "    step: 250; loss: 2.264; l2dist: 1.396\n",
      "    step: 300; loss: 2.176; l2dist: 1.365\n",
      "    step: 350; loss: 2.129; l2dist: 1.354\n",
      "    step: 400; loss: 2.054; l2dist: 1.332\n",
      "    step: 450; loss: 2.064; l2dist: 1.339\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.259; l2dist: 0.000\n",
      "    step: 50; loss: 7.857; l2dist: 2.365\n",
      "    step: 100; loss: 3.365; l2dist: 1.694\n",
      "    step: 150; loss: 2.478; l2dist: 1.458\n",
      "    step: 200; loss: 2.227; l2dist: 1.376\n",
      "    step: 250; loss: 2.075; l2dist: 1.339\n",
      "    step: 300; loss: 2.048; l2dist: 1.322\n",
      "    step: 350; loss: 1.992; l2dist: 1.310\n",
      "    step: 400; loss: 1.954; l2dist: 1.298\n",
      "    step: 450; loss: 1.951; l2dist: 1.292\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.873; l2dist: 0.000\n",
      "    step: 50; loss: 7.000; l2dist: 2.136\n",
      "    step: 100; loss: 3.196; l2dist: 1.596\n",
      "    step: 150; loss: 2.386; l2dist: 1.412\n",
      "    step: 200; loss: 2.161; l2dist: 1.352\n",
      "    step: 250; loss: 2.046; l2dist: 1.320\n",
      "    step: 300; loss: 1.997; l2dist: 1.309\n",
      "    step: 350; loss: 1.981; l2dist: 1.301\n",
      "    step: 400; loss: 1.957; l2dist: 1.293\n",
      "    step: 450; loss: 1.944; l2dist: 1.282\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.033; l2dist: 0.000\n",
      "    step: 50; loss: 6.417; l2dist: 1.990\n",
      "    step: 100; loss: 3.084; l2dist: 1.529\n",
      "    step: 150; loss: 2.325; l2dist: 1.355\n",
      "    step: 200; loss: 2.113; l2dist: 1.304\n",
      "    step: 250; loss: 2.015; l2dist: 1.280\n",
      "    step: 300; loss: 1.975; l2dist: 1.265\n",
      "    step: 350; loss: 1.962; l2dist: 1.264\n",
      "    step: 400; loss: 1.947; l2dist: 1.257\n",
      "    step: 450; loss: 1.933; l2dist: 1.254\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.796; l2dist: 0.000\n",
      "    step: 50; loss: 6.367; l2dist: 1.981\n",
      "    step: 100; loss: 3.073; l2dist: 1.537\n",
      "    step: 150; loss: 2.313; l2dist: 1.362\n",
      "    step: 200; loss: 2.118; l2dist: 1.322\n",
      "    step: 250; loss: 2.019; l2dist: 1.291\n",
      "    step: 300; loss: 1.986; l2dist: 1.275\n",
      "    step: 350; loss: 1.963; l2dist: 1.263\n",
      "    step: 400; loss: 1.944; l2dist: 1.264\n",
      "    step: 450; loss: 1.922; l2dist: 1.258\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.357; l2dist: 0.000\n",
      "    step: 50; loss: 6.253; l2dist: 1.949\n",
      "    step: 100; loss: 3.060; l2dist: 1.523\n",
      "    step: 150; loss: 2.313; l2dist: 1.358\n",
      "    step: 200; loss: 2.100; l2dist: 1.304\n",
      "    step: 250; loss: 2.012; l2dist: 1.284\n",
      "    step: 300; loss: 1.979; l2dist: 1.279\n",
      "    step: 350; loss: 1.951; l2dist: 1.266\n",
      "    step: 400; loss: 1.969; l2dist: 1.278\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.652; l2dist: 0.000\n",
      "    step: 50; loss: 6.212; l2dist: 1.936\n",
      "    step: 100; loss: 3.071; l2dist: 1.522\n",
      "    step: 150; loss: 2.309; l2dist: 1.360\n",
      "    step: 200; loss: 2.117; l2dist: 1.325\n",
      "    step: 250; loss: 2.029; l2dist: 1.288\n",
      "    step: 300; loss: 1.973; l2dist: 1.281\n",
      "    step: 350; loss: 1.969; l2dist: 1.275\n",
      "    step: 400; loss: 1.945; l2dist: 1.276\n",
      "    step: 450; loss: 1.934; l2dist: 1.267\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.081; l2dist: 0.000\n",
      "    step: 50; loss: 6.265; l2dist: 1.957\n",
      "    step: 100; loss: 3.080; l2dist: 1.531\n",
      "    step: 150; loss: 2.316; l2dist: 1.367\n",
      "    step: 200; loss: 2.103; l2dist: 1.319\n",
      "    step: 250; loss: 2.027; l2dist: 1.300\n",
      "    step: 300; loss: 1.980; l2dist: 1.285\n",
      "    step: 350; loss: 1.959; l2dist: 1.277\n",
      "    step: 400; loss: 1.955; l2dist: 1.275\n",
      "    step: 450; loss: 1.943; l2dist: 1.273\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.186; l2dist: 0.000\n",
      "    step: 50; loss: 13.011; l2dist: 3.284\n",
      "    step: 100; loss: 5.313; l2dist: 2.224\n",
      "    step: 150; loss: 3.428; l2dist: 1.778\n",
      "    step: 200; loss: 2.725; l2dist: 1.584\n",
      "    step: 250; loss: 2.419; l2dist: 1.485\n",
      "    step: 300; loss: 2.266; l2dist: 1.441\n",
      "    step: 350; loss: 2.154; l2dist: 1.401\n",
      "    step: 400; loss: 2.091; l2dist: 1.381\n",
      "    step: 450; loss: 2.046; l2dist: 1.365\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 85.868; l2dist: 0.000\n",
      "    step: 50; loss: 10.552; l2dist: 2.897\n",
      "    step: 100; loss: 4.082; l2dist: 1.913\n",
      "    step: 150; loss: 2.826; l2dist: 1.595\n",
      "    step: 200; loss: 2.342; l2dist: 1.454\n",
      "    step: 250; loss: 2.151; l2dist: 1.398\n",
      "    step: 300; loss: 2.018; l2dist: 1.351\n",
      "    step: 350; loss: 2.001; l2dist: 1.342\n",
      "    step: 400; loss: 1.954; l2dist: 1.335\n",
      "    step: 450; loss: 1.958; l2dist: 1.331\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.760; l2dist: 0.000\n",
      "    step: 50; loss: 8.506; l2dist: 2.533\n",
      "    step: 100; loss: 3.282; l2dist: 1.724\n",
      "    step: 150; loss: 2.348; l2dist: 1.457\n",
      "    step: 200; loss: 2.085; l2dist: 1.373\n",
      "    step: 250; loss: 1.971; l2dist: 1.330\n",
      "    step: 300; loss: 1.917; l2dist: 1.322\n",
      "    step: 350; loss: 1.881; l2dist: 1.310\n",
      "    step: 400; loss: 1.867; l2dist: 1.307\n",
      "    step: 450; loss: 1.853; l2dist: 1.295\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.349; l2dist: 0.000\n",
      "    step: 50; loss: 7.208; l2dist: 2.239\n",
      "    step: 100; loss: 2.998; l2dist: 1.636\n",
      "    step: 150; loss: 2.174; l2dist: 1.403\n",
      "    step: 200; loss: 1.977; l2dist: 1.339\n",
      "    step: 250; loss: 1.899; l2dist: 1.314\n",
      "    step: 300; loss: 1.863; l2dist: 1.304\n",
      "    step: 350; loss: 1.855; l2dist: 1.290\n",
      "    step: 400; loss: 1.832; l2dist: 1.289\n",
      "    step: 450; loss: 1.899; l2dist: 1.295\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.287; l2dist: 0.000\n",
      "    step: 50; loss: 6.333; l2dist: 2.012\n",
      "    step: 100; loss: 2.850; l2dist: 1.544\n",
      "    step: 150; loss: 2.095; l2dist: 1.352\n",
      "    step: 200; loss: 1.918; l2dist: 1.306\n",
      "    step: 250; loss: 1.848; l2dist: 1.282\n",
      "    step: 300; loss: 1.816; l2dist: 1.276\n",
      "    step: 350; loss: 1.819; l2dist: 1.273\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.383; l2dist: 0.000\n",
      "    step: 50; loss: 5.827; l2dist: 1.880\n",
      "    step: 100; loss: 2.761; l2dist: 1.455\n",
      "    step: 150; loss: 2.064; l2dist: 1.300\n",
      "    step: 200; loss: 1.898; l2dist: 1.265\n",
      "    step: 250; loss: 1.839; l2dist: 1.252\n",
      "    step: 300; loss: 1.801; l2dist: 1.237\n",
      "    step: 350; loss: 1.807; l2dist: 1.240\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.185; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 5.832; l2dist: 1.872\n",
      "    step: 100; loss: 2.763; l2dist: 1.472\n",
      "    step: 150; loss: 2.047; l2dist: 1.311\n",
      "    step: 200; loss: 1.904; l2dist: 1.269\n",
      "    step: 250; loss: 1.822; l2dist: 1.253\n",
      "    step: 300; loss: 1.793; l2dist: 1.241\n",
      "    step: 350; loss: 1.778; l2dist: 1.242\n",
      "    step: 400; loss: 1.769; l2dist: 1.239\n",
      "    step: 450; loss: 1.767; l2dist: 1.236\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.685; l2dist: 0.000\n",
      "    step: 50; loss: 5.802; l2dist: 1.867\n",
      "    step: 100; loss: 2.769; l2dist: 1.466\n",
      "    step: 150; loss: 2.049; l2dist: 1.309\n",
      "    step: 200; loss: 1.898; l2dist: 1.274\n",
      "    step: 250; loss: 1.836; l2dist: 1.255\n",
      "    step: 300; loss: 1.810; l2dist: 1.247\n",
      "    step: 350; loss: 1.785; l2dist: 1.244\n",
      "    step: 400; loss: 1.773; l2dist: 1.241\n",
      "    step: 450; loss: 1.769; l2dist: 1.240\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.334; l2dist: 0.000\n",
      "    step: 50; loss: 5.791; l2dist: 1.857\n",
      "    step: 100; loss: 2.769; l2dist: 1.463\n",
      "    step: 150; loss: 2.047; l2dist: 1.305\n",
      "    step: 200; loss: 1.889; l2dist: 1.268\n",
      "    step: 250; loss: 1.839; l2dist: 1.253\n",
      "    step: 300; loss: 1.809; l2dist: 1.249\n",
      "    step: 350; loss: 1.794; l2dist: 1.245\n",
      "    step: 400; loss: 1.783; l2dist: 1.238\n",
      "    step: 450; loss: 1.781; l2dist: 1.244\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.527; l2dist: 0.000\n",
      "    step: 50; loss: 5.844; l2dist: 1.871\n",
      "    step: 100; loss: 2.778; l2dist: 1.475\n",
      "    step: 150; loss: 2.060; l2dist: 1.310\n",
      "    step: 200; loss: 1.906; l2dist: 1.276\n",
      "    step: 250; loss: 1.834; l2dist: 1.258\n",
      "    step: 300; loss: 1.806; l2dist: 1.253\n",
      "    step: 350; loss: 1.789; l2dist: 1.247\n",
      "    step: 400; loss: 1.789; l2dist: 1.254\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.398; l2dist: 0.000\n",
      "    step: 50; loss: 12.907; l2dist: 3.194\n",
      "    step: 100; loss: 5.063; l2dist: 2.149\n",
      "    step: 150; loss: 3.362; l2dist: 1.734\n",
      "    step: 200; loss: 2.712; l2dist: 1.535\n",
      "    step: 250; loss: 2.450; l2dist: 1.459\n",
      "    step: 300; loss: 2.305; l2dist: 1.414\n",
      "    step: 350; loss: 2.213; l2dist: 1.371\n",
      "    step: 400; loss: 2.189; l2dist: 1.368\n",
      "    step: 450; loss: 2.193; l2dist: 1.369\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 98.461; l2dist: 0.000\n",
      "    step: 50; loss: 10.811; l2dist: 2.900\n",
      "    step: 100; loss: 4.458; l2dist: 1.971\n",
      "    step: 150; loss: 3.104; l2dist: 1.633\n",
      "    step: 200; loss: 2.574; l2dist: 1.481\n",
      "    step: 250; loss: 2.332; l2dist: 1.412\n",
      "    step: 300; loss: 2.209; l2dist: 1.376\n",
      "    step: 350; loss: 2.118; l2dist: 1.345\n",
      "    step: 400; loss: 2.056; l2dist: 1.338\n",
      "    step: 450; loss: 2.039; l2dist: 1.332\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.521; l2dist: 0.000\n",
      "    step: 50; loss: 8.418; l2dist: 2.471\n",
      "    step: 100; loss: 3.452; l2dist: 1.728\n",
      "    step: 150; loss: 2.541; l2dist: 1.476\n",
      "    step: 200; loss: 2.227; l2dist: 1.382\n",
      "    step: 250; loss: 2.112; l2dist: 1.341\n",
      "    step: 300; loss: 2.023; l2dist: 1.323\n",
      "    step: 350; loss: 2.023; l2dist: 1.322\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.993; l2dist: 0.000\n",
      "    step: 50; loss: 6.905; l2dist: 2.140\n",
      "    step: 100; loss: 3.128; l2dist: 1.636\n",
      "    step: 150; loss: 2.333; l2dist: 1.411\n",
      "    step: 200; loss: 2.102; l2dist: 1.342\n",
      "    step: 250; loss: 2.020; l2dist: 1.320\n",
      "    step: 300; loss: 1.989; l2dist: 1.306\n",
      "    step: 350; loss: 2.003; l2dist: 1.299\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.247; l2dist: 0.000\n",
      "    step: 50; loss: 6.348; l2dist: 2.002\n",
      "    step: 100; loss: 3.013; l2dist: 1.556\n",
      "    step: 150; loss: 2.257; l2dist: 1.372\n",
      "    step: 200; loss: 2.066; l2dist: 1.314\n",
      "    step: 250; loss: 1.993; l2dist: 1.296\n",
      "    step: 300; loss: 1.953; l2dist: 1.285\n",
      "    step: 350; loss: 1.950; l2dist: 1.278\n",
      "    step: 400; loss: 1.939; l2dist: 1.272\n",
      "    step: 450; loss: 1.911; l2dist: 1.270\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.373; l2dist: 0.000\n",
      "    step: 50; loss: 6.051; l2dist: 1.909\n",
      "    step: 100; loss: 2.987; l2dist: 1.508\n",
      "    step: 150; loss: 2.241; l2dist: 1.338\n",
      "    step: 200; loss: 2.049; l2dist: 1.292\n",
      "    step: 250; loss: 1.988; l2dist: 1.275\n",
      "    step: 300; loss: 1.936; l2dist: 1.262\n",
      "    step: 350; loss: 1.928; l2dist: 1.255\n",
      "    step: 400; loss: 1.911; l2dist: 1.260\n",
      "    step: 450; loss: 1.906; l2dist: 1.249\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.435; l2dist: 0.000\n",
      "    step: 50; loss: 5.892; l2dist: 1.864\n",
      "    step: 100; loss: 2.957; l2dist: 1.488\n",
      "    step: 150; loss: 2.218; l2dist: 1.318\n",
      "    step: 200; loss: 2.037; l2dist: 1.269\n",
      "    step: 250; loss: 1.981; l2dist: 1.264\n",
      "    step: 300; loss: 1.938; l2dist: 1.248\n",
      "    step: 350; loss: 1.905; l2dist: 1.240\n",
      "    step: 400; loss: 1.904; l2dist: 1.236\n",
      "    step: 450; loss: 1.907; l2dist: 1.239\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.858; l2dist: 0.000\n",
      "    step: 50; loss: 5.910; l2dist: 1.870\n",
      "    step: 100; loss: 2.986; l2dist: 1.503\n",
      "    step: 150; loss: 2.240; l2dist: 1.341\n",
      "    step: 200; loss: 2.047; l2dist: 1.297\n",
      "    step: 250; loss: 1.972; l2dist: 1.274\n",
      "    step: 300; loss: 1.947; l2dist: 1.270\n",
      "    step: 350; loss: 1.930; l2dist: 1.268\n",
      "    step: 400; loss: 1.923; l2dist: 1.259\n",
      "    step: 450; loss: 1.899; l2dist: 1.254\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.623; l2dist: 0.000\n",
      "    step: 50; loss: 5.910; l2dist: 1.865\n",
      "    step: 100; loss: 2.997; l2dist: 1.509\n",
      "    step: 150; loss: 2.237; l2dist: 1.344\n",
      "    step: 200; loss: 2.052; l2dist: 1.298\n",
      "    step: 250; loss: 1.991; l2dist: 1.272\n",
      "    step: 300; loss: 1.951; l2dist: 1.268\n",
      "    step: 350; loss: 1.921; l2dist: 1.260\n",
      "    step: 400; loss: 1.917; l2dist: 1.257\n",
      "    step: 450; loss: 1.911; l2dist: 1.259\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.834; l2dist: 0.000\n",
      "    step: 50; loss: 5.943; l2dist: 1.873\n",
      "    step: 100; loss: 2.999; l2dist: 1.514\n",
      "    step: 150; loss: 2.238; l2dist: 1.343\n",
      "    step: 200; loss: 2.060; l2dist: 1.302\n",
      "    step: 250; loss: 1.977; l2dist: 1.281\n",
      "    step: 300; loss: 1.956; l2dist: 1.274\n",
      "    step: 350; loss: 1.932; l2dist: 1.266\n",
      "    step: 400; loss: 1.920; l2dist: 1.259\n",
      "    step: 450; loss: 1.918; l2dist: 1.263\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.249; l2dist: 0.000\n",
      "    step: 50; loss: 12.780; l2dist: 3.339\n",
      "    step: 100; loss: 5.762; l2dist: 2.309\n",
      "    step: 150; loss: 3.841; l2dist: 1.860\n",
      "    step: 200; loss: 3.080; l2dist: 1.656\n",
      "    step: 250; loss: 2.804; l2dist: 1.555\n",
      "    step: 300; loss: 2.645; l2dist: 1.517\n",
      "    step: 350; loss: 2.616; l2dist: 1.496\n",
      "    step: 400; loss: 2.486; l2dist: 1.467\n",
      "    step: 450; loss: 2.468; l2dist: 1.460\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 136.020; l2dist: 0.000\n",
      "    step: 50; loss: 12.227; l2dist: 3.218\n",
      "    step: 100; loss: 5.524; l2dist: 2.213\n",
      "    step: 150; loss: 3.778; l2dist: 1.812\n",
      "    step: 200; loss: 3.101; l2dist: 1.628\n",
      "    step: 250; loss: 2.756; l2dist: 1.543\n",
      "    step: 300; loss: 2.592; l2dist: 1.491\n",
      "    step: 350; loss: 2.518; l2dist: 1.475\n",
      "    step: 400; loss: 2.413; l2dist: 1.442\n",
      "    step: 450; loss: 2.422; l2dist: 1.450\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.063; l2dist: 0.000\n",
      "    step: 50; loss: 9.425; l2dist: 2.775\n",
      "    step: 100; loss: 4.219; l2dist: 1.915\n",
      "    step: 150; loss: 3.042; l2dist: 1.621\n",
      "    step: 200; loss: 2.652; l2dist: 1.513\n",
      "    step: 250; loss: 2.463; l2dist: 1.454\n",
      "    step: 300; loss: 2.379; l2dist: 1.430\n",
      "    step: 350; loss: 2.376; l2dist: 1.429\n",
      "    step: 400; loss: 2.296; l2dist: 1.403\n",
      "    step: 450; loss: 2.279; l2dist: 1.404\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.875; l2dist: 0.000\n",
      "    step: 50; loss: 7.828; l2dist: 2.423\n",
      "    step: 100; loss: 3.670; l2dist: 1.776\n",
      "    step: 150; loss: 2.731; l2dist: 1.534\n",
      "    step: 200; loss: 2.460; l2dist: 1.458\n",
      "    step: 250; loss: 2.336; l2dist: 1.412\n",
      "    step: 300; loss: 2.285; l2dist: 1.398\n",
      "    step: 350; loss: 2.254; l2dist: 1.386\n",
      "    step: 400; loss: 2.256; l2dist: 1.391\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.610; l2dist: 0.000\n",
      "    step: 50; loss: 6.963; l2dist: 2.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 3.503; l2dist: 1.682\n",
      "    step: 150; loss: 2.619; l2dist: 1.470\n",
      "    step: 200; loss: 2.359; l2dist: 1.412\n",
      "    step: 250; loss: 2.291; l2dist: 1.391\n",
      "    step: 300; loss: 2.241; l2dist: 1.381\n",
      "    step: 350; loss: 2.226; l2dist: 1.375\n",
      "    step: 400; loss: 2.217; l2dist: 1.368\n",
      "    step: 450; loss: 2.213; l2dist: 1.367\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.234; l2dist: 0.000\n",
      "    step: 50; loss: 6.495; l2dist: 2.071\n",
      "    step: 100; loss: 3.385; l2dist: 1.609\n",
      "    step: 150; loss: 2.542; l2dist: 1.427\n",
      "    step: 200; loss: 2.343; l2dist: 1.378\n",
      "    step: 250; loss: 2.272; l2dist: 1.358\n",
      "    step: 300; loss: 2.212; l2dist: 1.349\n",
      "    step: 350; loss: 2.195; l2dist: 1.346\n",
      "    step: 400; loss: 2.182; l2dist: 1.337\n",
      "    step: 450; loss: 2.171; l2dist: 1.338\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.604; l2dist: 0.000\n",
      "    step: 50; loss: 6.434; l2dist: 2.043\n",
      "    step: 100; loss: 3.338; l2dist: 1.593\n",
      "    step: 150; loss: 2.513; l2dist: 1.414\n",
      "    step: 200; loss: 2.314; l2dist: 1.363\n",
      "    step: 250; loss: 2.237; l2dist: 1.347\n",
      "    step: 300; loss: 2.214; l2dist: 1.337\n",
      "    step: 350; loss: 2.184; l2dist: 1.336\n",
      "    step: 400; loss: 2.159; l2dist: 1.328\n",
      "    step: 450; loss: 2.163; l2dist: 1.330\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.814; l2dist: 0.000\n",
      "    step: 50; loss: 6.385; l2dist: 2.032\n",
      "    step: 100; loss: 3.380; l2dist: 1.594\n",
      "    step: 150; loss: 2.543; l2dist: 1.422\n",
      "    step: 200; loss: 2.344; l2dist: 1.373\n",
      "    step: 250; loss: 2.281; l2dist: 1.353\n",
      "    step: 300; loss: 2.238; l2dist: 1.350\n",
      "    step: 350; loss: 2.216; l2dist: 1.341\n",
      "    step: 400; loss: 2.207; l2dist: 1.338\n",
      "    step: 450; loss: 2.197; l2dist: 1.338\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.081; l2dist: 0.000\n",
      "    step: 50; loss: 6.346; l2dist: 2.018\n",
      "    step: 100; loss: 3.370; l2dist: 1.588\n",
      "    step: 150; loss: 2.542; l2dist: 1.414\n",
      "    step: 200; loss: 2.341; l2dist: 1.362\n",
      "    step: 250; loss: 2.268; l2dist: 1.350\n",
      "    step: 300; loss: 2.242; l2dist: 1.346\n",
      "    step: 350; loss: 2.212; l2dist: 1.344\n",
      "    step: 400; loss: 2.208; l2dist: 1.339\n",
      "    step: 450; loss: 2.196; l2dist: 1.337\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.278; l2dist: 0.000\n",
      "    step: 50; loss: 6.403; l2dist: 2.025\n",
      "    step: 100; loss: 3.361; l2dist: 1.590\n",
      "    step: 150; loss: 2.529; l2dist: 1.417\n",
      "    step: 200; loss: 2.329; l2dist: 1.372\n",
      "    step: 250; loss: 2.259; l2dist: 1.356\n",
      "    step: 300; loss: 2.220; l2dist: 1.348\n",
      "    step: 350; loss: 2.217; l2dist: 1.344\n",
      "    step: 400; loss: 2.191; l2dist: 1.336\n",
      "    step: 450; loss: 2.195; l2dist: 1.337\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 124.505; l2dist: 0.000\n",
      "    step: 50; loss: 12.761; l2dist: 3.388\n",
      "    step: 100; loss: 5.874; l2dist: 2.337\n",
      "    step: 150; loss: 3.862; l2dist: 1.879\n",
      "    step: 200; loss: 3.121; l2dist: 1.683\n",
      "    step: 250; loss: 2.762; l2dist: 1.570\n",
      "    step: 300; loss: 2.607; l2dist: 1.527\n",
      "    step: 350; loss: 2.502; l2dist: 1.495\n",
      "    step: 400; loss: 2.494; l2dist: 1.473\n",
      "    step: 450; loss: 2.469; l2dist: 1.472\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 105.020; l2dist: 0.000\n",
      "    step: 50; loss: 10.875; l2dist: 3.094\n",
      "    step: 100; loss: 4.895; l2dist: 2.090\n",
      "    step: 150; loss: 3.398; l2dist: 1.726\n",
      "    step: 200; loss: 2.778; l2dist: 1.571\n",
      "    step: 250; loss: 2.544; l2dist: 1.500\n",
      "    step: 300; loss: 2.443; l2dist: 1.472\n",
      "    step: 350; loss: 2.393; l2dist: 1.453\n",
      "    step: 400; loss: 2.306; l2dist: 1.437\n",
      "    step: 450; loss: 2.292; l2dist: 1.429\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.680; l2dist: 0.000\n",
      "    step: 50; loss: 8.714; l2dist: 2.697\n",
      "    step: 100; loss: 3.891; l2dist: 1.861\n",
      "    step: 150; loss: 2.791; l2dist: 1.570\n",
      "    step: 200; loss: 2.489; l2dist: 1.486\n",
      "    step: 250; loss: 2.328; l2dist: 1.443\n",
      "    step: 300; loss: 2.279; l2dist: 1.419\n",
      "    step: 350; loss: 2.219; l2dist: 1.407\n",
      "    step: 400; loss: 2.214; l2dist: 1.406\n",
      "    step: 450; loss: 2.206; l2dist: 1.396\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.370; l2dist: 0.000\n",
      "    step: 50; loss: 7.623; l2dist: 2.428\n",
      "    step: 100; loss: 3.506; l2dist: 1.758\n",
      "    step: 150; loss: 2.609; l2dist: 1.511\n",
      "    step: 200; loss: 2.351; l2dist: 1.441\n",
      "    step: 250; loss: 2.268; l2dist: 1.415\n",
      "    step: 300; loss: 2.220; l2dist: 1.394\n",
      "    step: 350; loss: 2.200; l2dist: 1.398\n",
      "    step: 400; loss: 2.190; l2dist: 1.388\n",
      "    step: 450; loss: 2.178; l2dist: 1.386\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.315; l2dist: 0.000\n",
      "    step: 50; loss: 6.976; l2dist: 2.217\n",
      "    step: 100; loss: 3.358; l2dist: 1.680\n",
      "    step: 150; loss: 2.495; l2dist: 1.475\n",
      "    step: 200; loss: 2.275; l2dist: 1.406\n",
      "    step: 250; loss: 2.200; l2dist: 1.387\n",
      "    step: 300; loss: 2.169; l2dist: 1.374\n",
      "    step: 350; loss: 2.142; l2dist: 1.371\n",
      "    step: 400; loss: 2.160; l2dist: 1.363\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.210; l2dist: 0.000\n",
      "    step: 50; loss: 6.786; l2dist: 2.170\n",
      "    step: 100; loss: 3.297; l2dist: 1.642\n",
      "    step: 150; loss: 2.461; l2dist: 1.443\n",
      "    step: 200; loss: 2.264; l2dist: 1.393\n",
      "    step: 250; loss: 2.179; l2dist: 1.377\n",
      "    step: 300; loss: 2.156; l2dist: 1.364\n",
      "    step: 350; loss: 2.143; l2dist: 1.362\n",
      "    step: 400; loss: 2.125; l2dist: 1.360\n",
      "    step: 450; loss: 2.131; l2dist: 1.364\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.010; l2dist: 0.000\n",
      "    step: 50; loss: 6.627; l2dist: 2.112\n",
      "    step: 100; loss: 3.277; l2dist: 1.616\n",
      "    step: 150; loss: 2.485; l2dist: 1.417\n",
      "    step: 200; loss: 2.280; l2dist: 1.363\n",
      "    step: 250; loss: 2.207; l2dist: 1.350\n",
      "    step: 300; loss: 2.178; l2dist: 1.345\n",
      "    step: 350; loss: 2.166; l2dist: 1.346\n",
      "    step: 400; loss: 2.148; l2dist: 1.338\n",
      "    step: 450; loss: 2.144; l2dist: 1.336\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.306; l2dist: 0.000\n",
      "    step: 50; loss: 6.626; l2dist: 2.111\n",
      "    step: 100; loss: 3.294; l2dist: 1.625\n",
      "    step: 150; loss: 2.481; l2dist: 1.428\n",
      "    step: 200; loss: 2.297; l2dist: 1.395\n",
      "    step: 250; loss: 2.211; l2dist: 1.358\n",
      "    step: 300; loss: 2.185; l2dist: 1.357\n",
      "    step: 350; loss: 2.165; l2dist: 1.357\n",
      "    step: 400; loss: 2.162; l2dist: 1.349\n",
      "    step: 450; loss: 2.148; l2dist: 1.345\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.965; l2dist: 0.000\n",
      "    step: 50; loss: 6.608; l2dist: 2.106\n",
      "    step: 100; loss: 3.286; l2dist: 1.626\n",
      "    step: 150; loss: 2.472; l2dist: 1.425\n",
      "    step: 200; loss: 2.271; l2dist: 1.379\n",
      "    step: 250; loss: 2.209; l2dist: 1.366\n",
      "    step: 300; loss: 2.185; l2dist: 1.352\n",
      "    step: 350; loss: 2.166; l2dist: 1.348\n",
      "    step: 400; loss: 2.152; l2dist: 1.346\n",
      "    step: 450; loss: 2.158; l2dist: 1.346\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.184; l2dist: 0.000\n",
      "    step: 50; loss: 6.650; l2dist: 2.120\n",
      "    step: 100; loss: 3.297; l2dist: 1.631\n",
      "    step: 150; loss: 2.477; l2dist: 1.439\n",
      "    step: 200; loss: 2.280; l2dist: 1.385\n",
      "    step: 250; loss: 2.227; l2dist: 1.365\n",
      "    step: 300; loss: 2.220; l2dist: 1.375\n",
      "    step: 350; loss: 2.169; l2dist: 1.362\n",
      "    step: 400; loss: 2.162; l2dist: 1.358\n",
      "    step: 450; loss: 2.155; l2dist: 1.357\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 121.381; l2dist: 0.000\n",
      "    step: 50; loss: 15.151; l2dist: 3.412\n",
      "    step: 100; loss: 5.733; l2dist: 2.309\n",
      "    step: 150; loss: 3.749; l2dist: 1.848\n",
      "    step: 200; loss: 2.959; l2dist: 1.631\n",
      "    step: 250; loss: 2.615; l2dist: 1.531\n",
      "    step: 300; loss: 2.553; l2dist: 1.496\n",
      "    step: 350; loss: 2.454; l2dist: 1.466\n",
      "    step: 400; loss: 2.328; l2dist: 1.435\n",
      "    step: 450; loss: 2.279; l2dist: 1.419\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 100.091; l2dist: 0.000\n",
      "    step: 50; loss: 12.770; l2dist: 3.110\n",
      "    step: 100; loss: 4.929; l2dist: 2.090\n",
      "    step: 150; loss: 3.337; l2dist: 1.707\n",
      "    step: 200; loss: 2.776; l2dist: 1.554\n",
      "    step: 250; loss: 2.488; l2dist: 1.470\n",
      "    step: 300; loss: 2.385; l2dist: 1.434\n",
      "    step: 350; loss: 2.287; l2dist: 1.419\n",
      "    step: 400; loss: 2.266; l2dist: 1.406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 450; loss: 2.246; l2dist: 1.400\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.060; l2dist: 0.000\n",
      "    step: 50; loss: 9.625; l2dist: 2.661\n",
      "    step: 100; loss: 3.702; l2dist: 1.804\n",
      "    step: 150; loss: 2.718; l2dist: 1.538\n",
      "    step: 200; loss: 2.445; l2dist: 1.451\n",
      "    step: 250; loss: 2.288; l2dist: 1.414\n",
      "    step: 300; loss: 2.241; l2dist: 1.393\n",
      "    step: 350; loss: 2.171; l2dist: 1.369\n",
      "    step: 400; loss: 2.144; l2dist: 1.366\n",
      "    step: 450; loss: 2.150; l2dist: 1.370\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.751; l2dist: 0.000\n",
      "    step: 50; loss: 8.041; l2dist: 2.340\n",
      "    step: 100; loss: 3.309; l2dist: 1.683\n",
      "    step: 150; loss: 2.514; l2dist: 1.462\n",
      "    step: 200; loss: 2.294; l2dist: 1.411\n",
      "    step: 250; loss: 2.195; l2dist: 1.381\n",
      "    step: 300; loss: 2.163; l2dist: 1.370\n",
      "    step: 350; loss: 2.133; l2dist: 1.355\n",
      "    step: 400; loss: 2.100; l2dist: 1.355\n",
      "    step: 450; loss: 2.088; l2dist: 1.343\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.647; l2dist: 0.000\n",
      "    step: 50; loss: 7.079; l2dist: 2.093\n",
      "    step: 100; loss: 3.220; l2dist: 1.584\n",
      "    step: 150; loss: 2.457; l2dist: 1.425\n",
      "    step: 200; loss: 2.237; l2dist: 1.368\n",
      "    step: 250; loss: 2.161; l2dist: 1.352\n",
      "    step: 300; loss: 2.106; l2dist: 1.335\n",
      "    step: 350; loss: 2.086; l2dist: 1.322\n",
      "    step: 400; loss: 2.084; l2dist: 1.332\n",
      "    step: 450; loss: 2.088; l2dist: 1.318\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.177; l2dist: 0.000\n",
      "    step: 50; loss: 6.849; l2dist: 2.007\n",
      "    step: 100; loss: 3.154; l2dist: 1.544\n",
      "    step: 150; loss: 2.407; l2dist: 1.383\n",
      "    step: 200; loss: 2.213; l2dist: 1.339\n",
      "    step: 250; loss: 2.120; l2dist: 1.323\n",
      "    step: 300; loss: 2.082; l2dist: 1.315\n",
      "    step: 350; loss: 2.061; l2dist: 1.309\n",
      "    step: 400; loss: 2.065; l2dist: 1.305\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.466; l2dist: 0.000\n",
      "    step: 50; loss: 6.787; l2dist: 1.973\n",
      "    step: 100; loss: 3.179; l2dist: 1.552\n",
      "    step: 150; loss: 2.391; l2dist: 1.393\n",
      "    step: 200; loss: 2.199; l2dist: 1.353\n",
      "    step: 250; loss: 2.124; l2dist: 1.328\n",
      "    step: 300; loss: 2.089; l2dist: 1.315\n",
      "    step: 350; loss: 2.071; l2dist: 1.317\n",
      "    step: 400; loss: 2.056; l2dist: 1.312\n",
      "    step: 450; loss: 2.048; l2dist: 1.308\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.045; l2dist: 0.000\n",
      "    step: 50; loss: 6.715; l2dist: 1.952\n",
      "    step: 100; loss: 3.186; l2dist: 1.546\n",
      "    step: 150; loss: 2.398; l2dist: 1.386\n",
      "    step: 200; loss: 2.206; l2dist: 1.350\n",
      "    step: 250; loss: 2.126; l2dist: 1.327\n",
      "    step: 300; loss: 2.087; l2dist: 1.314\n",
      "    step: 350; loss: 2.068; l2dist: 1.310\n",
      "    step: 400; loss: 2.065; l2dist: 1.312\n",
      "    step: 450; loss: 2.050; l2dist: 1.304\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.546; l2dist: 0.000\n",
      "    step: 50; loss: 6.656; l2dist: 1.934\n",
      "    step: 100; loss: 3.177; l2dist: 1.539\n",
      "    step: 150; loss: 2.394; l2dist: 1.383\n",
      "    step: 200; loss: 2.184; l2dist: 1.336\n",
      "    step: 250; loss: 2.123; l2dist: 1.318\n",
      "    step: 300; loss: 2.084; l2dist: 1.313\n",
      "    step: 350; loss: 2.062; l2dist: 1.312\n",
      "    step: 400; loss: 2.056; l2dist: 1.302\n",
      "    step: 450; loss: 2.046; l2dist: 1.308\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.855; l2dist: 0.000\n",
      "    step: 50; loss: 6.718; l2dist: 1.950\n",
      "    step: 100; loss: 3.188; l2dist: 1.547\n",
      "    step: 150; loss: 2.396; l2dist: 1.388\n",
      "    step: 200; loss: 2.219; l2dist: 1.341\n",
      "    step: 250; loss: 2.124; l2dist: 1.327\n",
      "    step: 300; loss: 2.083; l2dist: 1.316\n",
      "    step: 350; loss: 2.065; l2dist: 1.313\n",
      "    step: 400; loss: 2.055; l2dist: 1.312\n",
      "    step: 450; loss: 2.051; l2dist: 1.312\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 126.057; l2dist: 0.000\n",
      "    step: 50; loss: 14.255; l2dist: 3.435\n",
      "    step: 100; loss: 5.966; l2dist: 2.368\n",
      "    step: 150; loss: 3.945; l2dist: 1.914\n",
      "    step: 200; loss: 3.113; l2dist: 1.695\n",
      "    step: 250; loss: 2.769; l2dist: 1.588\n",
      "    step: 300; loss: 2.647; l2dist: 1.532\n",
      "    step: 350; loss: 2.563; l2dist: 1.526\n",
      "    step: 400; loss: 2.553; l2dist: 1.519\n",
      "    step: 450; loss: 2.444; l2dist: 1.492\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 127.024; l2dist: 0.000\n",
      "    step: 50; loss: 12.966; l2dist: 3.251\n",
      "    step: 100; loss: 5.510; l2dist: 2.222\n",
      "    step: 150; loss: 3.772; l2dist: 1.826\n",
      "    step: 200; loss: 3.111; l2dist: 1.652\n",
      "    step: 250; loss: 2.791; l2dist: 1.568\n",
      "    step: 300; loss: 2.598; l2dist: 1.527\n",
      "    step: 350; loss: 2.553; l2dist: 1.503\n",
      "    step: 400; loss: 2.473; l2dist: 1.491\n",
      "    step: 450; loss: 2.423; l2dist: 1.474\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.186; l2dist: 0.000\n",
      "    step: 50; loss: 10.146; l2dist: 2.827\n",
      "    step: 100; loss: 4.248; l2dist: 1.946\n",
      "    step: 150; loss: 3.095; l2dist: 1.654\n",
      "    step: 200; loss: 2.681; l2dist: 1.543\n",
      "    step: 250; loss: 2.504; l2dist: 1.501\n",
      "    step: 300; loss: 2.423; l2dist: 1.468\n",
      "    step: 350; loss: 2.356; l2dist: 1.459\n",
      "    step: 400; loss: 2.361; l2dist: 1.448\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.548; l2dist: 0.000\n",
      "    step: 50; loss: 8.743; l2dist: 2.547\n",
      "    step: 100; loss: 3.772; l2dist: 1.828\n",
      "    step: 150; loss: 2.802; l2dist: 1.573\n",
      "    step: 200; loss: 2.502; l2dist: 1.494\n",
      "    step: 250; loss: 2.391; l2dist: 1.463\n",
      "    step: 300; loss: 2.344; l2dist: 1.449\n",
      "    step: 350; loss: 2.315; l2dist: 1.446\n",
      "    step: 400; loss: 2.278; l2dist: 1.436\n",
      "    step: 450; loss: 2.256; l2dist: 1.422\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.215; l2dist: 0.000\n",
      "    step: 50; loss: 8.201; l2dist: 2.393\n",
      "    step: 100; loss: 3.623; l2dist: 1.754\n",
      "    step: 150; loss: 2.719; l2dist: 1.538\n",
      "    step: 200; loss: 2.462; l2dist: 1.472\n",
      "    step: 250; loss: 2.344; l2dist: 1.444\n",
      "    step: 300; loss: 2.296; l2dist: 1.424\n",
      "    step: 350; loss: 2.263; l2dist: 1.422\n",
      "    step: 400; loss: 2.241; l2dist: 1.414\n",
      "    step: 450; loss: 2.230; l2dist: 1.411\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.471; l2dist: 0.000\n",
      "    step: 50; loss: 7.706; l2dist: 2.287\n",
      "    step: 100; loss: 3.472; l2dist: 1.683\n",
      "    step: 150; loss: 2.658; l2dist: 1.493\n",
      "    step: 200; loss: 2.401; l2dist: 1.429\n",
      "    step: 250; loss: 2.314; l2dist: 1.405\n",
      "    step: 300; loss: 2.281; l2dist: 1.399\n",
      "    step: 350; loss: 2.217; l2dist: 1.384\n",
      "    step: 400; loss: 2.231; l2dist: 1.386\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.158; l2dist: 0.000\n",
      "    step: 50; loss: 7.556; l2dist: 2.256\n",
      "    step: 100; loss: 3.433; l2dist: 1.664\n",
      "    step: 150; loss: 2.628; l2dist: 1.482\n",
      "    step: 200; loss: 2.391; l2dist: 1.425\n",
      "    step: 250; loss: 2.313; l2dist: 1.398\n",
      "    step: 300; loss: 2.289; l2dist: 1.400\n",
      "    step: 350; loss: 2.235; l2dist: 1.384\n",
      "    step: 400; loss: 2.225; l2dist: 1.387\n",
      "    step: 450; loss: 2.219; l2dist: 1.378\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.277; l2dist: 0.000\n",
      "    step: 50; loss: 7.482; l2dist: 2.241\n",
      "    step: 100; loss: 3.421; l2dist: 1.661\n",
      "    step: 150; loss: 2.619; l2dist: 1.481\n",
      "    step: 200; loss: 2.389; l2dist: 1.425\n",
      "    step: 250; loss: 2.286; l2dist: 1.395\n",
      "    step: 300; loss: 2.248; l2dist: 1.387\n",
      "    step: 350; loss: 2.216; l2dist: 1.379\n",
      "    step: 400; loss: 2.245; l2dist: 1.385\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.748; l2dist: 0.000\n",
      "    step: 50; loss: 7.453; l2dist: 2.236\n",
      "    step: 100; loss: 3.429; l2dist: 1.666\n",
      "    step: 150; loss: 2.614; l2dist: 1.478\n",
      "    step: 200; loss: 2.377; l2dist: 1.423\n",
      "    step: 250; loss: 2.297; l2dist: 1.398\n",
      "    step: 300; loss: 2.255; l2dist: 1.400\n",
      "    step: 350; loss: 2.228; l2dist: 1.387\n",
      "    step: 400; loss: 2.218; l2dist: 1.381\n",
      "    step: 450; loss: 2.207; l2dist: 1.382\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.898; l2dist: 0.000\n",
      "    step: 50; loss: 7.482; l2dist: 2.245\n",
      "    step: 100; loss: 3.434; l2dist: 1.674\n",
      "    step: 150; loss: 2.629; l2dist: 1.492\n",
      "    step: 200; loss: 2.394; l2dist: 1.428\n",
      "    step: 250; loss: 2.303; l2dist: 1.412\n",
      "    step: 300; loss: 2.250; l2dist: 1.399\n",
      "    step: 350; loss: 2.233; l2dist: 1.394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 2.225; l2dist: 1.389\n",
      "    step: 450; loss: 2.223; l2dist: 1.393\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.710; l2dist: 0.000\n",
      "    step: 50; loss: 12.560; l2dist: 3.247\n",
      "    step: 100; loss: 5.202; l2dist: 2.162\n",
      "    step: 150; loss: 3.443; l2dist: 1.734\n",
      "    step: 200; loss: 2.756; l2dist: 1.530\n",
      "    step: 250; loss: 2.489; l2dist: 1.443\n",
      "    step: 300; loss: 2.369; l2dist: 1.401\n",
      "    step: 350; loss: 2.356; l2dist: 1.390\n",
      "    step: 400; loss: 2.278; l2dist: 1.365\n",
      "    step: 450; loss: 2.304; l2dist: 1.372\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 78.766; l2dist: 0.000\n",
      "    step: 50; loss: 10.431; l2dist: 2.883\n",
      "    step: 100; loss: 4.025; l2dist: 1.859\n",
      "    step: 150; loss: 2.864; l2dist: 1.544\n",
      "    step: 200; loss: 2.471; l2dist: 1.420\n",
      "    step: 250; loss: 2.291; l2dist: 1.368\n",
      "    step: 300; loss: 2.241; l2dist: 1.348\n",
      "    step: 350; loss: 2.178; l2dist: 1.336\n",
      "    step: 400; loss: 2.194; l2dist: 1.332\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.949; l2dist: 0.000\n",
      "    step: 50; loss: 8.285; l2dist: 2.459\n",
      "    step: 100; loss: 3.298; l2dist: 1.670\n",
      "    step: 150; loss: 2.496; l2dist: 1.436\n",
      "    step: 200; loss: 2.272; l2dist: 1.362\n",
      "    step: 250; loss: 2.179; l2dist: 1.333\n",
      "    step: 300; loss: 2.143; l2dist: 1.318\n",
      "    step: 350; loss: 2.102; l2dist: 1.306\n",
      "    step: 400; loss: 2.124; l2dist: 1.308\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.797; l2dist: 0.000\n",
      "    step: 50; loss: 7.076; l2dist: 2.189\n",
      "    step: 100; loss: 3.082; l2dist: 1.601\n",
      "    step: 150; loss: 2.364; l2dist: 1.390\n",
      "    step: 200; loss: 2.184; l2dist: 1.331\n",
      "    step: 250; loss: 2.112; l2dist: 1.306\n",
      "    step: 300; loss: 2.076; l2dist: 1.295\n",
      "    step: 350; loss: 2.094; l2dist: 1.304\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.166; l2dist: 0.000\n",
      "    step: 50; loss: 6.234; l2dist: 1.973\n",
      "    step: 100; loss: 3.056; l2dist: 1.551\n",
      "    step: 150; loss: 2.315; l2dist: 1.359\n",
      "    step: 200; loss: 2.149; l2dist: 1.312\n",
      "    step: 250; loss: 2.086; l2dist: 1.296\n",
      "    step: 300; loss: 2.073; l2dist: 1.277\n",
      "    step: 350; loss: 2.041; l2dist: 1.278\n",
      "    step: 400; loss: 2.029; l2dist: 1.272\n",
      "    step: 450; loss: 2.027; l2dist: 1.276\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.070; l2dist: 0.000\n",
      "    step: 50; loss: 5.845; l2dist: 1.869\n",
      "    step: 100; loss: 3.003; l2dist: 1.482\n",
      "    step: 150; loss: 2.304; l2dist: 1.331\n",
      "    step: 200; loss: 2.138; l2dist: 1.290\n",
      "    step: 250; loss: 2.077; l2dist: 1.275\n",
      "    step: 300; loss: 2.047; l2dist: 1.263\n",
      "    step: 350; loss: 2.038; l2dist: 1.264\n",
      "    step: 400; loss: 2.024; l2dist: 1.261\n",
      "    step: 450; loss: 2.025; l2dist: 1.255\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.240; l2dist: 0.000\n",
      "    step: 50; loss: 5.808; l2dist: 1.863\n",
      "    step: 100; loss: 3.000; l2dist: 1.485\n",
      "    step: 150; loss: 2.296; l2dist: 1.321\n",
      "    step: 200; loss: 2.130; l2dist: 1.277\n",
      "    step: 250; loss: 2.076; l2dist: 1.262\n",
      "    step: 300; loss: 2.049; l2dist: 1.254\n",
      "    step: 350; loss: 2.032; l2dist: 1.251\n",
      "    step: 400; loss: 2.022; l2dist: 1.256\n",
      "    step: 450; loss: 2.018; l2dist: 1.248\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.612; l2dist: 0.000\n",
      "    step: 50; loss: 5.763; l2dist: 1.849\n",
      "    step: 100; loss: 3.009; l2dist: 1.474\n",
      "    step: 150; loss: 2.299; l2dist: 1.318\n",
      "    step: 200; loss: 2.152; l2dist: 1.281\n",
      "    step: 250; loss: 2.079; l2dist: 1.266\n",
      "    step: 300; loss: 2.054; l2dist: 1.260\n",
      "    step: 350; loss: 2.032; l2dist: 1.258\n",
      "    step: 400; loss: 2.032; l2dist: 1.257\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.341; l2dist: 0.000\n",
      "    step: 50; loss: 5.745; l2dist: 1.843\n",
      "    step: 100; loss: 3.010; l2dist: 1.470\n",
      "    step: 150; loss: 2.300; l2dist: 1.316\n",
      "    step: 200; loss: 2.133; l2dist: 1.277\n",
      "    step: 250; loss: 2.071; l2dist: 1.265\n",
      "    step: 300; loss: 2.046; l2dist: 1.263\n",
      "    step: 350; loss: 2.038; l2dist: 1.257\n",
      "    step: 400; loss: 2.019; l2dist: 1.250\n",
      "    step: 450; loss: 2.021; l2dist: 1.252\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.482; l2dist: 0.000\n",
      "    step: 50; loss: 5.779; l2dist: 1.854\n",
      "    step: 100; loss: 3.019; l2dist: 1.482\n",
      "    step: 150; loss: 2.308; l2dist: 1.325\n",
      "    step: 200; loss: 2.141; l2dist: 1.285\n",
      "    step: 250; loss: 2.077; l2dist: 1.270\n",
      "    step: 300; loss: 2.051; l2dist: 1.265\n",
      "    step: 350; loss: 2.049; l2dist: 1.262\n",
      "    step: 400; loss: 2.034; l2dist: 1.262\n",
      "    step: 450; loss: 2.018; l2dist: 1.257\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 114.668; l2dist: 0.000\n",
      "    step: 50; loss: 11.916; l2dist: 3.246\n",
      "    step: 100; loss: 5.048; l2dist: 2.169\n",
      "    step: 150; loss: 3.335; l2dist: 1.744\n",
      "    step: 200; loss: 2.679; l2dist: 1.550\n",
      "    step: 250; loss: 2.376; l2dist: 1.457\n",
      "    step: 300; loss: 2.263; l2dist: 1.409\n",
      "    step: 350; loss: 2.178; l2dist: 1.383\n",
      "    step: 400; loss: 2.135; l2dist: 1.373\n",
      "    step: 450; loss: 2.136; l2dist: 1.368\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 90.977; l2dist: 0.000\n",
      "    step: 50; loss: 10.040; l2dist: 2.924\n",
      "    step: 100; loss: 4.136; l2dist: 1.932\n",
      "    step: 150; loss: 2.870; l2dist: 1.590\n",
      "    step: 200; loss: 2.385; l2dist: 1.444\n",
      "    step: 250; loss: 2.156; l2dist: 1.380\n",
      "    step: 300; loss: 2.104; l2dist: 1.365\n",
      "    step: 350; loss: 2.016; l2dist: 1.331\n",
      "    step: 400; loss: 2.021; l2dist: 1.328\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.721; l2dist: 0.000\n",
      "    step: 50; loss: 8.107; l2dist: 2.536\n",
      "    step: 100; loss: 3.320; l2dist: 1.717\n",
      "    step: 150; loss: 2.443; l2dist: 1.470\n",
      "    step: 200; loss: 2.163; l2dist: 1.374\n",
      "    step: 250; loss: 2.047; l2dist: 1.341\n",
      "    step: 300; loss: 1.994; l2dist: 1.327\n",
      "    step: 350; loss: 1.961; l2dist: 1.314\n",
      "    step: 400; loss: 1.935; l2dist: 1.300\n",
      "    step: 450; loss: 1.946; l2dist: 1.301\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.128; l2dist: 0.000\n",
      "    step: 50; loss: 6.902; l2dist: 2.232\n",
      "    step: 100; loss: 3.051; l2dist: 1.631\n",
      "    step: 150; loss: 2.272; l2dist: 1.404\n",
      "    step: 200; loss: 2.066; l2dist: 1.342\n",
      "    step: 250; loss: 1.980; l2dist: 1.310\n",
      "    step: 300; loss: 1.928; l2dist: 1.297\n",
      "    step: 350; loss: 1.920; l2dist: 1.291\n",
      "    step: 400; loss: 1.907; l2dist: 1.282\n",
      "    step: 450; loss: 1.889; l2dist: 1.279\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.807; l2dist: 0.000\n",
      "    step: 50; loss: 6.162; l2dist: 2.034\n",
      "    step: 100; loss: 3.017; l2dist: 1.567\n",
      "    step: 150; loss: 2.236; l2dist: 1.376\n",
      "    step: 200; loss: 2.042; l2dist: 1.319\n",
      "    step: 250; loss: 1.969; l2dist: 1.300\n",
      "    step: 300; loss: 1.929; l2dist: 1.285\n",
      "    step: 350; loss: 1.915; l2dist: 1.275\n",
      "    step: 400; loss: 1.903; l2dist: 1.278\n",
      "    step: 450; loss: 1.897; l2dist: 1.273\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.067; l2dist: 0.000\n",
      "    step: 50; loss: 5.818; l2dist: 1.925\n",
      "    step: 100; loss: 2.928; l2dist: 1.510\n",
      "    step: 150; loss: 2.202; l2dist: 1.336\n",
      "    step: 200; loss: 2.005; l2dist: 1.289\n",
      "    step: 250; loss: 1.934; l2dist: 1.263\n",
      "    step: 300; loss: 1.894; l2dist: 1.260\n",
      "    step: 350; loss: 1.875; l2dist: 1.250\n",
      "    step: 400; loss: 1.864; l2dist: 1.250\n",
      "    step: 450; loss: 1.856; l2dist: 1.243\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.596; l2dist: 0.000\n",
      "    step: 50; loss: 5.697; l2dist: 1.886\n",
      "    step: 100; loss: 2.917; l2dist: 1.487\n",
      "    step: 150; loss: 2.174; l2dist: 1.318\n",
      "    step: 200; loss: 1.992; l2dist: 1.270\n",
      "    step: 250; loss: 1.923; l2dist: 1.254\n",
      "    step: 300; loss: 1.884; l2dist: 1.240\n",
      "    step: 350; loss: 1.862; l2dist: 1.238\n",
      "    step: 400; loss: 1.852; l2dist: 1.237\n",
      "    step: 450; loss: 1.850; l2dist: 1.235\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.783; l2dist: 0.000\n",
      "    step: 50; loss: 5.633; l2dist: 1.864\n",
      "    step: 100; loss: 2.922; l2dist: 1.481\n",
      "    step: 150; loss: 2.167; l2dist: 1.315\n",
      "    step: 200; loss: 1.984; l2dist: 1.277\n",
      "    step: 250; loss: 1.914; l2dist: 1.255\n",
      "    step: 300; loss: 1.875; l2dist: 1.244\n",
      "    step: 350; loss: 1.859; l2dist: 1.241\n",
      "    step: 400; loss: 1.858; l2dist: 1.240\n",
      "    step: 450; loss: 1.856; l2dist: 1.235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.494; l2dist: 0.000\n",
      "    step: 50; loss: 5.613; l2dist: 1.858\n",
      "    step: 100; loss: 2.918; l2dist: 1.477\n",
      "    step: 150; loss: 2.163; l2dist: 1.311\n",
      "    step: 200; loss: 1.980; l2dist: 1.270\n",
      "    step: 250; loss: 1.912; l2dist: 1.256\n",
      "    step: 300; loss: 1.876; l2dist: 1.241\n",
      "    step: 350; loss: 1.868; l2dist: 1.239\n",
      "    step: 400; loss: 1.852; l2dist: 1.233\n",
      "    step: 450; loss: 1.848; l2dist: 1.228\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.623; l2dist: 0.000\n",
      "    step: 50; loss: 5.644; l2dist: 1.869\n",
      "    step: 100; loss: 2.921; l2dist: 1.484\n",
      "    step: 150; loss: 2.163; l2dist: 1.321\n",
      "    step: 200; loss: 1.992; l2dist: 1.274\n",
      "    step: 250; loss: 1.915; l2dist: 1.252\n",
      "    step: 300; loss: 1.893; l2dist: 1.247\n",
      "    step: 350; loss: 1.867; l2dist: 1.242\n",
      "    step: 400; loss: 1.850; l2dist: 1.240\n",
      "    step: 450; loss: 1.846; l2dist: 1.235\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 121.583; l2dist: 0.000\n",
      "    step: 50; loss: 13.865; l2dist: 3.308\n",
      "    step: 100; loss: 5.719; l2dist: 2.306\n",
      "    step: 150; loss: 3.749; l2dist: 1.846\n",
      "    step: 200; loss: 3.021; l2dist: 1.653\n",
      "    step: 250; loss: 2.757; l2dist: 1.576\n",
      "    step: 300; loss: 2.539; l2dist: 1.509\n",
      "    step: 350; loss: 2.421; l2dist: 1.459\n",
      "    step: 400; loss: 2.364; l2dist: 1.450\n",
      "    step: 450; loss: 2.307; l2dist: 1.430\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 121.424; l2dist: 0.000\n",
      "    step: 50; loss: 12.970; l2dist: 3.132\n",
      "    step: 100; loss: 5.584; l2dist: 2.201\n",
      "    step: 150; loss: 3.751; l2dist: 1.794\n",
      "    step: 200; loss: 3.077; l2dist: 1.622\n",
      "    step: 250; loss: 2.718; l2dist: 1.529\n",
      "    step: 300; loss: 2.521; l2dist: 1.484\n",
      "    step: 350; loss: 2.418; l2dist: 1.452\n",
      "    step: 400; loss: 2.416; l2dist: 1.453\n",
      "    step: 450; loss: 2.354; l2dist: 1.434\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.611; l2dist: 0.000\n",
      "    step: 50; loss: 10.074; l2dist: 2.745\n",
      "    step: 100; loss: 4.177; l2dist: 1.912\n",
      "    step: 150; loss: 2.993; l2dist: 1.614\n",
      "    step: 200; loss: 2.589; l2dist: 1.505\n",
      "    step: 250; loss: 2.388; l2dist: 1.454\n",
      "    step: 300; loss: 2.299; l2dist: 1.421\n",
      "    step: 350; loss: 2.297; l2dist: 1.423\n",
      "    step: 400; loss: 2.208; l2dist: 1.389\n",
      "    step: 450; loss: 2.203; l2dist: 1.391\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.601; l2dist: 0.000\n",
      "    step: 50; loss: 8.264; l2dist: 2.446\n",
      "    step: 100; loss: 3.605; l2dist: 1.779\n",
      "    step: 150; loss: 2.670; l2dist: 1.531\n",
      "    step: 200; loss: 2.390; l2dist: 1.440\n",
      "    step: 250; loss: 2.250; l2dist: 1.411\n",
      "    step: 300; loss: 2.196; l2dist: 1.388\n",
      "    step: 350; loss: 2.186; l2dist: 1.385\n",
      "    step: 400; loss: 2.147; l2dist: 1.372\n",
      "    step: 450; loss: 2.136; l2dist: 1.369\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.001; l2dist: 0.000\n",
      "    step: 50; loss: 7.262; l2dist: 2.256\n",
      "    step: 100; loss: 3.374; l2dist: 1.677\n",
      "    step: 150; loss: 2.510; l2dist: 1.462\n",
      "    step: 200; loss: 2.293; l2dist: 1.403\n",
      "    step: 250; loss: 2.180; l2dist: 1.373\n",
      "    step: 300; loss: 2.136; l2dist: 1.362\n",
      "    step: 350; loss: 2.103; l2dist: 1.352\n",
      "    step: 400; loss: 2.108; l2dist: 1.343\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.099; l2dist: 0.000\n",
      "    step: 50; loss: 6.831; l2dist: 2.159\n",
      "    step: 100; loss: 3.241; l2dist: 1.614\n",
      "    step: 150; loss: 2.477; l2dist: 1.426\n",
      "    step: 200; loss: 2.238; l2dist: 1.368\n",
      "    step: 250; loss: 2.164; l2dist: 1.349\n",
      "    step: 300; loss: 2.107; l2dist: 1.328\n",
      "    step: 350; loss: 2.089; l2dist: 1.324\n",
      "    step: 400; loss: 2.069; l2dist: 1.318\n",
      "    step: 450; loss: 2.058; l2dist: 1.320\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.180; l2dist: 0.000\n",
      "    step: 50; loss: 6.751; l2dist: 2.132\n",
      "    step: 100; loss: 3.204; l2dist: 1.602\n",
      "    step: 150; loss: 2.461; l2dist: 1.416\n",
      "    step: 200; loss: 2.229; l2dist: 1.358\n",
      "    step: 250; loss: 2.136; l2dist: 1.336\n",
      "    step: 300; loss: 2.089; l2dist: 1.323\n",
      "    step: 350; loss: 2.083; l2dist: 1.317\n",
      "    step: 400; loss: 2.069; l2dist: 1.313\n",
      "    step: 450; loss: 2.051; l2dist: 1.316\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.384; l2dist: 0.000\n",
      "    step: 50; loss: 6.692; l2dist: 2.125\n",
      "    step: 100; loss: 3.201; l2dist: 1.608\n",
      "    step: 150; loss: 2.457; l2dist: 1.425\n",
      "    step: 200; loss: 2.224; l2dist: 1.368\n",
      "    step: 250; loss: 2.153; l2dist: 1.345\n",
      "    step: 300; loss: 2.095; l2dist: 1.331\n",
      "    step: 350; loss: 2.075; l2dist: 1.327\n",
      "    step: 400; loss: 2.076; l2dist: 1.331\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.678; l2dist: 0.000\n",
      "    step: 50; loss: 6.641; l2dist: 2.114\n",
      "    step: 100; loss: 3.195; l2dist: 1.598\n",
      "    step: 150; loss: 2.456; l2dist: 1.421\n",
      "    step: 200; loss: 2.232; l2dist: 1.369\n",
      "    step: 250; loss: 2.150; l2dist: 1.349\n",
      "    step: 300; loss: 2.102; l2dist: 1.335\n",
      "    step: 350; loss: 2.088; l2dist: 1.328\n",
      "    step: 400; loss: 2.074; l2dist: 1.325\n",
      "    step: 450; loss: 2.061; l2dist: 1.322\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.035; l2dist: 0.000\n",
      "    step: 50; loss: 6.679; l2dist: 2.124\n",
      "    step: 100; loss: 3.201; l2dist: 1.610\n",
      "    step: 150; loss: 2.459; l2dist: 1.424\n",
      "    step: 200; loss: 2.229; l2dist: 1.373\n",
      "    step: 250; loss: 2.152; l2dist: 1.347\n",
      "    step: 300; loss: 2.112; l2dist: 1.343\n",
      "    step: 350; loss: 2.083; l2dist: 1.333\n",
      "    step: 400; loss: 2.075; l2dist: 1.327\n",
      "    step: 450; loss: 2.063; l2dist: 1.323\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.065; l2dist: 0.000\n",
      "    step: 50; loss: 10.999; l2dist: 3.161\n",
      "    step: 100; loss: 4.832; l2dist: 2.104\n",
      "    step: 150; loss: 3.176; l2dist: 1.691\n",
      "    step: 200; loss: 2.602; l2dist: 1.518\n",
      "    step: 250; loss: 2.370; l2dist: 1.437\n",
      "    step: 300; loss: 2.260; l2dist: 1.408\n",
      "    step: 350; loss: 2.215; l2dist: 1.392\n",
      "    step: 400; loss: 2.167; l2dist: 1.369\n",
      "    step: 450; loss: 2.158; l2dist: 1.366\n",
      "binary step: 0; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.033; l2dist: 0.000\n",
      "    step: 50; loss: 8.147; l2dist: 2.634\n",
      "    step: 100; loss: 3.432; l2dist: 1.760\n",
      "    step: 150; loss: 2.479; l2dist: 1.483\n",
      "    step: 200; loss: 2.233; l2dist: 1.387\n",
      "    step: 250; loss: 2.102; l2dist: 1.351\n",
      "    step: 300; loss: 2.047; l2dist: 1.334\n",
      "    step: 350; loss: 2.018; l2dist: 1.324\n",
      "    step: 400; loss: 2.028; l2dist: 1.331\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.129; l2dist: 0.000\n",
      "    step: 50; loss: 6.822; l2dist: 2.345\n",
      "    step: 100; loss: 2.999; l2dist: 1.636\n",
      "    step: 150; loss: 2.246; l2dist: 1.401\n",
      "    step: 200; loss: 2.059; l2dist: 1.342\n",
      "    step: 250; loss: 1.981; l2dist: 1.314\n",
      "    step: 300; loss: 1.937; l2dist: 1.298\n",
      "    step: 350; loss: 1.931; l2dist: 1.288\n",
      "    step: 400; loss: 1.920; l2dist: 1.289\n",
      "    step: 450; loss: 1.913; l2dist: 1.291\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.942; l2dist: 0.000\n",
      "    step: 50; loss: 6.116; l2dist: 2.120\n",
      "    step: 100; loss: 2.910; l2dist: 1.594\n",
      "    step: 150; loss: 2.192; l2dist: 1.374\n",
      "    step: 200; loss: 2.004; l2dist: 1.316\n",
      "    step: 250; loss: 1.952; l2dist: 1.294\n",
      "    step: 300; loss: 1.917; l2dist: 1.279\n",
      "    step: 350; loss: 1.896; l2dist: 1.283\n",
      "    step: 400; loss: 1.895; l2dist: 1.277\n",
      "    step: 450; loss: 1.886; l2dist: 1.281\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 14.459; l2dist: 0.000\n",
      "    step: 50; loss: 5.512; l2dist: 1.887\n",
      "    step: 100; loss: 2.866; l2dist: 1.519\n",
      "    step: 150; loss: 2.145; l2dist: 1.338\n",
      "    step: 200; loss: 1.984; l2dist: 1.296\n",
      "    step: 250; loss: 1.915; l2dist: 1.274\n",
      "    step: 300; loss: 1.898; l2dist: 1.264\n",
      "    step: 350; loss: 1.875; l2dist: 1.263\n",
      "    step: 400; loss: 1.880; l2dist: 1.259\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 13.253; l2dist: 0.000\n",
      "    step: 50; loss: 5.326; l2dist: 1.792\n",
      "    step: 100; loss: 2.842; l2dist: 1.457\n",
      "    step: 150; loss: 2.136; l2dist: 1.294\n",
      "    step: 200; loss: 1.976; l2dist: 1.259\n",
      "    step: 250; loss: 1.911; l2dist: 1.248\n",
      "    step: 300; loss: 1.881; l2dist: 1.242\n",
      "    step: 350; loss: 1.871; l2dist: 1.243\n",
      "    step: 400; loss: 1.868; l2dist: 1.239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 450; loss: 1.853; l2dist: 1.235\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.691; l2dist: 0.000\n",
      "    step: 50; loss: 5.302; l2dist: 1.766\n",
      "    step: 100; loss: 2.844; l2dist: 1.452\n",
      "    step: 150; loss: 2.118; l2dist: 1.290\n",
      "    step: 200; loss: 1.964; l2dist: 1.254\n",
      "    step: 250; loss: 1.904; l2dist: 1.242\n",
      "    step: 300; loss: 1.876; l2dist: 1.228\n",
      "    step: 350; loss: 1.868; l2dist: 1.233\n",
      "    step: 400; loss: 1.857; l2dist: 1.227\n",
      "    step: 450; loss: 1.861; l2dist: 1.224\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.577; l2dist: 0.000\n",
      "    step: 50; loss: 5.285; l2dist: 1.754\n",
      "    step: 100; loss: 2.857; l2dist: 1.452\n",
      "    step: 150; loss: 2.125; l2dist: 1.289\n",
      "    step: 200; loss: 1.965; l2dist: 1.250\n",
      "    step: 250; loss: 1.919; l2dist: 1.232\n",
      "    step: 300; loss: 1.881; l2dist: 1.225\n",
      "    step: 350; loss: 1.875; l2dist: 1.228\n",
      "    step: 400; loss: 1.857; l2dist: 1.228\n",
      "    step: 450; loss: 1.863; l2dist: 1.220\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.563; l2dist: 0.000\n",
      "    step: 50; loss: 5.290; l2dist: 1.757\n",
      "    step: 100; loss: 2.862; l2dist: 1.454\n",
      "    step: 150; loss: 2.124; l2dist: 1.289\n",
      "    step: 200; loss: 1.971; l2dist: 1.248\n",
      "    step: 250; loss: 1.906; l2dist: 1.235\n",
      "    step: 300; loss: 1.886; l2dist: 1.233\n",
      "    step: 350; loss: 1.870; l2dist: 1.225\n",
      "    step: 400; loss: 1.858; l2dist: 1.221\n",
      "    step: 450; loss: 1.852; l2dist: 1.227\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.745; l2dist: 0.000\n",
      "    step: 50; loss: 5.329; l2dist: 1.772\n",
      "    step: 100; loss: 2.867; l2dist: 1.465\n",
      "    step: 150; loss: 2.128; l2dist: 1.296\n",
      "    step: 200; loss: 1.981; l2dist: 1.257\n",
      "    step: 250; loss: 1.918; l2dist: 1.239\n",
      "    step: 300; loss: 1.888; l2dist: 1.239\n",
      "    step: 350; loss: 1.874; l2dist: 1.232\n",
      "    step: 400; loss: 1.868; l2dist: 1.234\n",
      "    step: 450; loss: 1.862; l2dist: 1.232\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.754; l2dist: 0.000\n",
      "    step: 50; loss: 11.476; l2dist: 3.213\n",
      "    step: 100; loss: 4.934; l2dist: 2.133\n",
      "    step: 150; loss: 3.239; l2dist: 1.702\n",
      "    step: 200; loss: 2.621; l2dist: 1.525\n",
      "    step: 250; loss: 2.344; l2dist: 1.446\n",
      "    step: 300; loss: 2.244; l2dist: 1.400\n",
      "    step: 350; loss: 2.202; l2dist: 1.390\n",
      "    step: 400; loss: 2.104; l2dist: 1.361\n",
      "    step: 450; loss: 2.060; l2dist: 1.346\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 107.038; l2dist: 0.000\n",
      "    step: 50; loss: 10.254; l2dist: 2.961\n",
      "    step: 100; loss: 4.469; l2dist: 1.970\n",
      "    step: 150; loss: 2.995; l2dist: 1.605\n",
      "    step: 200; loss: 2.511; l2dist: 1.471\n",
      "    step: 250; loss: 2.256; l2dist: 1.397\n",
      "    step: 300; loss: 2.153; l2dist: 1.357\n",
      "    step: 350; loss: 2.048; l2dist: 1.336\n",
      "    step: 400; loss: 2.047; l2dist: 1.336\n",
      "    step: 450; loss: 2.007; l2dist: 1.323\n",
      "binary step: 1; number of successful adv: 99/100\n",
      "    step: 0; loss: 189.854; l2dist: 0.000\n",
      "    step: 50; loss: 11.655; l2dist: 3.166\n",
      "    step: 100; loss: 5.535; l2dist: 2.202\n",
      "    step: 150; loss: 3.592; l2dist: 1.750\n",
      "    step: 200; loss: 2.814; l2dist: 1.538\n",
      "    step: 250; loss: 2.479; l2dist: 1.445\n",
      "    step: 300; loss: 2.303; l2dist: 1.387\n",
      "    step: 350; loss: 2.196; l2dist: 1.357\n",
      "    step: 400; loss: 2.145; l2dist: 1.345\n",
      "    step: 450; loss: 2.094; l2dist: 1.330\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 108.801; l2dist: 0.000\n",
      "    step: 50; loss: 8.849; l2dist: 2.655\n",
      "    step: 100; loss: 3.969; l2dist: 1.849\n",
      "    step: 150; loss: 2.764; l2dist: 1.529\n",
      "    step: 200; loss: 2.376; l2dist: 1.410\n",
      "    step: 250; loss: 2.209; l2dist: 1.360\n",
      "    step: 300; loss: 2.129; l2dist: 1.334\n",
      "    step: 350; loss: 2.059; l2dist: 1.319\n",
      "    step: 400; loss: 2.032; l2dist: 1.312\n",
      "    step: 450; loss: 2.000; l2dist: 1.304\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.141; l2dist: 0.000\n",
      "    step: 50; loss: 7.002; l2dist: 2.273\n",
      "    step: 100; loss: 3.320; l2dist: 1.649\n",
      "    step: 150; loss: 2.452; l2dist: 1.420\n",
      "    step: 200; loss: 2.192; l2dist: 1.343\n",
      "    step: 250; loss: 2.066; l2dist: 1.312\n",
      "    step: 300; loss: 2.006; l2dist: 1.294\n",
      "    step: 350; loss: 1.967; l2dist: 1.286\n",
      "    step: 400; loss: 1.942; l2dist: 1.279\n",
      "    step: 450; loss: 1.914; l2dist: 1.277\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.834; l2dist: 0.000\n",
      "    step: 50; loss: 6.176; l2dist: 2.053\n",
      "    step: 100; loss: 3.021; l2dist: 1.525\n",
      "    step: 150; loss: 2.284; l2dist: 1.340\n",
      "    step: 200; loss: 2.058; l2dist: 1.290\n",
      "    step: 250; loss: 1.955; l2dist: 1.263\n",
      "    step: 300; loss: 1.908; l2dist: 1.250\n",
      "    step: 350; loss: 1.878; l2dist: 1.241\n",
      "    step: 400; loss: 1.851; l2dist: 1.239\n",
      "    step: 450; loss: 1.833; l2dist: 1.235\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.127; l2dist: 0.000\n",
      "    step: 50; loss: 5.934; l2dist: 1.976\n",
      "    step: 100; loss: 2.953; l2dist: 1.497\n",
      "    step: 150; loss: 2.231; l2dist: 1.321\n",
      "    step: 200; loss: 2.021; l2dist: 1.264\n",
      "    step: 250; loss: 1.933; l2dist: 1.242\n",
      "    step: 300; loss: 1.885; l2dist: 1.236\n",
      "    step: 350; loss: 1.862; l2dist: 1.232\n",
      "    step: 400; loss: 1.841; l2dist: 1.224\n",
      "    step: 450; loss: 1.843; l2dist: 1.225\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.616; l2dist: 0.000\n",
      "    step: 50; loss: 5.863; l2dist: 1.960\n",
      "    step: 100; loss: 2.932; l2dist: 1.491\n",
      "    step: 150; loss: 2.214; l2dist: 1.317\n",
      "    step: 200; loss: 2.013; l2dist: 1.269\n",
      "    step: 250; loss: 1.934; l2dist: 1.247\n",
      "    step: 300; loss: 1.901; l2dist: 1.240\n",
      "    step: 350; loss: 1.862; l2dist: 1.231\n",
      "    step: 400; loss: 1.852; l2dist: 1.235\n",
      "    step: 450; loss: 1.835; l2dist: 1.225\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.254; l2dist: 0.000\n",
      "    step: 50; loss: 5.791; l2dist: 1.939\n",
      "    step: 100; loss: 2.913; l2dist: 1.485\n",
      "    step: 150; loss: 2.201; l2dist: 1.315\n",
      "    step: 200; loss: 2.013; l2dist: 1.274\n",
      "    step: 250; loss: 1.923; l2dist: 1.249\n",
      "    step: 300; loss: 1.877; l2dist: 1.238\n",
      "    step: 350; loss: 1.867; l2dist: 1.237\n",
      "    step: 400; loss: 1.847; l2dist: 1.229\n",
      "    step: 450; loss: 1.830; l2dist: 1.228\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.539; l2dist: 0.000\n",
      "    step: 50; loss: 5.829; l2dist: 1.959\n",
      "    step: 100; loss: 2.925; l2dist: 1.495\n",
      "    step: 150; loss: 2.208; l2dist: 1.323\n",
      "    step: 200; loss: 2.018; l2dist: 1.273\n",
      "    step: 250; loss: 1.923; l2dist: 1.256\n",
      "    step: 300; loss: 1.891; l2dist: 1.243\n",
      "    step: 350; loss: 1.863; l2dist: 1.237\n",
      "    step: 400; loss: 1.845; l2dist: 1.236\n",
      "    step: 450; loss: 1.835; l2dist: 1.237\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 119.126; l2dist: 0.000\n",
      "    step: 50; loss: 12.950; l2dist: 3.267\n",
      "    step: 100; loss: 5.302; l2dist: 2.203\n",
      "    step: 150; loss: 3.508; l2dist: 1.776\n",
      "    step: 200; loss: 2.814; l2dist: 1.571\n",
      "    step: 250; loss: 2.520; l2dist: 1.491\n",
      "    step: 300; loss: 2.395; l2dist: 1.451\n",
      "    step: 350; loss: 2.370; l2dist: 1.436\n",
      "    step: 400; loss: 2.295; l2dist: 1.412\n",
      "    step: 450; loss: 2.256; l2dist: 1.407\n",
      "binary step: 0; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.563; l2dist: 0.000\n",
      "    step: 50; loss: 9.447; l2dist: 2.724\n",
      "    step: 100; loss: 3.721; l2dist: 1.831\n",
      "    step: 150; loss: 2.676; l2dist: 1.537\n",
      "    step: 200; loss: 2.357; l2dist: 1.436\n",
      "    step: 250; loss: 2.232; l2dist: 1.402\n",
      "    step: 300; loss: 2.153; l2dist: 1.376\n",
      "    step: 350; loss: 2.145; l2dist: 1.371\n",
      "    step: 400; loss: 2.141; l2dist: 1.360\n",
      "    step: 450; loss: 2.113; l2dist: 1.364\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.219; l2dist: 0.000\n",
      "    step: 50; loss: 8.084; l2dist: 2.466\n",
      "    step: 100; loss: 3.272; l2dist: 1.700\n",
      "    step: 150; loss: 2.436; l2dist: 1.462\n",
      "    step: 200; loss: 2.255; l2dist: 1.395\n",
      "    step: 250; loss: 2.140; l2dist: 1.369\n",
      "    step: 300; loss: 2.104; l2dist: 1.356\n",
      "    step: 350; loss: 2.091; l2dist: 1.352\n",
      "    step: 400; loss: 2.081; l2dist: 1.352\n",
      "    step: 450; loss: 2.057; l2dist: 1.339\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.824; l2dist: 0.000\n",
      "    step: 50; loss: 7.136; l2dist: 2.241\n",
      "    step: 100; loss: 3.065; l2dist: 1.638\n",
      "    step: 150; loss: 2.372; l2dist: 1.433\n",
      "    step: 200; loss: 2.164; l2dist: 1.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 250; loss: 2.105; l2dist: 1.353\n",
      "    step: 300; loss: 2.075; l2dist: 1.335\n",
      "    step: 350; loss: 2.046; l2dist: 1.335\n",
      "    step: 400; loss: 2.039; l2dist: 1.327\n",
      "    step: 450; loss: 2.023; l2dist: 1.322\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.349; l2dist: 0.000\n",
      "    step: 50; loss: 6.321; l2dist: 2.015\n",
      "    step: 100; loss: 3.054; l2dist: 1.603\n",
      "    step: 150; loss: 2.299; l2dist: 1.404\n",
      "    step: 200; loss: 2.117; l2dist: 1.348\n",
      "    step: 250; loss: 2.058; l2dist: 1.320\n",
      "    step: 300; loss: 2.025; l2dist: 1.311\n",
      "    step: 350; loss: 2.004; l2dist: 1.305\n",
      "    step: 400; loss: 1.999; l2dist: 1.310\n",
      "    step: 450; loss: 1.999; l2dist: 1.298\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.307; l2dist: 0.000\n",
      "    step: 50; loss: 6.178; l2dist: 1.936\n",
      "    step: 100; loss: 3.011; l2dist: 1.525\n",
      "    step: 150; loss: 2.289; l2dist: 1.358\n",
      "    step: 200; loss: 2.101; l2dist: 1.315\n",
      "    step: 250; loss: 2.041; l2dist: 1.297\n",
      "    step: 300; loss: 2.010; l2dist: 1.292\n",
      "    step: 350; loss: 1.996; l2dist: 1.282\n",
      "    step: 400; loss: 1.988; l2dist: 1.285\n",
      "    step: 450; loss: 1.981; l2dist: 1.285\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.763; l2dist: 0.000\n",
      "    step: 50; loss: 6.158; l2dist: 1.921\n",
      "    step: 100; loss: 3.007; l2dist: 1.529\n",
      "    step: 150; loss: 2.261; l2dist: 1.354\n",
      "    step: 200; loss: 2.098; l2dist: 1.316\n",
      "    step: 250; loss: 2.048; l2dist: 1.289\n",
      "    step: 300; loss: 2.013; l2dist: 1.288\n",
      "    step: 350; loss: 1.992; l2dist: 1.285\n",
      "    step: 400; loss: 1.981; l2dist: 1.284\n",
      "    step: 450; loss: 1.969; l2dist: 1.274\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.646; l2dist: 0.000\n",
      "    step: 50; loss: 6.194; l2dist: 1.923\n",
      "    step: 100; loss: 3.031; l2dist: 1.530\n",
      "    step: 150; loss: 2.279; l2dist: 1.362\n",
      "    step: 200; loss: 2.111; l2dist: 1.316\n",
      "    step: 250; loss: 2.050; l2dist: 1.299\n",
      "    step: 300; loss: 2.012; l2dist: 1.293\n",
      "    step: 350; loss: 1.995; l2dist: 1.291\n",
      "    step: 400; loss: 1.987; l2dist: 1.285\n",
      "    step: 450; loss: 1.982; l2dist: 1.287\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.598; l2dist: 0.000\n",
      "    step: 50; loss: 6.206; l2dist: 1.916\n",
      "    step: 100; loss: 3.026; l2dist: 1.526\n",
      "    step: 150; loss: 2.281; l2dist: 1.352\n",
      "    step: 200; loss: 2.120; l2dist: 1.326\n",
      "    step: 250; loss: 2.042; l2dist: 1.296\n",
      "    step: 300; loss: 2.014; l2dist: 1.292\n",
      "    step: 350; loss: 2.000; l2dist: 1.283\n",
      "    step: 400; loss: 1.993; l2dist: 1.283\n",
      "    step: 450; loss: 1.988; l2dist: 1.277\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.730; l2dist: 0.000\n",
      "    step: 50; loss: 6.240; l2dist: 1.924\n",
      "    step: 100; loss: 3.034; l2dist: 1.534\n",
      "    step: 150; loss: 2.296; l2dist: 1.363\n",
      "    step: 200; loss: 2.110; l2dist: 1.324\n",
      "    step: 250; loss: 2.059; l2dist: 1.303\n",
      "    step: 300; loss: 2.016; l2dist: 1.296\n",
      "    step: 350; loss: 2.009; l2dist: 1.294\n",
      "    step: 400; loss: 1.990; l2dist: 1.286\n",
      "    step: 450; loss: 1.994; l2dist: 1.284\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.362; l2dist: 0.000\n",
      "    step: 50; loss: 11.560; l2dist: 3.081\n",
      "    step: 100; loss: 4.912; l2dist: 2.113\n",
      "    step: 150; loss: 3.376; l2dist: 1.730\n",
      "    step: 200; loss: 2.680; l2dist: 1.525\n",
      "    step: 250; loss: 2.362; l2dist: 1.442\n",
      "    step: 300; loss: 2.223; l2dist: 1.389\n",
      "    step: 350; loss: 2.100; l2dist: 1.347\n",
      "    step: 400; loss: 2.069; l2dist: 1.346\n",
      "    step: 450; loss: 2.102; l2dist: 1.353\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 89.298; l2dist: 0.000\n",
      "    step: 50; loss: 9.843; l2dist: 2.787\n",
      "    step: 100; loss: 4.282; l2dist: 1.929\n",
      "    step: 150; loss: 2.999; l2dist: 1.607\n",
      "    step: 200; loss: 2.462; l2dist: 1.448\n",
      "    step: 250; loss: 2.229; l2dist: 1.381\n",
      "    step: 300; loss: 2.101; l2dist: 1.344\n",
      "    step: 350; loss: 2.038; l2dist: 1.325\n",
      "    step: 400; loss: 2.013; l2dist: 1.325\n",
      "    step: 450; loss: 2.007; l2dist: 1.317\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.287; l2dist: 0.000\n",
      "    step: 50; loss: 7.844; l2dist: 2.418\n",
      "    step: 100; loss: 3.422; l2dist: 1.723\n",
      "    step: 150; loss: 2.496; l2dist: 1.459\n",
      "    step: 200; loss: 2.188; l2dist: 1.371\n",
      "    step: 250; loss: 2.042; l2dist: 1.330\n",
      "    step: 300; loss: 1.998; l2dist: 1.304\n",
      "    step: 350; loss: 1.932; l2dist: 1.292\n",
      "    step: 400; loss: 1.920; l2dist: 1.288\n",
      "    step: 450; loss: 1.898; l2dist: 1.282\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.834; l2dist: 0.000\n",
      "    step: 50; loss: 7.068; l2dist: 2.235\n",
      "    step: 100; loss: 3.110; l2dist: 1.636\n",
      "    step: 150; loss: 2.306; l2dist: 1.410\n",
      "    step: 200; loss: 2.053; l2dist: 1.333\n",
      "    step: 250; loss: 1.971; l2dist: 1.295\n",
      "    step: 300; loss: 1.908; l2dist: 1.286\n",
      "    step: 350; loss: 1.893; l2dist: 1.281\n",
      "    step: 400; loss: 1.877; l2dist: 1.276\n",
      "    step: 450; loss: 1.872; l2dist: 1.273\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.022; l2dist: 0.000\n",
      "    step: 50; loss: 6.437; l2dist: 2.069\n",
      "    step: 100; loss: 2.997; l2dist: 1.580\n",
      "    step: 150; loss: 2.229; l2dist: 1.382\n",
      "    step: 200; loss: 2.013; l2dist: 1.312\n",
      "    step: 250; loss: 1.925; l2dist: 1.290\n",
      "    step: 300; loss: 1.895; l2dist: 1.271\n",
      "    step: 350; loss: 1.877; l2dist: 1.265\n",
      "    step: 400; loss: 1.856; l2dist: 1.257\n",
      "    step: 450; loss: 1.849; l2dist: 1.257\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.705; l2dist: 0.000\n",
      "    step: 50; loss: 6.004; l2dist: 1.938\n",
      "    step: 100; loss: 2.871; l2dist: 1.487\n",
      "    step: 150; loss: 2.176; l2dist: 1.329\n",
      "    step: 200; loss: 1.976; l2dist: 1.277\n",
      "    step: 250; loss: 1.899; l2dist: 1.257\n",
      "    step: 300; loss: 1.858; l2dist: 1.239\n",
      "    step: 350; loss: 1.834; l2dist: 1.237\n",
      "    step: 400; loss: 1.829; l2dist: 1.233\n",
      "    step: 450; loss: 1.828; l2dist: 1.234\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.069; l2dist: 0.000\n",
      "    step: 50; loss: 5.878; l2dist: 1.899\n",
      "    step: 100; loss: 2.849; l2dist: 1.466\n",
      "    step: 150; loss: 2.169; l2dist: 1.312\n",
      "    step: 200; loss: 1.967; l2dist: 1.261\n",
      "    step: 250; loss: 1.898; l2dist: 1.240\n",
      "    step: 300; loss: 1.868; l2dist: 1.230\n",
      "    step: 350; loss: 1.842; l2dist: 1.224\n",
      "    step: 400; loss: 1.834; l2dist: 1.227\n",
      "    step: 450; loss: 1.827; l2dist: 1.223\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.645; l2dist: 0.000\n",
      "    step: 50; loss: 5.827; l2dist: 1.884\n",
      "    step: 100; loss: 2.846; l2dist: 1.461\n",
      "    step: 150; loss: 2.159; l2dist: 1.300\n",
      "    step: 200; loss: 1.965; l2dist: 1.250\n",
      "    step: 250; loss: 1.894; l2dist: 1.235\n",
      "    step: 300; loss: 1.854; l2dist: 1.223\n",
      "    step: 350; loss: 1.838; l2dist: 1.222\n",
      "    step: 400; loss: 1.824; l2dist: 1.219\n",
      "    step: 450; loss: 1.824; l2dist: 1.217\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.234; l2dist: 0.000\n",
      "    step: 50; loss: 5.793; l2dist: 1.871\n",
      "    step: 100; loss: 2.843; l2dist: 1.463\n",
      "    step: 150; loss: 2.158; l2dist: 1.300\n",
      "    step: 200; loss: 1.961; l2dist: 1.253\n",
      "    step: 250; loss: 1.896; l2dist: 1.235\n",
      "    step: 300; loss: 1.860; l2dist: 1.231\n",
      "    step: 350; loss: 1.839; l2dist: 1.220\n",
      "    step: 400; loss: 1.829; l2dist: 1.218\n",
      "    step: 450; loss: 1.822; l2dist: 1.219\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.474; l2dist: 0.000\n",
      "    step: 50; loss: 5.832; l2dist: 1.885\n",
      "    step: 100; loss: 2.844; l2dist: 1.468\n",
      "    step: 150; loss: 2.166; l2dist: 1.315\n",
      "    step: 200; loss: 1.963; l2dist: 1.260\n",
      "    step: 250; loss: 1.894; l2dist: 1.242\n",
      "    step: 300; loss: 1.861; l2dist: 1.235\n",
      "    step: 350; loss: 1.845; l2dist: 1.231\n",
      "    step: 400; loss: 1.835; l2dist: 1.222\n",
      "    step: 450; loss: 1.835; l2dist: 1.225\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 117.666; l2dist: 0.000\n",
      "    step: 50; loss: 13.063; l2dist: 3.289\n",
      "    step: 100; loss: 5.493; l2dist: 2.249\n",
      "    step: 150; loss: 3.599; l2dist: 1.802\n",
      "    step: 200; loss: 2.930; l2dist: 1.611\n",
      "    step: 250; loss: 2.588; l2dist: 1.518\n",
      "    step: 300; loss: 2.458; l2dist: 1.462\n",
      "    step: 350; loss: 2.367; l2dist: 1.430\n",
      "    step: 400; loss: 2.284; l2dist: 1.419\n",
      "    step: 450; loss: 2.274; l2dist: 1.411\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 107.436; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 11.448; l2dist: 3.046\n",
      "    step: 100; loss: 4.760; l2dist: 2.047\n",
      "    step: 150; loss: 3.297; l2dist: 1.683\n",
      "    step: 200; loss: 2.727; l2dist: 1.531\n",
      "    step: 250; loss: 2.474; l2dist: 1.457\n",
      "    step: 300; loss: 2.329; l2dist: 1.421\n",
      "    step: 350; loss: 2.279; l2dist: 1.409\n",
      "    step: 400; loss: 2.207; l2dist: 1.383\n",
      "    step: 450; loss: 2.172; l2dist: 1.376\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.307; l2dist: 0.000\n",
      "    step: 50; loss: 8.909; l2dist: 2.616\n",
      "    step: 100; loss: 3.730; l2dist: 1.803\n",
      "    step: 150; loss: 2.728; l2dist: 1.536\n",
      "    step: 200; loss: 2.410; l2dist: 1.452\n",
      "    step: 250; loss: 2.257; l2dist: 1.398\n",
      "    step: 300; loss: 2.158; l2dist: 1.369\n",
      "    step: 350; loss: 2.115; l2dist: 1.359\n",
      "    step: 400; loss: 2.102; l2dist: 1.361\n",
      "    step: 450; loss: 2.101; l2dist: 1.357\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.309; l2dist: 0.000\n",
      "    step: 50; loss: 7.398; l2dist: 2.319\n",
      "    step: 100; loss: 3.382; l2dist: 1.715\n",
      "    step: 150; loss: 2.507; l2dist: 1.471\n",
      "    step: 200; loss: 2.264; l2dist: 1.397\n",
      "    step: 250; loss: 2.153; l2dist: 1.369\n",
      "    step: 300; loss: 2.099; l2dist: 1.353\n",
      "    step: 350; loss: 2.071; l2dist: 1.341\n",
      "    step: 400; loss: 2.069; l2dist: 1.341\n",
      "    step: 450; loss: 2.048; l2dist: 1.336\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.609; l2dist: 0.000\n",
      "    step: 50; loss: 6.322; l2dist: 2.041\n",
      "    step: 100; loss: 3.257; l2dist: 1.595\n",
      "    step: 150; loss: 2.435; l2dist: 1.420\n",
      "    step: 200; loss: 2.187; l2dist: 1.356\n",
      "    step: 250; loss: 2.104; l2dist: 1.334\n",
      "    step: 300; loss: 2.055; l2dist: 1.317\n",
      "    step: 350; loss: 2.042; l2dist: 1.315\n",
      "    step: 400; loss: 2.037; l2dist: 1.309\n",
      "    step: 450; loss: 2.022; l2dist: 1.308\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.961; l2dist: 0.000\n",
      "    step: 50; loss: 6.146; l2dist: 1.978\n",
      "    step: 100; loss: 3.147; l2dist: 1.539\n",
      "    step: 150; loss: 2.359; l2dist: 1.367\n",
      "    step: 200; loss: 2.136; l2dist: 1.312\n",
      "    step: 250; loss: 2.058; l2dist: 1.293\n",
      "    step: 300; loss: 2.017; l2dist: 1.281\n",
      "    step: 350; loss: 2.006; l2dist: 1.277\n",
      "    step: 400; loss: 2.002; l2dist: 1.277\n",
      "    step: 450; loss: 1.987; l2dist: 1.273\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.883; l2dist: 0.000\n",
      "    step: 50; loss: 6.124; l2dist: 1.964\n",
      "    step: 100; loss: 3.164; l2dist: 1.545\n",
      "    step: 150; loss: 2.370; l2dist: 1.384\n",
      "    step: 200; loss: 2.155; l2dist: 1.337\n",
      "    step: 250; loss: 2.080; l2dist: 1.311\n",
      "    step: 300; loss: 2.042; l2dist: 1.307\n",
      "    step: 350; loss: 2.027; l2dist: 1.293\n",
      "    step: 400; loss: 2.008; l2dist: 1.294\n",
      "    step: 450; loss: 1.995; l2dist: 1.291\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.737; l2dist: 0.000\n",
      "    step: 50; loss: 6.054; l2dist: 1.942\n",
      "    step: 100; loss: 3.143; l2dist: 1.538\n",
      "    step: 150; loss: 2.354; l2dist: 1.372\n",
      "    step: 200; loss: 2.144; l2dist: 1.327\n",
      "    step: 250; loss: 2.068; l2dist: 1.308\n",
      "    step: 300; loss: 2.029; l2dist: 1.297\n",
      "    step: 350; loss: 2.012; l2dist: 1.294\n",
      "    step: 400; loss: 2.009; l2dist: 1.294\n",
      "    step: 450; loss: 2.005; l2dist: 1.291\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.323; l2dist: 0.000\n",
      "    step: 50; loss: 6.043; l2dist: 1.937\n",
      "    step: 100; loss: 3.145; l2dist: 1.540\n",
      "    step: 150; loss: 2.350; l2dist: 1.375\n",
      "    step: 200; loss: 2.141; l2dist: 1.329\n",
      "    step: 250; loss: 2.066; l2dist: 1.309\n",
      "    step: 300; loss: 2.038; l2dist: 1.295\n",
      "    step: 350; loss: 2.017; l2dist: 1.301\n",
      "    step: 400; loss: 2.007; l2dist: 1.299\n",
      "    step: 450; loss: 2.001; l2dist: 1.291\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.610; l2dist: 0.000\n",
      "    step: 50; loss: 6.073; l2dist: 1.953\n",
      "    step: 100; loss: 3.159; l2dist: 1.549\n",
      "    step: 150; loss: 2.354; l2dist: 1.387\n",
      "    step: 200; loss: 2.146; l2dist: 1.334\n",
      "    step: 250; loss: 2.084; l2dist: 1.319\n",
      "    step: 300; loss: 2.038; l2dist: 1.307\n",
      "    step: 350; loss: 2.028; l2dist: 1.302\n",
      "    step: 400; loss: 2.007; l2dist: 1.298\n",
      "    step: 450; loss: 2.005; l2dist: 1.300\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.306; l2dist: 0.000\n",
      "    step: 50; loss: 11.871; l2dist: 3.244\n",
      "    step: 100; loss: 5.147; l2dist: 2.195\n",
      "    step: 150; loss: 3.338; l2dist: 1.756\n",
      "    step: 200; loss: 2.686; l2dist: 1.565\n",
      "    step: 250; loss: 2.425; l2dist: 1.473\n",
      "    step: 300; loss: 2.305; l2dist: 1.438\n",
      "    step: 350; loss: 2.225; l2dist: 1.406\n",
      "    step: 400; loss: 2.199; l2dist: 1.385\n",
      "    step: 450; loss: 2.123; l2dist: 1.383\n",
      "binary step: 0; number of successful adv: 90/100\n",
      "    step: 0; loss: 167.281; l2dist: 0.000\n",
      "    step: 50; loss: 12.880; l2dist: 3.354\n",
      "    step: 100; loss: 6.169; l2dist: 2.340\n",
      "    step: 150; loss: 4.124; l2dist: 1.893\n",
      "    step: 200; loss: 3.235; l2dist: 1.678\n",
      "    step: 250; loss: 2.768; l2dist: 1.555\n",
      "    step: 300; loss: 2.520; l2dist: 1.491\n",
      "    step: 350; loss: 2.328; l2dist: 1.439\n",
      "    step: 400; loss: 2.265; l2dist: 1.419\n",
      "    step: 450; loss: 2.241; l2dist: 1.415\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.844; l2dist: 0.000\n",
      "    step: 50; loss: 9.826; l2dist: 2.882\n",
      "    step: 100; loss: 4.418; l2dist: 1.977\n",
      "    step: 150; loss: 3.054; l2dist: 1.637\n",
      "    step: 200; loss: 2.550; l2dist: 1.501\n",
      "    step: 250; loss: 2.313; l2dist: 1.434\n",
      "    step: 300; loss: 2.192; l2dist: 1.392\n",
      "    step: 350; loss: 2.135; l2dist: 1.372\n",
      "    step: 400; loss: 2.098; l2dist: 1.366\n",
      "    step: 450; loss: 2.041; l2dist: 1.347\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 57.361; l2dist: 0.000\n",
      "    step: 50; loss: 8.066; l2dist: 2.540\n",
      "    step: 100; loss: 3.581; l2dist: 1.777\n",
      "    step: 150; loss: 2.559; l2dist: 1.508\n",
      "    step: 200; loss: 2.231; l2dist: 1.412\n",
      "    step: 250; loss: 2.113; l2dist: 1.370\n",
      "    step: 300; loss: 2.024; l2dist: 1.342\n",
      "    step: 350; loss: 2.007; l2dist: 1.337\n",
      "    step: 400; loss: 1.973; l2dist: 1.327\n",
      "    step: 450; loss: 1.964; l2dist: 1.327\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.265; l2dist: 0.000\n",
      "    step: 50; loss: 6.948; l2dist: 2.239\n",
      "    step: 100; loss: 3.267; l2dist: 1.647\n",
      "    step: 150; loss: 2.391; l2dist: 1.432\n",
      "    step: 200; loss: 2.130; l2dist: 1.359\n",
      "    step: 250; loss: 2.030; l2dist: 1.329\n",
      "    step: 300; loss: 1.983; l2dist: 1.312\n",
      "    step: 350; loss: 2.018; l2dist: 1.315\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.568; l2dist: 0.000\n",
      "    step: 50; loss: 6.421; l2dist: 2.132\n",
      "    step: 100; loss: 3.124; l2dist: 1.578\n",
      "    step: 150; loss: 2.305; l2dist: 1.385\n",
      "    step: 200; loss: 2.083; l2dist: 1.324\n",
      "    step: 250; loss: 1.995; l2dist: 1.300\n",
      "    step: 300; loss: 1.958; l2dist: 1.292\n",
      "    step: 350; loss: 1.961; l2dist: 1.287\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.510; l2dist: 0.000\n",
      "    step: 50; loss: 6.295; l2dist: 2.084\n",
      "    step: 100; loss: 3.067; l2dist: 1.553\n",
      "    step: 150; loss: 2.256; l2dist: 1.365\n",
      "    step: 200; loss: 2.047; l2dist: 1.307\n",
      "    step: 250; loss: 1.975; l2dist: 1.285\n",
      "    step: 300; loss: 1.939; l2dist: 1.279\n",
      "    step: 350; loss: 1.923; l2dist: 1.273\n",
      "    step: 400; loss: 1.912; l2dist: 1.270\n",
      "    step: 450; loss: 1.898; l2dist: 1.270\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.719; l2dist: 0.000\n",
      "    step: 50; loss: 6.217; l2dist: 2.072\n",
      "    step: 100; loss: 3.073; l2dist: 1.560\n",
      "    step: 150; loss: 2.256; l2dist: 1.362\n",
      "    step: 200; loss: 2.060; l2dist: 1.307\n",
      "    step: 250; loss: 1.973; l2dist: 1.289\n",
      "    step: 300; loss: 1.941; l2dist: 1.281\n",
      "    step: 350; loss: 1.923; l2dist: 1.275\n",
      "    step: 400; loss: 1.913; l2dist: 1.276\n",
      "    step: 450; loss: 1.918; l2dist: 1.278\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.730; l2dist: 0.000\n",
      "    step: 50; loss: 6.144; l2dist: 2.051\n",
      "    step: 100; loss: 3.055; l2dist: 1.548\n",
      "    step: 150; loss: 2.247; l2dist: 1.365\n",
      "    step: 200; loss: 2.043; l2dist: 1.303\n",
      "    step: 250; loss: 1.974; l2dist: 1.287\n",
      "    step: 300; loss: 1.948; l2dist: 1.277\n",
      "    step: 350; loss: 1.923; l2dist: 1.271\n",
      "    step: 400; loss: 1.895; l2dist: 1.264\n",
      "    step: 450; loss: 1.902; l2dist: 1.265\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.885; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 6.174; l2dist: 2.060\n",
      "    step: 100; loss: 3.069; l2dist: 1.554\n",
      "    step: 150; loss: 2.239; l2dist: 1.358\n",
      "    step: 200; loss: 2.052; l2dist: 1.305\n",
      "    step: 250; loss: 1.961; l2dist: 1.289\n",
      "    step: 300; loss: 1.937; l2dist: 1.277\n",
      "    step: 350; loss: 1.906; l2dist: 1.270\n",
      "    step: 400; loss: 1.906; l2dist: 1.270\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 117.452; l2dist: 0.000\n",
      "    step: 50; loss: 12.977; l2dist: 3.298\n",
      "    step: 100; loss: 5.412; l2dist: 2.241\n",
      "    step: 150; loss: 3.495; l2dist: 1.776\n",
      "    step: 200; loss: 2.787; l2dist: 1.568\n",
      "    step: 250; loss: 2.454; l2dist: 1.468\n",
      "    step: 300; loss: 2.291; l2dist: 1.413\n",
      "    step: 350; loss: 2.247; l2dist: 1.384\n",
      "    step: 400; loss: 2.218; l2dist: 1.372\n",
      "    step: 450; loss: 2.183; l2dist: 1.363\n",
      "binary step: 0; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.726; l2dist: 0.000\n",
      "    step: 50; loss: 9.684; l2dist: 2.765\n",
      "    step: 100; loss: 3.628; l2dist: 1.812\n",
      "    step: 150; loss: 2.589; l2dist: 1.506\n",
      "    step: 200; loss: 2.267; l2dist: 1.412\n",
      "    step: 250; loss: 2.128; l2dist: 1.358\n",
      "    step: 300; loss: 2.106; l2dist: 1.339\n",
      "    step: 350; loss: 2.069; l2dist: 1.328\n",
      "    step: 400; loss: 2.025; l2dist: 1.313\n",
      "    step: 450; loss: 2.007; l2dist: 1.318\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.663; l2dist: 0.000\n",
      "    step: 50; loss: 7.845; l2dist: 2.432\n",
      "    step: 100; loss: 3.086; l2dist: 1.655\n",
      "    step: 150; loss: 2.330; l2dist: 1.429\n",
      "    step: 200; loss: 2.107; l2dist: 1.354\n",
      "    step: 250; loss: 2.024; l2dist: 1.326\n",
      "    step: 300; loss: 1.983; l2dist: 1.304\n",
      "    step: 350; loss: 1.979; l2dist: 1.302\n",
      "    step: 400; loss: 1.958; l2dist: 1.302\n",
      "    step: 450; loss: 1.949; l2dist: 1.298\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.665; l2dist: 0.000\n",
      "    step: 50; loss: 6.961; l2dist: 2.207\n",
      "    step: 100; loss: 3.013; l2dist: 1.608\n",
      "    step: 150; loss: 2.270; l2dist: 1.399\n",
      "    step: 200; loss: 2.067; l2dist: 1.329\n",
      "    step: 250; loss: 2.001; l2dist: 1.302\n",
      "    step: 300; loss: 1.955; l2dist: 1.296\n",
      "    step: 350; loss: 1.938; l2dist: 1.286\n",
      "    step: 400; loss: 1.925; l2dist: 1.288\n",
      "    step: 450; loss: 1.926; l2dist: 1.283\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.364; l2dist: 0.000\n",
      "    step: 50; loss: 6.237; l2dist: 1.986\n",
      "    step: 100; loss: 2.985; l2dist: 1.548\n",
      "    step: 150; loss: 2.243; l2dist: 1.373\n",
      "    step: 200; loss: 2.049; l2dist: 1.311\n",
      "    step: 250; loss: 1.968; l2dist: 1.295\n",
      "    step: 300; loss: 1.941; l2dist: 1.280\n",
      "    step: 350; loss: 1.921; l2dist: 1.274\n",
      "    step: 400; loss: 1.916; l2dist: 1.276\n",
      "    step: 450; loss: 1.914; l2dist: 1.272\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.038; l2dist: 0.000\n",
      "    step: 50; loss: 6.024; l2dist: 1.903\n",
      "    step: 100; loss: 2.950; l2dist: 1.493\n",
      "    step: 150; loss: 2.227; l2dist: 1.338\n",
      "    step: 200; loss: 2.032; l2dist: 1.294\n",
      "    step: 250; loss: 1.957; l2dist: 1.273\n",
      "    step: 300; loss: 1.925; l2dist: 1.269\n",
      "    step: 350; loss: 1.910; l2dist: 1.259\n",
      "    step: 400; loss: 1.898; l2dist: 1.259\n",
      "    step: 450; loss: 1.896; l2dist: 1.255\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.453; l2dist: 0.000\n",
      "    step: 50; loss: 5.925; l2dist: 1.862\n",
      "    step: 100; loss: 2.943; l2dist: 1.478\n",
      "    step: 150; loss: 2.217; l2dist: 1.321\n",
      "    step: 200; loss: 2.023; l2dist: 1.273\n",
      "    step: 250; loss: 1.953; l2dist: 1.258\n",
      "    step: 300; loss: 1.919; l2dist: 1.250\n",
      "    step: 350; loss: 1.912; l2dist: 1.244\n",
      "    step: 400; loss: 1.896; l2dist: 1.247\n",
      "    step: 450; loss: 1.893; l2dist: 1.240\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.310; l2dist: 0.000\n",
      "    step: 50; loss: 5.894; l2dist: 1.851\n",
      "    step: 100; loss: 2.975; l2dist: 1.471\n",
      "    step: 150; loss: 2.242; l2dist: 1.329\n",
      "    step: 200; loss: 2.047; l2dist: 1.284\n",
      "    step: 250; loss: 1.976; l2dist: 1.269\n",
      "    step: 300; loss: 1.941; l2dist: 1.263\n",
      "    step: 350; loss: 1.930; l2dist: 1.254\n",
      "    step: 400; loss: 1.914; l2dist: 1.252\n",
      "    step: 450; loss: 1.908; l2dist: 1.254\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.175; l2dist: 0.000\n",
      "    step: 50; loss: 5.865; l2dist: 1.842\n",
      "    step: 100; loss: 2.971; l2dist: 1.471\n",
      "    step: 150; loss: 2.244; l2dist: 1.335\n",
      "    step: 200; loss: 2.041; l2dist: 1.283\n",
      "    step: 250; loss: 1.978; l2dist: 1.270\n",
      "    step: 300; loss: 1.940; l2dist: 1.261\n",
      "    step: 350; loss: 1.926; l2dist: 1.255\n",
      "    step: 400; loss: 1.918; l2dist: 1.250\n",
      "    step: 450; loss: 1.910; l2dist: 1.251\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.347; l2dist: 0.000\n",
      "    step: 50; loss: 5.910; l2dist: 1.853\n",
      "    step: 100; loss: 2.968; l2dist: 1.482\n",
      "    step: 150; loss: 2.231; l2dist: 1.335\n",
      "    step: 200; loss: 2.033; l2dist: 1.286\n",
      "    step: 250; loss: 1.971; l2dist: 1.267\n",
      "    step: 300; loss: 1.932; l2dist: 1.267\n",
      "    step: 350; loss: 1.922; l2dist: 1.258\n",
      "    step: 400; loss: 1.911; l2dist: 1.252\n",
      "    step: 450; loss: 1.902; l2dist: 1.257\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.477; l2dist: 0.000\n",
      "    step: 50; loss: 10.442; l2dist: 3.070\n",
      "    step: 100; loss: 4.705; l2dist: 2.077\n",
      "    step: 150; loss: 3.088; l2dist: 1.668\n",
      "    step: 200; loss: 2.575; l2dist: 1.515\n",
      "    step: 250; loss: 2.271; l2dist: 1.407\n",
      "    step: 300; loss: 2.130; l2dist: 1.362\n",
      "    step: 350; loss: 2.024; l2dist: 1.327\n",
      "    step: 400; loss: 2.020; l2dist: 1.334\n",
      "    step: 450; loss: 2.013; l2dist: 1.320\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 119.536; l2dist: 0.000\n",
      "    step: 50; loss: 10.269; l2dist: 2.977\n",
      "    step: 100; loss: 4.791; l2dist: 2.041\n",
      "    step: 150; loss: 3.214; l2dist: 1.649\n",
      "    step: 200; loss: 2.604; l2dist: 1.485\n",
      "    step: 250; loss: 2.288; l2dist: 1.394\n",
      "    step: 300; loss: 2.159; l2dist: 1.361\n",
      "    step: 350; loss: 2.037; l2dist: 1.326\n",
      "    step: 400; loss: 2.025; l2dist: 1.314\n",
      "    step: 450; loss: 2.012; l2dist: 1.311\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.867; l2dist: 0.000\n",
      "    step: 50; loss: 7.919; l2dist: 2.538\n",
      "    step: 100; loss: 3.491; l2dist: 1.742\n",
      "    step: 150; loss: 2.480; l2dist: 1.462\n",
      "    step: 200; loss: 2.144; l2dist: 1.365\n",
      "    step: 250; loss: 1.992; l2dist: 1.313\n",
      "    step: 300; loss: 1.921; l2dist: 1.293\n",
      "    step: 350; loss: 1.892; l2dist: 1.281\n",
      "    step: 400; loss: 1.865; l2dist: 1.274\n",
      "    step: 450; loss: 1.881; l2dist: 1.275\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.548; l2dist: 0.000\n",
      "    step: 50; loss: 6.685; l2dist: 2.225\n",
      "    step: 100; loss: 3.046; l2dist: 1.616\n",
      "    step: 150; loss: 2.255; l2dist: 1.393\n",
      "    step: 200; loss: 2.000; l2dist: 1.314\n",
      "    step: 250; loss: 1.907; l2dist: 1.283\n",
      "    step: 300; loss: 1.862; l2dist: 1.266\n",
      "    step: 350; loss: 1.823; l2dist: 1.258\n",
      "    step: 400; loss: 1.818; l2dist: 1.251\n",
      "    step: 450; loss: 1.803; l2dist: 1.252\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.511; l2dist: 0.000\n",
      "    step: 50; loss: 6.017; l2dist: 2.053\n",
      "    step: 100; loss: 2.876; l2dist: 1.540\n",
      "    step: 150; loss: 2.148; l2dist: 1.351\n",
      "    step: 200; loss: 1.938; l2dist: 1.290\n",
      "    step: 250; loss: 1.858; l2dist: 1.260\n",
      "    step: 300; loss: 1.832; l2dist: 1.246\n",
      "    step: 350; loss: 1.809; l2dist: 1.239\n",
      "    step: 400; loss: 1.791; l2dist: 1.240\n",
      "    step: 450; loss: 1.774; l2dist: 1.229\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.648; l2dist: 0.000\n",
      "    step: 50; loss: 5.643; l2dist: 1.935\n",
      "    step: 100; loss: 2.757; l2dist: 1.476\n",
      "    step: 150; loss: 2.082; l2dist: 1.297\n",
      "    step: 200; loss: 1.904; l2dist: 1.245\n",
      "    step: 250; loss: 1.827; l2dist: 1.221\n",
      "    step: 300; loss: 1.786; l2dist: 1.216\n",
      "    step: 350; loss: 1.770; l2dist: 1.209\n",
      "    step: 400; loss: 1.763; l2dist: 1.205\n",
      "    step: 450; loss: 1.757; l2dist: 1.209\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.872; l2dist: 0.000\n",
      "    step: 50; loss: 5.474; l2dist: 1.876\n",
      "    step: 100; loss: 2.748; l2dist: 1.456\n",
      "    step: 150; loss: 2.069; l2dist: 1.276\n",
      "    step: 200; loss: 1.903; l2dist: 1.238\n",
      "    step: 250; loss: 1.835; l2dist: 1.214\n",
      "    step: 300; loss: 1.805; l2dist: 1.211\n",
      "    step: 350; loss: 1.768; l2dist: 1.201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 1.776; l2dist: 1.204\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.796; l2dist: 0.000\n",
      "    step: 50; loss: 5.441; l2dist: 1.869\n",
      "    step: 100; loss: 2.751; l2dist: 1.454\n",
      "    step: 150; loss: 2.059; l2dist: 1.280\n",
      "    step: 200; loss: 1.889; l2dist: 1.230\n",
      "    step: 250; loss: 1.825; l2dist: 1.220\n",
      "    step: 300; loss: 1.792; l2dist: 1.204\n",
      "    step: 350; loss: 1.763; l2dist: 1.206\n",
      "    step: 400; loss: 1.755; l2dist: 1.203\n",
      "    step: 450; loss: 1.741; l2dist: 1.199\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.366; l2dist: 0.000\n",
      "    step: 50; loss: 5.421; l2dist: 1.863\n",
      "    step: 100; loss: 2.754; l2dist: 1.456\n",
      "    step: 150; loss: 2.064; l2dist: 1.275\n",
      "    step: 200; loss: 1.883; l2dist: 1.237\n",
      "    step: 250; loss: 1.824; l2dist: 1.221\n",
      "    step: 300; loss: 1.783; l2dist: 1.212\n",
      "    step: 350; loss: 1.765; l2dist: 1.205\n",
      "    step: 400; loss: 1.752; l2dist: 1.202\n",
      "    step: 450; loss: 1.754; l2dist: 1.204\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.510; l2dist: 0.000\n",
      "    step: 50; loss: 5.450; l2dist: 1.875\n",
      "    step: 100; loss: 2.757; l2dist: 1.460\n",
      "    step: 150; loss: 2.062; l2dist: 1.288\n",
      "    step: 200; loss: 1.896; l2dist: 1.239\n",
      "    step: 250; loss: 1.828; l2dist: 1.225\n",
      "    step: 300; loss: 1.794; l2dist: 1.227\n",
      "    step: 350; loss: 1.769; l2dist: 1.215\n",
      "    step: 400; loss: 1.757; l2dist: 1.211\n",
      "    step: 450; loss: 1.757; l2dist: 1.209\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.821; l2dist: 0.000\n",
      "    step: 50; loss: 12.324; l2dist: 3.190\n",
      "    step: 100; loss: 5.096; l2dist: 2.135\n",
      "    step: 150; loss: 3.385; l2dist: 1.723\n",
      "    step: 200; loss: 2.737; l2dist: 1.539\n",
      "    step: 250; loss: 2.463; l2dist: 1.453\n",
      "    step: 300; loss: 2.293; l2dist: 1.416\n",
      "    step: 350; loss: 2.257; l2dist: 1.382\n",
      "    step: 400; loss: 2.226; l2dist: 1.387\n",
      "    step: 450; loss: 2.206; l2dist: 1.372\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 76.534; l2dist: 0.000\n",
      "    step: 50; loss: 10.092; l2dist: 2.800\n",
      "    step: 100; loss: 4.052; l2dist: 1.874\n",
      "    step: 150; loss: 2.830; l2dist: 1.560\n",
      "    step: 200; loss: 2.448; l2dist: 1.449\n",
      "    step: 250; loss: 2.238; l2dist: 1.389\n",
      "    step: 300; loss: 2.163; l2dist: 1.366\n",
      "    step: 350; loss: 2.094; l2dist: 1.343\n",
      "    step: 400; loss: 2.120; l2dist: 1.334\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.712; l2dist: 0.000\n",
      "    step: 50; loss: 8.274; l2dist: 2.446\n",
      "    step: 100; loss: 3.426; l2dist: 1.710\n",
      "    step: 150; loss: 2.533; l2dist: 1.474\n",
      "    step: 200; loss: 2.264; l2dist: 1.381\n",
      "    step: 250; loss: 2.116; l2dist: 1.347\n",
      "    step: 300; loss: 2.067; l2dist: 1.332\n",
      "    step: 350; loss: 2.068; l2dist: 1.323\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.361; l2dist: 0.000\n",
      "    step: 50; loss: 7.353; l2dist: 2.234\n",
      "    step: 100; loss: 3.163; l2dist: 1.645\n",
      "    step: 150; loss: 2.360; l2dist: 1.421\n",
      "    step: 200; loss: 2.167; l2dist: 1.361\n",
      "    step: 250; loss: 2.080; l2dist: 1.325\n",
      "    step: 300; loss: 2.039; l2dist: 1.318\n",
      "    step: 350; loss: 2.023; l2dist: 1.309\n",
      "    step: 400; loss: 2.002; l2dist: 1.305\n",
      "    step: 450; loss: 1.993; l2dist: 1.303\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.657; l2dist: 0.000\n",
      "    step: 50; loss: 6.582; l2dist: 2.052\n",
      "    step: 100; loss: 3.112; l2dist: 1.591\n",
      "    step: 150; loss: 2.332; l2dist: 1.401\n",
      "    step: 200; loss: 2.135; l2dist: 1.337\n",
      "    step: 250; loss: 2.063; l2dist: 1.311\n",
      "    step: 300; loss: 2.024; l2dist: 1.307\n",
      "    step: 350; loss: 1.998; l2dist: 1.294\n",
      "    step: 400; loss: 1.976; l2dist: 1.290\n",
      "    step: 450; loss: 1.980; l2dist: 1.288\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.398; l2dist: 0.000\n",
      "    step: 50; loss: 6.342; l2dist: 1.949\n",
      "    step: 100; loss: 3.037; l2dist: 1.518\n",
      "    step: 150; loss: 2.281; l2dist: 1.349\n",
      "    step: 200; loss: 2.088; l2dist: 1.299\n",
      "    step: 250; loss: 2.028; l2dist: 1.283\n",
      "    step: 300; loss: 1.992; l2dist: 1.271\n",
      "    step: 350; loss: 1.965; l2dist: 1.265\n",
      "    step: 400; loss: 1.971; l2dist: 1.266\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.799; l2dist: 0.000\n",
      "    step: 50; loss: 6.280; l2dist: 1.933\n",
      "    step: 100; loss: 3.029; l2dist: 1.515\n",
      "    step: 150; loss: 2.265; l2dist: 1.334\n",
      "    step: 200; loss: 2.089; l2dist: 1.288\n",
      "    step: 250; loss: 2.017; l2dist: 1.266\n",
      "    step: 300; loss: 1.999; l2dist: 1.267\n",
      "    step: 350; loss: 1.979; l2dist: 1.254\n",
      "    step: 400; loss: 1.955; l2dist: 1.256\n",
      "    step: 450; loss: 1.951; l2dist: 1.249\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.206; l2dist: 0.000\n",
      "    step: 50; loss: 6.223; l2dist: 1.915\n",
      "    step: 100; loss: 3.034; l2dist: 1.505\n",
      "    step: 150; loss: 2.264; l2dist: 1.326\n",
      "    step: 200; loss: 2.091; l2dist: 1.275\n",
      "    step: 250; loss: 2.009; l2dist: 1.258\n",
      "    step: 300; loss: 1.980; l2dist: 1.256\n",
      "    step: 350; loss: 1.964; l2dist: 1.250\n",
      "    step: 400; loss: 1.958; l2dist: 1.247\n",
      "    step: 450; loss: 1.950; l2dist: 1.246\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.003; l2dist: 0.000\n",
      "    step: 50; loss: 6.214; l2dist: 1.914\n",
      "    step: 100; loss: 3.035; l2dist: 1.504\n",
      "    step: 150; loss: 2.266; l2dist: 1.327\n",
      "    step: 200; loss: 2.100; l2dist: 1.284\n",
      "    step: 250; loss: 2.025; l2dist: 1.267\n",
      "    step: 300; loss: 1.987; l2dist: 1.259\n",
      "    step: 350; loss: 1.965; l2dist: 1.255\n",
      "    step: 400; loss: 1.964; l2dist: 1.248\n",
      "    step: 450; loss: 1.966; l2dist: 1.252\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.183; l2dist: 0.000\n",
      "    step: 50; loss: 6.259; l2dist: 1.925\n",
      "    step: 100; loss: 3.036; l2dist: 1.512\n",
      "    step: 150; loss: 2.275; l2dist: 1.336\n",
      "    step: 200; loss: 2.101; l2dist: 1.295\n",
      "    step: 250; loss: 2.018; l2dist: 1.276\n",
      "    step: 300; loss: 1.988; l2dist: 1.269\n",
      "    step: 350; loss: 1.974; l2dist: 1.262\n",
      "    step: 400; loss: 1.964; l2dist: 1.262\n",
      "    step: 450; loss: 1.959; l2dist: 1.258\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 112.916; l2dist: 0.000\n",
      "    step: 50; loss: 12.580; l2dist: 3.221\n",
      "    step: 100; loss: 5.263; l2dist: 2.186\n",
      "    step: 150; loss: 3.544; l2dist: 1.783\n",
      "    step: 200; loss: 2.932; l2dist: 1.600\n",
      "    step: 250; loss: 2.653; l2dist: 1.530\n",
      "    step: 300; loss: 2.513; l2dist: 1.475\n",
      "    step: 350; loss: 2.397; l2dist: 1.452\n",
      "    step: 400; loss: 2.384; l2dist: 1.440\n",
      "    step: 450; loss: 2.302; l2dist: 1.414\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 88.914; l2dist: 0.000\n",
      "    step: 50; loss: 10.641; l2dist: 2.883\n",
      "    step: 100; loss: 4.429; l2dist: 1.969\n",
      "    step: 150; loss: 3.124; l2dist: 1.641\n",
      "    step: 200; loss: 2.647; l2dist: 1.514\n",
      "    step: 250; loss: 2.445; l2dist: 1.455\n",
      "    step: 300; loss: 2.370; l2dist: 1.420\n",
      "    step: 350; loss: 2.272; l2dist: 1.408\n",
      "    step: 400; loss: 2.226; l2dist: 1.391\n",
      "    step: 450; loss: 2.213; l2dist: 1.383\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.455; l2dist: 0.000\n",
      "    step: 50; loss: 8.012; l2dist: 2.454\n",
      "    step: 100; loss: 3.476; l2dist: 1.748\n",
      "    step: 150; loss: 2.588; l2dist: 1.503\n",
      "    step: 200; loss: 2.341; l2dist: 1.424\n",
      "    step: 250; loss: 2.202; l2dist: 1.382\n",
      "    step: 300; loss: 2.169; l2dist: 1.374\n",
      "    step: 350; loss: 2.143; l2dist: 1.371\n",
      "    step: 400; loss: 2.131; l2dist: 1.361\n",
      "    step: 450; loss: 2.086; l2dist: 1.353\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.153; l2dist: 0.000\n",
      "    step: 50; loss: 6.915; l2dist: 2.218\n",
      "    step: 100; loss: 3.267; l2dist: 1.673\n",
      "    step: 150; loss: 2.447; l2dist: 1.454\n",
      "    step: 200; loss: 2.250; l2dist: 1.398\n",
      "    step: 250; loss: 2.145; l2dist: 1.358\n",
      "    step: 300; loss: 2.122; l2dist: 1.348\n",
      "    step: 350; loss: 2.082; l2dist: 1.350\n",
      "    step: 400; loss: 2.062; l2dist: 1.333\n",
      "    step: 450; loss: 2.067; l2dist: 1.338\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.230; l2dist: 0.000\n",
      "    step: 50; loss: 6.195; l2dist: 2.027\n",
      "    step: 100; loss: 3.180; l2dist: 1.582\n",
      "    step: 150; loss: 2.395; l2dist: 1.405\n",
      "    step: 200; loss: 2.196; l2dist: 1.360\n",
      "    step: 250; loss: 2.105; l2dist: 1.335\n",
      "    step: 300; loss: 2.073; l2dist: 1.321\n",
      "    step: 350; loss: 2.037; l2dist: 1.318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 2.051; l2dist: 1.316\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.509; l2dist: 0.000\n",
      "    step: 50; loss: 5.817; l2dist: 1.902\n",
      "    step: 100; loss: 3.051; l2dist: 1.507\n",
      "    step: 150; loss: 2.329; l2dist: 1.349\n",
      "    step: 200; loss: 2.142; l2dist: 1.310\n",
      "    step: 250; loss: 2.066; l2dist: 1.285\n",
      "    step: 300; loss: 2.033; l2dist: 1.280\n",
      "    step: 350; loss: 2.006; l2dist: 1.282\n",
      "    step: 400; loss: 1.993; l2dist: 1.276\n",
      "    step: 450; loss: 1.996; l2dist: 1.270\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.142; l2dist: 0.000\n",
      "    step: 50; loss: 5.892; l2dist: 1.898\n",
      "    step: 100; loss: 3.102; l2dist: 1.506\n",
      "    step: 150; loss: 2.404; l2dist: 1.366\n",
      "    step: 200; loss: 2.188; l2dist: 1.312\n",
      "    step: 250; loss: 2.105; l2dist: 1.301\n",
      "    step: 300; loss: 2.063; l2dist: 1.293\n",
      "    step: 350; loss: 2.050; l2dist: 1.290\n",
      "    step: 400; loss: 2.034; l2dist: 1.284\n",
      "    step: 450; loss: 2.025; l2dist: 1.285\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.651; l2dist: 0.000\n",
      "    step: 50; loss: 5.899; l2dist: 1.899\n",
      "    step: 100; loss: 3.113; l2dist: 1.515\n",
      "    step: 150; loss: 2.377; l2dist: 1.358\n",
      "    step: 200; loss: 2.183; l2dist: 1.326\n",
      "    step: 250; loss: 2.108; l2dist: 1.303\n",
      "    step: 300; loss: 2.077; l2dist: 1.292\n",
      "    step: 350; loss: 2.051; l2dist: 1.287\n",
      "    step: 400; loss: 2.039; l2dist: 1.287\n",
      "    step: 450; loss: 2.035; l2dist: 1.288\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.373; l2dist: 0.000\n",
      "    step: 50; loss: 5.886; l2dist: 1.891\n",
      "    step: 100; loss: 3.117; l2dist: 1.515\n",
      "    step: 150; loss: 2.367; l2dist: 1.364\n",
      "    step: 200; loss: 2.189; l2dist: 1.316\n",
      "    step: 250; loss: 2.112; l2dist: 1.299\n",
      "    step: 300; loss: 2.074; l2dist: 1.291\n",
      "    step: 350; loss: 2.049; l2dist: 1.286\n",
      "    step: 400; loss: 2.052; l2dist: 1.286\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.618; l2dist: 0.000\n",
      "    step: 50; loss: 5.924; l2dist: 1.904\n",
      "    step: 100; loss: 3.125; l2dist: 1.522\n",
      "    step: 150; loss: 2.371; l2dist: 1.364\n",
      "    step: 200; loss: 2.188; l2dist: 1.321\n",
      "    step: 250; loss: 2.113; l2dist: 1.307\n",
      "    step: 300; loss: 2.087; l2dist: 1.305\n",
      "    step: 350; loss: 2.052; l2dist: 1.292\n",
      "    step: 400; loss: 2.036; l2dist: 1.293\n",
      "    step: 450; loss: 2.031; l2dist: 1.289\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 126.167; l2dist: 0.000\n",
      "    step: 50; loss: 14.163; l2dist: 3.394\n",
      "    step: 100; loss: 5.709; l2dist: 2.321\n",
      "    step: 150; loss: 3.784; l2dist: 1.862\n",
      "    step: 200; loss: 3.027; l2dist: 1.667\n",
      "    step: 250; loss: 2.701; l2dist: 1.567\n",
      "    step: 300; loss: 2.563; l2dist: 1.522\n",
      "    step: 350; loss: 2.474; l2dist: 1.488\n",
      "    step: 400; loss: 2.376; l2dist: 1.466\n",
      "    step: 450; loss: 2.351; l2dist: 1.457\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 156.369; l2dist: 0.000\n",
      "    step: 50; loss: 13.599; l2dist: 3.295\n",
      "    step: 100; loss: 5.948; l2dist: 2.301\n",
      "    step: 150; loss: 4.065; l2dist: 1.888\n",
      "    step: 200; loss: 3.254; l2dist: 1.688\n",
      "    step: 250; loss: 2.836; l2dist: 1.580\n",
      "    step: 300; loss: 2.636; l2dist: 1.528\n",
      "    step: 350; loss: 2.480; l2dist: 1.495\n",
      "    step: 400; loss: 2.383; l2dist: 1.462\n",
      "    step: 450; loss: 2.392; l2dist: 1.463\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 87.075; l2dist: 0.000\n",
      "    step: 50; loss: 10.537; l2dist: 2.840\n",
      "    step: 100; loss: 4.427; l2dist: 1.976\n",
      "    step: 150; loss: 3.152; l2dist: 1.672\n",
      "    step: 200; loss: 2.683; l2dist: 1.540\n",
      "    step: 250; loss: 2.460; l2dist: 1.481\n",
      "    step: 300; loss: 2.362; l2dist: 1.458\n",
      "    step: 350; loss: 2.309; l2dist: 1.435\n",
      "    step: 400; loss: 2.271; l2dist: 1.424\n",
      "    step: 450; loss: 2.229; l2dist: 1.415\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.437; l2dist: 0.000\n",
      "    step: 50; loss: 8.380; l2dist: 2.431\n",
      "    step: 100; loss: 3.740; l2dist: 1.802\n",
      "    step: 150; loss: 2.729; l2dist: 1.555\n",
      "    step: 200; loss: 2.425; l2dist: 1.461\n",
      "    step: 250; loss: 2.290; l2dist: 1.433\n",
      "    step: 300; loss: 2.217; l2dist: 1.407\n",
      "    step: 350; loss: 2.203; l2dist: 1.402\n",
      "    step: 400; loss: 2.204; l2dist: 1.400\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.511; l2dist: 0.000\n",
      "    step: 50; loss: 7.424; l2dist: 2.230\n",
      "    step: 100; loss: 3.449; l2dist: 1.686\n",
      "    step: 150; loss: 2.552; l2dist: 1.483\n",
      "    step: 200; loss: 2.311; l2dist: 1.416\n",
      "    step: 250; loss: 2.191; l2dist: 1.382\n",
      "    step: 300; loss: 2.164; l2dist: 1.370\n",
      "    step: 350; loss: 2.132; l2dist: 1.362\n",
      "    step: 400; loss: 2.111; l2dist: 1.357\n",
      "    step: 450; loss: 2.103; l2dist: 1.355\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.276; l2dist: 0.000\n",
      "    step: 50; loss: 6.734; l2dist: 2.070\n",
      "    step: 100; loss: 3.240; l2dist: 1.574\n",
      "    step: 150; loss: 2.456; l2dist: 1.411\n",
      "    step: 200; loss: 2.253; l2dist: 1.361\n",
      "    step: 250; loss: 2.155; l2dist: 1.340\n",
      "    step: 300; loss: 2.136; l2dist: 1.335\n",
      "    step: 350; loss: 2.104; l2dist: 1.328\n",
      "    step: 400; loss: 2.091; l2dist: 1.321\n",
      "    step: 450; loss: 2.081; l2dist: 1.320\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.350; l2dist: 0.000\n",
      "    step: 50; loss: 6.579; l2dist: 2.045\n",
      "    step: 100; loss: 3.202; l2dist: 1.576\n",
      "    step: 150; loss: 2.437; l2dist: 1.404\n",
      "    step: 200; loss: 2.241; l2dist: 1.360\n",
      "    step: 250; loss: 2.157; l2dist: 1.346\n",
      "    step: 300; loss: 2.110; l2dist: 1.329\n",
      "    step: 350; loss: 2.088; l2dist: 1.327\n",
      "    step: 400; loss: 2.080; l2dist: 1.315\n",
      "    step: 450; loss: 2.076; l2dist: 1.326\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.863; l2dist: 0.000\n",
      "    step: 50; loss: 6.491; l2dist: 2.025\n",
      "    step: 100; loss: 3.185; l2dist: 1.561\n",
      "    step: 150; loss: 2.422; l2dist: 1.399\n",
      "    step: 200; loss: 2.235; l2dist: 1.358\n",
      "    step: 250; loss: 2.153; l2dist: 1.331\n",
      "    step: 300; loss: 2.110; l2dist: 1.322\n",
      "    step: 350; loss: 2.095; l2dist: 1.319\n",
      "    step: 400; loss: 2.085; l2dist: 1.319\n",
      "    step: 450; loss: 2.086; l2dist: 1.319\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.146; l2dist: 0.000\n",
      "    step: 50; loss: 6.450; l2dist: 2.019\n",
      "    step: 100; loss: 3.184; l2dist: 1.554\n",
      "    step: 150; loss: 2.418; l2dist: 1.391\n",
      "    step: 200; loss: 2.229; l2dist: 1.352\n",
      "    step: 250; loss: 2.154; l2dist: 1.338\n",
      "    step: 300; loss: 2.104; l2dist: 1.323\n",
      "    step: 350; loss: 2.085; l2dist: 1.325\n",
      "    step: 400; loss: 2.077; l2dist: 1.322\n",
      "    step: 450; loss: 2.071; l2dist: 1.321\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.378; l2dist: 0.000\n",
      "    step: 50; loss: 6.500; l2dist: 2.033\n",
      "    step: 100; loss: 3.200; l2dist: 1.565\n",
      "    step: 150; loss: 2.432; l2dist: 1.406\n",
      "    step: 200; loss: 2.238; l2dist: 1.363\n",
      "    step: 250; loss: 2.143; l2dist: 1.345\n",
      "    step: 300; loss: 2.135; l2dist: 1.339\n",
      "    step: 350; loss: 2.097; l2dist: 1.325\n",
      "    step: 400; loss: 2.076; l2dist: 1.328\n",
      "    step: 450; loss: 2.090; l2dist: 1.330\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 116.222; l2dist: 0.000\n",
      "    step: 50; loss: 12.290; l2dist: 3.268\n",
      "    step: 100; loss: 5.358; l2dist: 2.222\n",
      "    step: 150; loss: 3.520; l2dist: 1.779\n",
      "    step: 200; loss: 2.830; l2dist: 1.575\n",
      "    step: 250; loss: 2.572; l2dist: 1.498\n",
      "    step: 300; loss: 2.416; l2dist: 1.445\n",
      "    step: 350; loss: 2.318; l2dist: 1.425\n",
      "    step: 400; loss: 2.243; l2dist: 1.388\n",
      "    step: 450; loss: 2.260; l2dist: 1.400\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 118.277; l2dist: 0.000\n",
      "    step: 50; loss: 11.571; l2dist: 3.135\n",
      "    step: 100; loss: 5.122; l2dist: 2.117\n",
      "    step: 150; loss: 3.503; l2dist: 1.729\n",
      "    step: 200; loss: 2.838; l2dist: 1.554\n",
      "    step: 250; loss: 2.574; l2dist: 1.484\n",
      "    step: 300; loss: 2.440; l2dist: 1.444\n",
      "    step: 350; loss: 2.349; l2dist: 1.418\n",
      "    step: 400; loss: 2.313; l2dist: 1.407\n",
      "    step: 450; loss: 2.318; l2dist: 1.402\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.139; l2dist: 0.000\n",
      "    step: 50; loss: 8.853; l2dist: 2.657\n",
      "    step: 100; loss: 3.839; l2dist: 1.816\n",
      "    step: 150; loss: 2.776; l2dist: 1.539\n",
      "    step: 200; loss: 2.455; l2dist: 1.450\n",
      "    step: 250; loss: 2.307; l2dist: 1.413\n",
      "    step: 300; loss: 2.229; l2dist: 1.382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 2.218; l2dist: 1.376\n",
      "    step: 400; loss: 2.191; l2dist: 1.364\n",
      "    step: 450; loss: 2.161; l2dist: 1.363\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.538; l2dist: 0.000\n",
      "    step: 50; loss: 7.441; l2dist: 2.335\n",
      "    step: 100; loss: 3.401; l2dist: 1.707\n",
      "    step: 150; loss: 2.565; l2dist: 1.483\n",
      "    step: 200; loss: 2.295; l2dist: 1.403\n",
      "    step: 250; loss: 2.205; l2dist: 1.375\n",
      "    step: 300; loss: 2.159; l2dist: 1.362\n",
      "    step: 350; loss: 2.146; l2dist: 1.354\n",
      "    step: 400; loss: 2.124; l2dist: 1.349\n",
      "    step: 450; loss: 2.111; l2dist: 1.342\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.191; l2dist: 0.000\n",
      "    step: 50; loss: 6.868; l2dist: 2.198\n",
      "    step: 100; loss: 3.244; l2dist: 1.639\n",
      "    step: 150; loss: 2.452; l2dist: 1.433\n",
      "    step: 200; loss: 2.262; l2dist: 1.381\n",
      "    step: 250; loss: 2.183; l2dist: 1.357\n",
      "    step: 300; loss: 2.136; l2dist: 1.347\n",
      "    step: 350; loss: 2.125; l2dist: 1.335\n",
      "    step: 400; loss: 2.111; l2dist: 1.329\n",
      "    step: 450; loss: 2.108; l2dist: 1.339\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.613; l2dist: 0.000\n",
      "    step: 50; loss: 6.564; l2dist: 2.102\n",
      "    step: 100; loss: 3.226; l2dist: 1.600\n",
      "    step: 150; loss: 2.452; l2dist: 1.410\n",
      "    step: 200; loss: 2.254; l2dist: 1.358\n",
      "    step: 250; loss: 2.172; l2dist: 1.340\n",
      "    step: 300; loss: 2.146; l2dist: 1.333\n",
      "    step: 350; loss: 2.118; l2dist: 1.328\n",
      "    step: 400; loss: 2.113; l2dist: 1.320\n",
      "    step: 450; loss: 2.100; l2dist: 1.322\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.126; l2dist: 0.000\n",
      "    step: 50; loss: 6.404; l2dist: 2.062\n",
      "    step: 100; loss: 3.213; l2dist: 1.589\n",
      "    step: 150; loss: 2.429; l2dist: 1.407\n",
      "    step: 200; loss: 2.246; l2dist: 1.361\n",
      "    step: 250; loss: 2.175; l2dist: 1.341\n",
      "    step: 300; loss: 2.139; l2dist: 1.342\n",
      "    step: 350; loss: 2.110; l2dist: 1.329\n",
      "    step: 400; loss: 2.093; l2dist: 1.324\n",
      "    step: 450; loss: 2.085; l2dist: 1.322\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.076; l2dist: 0.000\n",
      "    step: 50; loss: 6.357; l2dist: 2.054\n",
      "    step: 100; loss: 3.189; l2dist: 1.575\n",
      "    step: 150; loss: 2.414; l2dist: 1.404\n",
      "    step: 200; loss: 2.227; l2dist: 1.357\n",
      "    step: 250; loss: 2.154; l2dist: 1.337\n",
      "    step: 300; loss: 2.120; l2dist: 1.326\n",
      "    step: 350; loss: 2.104; l2dist: 1.321\n",
      "    step: 400; loss: 2.089; l2dist: 1.314\n",
      "    step: 450; loss: 2.092; l2dist: 1.326\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.429; l2dist: 0.000\n",
      "    step: 50; loss: 6.326; l2dist: 2.043\n",
      "    step: 100; loss: 3.172; l2dist: 1.568\n",
      "    step: 150; loss: 2.411; l2dist: 1.399\n",
      "    step: 200; loss: 2.224; l2dist: 1.357\n",
      "    step: 250; loss: 2.160; l2dist: 1.333\n",
      "    step: 300; loss: 2.121; l2dist: 1.327\n",
      "    step: 350; loss: 2.111; l2dist: 1.326\n",
      "    step: 400; loss: 2.094; l2dist: 1.314\n",
      "    step: 450; loss: 2.082; l2dist: 1.320\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.690; l2dist: 0.000\n",
      "    step: 50; loss: 6.359; l2dist: 2.054\n",
      "    step: 100; loss: 3.190; l2dist: 1.576\n",
      "    step: 150; loss: 2.419; l2dist: 1.406\n",
      "    step: 200; loss: 2.233; l2dist: 1.361\n",
      "    step: 250; loss: 2.164; l2dist: 1.338\n",
      "    step: 300; loss: 2.129; l2dist: 1.335\n",
      "    step: 350; loss: 2.107; l2dist: 1.325\n",
      "    step: 400; loss: 2.099; l2dist: 1.324\n",
      "    step: 450; loss: 2.085; l2dist: 1.319\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 109.350; l2dist: 0.000\n",
      "    step: 50; loss: 10.684; l2dist: 3.063\n",
      "    step: 100; loss: 4.674; l2dist: 2.069\n",
      "    step: 150; loss: 3.092; l2dist: 1.660\n",
      "    step: 200; loss: 2.385; l2dist: 1.454\n",
      "    step: 250; loss: 2.162; l2dist: 1.377\n",
      "    step: 300; loss: 2.042; l2dist: 1.338\n",
      "    step: 350; loss: 2.005; l2dist: 1.301\n",
      "    step: 400; loss: 1.965; l2dist: 1.299\n",
      "    step: 450; loss: 1.930; l2dist: 1.293\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 123.934; l2dist: 0.000\n",
      "    step: 50; loss: 10.283; l2dist: 2.938\n",
      "    step: 100; loss: 4.768; l2dist: 2.018\n",
      "    step: 150; loss: 3.229; l2dist: 1.644\n",
      "    step: 200; loss: 2.537; l2dist: 1.466\n",
      "    step: 250; loss: 2.208; l2dist: 1.374\n",
      "    step: 300; loss: 2.052; l2dist: 1.328\n",
      "    step: 350; loss: 1.968; l2dist: 1.304\n",
      "    step: 400; loss: 1.930; l2dist: 1.281\n",
      "    step: 450; loss: 1.903; l2dist: 1.268\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.826; l2dist: 0.000\n",
      "    step: 50; loss: 7.943; l2dist: 2.496\n",
      "    step: 100; loss: 3.508; l2dist: 1.727\n",
      "    step: 150; loss: 2.471; l2dist: 1.443\n",
      "    step: 200; loss: 2.092; l2dist: 1.335\n",
      "    step: 250; loss: 1.986; l2dist: 1.303\n",
      "    step: 300; loss: 1.875; l2dist: 1.264\n",
      "    step: 350; loss: 1.822; l2dist: 1.247\n",
      "    step: 400; loss: 1.781; l2dist: 1.237\n",
      "    step: 450; loss: 1.764; l2dist: 1.228\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.283; l2dist: 0.000\n",
      "    step: 50; loss: 6.739; l2dist: 2.278\n",
      "    step: 100; loss: 3.047; l2dist: 1.620\n",
      "    step: 150; loss: 2.201; l2dist: 1.375\n",
      "    step: 200; loss: 1.952; l2dist: 1.296\n",
      "    step: 250; loss: 1.850; l2dist: 1.264\n",
      "    step: 300; loss: 1.789; l2dist: 1.239\n",
      "    step: 350; loss: 1.759; l2dist: 1.234\n",
      "    step: 400; loss: 1.746; l2dist: 1.228\n",
      "    step: 450; loss: 1.768; l2dist: 1.239\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.448; l2dist: 0.000\n",
      "    step: 50; loss: 5.898; l2dist: 2.066\n",
      "    step: 100; loss: 2.859; l2dist: 1.547\n",
      "    step: 150; loss: 2.062; l2dist: 1.330\n",
      "    step: 200; loss: 1.870; l2dist: 1.267\n",
      "    step: 250; loss: 1.788; l2dist: 1.235\n",
      "    step: 300; loss: 1.745; l2dist: 1.222\n",
      "    step: 350; loss: 1.719; l2dist: 1.215\n",
      "    step: 400; loss: 1.716; l2dist: 1.210\n",
      "    step: 450; loss: 1.700; l2dist: 1.203\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.967; l2dist: 0.000\n",
      "    step: 50; loss: 5.619; l2dist: 1.976\n",
      "    step: 100; loss: 2.745; l2dist: 1.483\n",
      "    step: 150; loss: 2.026; l2dist: 1.289\n",
      "    step: 200; loss: 1.838; l2dist: 1.237\n",
      "    step: 250; loss: 1.769; l2dist: 1.211\n",
      "    step: 300; loss: 1.725; l2dist: 1.197\n",
      "    step: 350; loss: 1.714; l2dist: 1.196\n",
      "    step: 400; loss: 1.703; l2dist: 1.193\n",
      "    step: 450; loss: 1.691; l2dist: 1.186\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.203; l2dist: 0.000\n",
      "    step: 50; loss: 5.482; l2dist: 1.935\n",
      "    step: 100; loss: 2.723; l2dist: 1.455\n",
      "    step: 150; loss: 2.002; l2dist: 1.274\n",
      "    step: 200; loss: 1.830; l2dist: 1.228\n",
      "    step: 250; loss: 1.760; l2dist: 1.205\n",
      "    step: 300; loss: 1.727; l2dist: 1.194\n",
      "    step: 350; loss: 1.705; l2dist: 1.189\n",
      "    step: 400; loss: 1.698; l2dist: 1.185\n",
      "    step: 450; loss: 1.706; l2dist: 1.188\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.114; l2dist: 0.000\n",
      "    step: 50; loss: 5.466; l2dist: 1.924\n",
      "    step: 100; loss: 2.705; l2dist: 1.451\n",
      "    step: 150; loss: 1.993; l2dist: 1.269\n",
      "    step: 200; loss: 1.830; l2dist: 1.222\n",
      "    step: 250; loss: 1.747; l2dist: 1.199\n",
      "    step: 300; loss: 1.721; l2dist: 1.193\n",
      "    step: 350; loss: 1.710; l2dist: 1.195\n",
      "    step: 400; loss: 1.695; l2dist: 1.184\n",
      "    step: 450; loss: 1.687; l2dist: 1.180\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.497; l2dist: 0.000\n",
      "    step: 50; loss: 5.448; l2dist: 1.919\n",
      "    step: 100; loss: 2.711; l2dist: 1.454\n",
      "    step: 150; loss: 1.988; l2dist: 1.272\n",
      "    step: 200; loss: 1.818; l2dist: 1.225\n",
      "    step: 250; loss: 1.764; l2dist: 1.200\n",
      "    step: 300; loss: 1.720; l2dist: 1.198\n",
      "    step: 350; loss: 1.715; l2dist: 1.189\n",
      "    step: 400; loss: 1.697; l2dist: 1.187\n",
      "    step: 450; loss: 1.704; l2dist: 1.189\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.621; l2dist: 0.000\n",
      "    step: 50; loss: 5.470; l2dist: 1.926\n",
      "    step: 100; loss: 2.716; l2dist: 1.458\n",
      "    step: 150; loss: 1.988; l2dist: 1.283\n",
      "    step: 200; loss: 1.824; l2dist: 1.231\n",
      "    step: 250; loss: 1.760; l2dist: 1.214\n",
      "    step: 300; loss: 1.730; l2dist: 1.197\n",
      "    step: 350; loss: 1.708; l2dist: 1.194\n",
      "    step: 400; loss: 1.692; l2dist: 1.185\n",
      "    step: 450; loss: 1.709; l2dist: 1.187\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 121.201; l2dist: 0.000\n",
      "    step: 50; loss: 12.626; l2dist: 3.378\n",
      "    step: 100; loss: 5.654; l2dist: 2.279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 3.752; l2dist: 1.843\n",
      "    step: 200; loss: 3.044; l2dist: 1.650\n",
      "    step: 250; loss: 2.721; l2dist: 1.550\n",
      "    step: 300; loss: 2.530; l2dist: 1.493\n",
      "    step: 350; loss: 2.466; l2dist: 1.476\n",
      "    step: 400; loss: 2.449; l2dist: 1.472\n",
      "    step: 450; loss: 2.399; l2dist: 1.452\n",
      "binary step: 0; number of successful adv: 92/100\n",
      "    step: 0; loss: 165.769; l2dist: 0.000\n",
      "    step: 50; loss: 13.428; l2dist: 3.426\n",
      "    step: 100; loss: 6.690; l2dist: 2.406\n",
      "    step: 150; loss: 4.475; l2dist: 1.942\n",
      "    step: 200; loss: 3.485; l2dist: 1.716\n",
      "    step: 250; loss: 2.960; l2dist: 1.595\n",
      "    step: 300; loss: 2.730; l2dist: 1.528\n",
      "    step: 350; loss: 2.574; l2dist: 1.498\n",
      "    step: 400; loss: 2.511; l2dist: 1.478\n",
      "    step: 450; loss: 2.430; l2dist: 1.453\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.306; l2dist: 0.000\n",
      "    step: 50; loss: 10.282; l2dist: 2.941\n",
      "    step: 100; loss: 4.683; l2dist: 2.006\n",
      "    step: 150; loss: 3.237; l2dist: 1.667\n",
      "    step: 200; loss: 2.719; l2dist: 1.534\n",
      "    step: 250; loss: 2.507; l2dist: 1.479\n",
      "    step: 300; loss: 2.381; l2dist: 1.438\n",
      "    step: 350; loss: 2.334; l2dist: 1.428\n",
      "    step: 400; loss: 2.291; l2dist: 1.421\n",
      "    step: 450; loss: 2.248; l2dist: 1.405\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.293; l2dist: 0.000\n",
      "    step: 50; loss: 8.526; l2dist: 2.549\n",
      "    step: 100; loss: 3.817; l2dist: 1.808\n",
      "    step: 150; loss: 2.788; l2dist: 1.555\n",
      "    step: 200; loss: 2.503; l2dist: 1.469\n",
      "    step: 250; loss: 2.329; l2dist: 1.424\n",
      "    step: 300; loss: 2.261; l2dist: 1.411\n",
      "    step: 350; loss: 2.325; l2dist: 1.410\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.693; l2dist: 0.000\n",
      "    step: 50; loss: 7.809; l2dist: 2.332\n",
      "    step: 100; loss: 3.604; l2dist: 1.688\n",
      "    step: 150; loss: 2.725; l2dist: 1.492\n",
      "    step: 200; loss: 2.434; l2dist: 1.417\n",
      "    step: 250; loss: 2.333; l2dist: 1.395\n",
      "    step: 300; loss: 2.280; l2dist: 1.380\n",
      "    step: 350; loss: 2.241; l2dist: 1.376\n",
      "    step: 400; loss: 2.230; l2dist: 1.370\n",
      "    step: 450; loss: 2.218; l2dist: 1.366\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.453; l2dist: 0.000\n",
      "    step: 50; loss: 7.197; l2dist: 2.171\n",
      "    step: 100; loss: 3.399; l2dist: 1.565\n",
      "    step: 150; loss: 2.621; l2dist: 1.407\n",
      "    step: 200; loss: 2.376; l2dist: 1.361\n",
      "    step: 250; loss: 2.284; l2dist: 1.331\n",
      "    step: 300; loss: 2.229; l2dist: 1.336\n",
      "    step: 350; loss: 2.204; l2dist: 1.322\n",
      "    step: 400; loss: 2.183; l2dist: 1.322\n",
      "    step: 450; loss: 2.193; l2dist: 1.324\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.838; l2dist: 0.000\n",
      "    step: 50; loss: 7.014; l2dist: 2.129\n",
      "    step: 100; loss: 3.330; l2dist: 1.556\n",
      "    step: 150; loss: 2.567; l2dist: 1.400\n",
      "    step: 200; loss: 2.345; l2dist: 1.353\n",
      "    step: 250; loss: 2.245; l2dist: 1.328\n",
      "    step: 300; loss: 2.197; l2dist: 1.324\n",
      "    step: 350; loss: 2.168; l2dist: 1.319\n",
      "    step: 400; loss: 2.158; l2dist: 1.314\n",
      "    step: 450; loss: 2.146; l2dist: 1.319\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.494; l2dist: 0.000\n",
      "    step: 50; loss: 6.931; l2dist: 2.113\n",
      "    step: 100; loss: 3.341; l2dist: 1.544\n",
      "    step: 150; loss: 2.568; l2dist: 1.392\n",
      "    step: 200; loss: 2.341; l2dist: 1.346\n",
      "    step: 250; loss: 2.244; l2dist: 1.328\n",
      "    step: 300; loss: 2.204; l2dist: 1.322\n",
      "    step: 350; loss: 2.174; l2dist: 1.315\n",
      "    step: 400; loss: 2.200; l2dist: 1.323\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.848; l2dist: 0.000\n",
      "    step: 50; loss: 6.905; l2dist: 2.103\n",
      "    step: 100; loss: 3.328; l2dist: 1.550\n",
      "    step: 150; loss: 2.549; l2dist: 1.393\n",
      "    step: 200; loss: 2.336; l2dist: 1.348\n",
      "    step: 250; loss: 2.238; l2dist: 1.334\n",
      "    step: 300; loss: 2.186; l2dist: 1.322\n",
      "    step: 350; loss: 2.165; l2dist: 1.320\n",
      "    step: 400; loss: 2.163; l2dist: 1.314\n",
      "    step: 450; loss: 2.151; l2dist: 1.317\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.034; l2dist: 0.000\n",
      "    step: 50; loss: 6.947; l2dist: 2.115\n",
      "    step: 100; loss: 3.342; l2dist: 1.561\n",
      "    step: 150; loss: 2.566; l2dist: 1.404\n",
      "    step: 200; loss: 2.345; l2dist: 1.360\n",
      "    step: 250; loss: 2.245; l2dist: 1.339\n",
      "    step: 300; loss: 2.217; l2dist: 1.340\n",
      "    step: 350; loss: 2.197; l2dist: 1.333\n",
      "    step: 400; loss: 2.172; l2dist: 1.329\n",
      "    step: 450; loss: 2.168; l2dist: 1.324\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 118.574; l2dist: 0.000\n",
      "    step: 50; loss: 12.529; l2dist: 3.294\n",
      "    step: 100; loss: 5.528; l2dist: 2.271\n",
      "    step: 150; loss: 3.717; l2dist: 1.848\n",
      "    step: 200; loss: 3.017; l2dist: 1.647\n",
      "    step: 250; loss: 2.740; l2dist: 1.550\n",
      "    step: 300; loss: 2.469; l2dist: 1.480\n",
      "    step: 350; loss: 2.400; l2dist: 1.459\n",
      "    step: 400; loss: 2.385; l2dist: 1.456\n",
      "    step: 450; loss: 2.333; l2dist: 1.442\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 73.822; l2dist: 0.000\n",
      "    step: 50; loss: 9.937; l2dist: 2.876\n",
      "    step: 100; loss: 4.252; l2dist: 1.952\n",
      "    step: 150; loss: 3.056; l2dist: 1.639\n",
      "    step: 200; loss: 2.677; l2dist: 1.522\n",
      "    step: 250; loss: 2.430; l2dist: 1.457\n",
      "    step: 300; loss: 2.344; l2dist: 1.431\n",
      "    step: 350; loss: 2.304; l2dist: 1.424\n",
      "    step: 400; loss: 2.265; l2dist: 1.409\n",
      "    step: 450; loss: 2.269; l2dist: 1.419\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.433; l2dist: 0.000\n",
      "    step: 50; loss: 7.951; l2dist: 2.500\n",
      "    step: 100; loss: 3.546; l2dist: 1.764\n",
      "    step: 150; loss: 2.645; l2dist: 1.527\n",
      "    step: 200; loss: 2.374; l2dist: 1.454\n",
      "    step: 250; loss: 2.290; l2dist: 1.413\n",
      "    step: 300; loss: 2.224; l2dist: 1.400\n",
      "    step: 350; loss: 2.193; l2dist: 1.394\n",
      "    step: 400; loss: 2.183; l2dist: 1.390\n",
      "    step: 450; loss: 2.158; l2dist: 1.377\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.947; l2dist: 0.000\n",
      "    step: 50; loss: 6.726; l2dist: 2.178\n",
      "    step: 100; loss: 3.326; l2dist: 1.709\n",
      "    step: 150; loss: 2.472; l2dist: 1.475\n",
      "    step: 200; loss: 2.275; l2dist: 1.416\n",
      "    step: 250; loss: 2.192; l2dist: 1.392\n",
      "    step: 300; loss: 2.183; l2dist: 1.385\n",
      "    step: 350; loss: 2.134; l2dist: 1.373\n",
      "    step: 400; loss: 2.123; l2dist: 1.373\n",
      "    step: 450; loss: 2.114; l2dist: 1.368\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.952; l2dist: 0.000\n",
      "    step: 50; loss: 5.950; l2dist: 1.970\n",
      "    step: 100; loss: 3.287; l2dist: 1.622\n",
      "    step: 150; loss: 2.439; l2dist: 1.437\n",
      "    step: 200; loss: 2.232; l2dist: 1.389\n",
      "    step: 250; loss: 2.164; l2dist: 1.366\n",
      "    step: 300; loss: 2.131; l2dist: 1.353\n",
      "    step: 350; loss: 2.098; l2dist: 1.353\n",
      "    step: 400; loss: 2.096; l2dist: 1.350\n",
      "    step: 450; loss: 2.091; l2dist: 1.344\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.056; l2dist: 0.000\n",
      "    step: 50; loss: 5.493; l2dist: 1.824\n",
      "    step: 100; loss: 3.174; l2dist: 1.520\n",
      "    step: 150; loss: 2.398; l2dist: 1.369\n",
      "    step: 200; loss: 2.188; l2dist: 1.331\n",
      "    step: 250; loss: 2.119; l2dist: 1.323\n",
      "    step: 300; loss: 2.082; l2dist: 1.312\n",
      "    step: 350; loss: 2.066; l2dist: 1.305\n",
      "    step: 400; loss: 2.054; l2dist: 1.305\n",
      "    step: 450; loss: 2.044; l2dist: 1.307\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.068; l2dist: 0.000\n",
      "    step: 50; loss: 5.514; l2dist: 1.813\n",
      "    step: 100; loss: 3.216; l2dist: 1.527\n",
      "    step: 150; loss: 2.410; l2dist: 1.382\n",
      "    step: 200; loss: 2.204; l2dist: 1.337\n",
      "    step: 250; loss: 2.128; l2dist: 1.322\n",
      "    step: 300; loss: 2.094; l2dist: 1.318\n",
      "    step: 350; loss: 2.077; l2dist: 1.314\n",
      "    step: 400; loss: 2.060; l2dist: 1.311\n",
      "    step: 450; loss: 2.059; l2dist: 1.314\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.132; l2dist: 0.000\n",
      "    step: 50; loss: 5.536; l2dist: 1.815\n",
      "    step: 100; loss: 3.244; l2dist: 1.540\n",
      "    step: 150; loss: 2.427; l2dist: 1.390\n",
      "    step: 200; loss: 2.203; l2dist: 1.350\n",
      "    step: 250; loss: 2.131; l2dist: 1.327\n",
      "    step: 300; loss: 2.098; l2dist: 1.328\n",
      "    step: 350; loss: 2.085; l2dist: 1.321\n",
      "    step: 400; loss: 2.066; l2dist: 1.317\n",
      "    step: 450; loss: 2.059; l2dist: 1.319\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.870; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 5.519; l2dist: 1.811\n",
      "    step: 100; loss: 3.256; l2dist: 1.547\n",
      "    step: 150; loss: 2.422; l2dist: 1.394\n",
      "    step: 200; loss: 2.214; l2dist: 1.352\n",
      "    step: 250; loss: 2.135; l2dist: 1.338\n",
      "    step: 300; loss: 2.109; l2dist: 1.324\n",
      "    step: 350; loss: 2.079; l2dist: 1.323\n",
      "    step: 400; loss: 2.074; l2dist: 1.323\n",
      "    step: 450; loss: 2.064; l2dist: 1.319\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.171; l2dist: 0.000\n",
      "    step: 50; loss: 5.581; l2dist: 1.830\n",
      "    step: 100; loss: 3.260; l2dist: 1.556\n",
      "    step: 150; loss: 2.426; l2dist: 1.401\n",
      "    step: 200; loss: 2.217; l2dist: 1.357\n",
      "    step: 250; loss: 2.139; l2dist: 1.346\n",
      "    step: 300; loss: 2.111; l2dist: 1.331\n",
      "    step: 350; loss: 2.086; l2dist: 1.335\n",
      "    step: 400; loss: 2.080; l2dist: 1.324\n",
      "    step: 450; loss: 2.071; l2dist: 1.332\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 117.455; l2dist: 0.000\n",
      "    step: 50; loss: 13.962; l2dist: 3.367\n",
      "    step: 100; loss: 5.787; l2dist: 2.289\n",
      "    step: 150; loss: 3.841; l2dist: 1.829\n",
      "    step: 200; loss: 3.106; l2dist: 1.626\n",
      "    step: 250; loss: 2.795; l2dist: 1.533\n",
      "    step: 300; loss: 2.675; l2dist: 1.493\n",
      "    step: 350; loss: 2.601; l2dist: 1.479\n",
      "    step: 400; loss: 2.531; l2dist: 1.454\n",
      "    step: 450; loss: 2.486; l2dist: 1.449\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 92.669; l2dist: 0.000\n",
      "    step: 50; loss: 11.969; l2dist: 3.054\n",
      "    step: 100; loss: 4.762; l2dist: 2.014\n",
      "    step: 150; loss: 3.337; l2dist: 1.666\n",
      "    step: 200; loss: 2.846; l2dist: 1.540\n",
      "    step: 250; loss: 2.628; l2dist: 1.480\n",
      "    step: 300; loss: 2.480; l2dist: 1.438\n",
      "    step: 350; loss: 2.452; l2dist: 1.418\n",
      "    step: 400; loss: 2.397; l2dist: 1.404\n",
      "    step: 450; loss: 2.374; l2dist: 1.405\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.931; l2dist: 0.000\n",
      "    step: 50; loss: 9.598; l2dist: 2.647\n",
      "    step: 100; loss: 3.930; l2dist: 1.818\n",
      "    step: 150; loss: 2.881; l2dist: 1.552\n",
      "    step: 200; loss: 2.566; l2dist: 1.460\n",
      "    step: 250; loss: 2.452; l2dist: 1.424\n",
      "    step: 300; loss: 2.420; l2dist: 1.411\n",
      "    step: 350; loss: 2.354; l2dist: 1.392\n",
      "    step: 400; loss: 2.361; l2dist: 1.392\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.161; l2dist: 0.000\n",
      "    step: 50; loss: 8.101; l2dist: 2.373\n",
      "    step: 100; loss: 3.594; l2dist: 1.719\n",
      "    step: 150; loss: 2.708; l2dist: 1.486\n",
      "    step: 200; loss: 2.485; l2dist: 1.419\n",
      "    step: 250; loss: 2.387; l2dist: 1.395\n",
      "    step: 300; loss: 2.362; l2dist: 1.381\n",
      "    step: 350; loss: 2.320; l2dist: 1.372\n",
      "    step: 400; loss: 2.307; l2dist: 1.371\n",
      "    step: 450; loss: 2.286; l2dist: 1.367\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.434; l2dist: 0.000\n",
      "    step: 50; loss: 7.631; l2dist: 2.232\n",
      "    step: 100; loss: 3.511; l2dist: 1.664\n",
      "    step: 150; loss: 2.653; l2dist: 1.470\n",
      "    step: 200; loss: 2.441; l2dist: 1.408\n",
      "    step: 250; loss: 2.371; l2dist: 1.378\n",
      "    step: 300; loss: 2.317; l2dist: 1.369\n",
      "    step: 350; loss: 2.302; l2dist: 1.362\n",
      "    step: 400; loss: 2.270; l2dist: 1.356\n",
      "    step: 450; loss: 2.261; l2dist: 1.358\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.680; l2dist: 0.000\n",
      "    step: 50; loss: 7.374; l2dist: 2.161\n",
      "    step: 100; loss: 3.459; l2dist: 1.623\n",
      "    step: 150; loss: 2.642; l2dist: 1.439\n",
      "    step: 200; loss: 2.431; l2dist: 1.394\n",
      "    step: 250; loss: 2.350; l2dist: 1.370\n",
      "    step: 300; loss: 2.317; l2dist: 1.364\n",
      "    step: 350; loss: 2.294; l2dist: 1.357\n",
      "    step: 400; loss: 2.287; l2dist: 1.352\n",
      "    step: 450; loss: 2.277; l2dist: 1.350\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.156; l2dist: 0.000\n",
      "    step: 50; loss: 7.211; l2dist: 2.132\n",
      "    step: 100; loss: 3.418; l2dist: 1.604\n",
      "    step: 150; loss: 2.611; l2dist: 1.426\n",
      "    step: 200; loss: 2.413; l2dist: 1.375\n",
      "    step: 250; loss: 2.327; l2dist: 1.357\n",
      "    step: 300; loss: 2.287; l2dist: 1.351\n",
      "    step: 350; loss: 2.269; l2dist: 1.340\n",
      "    step: 400; loss: 2.260; l2dist: 1.341\n",
      "    step: 450; loss: 2.258; l2dist: 1.342\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.916; l2dist: 0.000\n",
      "    step: 50; loss: 7.226; l2dist: 2.132\n",
      "    step: 100; loss: 3.442; l2dist: 1.611\n",
      "    step: 150; loss: 2.624; l2dist: 1.431\n",
      "    step: 200; loss: 2.425; l2dist: 1.381\n",
      "    step: 250; loss: 2.354; l2dist: 1.362\n",
      "    step: 300; loss: 2.312; l2dist: 1.349\n",
      "    step: 350; loss: 2.308; l2dist: 1.350\n",
      "    step: 400; loss: 2.283; l2dist: 1.344\n",
      "    step: 450; loss: 2.277; l2dist: 1.342\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.731; l2dist: 0.000\n",
      "    step: 50; loss: 7.219; l2dist: 2.129\n",
      "    step: 100; loss: 3.443; l2dist: 1.607\n",
      "    step: 150; loss: 2.626; l2dist: 1.429\n",
      "    step: 200; loss: 2.437; l2dist: 1.383\n",
      "    step: 250; loss: 2.350; l2dist: 1.365\n",
      "    step: 300; loss: 2.317; l2dist: 1.352\n",
      "    step: 350; loss: 2.315; l2dist: 1.353\n",
      "    step: 400; loss: 2.288; l2dist: 1.353\n",
      "    step: 450; loss: 2.281; l2dist: 1.346\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.870; l2dist: 0.000\n",
      "    step: 50; loss: 7.227; l2dist: 2.136\n",
      "    step: 100; loss: 3.446; l2dist: 1.611\n",
      "    step: 150; loss: 2.640; l2dist: 1.429\n",
      "    step: 200; loss: 2.418; l2dist: 1.386\n",
      "    step: 250; loss: 2.349; l2dist: 1.366\n",
      "    step: 300; loss: 2.312; l2dist: 1.358\n",
      "    step: 350; loss: 2.296; l2dist: 1.354\n",
      "    step: 400; loss: 2.279; l2dist: 1.351\n",
      "    step: 450; loss: 2.269; l2dist: 1.349\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 116.372; l2dist: 0.000\n",
      "    step: 50; loss: 11.788; l2dist: 3.173\n",
      "    step: 100; loss: 4.957; l2dist: 2.143\n",
      "    step: 150; loss: 3.260; l2dist: 1.729\n",
      "    step: 200; loss: 2.621; l2dist: 1.545\n",
      "    step: 250; loss: 2.326; l2dist: 1.443\n",
      "    step: 300; loss: 2.219; l2dist: 1.419\n",
      "    step: 350; loss: 2.123; l2dist: 1.378\n",
      "    step: 400; loss: 2.091; l2dist: 1.370\n",
      "    step: 450; loss: 2.079; l2dist: 1.364\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 86.364; l2dist: 0.000\n",
      "    step: 50; loss: 9.459; l2dist: 2.787\n",
      "    step: 100; loss: 3.893; l2dist: 1.860\n",
      "    step: 150; loss: 2.764; l2dist: 1.556\n",
      "    step: 200; loss: 2.353; l2dist: 1.435\n",
      "    step: 250; loss: 2.200; l2dist: 1.387\n",
      "    step: 300; loss: 2.152; l2dist: 1.379\n",
      "    step: 350; loss: 2.049; l2dist: 1.345\n",
      "    step: 400; loss: 2.009; l2dist: 1.334\n",
      "    step: 450; loss: 2.000; l2dist: 1.334\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.508; l2dist: 0.000\n",
      "    step: 50; loss: 7.484; l2dist: 2.418\n",
      "    step: 100; loss: 3.224; l2dist: 1.689\n",
      "    step: 150; loss: 2.398; l2dist: 1.452\n",
      "    step: 200; loss: 2.160; l2dist: 1.386\n",
      "    step: 250; loss: 2.046; l2dist: 1.345\n",
      "    step: 300; loss: 1.999; l2dist: 1.333\n",
      "    step: 350; loss: 1.968; l2dist: 1.323\n",
      "    step: 400; loss: 1.957; l2dist: 1.318\n",
      "    step: 450; loss: 1.905; l2dist: 1.304\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.635; l2dist: 0.000\n",
      "    step: 50; loss: 6.360; l2dist: 2.160\n",
      "    step: 100; loss: 2.985; l2dist: 1.618\n",
      "    step: 150; loss: 2.224; l2dist: 1.401\n",
      "    step: 200; loss: 2.020; l2dist: 1.337\n",
      "    step: 250; loss: 1.947; l2dist: 1.315\n",
      "    step: 300; loss: 1.918; l2dist: 1.303\n",
      "    step: 350; loss: 1.881; l2dist: 1.294\n",
      "    step: 400; loss: 1.870; l2dist: 1.288\n",
      "    step: 450; loss: 1.890; l2dist: 1.291\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.634; l2dist: 0.000\n",
      "    step: 50; loss: 5.636; l2dist: 1.932\n",
      "    step: 100; loss: 2.905; l2dist: 1.510\n",
      "    step: 150; loss: 2.154; l2dist: 1.346\n",
      "    step: 200; loss: 1.977; l2dist: 1.299\n",
      "    step: 250; loss: 1.901; l2dist: 1.278\n",
      "    step: 300; loss: 1.856; l2dist: 1.265\n",
      "    step: 350; loss: 1.833; l2dist: 1.267\n",
      "    step: 400; loss: 1.837; l2dist: 1.266\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.878; l2dist: 0.000\n",
      "    step: 50; loss: 5.273; l2dist: 1.808\n",
      "    step: 100; loss: 2.788; l2dist: 1.438\n",
      "    step: 150; loss: 2.097; l2dist: 1.291\n",
      "    step: 200; loss: 1.916; l2dist: 1.251\n",
      "    step: 250; loss: 1.847; l2dist: 1.236\n",
      "    step: 300; loss: 1.808; l2dist: 1.224\n",
      "    step: 350; loss: 1.788; l2dist: 1.224\n",
      "    step: 400; loss: 1.795; l2dist: 1.225\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.781; l2dist: 0.000\n",
      "    step: 50; loss: 5.238; l2dist: 1.794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 2.772; l2dist: 1.428\n",
      "    step: 150; loss: 2.069; l2dist: 1.281\n",
      "    step: 200; loss: 1.897; l2dist: 1.238\n",
      "    step: 250; loss: 1.849; l2dist: 1.224\n",
      "    step: 300; loss: 1.799; l2dist: 1.212\n",
      "    step: 350; loss: 1.786; l2dist: 1.213\n",
      "    step: 400; loss: 1.772; l2dist: 1.209\n",
      "    step: 450; loss: 1.776; l2dist: 1.208\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.735; l2dist: 0.000\n",
      "    step: 50; loss: 5.251; l2dist: 1.797\n",
      "    step: 100; loss: 2.784; l2dist: 1.432\n",
      "    step: 150; loss: 2.080; l2dist: 1.288\n",
      "    step: 200; loss: 1.905; l2dist: 1.247\n",
      "    step: 250; loss: 1.859; l2dist: 1.232\n",
      "    step: 300; loss: 1.825; l2dist: 1.230\n",
      "    step: 350; loss: 1.795; l2dist: 1.215\n",
      "    step: 400; loss: 1.797; l2dist: 1.221\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.383; l2dist: 0.000\n",
      "    step: 50; loss: 5.221; l2dist: 1.790\n",
      "    step: 100; loss: 2.779; l2dist: 1.430\n",
      "    step: 150; loss: 2.080; l2dist: 1.287\n",
      "    step: 200; loss: 1.903; l2dist: 1.244\n",
      "    step: 250; loss: 1.839; l2dist: 1.230\n",
      "    step: 300; loss: 1.815; l2dist: 1.218\n",
      "    step: 350; loss: 1.801; l2dist: 1.221\n",
      "    step: 400; loss: 1.790; l2dist: 1.212\n",
      "    step: 450; loss: 1.793; l2dist: 1.215\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.565; l2dist: 0.000\n",
      "    step: 50; loss: 5.254; l2dist: 1.804\n",
      "    step: 100; loss: 2.786; l2dist: 1.445\n",
      "    step: 150; loss: 2.085; l2dist: 1.292\n",
      "    step: 200; loss: 1.907; l2dist: 1.255\n",
      "    step: 250; loss: 1.839; l2dist: 1.237\n",
      "    step: 300; loss: 1.815; l2dist: 1.236\n",
      "    step: 350; loss: 1.795; l2dist: 1.225\n",
      "    step: 400; loss: 1.786; l2dist: 1.221\n",
      "    step: 450; loss: 1.780; l2dist: 1.224\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 107.708; l2dist: 0.000\n",
      "    step: 50; loss: 12.421; l2dist: 3.141\n",
      "    step: 100; loss: 5.014; l2dist: 2.139\n",
      "    step: 150; loss: 3.310; l2dist: 1.713\n",
      "    step: 200; loss: 2.705; l2dist: 1.531\n",
      "    step: 250; loss: 2.359; l2dist: 1.432\n",
      "    step: 300; loss: 2.212; l2dist: 1.381\n",
      "    step: 350; loss: 2.185; l2dist: 1.373\n",
      "    step: 400; loss: 2.120; l2dist: 1.334\n",
      "    step: 450; loss: 2.088; l2dist: 1.334\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 97.347; l2dist: 0.000\n",
      "    step: 50; loss: 11.237; l2dist: 2.873\n",
      "    step: 100; loss: 4.570; l2dist: 1.971\n",
      "    step: 150; loss: 3.196; l2dist: 1.621\n",
      "    step: 200; loss: 2.617; l2dist: 1.472\n",
      "    step: 250; loss: 2.376; l2dist: 1.397\n",
      "    step: 300; loss: 2.230; l2dist: 1.363\n",
      "    step: 350; loss: 2.167; l2dist: 1.346\n",
      "    step: 400; loss: 2.091; l2dist: 1.328\n",
      "    step: 450; loss: 2.076; l2dist: 1.320\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.644; l2dist: 0.000\n",
      "    step: 50; loss: 8.688; l2dist: 2.481\n",
      "    step: 100; loss: 3.566; l2dist: 1.727\n",
      "    step: 150; loss: 2.581; l2dist: 1.471\n",
      "    step: 200; loss: 2.251; l2dist: 1.372\n",
      "    step: 250; loss: 2.091; l2dist: 1.325\n",
      "    step: 300; loss: 2.023; l2dist: 1.297\n",
      "    step: 350; loss: 1.981; l2dist: 1.289\n",
      "    step: 400; loss: 1.972; l2dist: 1.286\n",
      "    step: 450; loss: 1.936; l2dist: 1.276\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.333; l2dist: 0.000\n",
      "    step: 50; loss: 7.242; l2dist: 2.215\n",
      "    step: 100; loss: 3.164; l2dist: 1.616\n",
      "    step: 150; loss: 2.357; l2dist: 1.398\n",
      "    step: 200; loss: 2.091; l2dist: 1.325\n",
      "    step: 250; loss: 2.026; l2dist: 1.303\n",
      "    step: 300; loss: 1.939; l2dist: 1.279\n",
      "    step: 350; loss: 1.913; l2dist: 1.266\n",
      "    step: 400; loss: 1.914; l2dist: 1.258\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.131; l2dist: 0.000\n",
      "    step: 50; loss: 6.667; l2dist: 2.097\n",
      "    step: 100; loss: 3.036; l2dist: 1.557\n",
      "    step: 150; loss: 2.262; l2dist: 1.358\n",
      "    step: 200; loss: 2.045; l2dist: 1.296\n",
      "    step: 250; loss: 1.953; l2dist: 1.273\n",
      "    step: 300; loss: 1.898; l2dist: 1.258\n",
      "    step: 350; loss: 1.884; l2dist: 1.253\n",
      "    step: 400; loss: 1.862; l2dist: 1.242\n",
      "    step: 450; loss: 1.855; l2dist: 1.251\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.045; l2dist: 0.000\n",
      "    step: 50; loss: 6.481; l2dist: 2.031\n",
      "    step: 100; loss: 2.936; l2dist: 1.498\n",
      "    step: 150; loss: 2.227; l2dist: 1.334\n",
      "    step: 200; loss: 2.016; l2dist: 1.280\n",
      "    step: 250; loss: 1.927; l2dist: 1.254\n",
      "    step: 300; loss: 1.867; l2dist: 1.239\n",
      "    step: 350; loss: 1.854; l2dist: 1.229\n",
      "    step: 400; loss: 1.849; l2dist: 1.227\n",
      "    step: 450; loss: 1.825; l2dist: 1.225\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.320; l2dist: 0.000\n",
      "    step: 50; loss: 6.435; l2dist: 2.014\n",
      "    step: 100; loss: 2.975; l2dist: 1.505\n",
      "    step: 150; loss: 2.248; l2dist: 1.342\n",
      "    step: 200; loss: 2.036; l2dist: 1.283\n",
      "    step: 250; loss: 1.957; l2dist: 1.261\n",
      "    step: 300; loss: 1.902; l2dist: 1.246\n",
      "    step: 350; loss: 1.888; l2dist: 1.239\n",
      "    step: 400; loss: 1.866; l2dist: 1.234\n",
      "    step: 450; loss: 1.878; l2dist: 1.231\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.406; l2dist: 0.000\n",
      "    step: 50; loss: 6.352; l2dist: 1.996\n",
      "    step: 100; loss: 2.942; l2dist: 1.480\n",
      "    step: 150; loss: 2.233; l2dist: 1.323\n",
      "    step: 200; loss: 2.021; l2dist: 1.269\n",
      "    step: 250; loss: 1.936; l2dist: 1.247\n",
      "    step: 300; loss: 1.890; l2dist: 1.236\n",
      "    step: 350; loss: 1.874; l2dist: 1.233\n",
      "    step: 400; loss: 1.855; l2dist: 1.227\n",
      "    step: 450; loss: 1.855; l2dist: 1.222\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.970; l2dist: 0.000\n",
      "    step: 50; loss: 6.332; l2dist: 1.994\n",
      "    step: 100; loss: 2.944; l2dist: 1.480\n",
      "    step: 150; loss: 2.230; l2dist: 1.325\n",
      "    step: 200; loss: 2.026; l2dist: 1.264\n",
      "    step: 250; loss: 1.936; l2dist: 1.241\n",
      "    step: 300; loss: 1.897; l2dist: 1.234\n",
      "    step: 350; loss: 1.867; l2dist: 1.229\n",
      "    step: 400; loss: 1.863; l2dist: 1.223\n",
      "    step: 450; loss: 1.885; l2dist: 1.232\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.098; l2dist: 0.000\n",
      "    step: 50; loss: 6.352; l2dist: 1.998\n",
      "    step: 100; loss: 2.953; l2dist: 1.486\n",
      "    step: 150; loss: 2.234; l2dist: 1.327\n",
      "    step: 200; loss: 2.030; l2dist: 1.277\n",
      "    step: 250; loss: 1.943; l2dist: 1.249\n",
      "    step: 300; loss: 1.893; l2dist: 1.237\n",
      "    step: 350; loss: 1.877; l2dist: 1.231\n",
      "    step: 400; loss: 1.868; l2dist: 1.229\n",
      "    step: 450; loss: 1.850; l2dist: 1.226\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 127.263; l2dist: 0.000\n",
      "    step: 50; loss: 15.075; l2dist: 3.569\n",
      "    step: 100; loss: 6.444; l2dist: 2.454\n",
      "    step: 150; loss: 4.264; l2dist: 1.973\n",
      "    step: 200; loss: 3.317; l2dist: 1.724\n",
      "    step: 250; loss: 2.940; l2dist: 1.621\n",
      "    step: 300; loss: 2.749; l2dist: 1.568\n",
      "    step: 350; loss: 2.641; l2dist: 1.521\n",
      "    step: 400; loss: 2.606; l2dist: 1.509\n",
      "    step: 450; loss: 2.558; l2dist: 1.492\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 89.852; l2dist: 0.000\n",
      "    step: 50; loss: 12.278; l2dist: 3.160\n",
      "    step: 100; loss: 4.990; l2dist: 2.109\n",
      "    step: 150; loss: 3.406; l2dist: 1.729\n",
      "    step: 200; loss: 2.901; l2dist: 1.593\n",
      "    step: 250; loss: 2.642; l2dist: 1.514\n",
      "    step: 300; loss: 2.542; l2dist: 1.484\n",
      "    step: 350; loss: 2.492; l2dist: 1.470\n",
      "    step: 400; loss: 2.486; l2dist: 1.472\n",
      "    step: 450; loss: 2.430; l2dist: 1.463\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.959; l2dist: 0.000\n",
      "    step: 50; loss: 9.876; l2dist: 2.723\n",
      "    step: 100; loss: 3.906; l2dist: 1.861\n",
      "    step: 150; loss: 2.880; l2dist: 1.593\n",
      "    step: 200; loss: 2.574; l2dist: 1.498\n",
      "    step: 250; loss: 2.463; l2dist: 1.467\n",
      "    step: 300; loss: 2.417; l2dist: 1.444\n",
      "    step: 350; loss: 2.385; l2dist: 1.441\n",
      "    step: 400; loss: 2.363; l2dist: 1.432\n",
      "    step: 450; loss: 2.356; l2dist: 1.433\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.787; l2dist: 0.000\n",
      "    step: 50; loss: 8.546; l2dist: 2.450\n",
      "    step: 100; loss: 3.624; l2dist: 1.774\n",
      "    step: 150; loss: 2.741; l2dist: 1.544\n",
      "    step: 200; loss: 2.513; l2dist: 1.478\n",
      "    step: 250; loss: 2.449; l2dist: 1.451\n",
      "    step: 300; loss: 2.393; l2dist: 1.440\n",
      "    step: 350; loss: 2.355; l2dist: 1.426\n",
      "    step: 400; loss: 2.342; l2dist: 1.422\n",
      "    step: 450; loss: 2.309; l2dist: 1.421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.656; l2dist: 0.000\n",
      "    step: 50; loss: 7.630; l2dist: 2.216\n",
      "    step: 100; loss: 3.577; l2dist: 1.715\n",
      "    step: 150; loss: 2.696; l2dist: 1.516\n",
      "    step: 200; loss: 2.476; l2dist: 1.449\n",
      "    step: 250; loss: 2.400; l2dist: 1.428\n",
      "    step: 300; loss: 2.355; l2dist: 1.415\n",
      "    step: 350; loss: 2.321; l2dist: 1.411\n",
      "    step: 400; loss: 2.294; l2dist: 1.407\n",
      "    step: 450; loss: 2.281; l2dist: 1.403\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.096; l2dist: 0.000\n",
      "    step: 50; loss: 7.393; l2dist: 2.162\n",
      "    step: 100; loss: 3.517; l2dist: 1.673\n",
      "    step: 150; loss: 2.670; l2dist: 1.482\n",
      "    step: 200; loss: 2.457; l2dist: 1.431\n",
      "    step: 250; loss: 2.381; l2dist: 1.412\n",
      "    step: 300; loss: 2.327; l2dist: 1.397\n",
      "    step: 350; loss: 2.307; l2dist: 1.395\n",
      "    step: 400; loss: 2.296; l2dist: 1.391\n",
      "    step: 450; loss: 2.275; l2dist: 1.387\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.339; l2dist: 0.000\n",
      "    step: 50; loss: 7.313; l2dist: 2.117\n",
      "    step: 100; loss: 3.529; l2dist: 1.664\n",
      "    step: 150; loss: 2.669; l2dist: 1.467\n",
      "    step: 200; loss: 2.448; l2dist: 1.424\n",
      "    step: 250; loss: 2.361; l2dist: 1.404\n",
      "    step: 300; loss: 2.317; l2dist: 1.391\n",
      "    step: 350; loss: 2.298; l2dist: 1.389\n",
      "    step: 400; loss: 2.283; l2dist: 1.385\n",
      "    step: 450; loss: 2.278; l2dist: 1.381\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.044; l2dist: 0.000\n",
      "    step: 50; loss: 7.260; l2dist: 2.101\n",
      "    step: 100; loss: 3.518; l2dist: 1.648\n",
      "    step: 150; loss: 2.665; l2dist: 1.464\n",
      "    step: 200; loss: 2.451; l2dist: 1.414\n",
      "    step: 250; loss: 2.368; l2dist: 1.395\n",
      "    step: 300; loss: 2.323; l2dist: 1.386\n",
      "    step: 350; loss: 2.309; l2dist: 1.377\n",
      "    step: 400; loss: 2.287; l2dist: 1.376\n",
      "    step: 450; loss: 2.268; l2dist: 1.373\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.915; l2dist: 0.000\n",
      "    step: 50; loss: 7.248; l2dist: 2.091\n",
      "    step: 100; loss: 3.521; l2dist: 1.643\n",
      "    step: 150; loss: 2.659; l2dist: 1.463\n",
      "    step: 200; loss: 2.435; l2dist: 1.411\n",
      "    step: 250; loss: 2.360; l2dist: 1.390\n",
      "    step: 300; loss: 2.304; l2dist: 1.383\n",
      "    step: 350; loss: 2.303; l2dist: 1.381\n",
      "    step: 400; loss: 2.284; l2dist: 1.372\n",
      "    step: 450; loss: 2.262; l2dist: 1.371\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.181; l2dist: 0.000\n",
      "    step: 50; loss: 7.298; l2dist: 2.106\n",
      "    step: 100; loss: 3.525; l2dist: 1.653\n",
      "    step: 150; loss: 2.662; l2dist: 1.473\n",
      "    step: 200; loss: 2.444; l2dist: 1.416\n",
      "    step: 250; loss: 2.367; l2dist: 1.397\n",
      "    step: 300; loss: 2.314; l2dist: 1.390\n",
      "    step: 350; loss: 2.286; l2dist: 1.384\n",
      "    step: 400; loss: 2.276; l2dist: 1.377\n",
      "    step: 450; loss: 2.266; l2dist: 1.376\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 124.539; l2dist: 0.000\n",
      "    step: 50; loss: 12.232; l2dist: 3.315\n",
      "    step: 100; loss: 5.570; l2dist: 2.282\n",
      "    step: 150; loss: 3.644; l2dist: 1.835\n",
      "    step: 200; loss: 2.912; l2dist: 1.629\n",
      "    step: 250; loss: 2.565; l2dist: 1.532\n",
      "    step: 300; loss: 2.410; l2dist: 1.479\n",
      "    step: 350; loss: 2.341; l2dist: 1.456\n",
      "    step: 400; loss: 2.279; l2dist: 1.437\n",
      "    step: 450; loss: 2.262; l2dist: 1.427\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 92.292; l2dist: 0.000\n",
      "    step: 50; loss: 9.990; l2dist: 2.968\n",
      "    step: 100; loss: 4.356; l2dist: 1.989\n",
      "    step: 150; loss: 3.042; l2dist: 1.652\n",
      "    step: 200; loss: 2.571; l2dist: 1.517\n",
      "    step: 250; loss: 2.371; l2dist: 1.450\n",
      "    step: 300; loss: 2.252; l2dist: 1.419\n",
      "    step: 350; loss: 2.205; l2dist: 1.412\n",
      "    step: 400; loss: 2.154; l2dist: 1.395\n",
      "    step: 450; loss: 2.134; l2dist: 1.387\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.510; l2dist: 0.000\n",
      "    step: 50; loss: 8.018; l2dist: 2.573\n",
      "    step: 100; loss: 3.474; l2dist: 1.774\n",
      "    step: 150; loss: 2.577; l2dist: 1.519\n",
      "    step: 200; loss: 2.300; l2dist: 1.435\n",
      "    step: 250; loss: 2.178; l2dist: 1.396\n",
      "    step: 300; loss: 2.125; l2dist: 1.379\n",
      "    step: 350; loss: 2.106; l2dist: 1.375\n",
      "    step: 400; loss: 2.105; l2dist: 1.367\n",
      "    step: 450; loss: 2.075; l2dist: 1.362\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.205; l2dist: 0.000\n",
      "    step: 50; loss: 6.829; l2dist: 2.295\n",
      "    step: 100; loss: 3.210; l2dist: 1.697\n",
      "    step: 150; loss: 2.399; l2dist: 1.472\n",
      "    step: 200; loss: 2.177; l2dist: 1.397\n",
      "    step: 250; loss: 2.105; l2dist: 1.370\n",
      "    step: 300; loss: 2.061; l2dist: 1.359\n",
      "    step: 350; loss: 2.035; l2dist: 1.347\n",
      "    step: 400; loss: 2.032; l2dist: 1.353\n",
      "    step: 450; loss: 2.030; l2dist: 1.345\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.941; l2dist: 0.000\n",
      "    step: 50; loss: 6.192; l2dist: 2.092\n",
      "    step: 100; loss: 3.156; l2dist: 1.620\n",
      "    step: 150; loss: 2.345; l2dist: 1.431\n",
      "    step: 200; loss: 2.153; l2dist: 1.381\n",
      "    step: 250; loss: 2.084; l2dist: 1.354\n",
      "    step: 300; loss: 2.045; l2dist: 1.343\n",
      "    step: 350; loss: 2.045; l2dist: 1.339\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.240; l2dist: 0.000\n",
      "    step: 50; loss: 5.865; l2dist: 1.990\n",
      "    step: 100; loss: 3.130; l2dist: 1.571\n",
      "    step: 150; loss: 2.340; l2dist: 1.396\n",
      "    step: 200; loss: 2.153; l2dist: 1.346\n",
      "    step: 250; loss: 2.089; l2dist: 1.337\n",
      "    step: 300; loss: 2.048; l2dist: 1.327\n",
      "    step: 350; loss: 2.023; l2dist: 1.320\n",
      "    step: 400; loss: 2.020; l2dist: 1.313\n",
      "    step: 450; loss: 2.010; l2dist: 1.316\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.858; l2dist: 0.000\n",
      "    step: 50; loss: 5.780; l2dist: 1.969\n",
      "    step: 100; loss: 3.122; l2dist: 1.564\n",
      "    step: 150; loss: 2.310; l2dist: 1.388\n",
      "    step: 200; loss: 2.130; l2dist: 1.346\n",
      "    step: 250; loss: 2.058; l2dist: 1.320\n",
      "    step: 300; loss: 2.026; l2dist: 1.320\n",
      "    step: 350; loss: 2.020; l2dist: 1.317\n",
      "    step: 400; loss: 2.004; l2dist: 1.312\n",
      "    step: 450; loss: 1.990; l2dist: 1.308\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.235; l2dist: 0.000\n",
      "    step: 50; loss: 5.780; l2dist: 1.971\n",
      "    step: 100; loss: 3.133; l2dist: 1.569\n",
      "    step: 150; loss: 2.314; l2dist: 1.396\n",
      "    step: 200; loss: 2.138; l2dist: 1.353\n",
      "    step: 250; loss: 2.078; l2dist: 1.333\n",
      "    step: 300; loss: 2.026; l2dist: 1.329\n",
      "    step: 350; loss: 2.016; l2dist: 1.320\n",
      "    step: 400; loss: 2.007; l2dist: 1.322\n",
      "    step: 450; loss: 2.001; l2dist: 1.317\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.898; l2dist: 0.000\n",
      "    step: 50; loss: 5.762; l2dist: 1.965\n",
      "    step: 100; loss: 3.137; l2dist: 1.565\n",
      "    step: 150; loss: 2.324; l2dist: 1.399\n",
      "    step: 200; loss: 2.146; l2dist: 1.347\n",
      "    step: 250; loss: 2.065; l2dist: 1.331\n",
      "    step: 300; loss: 2.044; l2dist: 1.328\n",
      "    step: 350; loss: 2.017; l2dist: 1.319\n",
      "    step: 400; loss: 2.003; l2dist: 1.318\n",
      "    step: 450; loss: 2.008; l2dist: 1.316\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.192; l2dist: 0.000\n",
      "    step: 50; loss: 5.803; l2dist: 1.981\n",
      "    step: 100; loss: 3.140; l2dist: 1.571\n",
      "    step: 150; loss: 2.330; l2dist: 1.399\n",
      "    step: 200; loss: 2.143; l2dist: 1.355\n",
      "    step: 250; loss: 2.076; l2dist: 1.334\n",
      "    step: 300; loss: 2.044; l2dist: 1.331\n",
      "    step: 350; loss: 2.026; l2dist: 1.332\n",
      "    step: 400; loss: 2.020; l2dist: 1.323\n",
      "    step: 450; loss: 2.002; l2dist: 1.323\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.156; l2dist: 0.000\n",
      "    step: 50; loss: 11.613; l2dist: 3.165\n",
      "    step: 100; loss: 4.936; l2dist: 2.134\n",
      "    step: 150; loss: 3.292; l2dist: 1.724\n",
      "    step: 200; loss: 2.641; l2dist: 1.533\n",
      "    step: 250; loss: 2.388; l2dist: 1.455\n",
      "    step: 300; loss: 2.288; l2dist: 1.426\n",
      "    step: 350; loss: 2.201; l2dist: 1.392\n",
      "    step: 400; loss: 2.215; l2dist: 1.398\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 82.744; l2dist: 0.000\n",
      "    step: 50; loss: 9.608; l2dist: 2.790\n",
      "    step: 100; loss: 4.047; l2dist: 1.878\n",
      "    step: 150; loss: 2.883; l2dist: 1.577\n",
      "    step: 200; loss: 2.451; l2dist: 1.456\n",
      "    step: 250; loss: 2.256; l2dist: 1.392\n",
      "    step: 300; loss: 2.202; l2dist: 1.379\n",
      "    step: 350; loss: 2.150; l2dist: 1.362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 2.124; l2dist: 1.349\n",
      "    step: 450; loss: 2.084; l2dist: 1.341\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.535; l2dist: 0.000\n",
      "    step: 50; loss: 7.565; l2dist: 2.391\n",
      "    step: 100; loss: 3.301; l2dist: 1.689\n",
      "    step: 150; loss: 2.475; l2dist: 1.454\n",
      "    step: 200; loss: 2.230; l2dist: 1.387\n",
      "    step: 250; loss: 2.112; l2dist: 1.357\n",
      "    step: 300; loss: 2.079; l2dist: 1.333\n",
      "    step: 350; loss: 2.030; l2dist: 1.324\n",
      "    step: 400; loss: 2.011; l2dist: 1.321\n",
      "    step: 450; loss: 1.995; l2dist: 1.317\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.521; l2dist: 0.000\n",
      "    step: 50; loss: 6.198; l2dist: 2.054\n",
      "    step: 100; loss: 3.072; l2dist: 1.616\n",
      "    step: 150; loss: 2.317; l2dist: 1.409\n",
      "    step: 200; loss: 2.127; l2dist: 1.353\n",
      "    step: 250; loss: 2.055; l2dist: 1.325\n",
      "    step: 300; loss: 2.017; l2dist: 1.325\n",
      "    step: 350; loss: 1.974; l2dist: 1.306\n",
      "    step: 400; loss: 1.968; l2dist: 1.297\n",
      "    step: 450; loss: 1.939; l2dist: 1.293\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.471; l2dist: 0.000\n",
      "    step: 50; loss: 5.347; l2dist: 1.801\n",
      "    step: 100; loss: 3.037; l2dist: 1.536\n",
      "    step: 150; loss: 2.260; l2dist: 1.373\n",
      "    step: 200; loss: 2.048; l2dist: 1.320\n",
      "    step: 250; loss: 1.982; l2dist: 1.300\n",
      "    step: 300; loss: 1.933; l2dist: 1.288\n",
      "    step: 350; loss: 1.916; l2dist: 1.279\n",
      "    step: 400; loss: 1.901; l2dist: 1.275\n",
      "    step: 450; loss: 1.919; l2dist: 1.276\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.014; l2dist: 0.000\n",
      "    step: 50; loss: 4.946; l2dist: 1.685\n",
      "    step: 100; loss: 3.041; l2dist: 1.465\n",
      "    step: 150; loss: 2.283; l2dist: 1.334\n",
      "    step: 200; loss: 2.085; l2dist: 1.297\n",
      "    step: 250; loss: 1.983; l2dist: 1.274\n",
      "    step: 300; loss: 1.945; l2dist: 1.267\n",
      "    step: 350; loss: 1.914; l2dist: 1.264\n",
      "    step: 400; loss: 1.915; l2dist: 1.261\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.156; l2dist: 0.000\n",
      "    step: 50; loss: 4.991; l2dist: 1.693\n",
      "    step: 100; loss: 3.061; l2dist: 1.483\n",
      "    step: 150; loss: 2.278; l2dist: 1.349\n",
      "    step: 200; loss: 2.079; l2dist: 1.309\n",
      "    step: 250; loss: 1.981; l2dist: 1.285\n",
      "    step: 300; loss: 1.940; l2dist: 1.279\n",
      "    step: 350; loss: 1.923; l2dist: 1.270\n",
      "    step: 400; loss: 1.931; l2dist: 1.269\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.639; l2dist: 0.000\n",
      "    step: 50; loss: 4.954; l2dist: 1.678\n",
      "    step: 100; loss: 3.067; l2dist: 1.482\n",
      "    step: 150; loss: 2.280; l2dist: 1.343\n",
      "    step: 200; loss: 2.077; l2dist: 1.306\n",
      "    step: 250; loss: 1.983; l2dist: 1.283\n",
      "    step: 300; loss: 1.950; l2dist: 1.273\n",
      "    step: 350; loss: 1.922; l2dist: 1.271\n",
      "    step: 400; loss: 1.906; l2dist: 1.269\n",
      "    step: 450; loss: 1.903; l2dist: 1.264\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.565; l2dist: 0.000\n",
      "    step: 50; loss: 4.931; l2dist: 1.675\n",
      "    step: 100; loss: 3.072; l2dist: 1.472\n",
      "    step: 150; loss: 2.286; l2dist: 1.343\n",
      "    step: 200; loss: 2.079; l2dist: 1.304\n",
      "    step: 250; loss: 1.982; l2dist: 1.279\n",
      "    step: 300; loss: 1.949; l2dist: 1.283\n",
      "    step: 350; loss: 1.924; l2dist: 1.268\n",
      "    step: 400; loss: 1.906; l2dist: 1.266\n",
      "    step: 450; loss: 1.912; l2dist: 1.262\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.778; l2dist: 0.000\n",
      "    step: 50; loss: 4.985; l2dist: 1.695\n",
      "    step: 100; loss: 3.080; l2dist: 1.489\n",
      "    step: 150; loss: 2.285; l2dist: 1.353\n",
      "    step: 200; loss: 2.082; l2dist: 1.310\n",
      "    step: 250; loss: 1.984; l2dist: 1.285\n",
      "    step: 300; loss: 1.950; l2dist: 1.280\n",
      "    step: 350; loss: 1.925; l2dist: 1.275\n",
      "    step: 400; loss: 1.916; l2dist: 1.266\n",
      "    step: 450; loss: 1.898; l2dist: 1.270\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.986; l2dist: 0.000\n",
      "    step: 50; loss: 11.223; l2dist: 3.190\n",
      "    step: 100; loss: 5.034; l2dist: 2.164\n",
      "    step: 150; loss: 3.342; l2dist: 1.748\n",
      "    step: 200; loss: 2.694; l2dist: 1.547\n",
      "    step: 250; loss: 2.475; l2dist: 1.482\n",
      "    step: 300; loss: 2.359; l2dist: 1.434\n",
      "    step: 350; loss: 2.256; l2dist: 1.402\n",
      "    step: 400; loss: 2.235; l2dist: 1.410\n",
      "    step: 450; loss: 2.210; l2dist: 1.395\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 104.853; l2dist: 0.000\n",
      "    step: 50; loss: 9.879; l2dist: 2.925\n",
      "    step: 100; loss: 4.347; l2dist: 1.964\n",
      "    step: 150; loss: 3.015; l2dist: 1.616\n",
      "    step: 200; loss: 2.536; l2dist: 1.478\n",
      "    step: 250; loss: 2.348; l2dist: 1.428\n",
      "    step: 300; loss: 2.267; l2dist: 1.403\n",
      "    step: 350; loss: 2.204; l2dist: 1.386\n",
      "    step: 400; loss: 2.130; l2dist: 1.364\n",
      "    step: 450; loss: 2.131; l2dist: 1.368\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.360; l2dist: 0.000\n",
      "    step: 50; loss: 7.461; l2dist: 2.443\n",
      "    step: 100; loss: 3.326; l2dist: 1.714\n",
      "    step: 150; loss: 2.498; l2dist: 1.476\n",
      "    step: 200; loss: 2.257; l2dist: 1.402\n",
      "    step: 250; loss: 2.161; l2dist: 1.369\n",
      "    step: 300; loss: 2.092; l2dist: 1.347\n",
      "    step: 350; loss: 2.069; l2dist: 1.343\n",
      "    step: 400; loss: 2.043; l2dist: 1.338\n",
      "    step: 450; loss: 2.036; l2dist: 1.331\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.217; l2dist: 0.000\n",
      "    step: 50; loss: 6.211; l2dist: 2.160\n",
      "    step: 100; loss: 3.057; l2dist: 1.618\n",
      "    step: 150; loss: 2.338; l2dist: 1.423\n",
      "    step: 200; loss: 2.152; l2dist: 1.366\n",
      "    step: 250; loss: 2.066; l2dist: 1.344\n",
      "    step: 300; loss: 2.028; l2dist: 1.324\n",
      "    step: 350; loss: 2.007; l2dist: 1.325\n",
      "    step: 400; loss: 1.993; l2dist: 1.320\n",
      "    step: 450; loss: 1.977; l2dist: 1.313\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.841; l2dist: 0.000\n",
      "    step: 50; loss: 5.631; l2dist: 1.981\n",
      "    step: 100; loss: 2.987; l2dist: 1.541\n",
      "    step: 150; loss: 2.293; l2dist: 1.383\n",
      "    step: 200; loss: 2.114; l2dist: 1.331\n",
      "    step: 250; loss: 2.038; l2dist: 1.313\n",
      "    step: 300; loss: 1.999; l2dist: 1.304\n",
      "    step: 350; loss: 1.996; l2dist: 1.301\n",
      "    step: 400; loss: 1.969; l2dist: 1.297\n",
      "    step: 450; loss: 1.961; l2dist: 1.288\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.385; l2dist: 0.000\n",
      "    step: 50; loss: 5.343; l2dist: 1.869\n",
      "    step: 100; loss: 2.976; l2dist: 1.505\n",
      "    step: 150; loss: 2.286; l2dist: 1.357\n",
      "    step: 200; loss: 2.117; l2dist: 1.319\n",
      "    step: 250; loss: 2.041; l2dist: 1.301\n",
      "    step: 300; loss: 2.006; l2dist: 1.295\n",
      "    step: 350; loss: 1.994; l2dist: 1.284\n",
      "    step: 400; loss: 1.972; l2dist: 1.284\n",
      "    step: 450; loss: 1.969; l2dist: 1.279\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.049; l2dist: 0.000\n",
      "    step: 50; loss: 5.180; l2dist: 1.806\n",
      "    step: 100; loss: 2.964; l2dist: 1.479\n",
      "    step: 150; loss: 2.265; l2dist: 1.338\n",
      "    step: 200; loss: 2.095; l2dist: 1.304\n",
      "    step: 250; loss: 2.029; l2dist: 1.284\n",
      "    step: 300; loss: 1.998; l2dist: 1.280\n",
      "    step: 350; loss: 1.972; l2dist: 1.276\n",
      "    step: 400; loss: 1.963; l2dist: 1.274\n",
      "    step: 450; loss: 1.954; l2dist: 1.274\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.499; l2dist: 0.000\n",
      "    step: 50; loss: 5.140; l2dist: 1.796\n",
      "    step: 100; loss: 2.976; l2dist: 1.471\n",
      "    step: 150; loss: 2.279; l2dist: 1.339\n",
      "    step: 200; loss: 2.114; l2dist: 1.297\n",
      "    step: 250; loss: 2.044; l2dist: 1.290\n",
      "    step: 300; loss: 2.009; l2dist: 1.281\n",
      "    step: 350; loss: 1.997; l2dist: 1.281\n",
      "    step: 400; loss: 1.977; l2dist: 1.278\n",
      "    step: 450; loss: 1.964; l2dist: 1.278\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.095; l2dist: 0.000\n",
      "    step: 50; loss: 5.128; l2dist: 1.790\n",
      "    step: 100; loss: 2.962; l2dist: 1.471\n",
      "    step: 150; loss: 2.274; l2dist: 1.333\n",
      "    step: 200; loss: 2.100; l2dist: 1.301\n",
      "    step: 250; loss: 2.029; l2dist: 1.283\n",
      "    step: 300; loss: 2.000; l2dist: 1.284\n",
      "    step: 350; loss: 1.970; l2dist: 1.270\n",
      "    step: 400; loss: 1.970; l2dist: 1.273\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.291; l2dist: 0.000\n",
      "    step: 50; loss: 5.172; l2dist: 1.806\n",
      "    step: 100; loss: 2.966; l2dist: 1.485\n",
      "    step: 150; loss: 2.267; l2dist: 1.342\n",
      "    step: 200; loss: 2.104; l2dist: 1.306\n",
      "    step: 250; loss: 2.028; l2dist: 1.290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 1.993; l2dist: 1.284\n",
      "    step: 350; loss: 1.975; l2dist: 1.285\n",
      "    step: 400; loss: 1.964; l2dist: 1.281\n",
      "    step: 450; loss: 1.962; l2dist: 1.276\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.943; l2dist: 0.000\n",
      "    step: 50; loss: 13.248; l2dist: 3.346\n",
      "    step: 100; loss: 5.697; l2dist: 2.289\n",
      "    step: 150; loss: 3.773; l2dist: 1.846\n",
      "    step: 200; loss: 2.979; l2dist: 1.631\n",
      "    step: 250; loss: 2.624; l2dist: 1.527\n",
      "    step: 300; loss: 2.472; l2dist: 1.479\n",
      "    step: 350; loss: 2.406; l2dist: 1.447\n",
      "    step: 400; loss: 2.346; l2dist: 1.443\n",
      "    step: 450; loss: 2.335; l2dist: 1.419\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 72.482; l2dist: 0.000\n",
      "    step: 50; loss: 10.147; l2dist: 2.863\n",
      "    step: 100; loss: 4.136; l2dist: 1.919\n",
      "    step: 150; loss: 2.910; l2dist: 1.604\n",
      "    step: 200; loss: 2.500; l2dist: 1.478\n",
      "    step: 250; loss: 2.335; l2dist: 1.425\n",
      "    step: 300; loss: 2.246; l2dist: 1.406\n",
      "    step: 350; loss: 2.232; l2dist: 1.396\n",
      "    step: 400; loss: 2.185; l2dist: 1.379\n",
      "    step: 450; loss: 2.159; l2dist: 1.372\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.560; l2dist: 0.000\n",
      "    step: 50; loss: 7.910; l2dist: 2.455\n",
      "    step: 100; loss: 3.398; l2dist: 1.735\n",
      "    step: 150; loss: 2.525; l2dist: 1.491\n",
      "    step: 200; loss: 2.280; l2dist: 1.413\n",
      "    step: 250; loss: 2.199; l2dist: 1.382\n",
      "    step: 300; loss: 2.141; l2dist: 1.374\n",
      "    step: 350; loss: 2.128; l2dist: 1.358\n",
      "    step: 400; loss: 2.104; l2dist: 1.360\n",
      "    step: 450; loss: 2.120; l2dist: 1.349\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.158; l2dist: 0.000\n",
      "    step: 50; loss: 6.903; l2dist: 2.221\n",
      "    step: 100; loss: 3.197; l2dist: 1.668\n",
      "    step: 150; loss: 2.410; l2dist: 1.450\n",
      "    step: 200; loss: 2.216; l2dist: 1.390\n",
      "    step: 250; loss: 2.129; l2dist: 1.357\n",
      "    step: 300; loss: 2.093; l2dist: 1.346\n",
      "    step: 350; loss: 2.060; l2dist: 1.339\n",
      "    step: 400; loss: 2.058; l2dist: 1.343\n",
      "    step: 450; loss: 2.049; l2dist: 1.336\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.931; l2dist: 0.000\n",
      "    step: 50; loss: 6.349; l2dist: 2.063\n",
      "    step: 100; loss: 3.142; l2dist: 1.605\n",
      "    step: 150; loss: 2.377; l2dist: 1.419\n",
      "    step: 200; loss: 2.173; l2dist: 1.358\n",
      "    step: 250; loss: 2.098; l2dist: 1.344\n",
      "    step: 300; loss: 2.061; l2dist: 1.327\n",
      "    step: 350; loss: 2.034; l2dist: 1.325\n",
      "    step: 400; loss: 2.032; l2dist: 1.316\n",
      "    step: 450; loss: 2.019; l2dist: 1.314\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.035; l2dist: 0.000\n",
      "    step: 50; loss: 6.092; l2dist: 1.980\n",
      "    step: 100; loss: 3.108; l2dist: 1.580\n",
      "    step: 150; loss: 2.343; l2dist: 1.393\n",
      "    step: 200; loss: 2.152; l2dist: 1.343\n",
      "    step: 250; loss: 2.084; l2dist: 1.327\n",
      "    step: 300; loss: 2.055; l2dist: 1.310\n",
      "    step: 350; loss: 2.024; l2dist: 1.308\n",
      "    step: 400; loss: 2.028; l2dist: 1.305\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.423; l2dist: 0.000\n",
      "    step: 50; loss: 6.057; l2dist: 1.968\n",
      "    step: 100; loss: 3.095; l2dist: 1.570\n",
      "    step: 150; loss: 2.341; l2dist: 1.388\n",
      "    step: 200; loss: 2.151; l2dist: 1.340\n",
      "    step: 250; loss: 2.086; l2dist: 1.321\n",
      "    step: 300; loss: 2.053; l2dist: 1.306\n",
      "    step: 350; loss: 2.042; l2dist: 1.309\n",
      "    step: 400; loss: 2.033; l2dist: 1.305\n",
      "    step: 450; loss: 2.021; l2dist: 1.298\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.078; l2dist: 0.000\n",
      "    step: 50; loss: 6.046; l2dist: 1.963\n",
      "    step: 100; loss: 3.097; l2dist: 1.566\n",
      "    step: 150; loss: 2.336; l2dist: 1.385\n",
      "    step: 200; loss: 2.155; l2dist: 1.338\n",
      "    step: 250; loss: 2.076; l2dist: 1.319\n",
      "    step: 300; loss: 2.063; l2dist: 1.311\n",
      "    step: 350; loss: 2.028; l2dist: 1.304\n",
      "    step: 400; loss: 2.015; l2dist: 1.304\n",
      "    step: 450; loss: 2.017; l2dist: 1.305\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.845; l2dist: 0.000\n",
      "    step: 50; loss: 6.012; l2dist: 1.957\n",
      "    step: 100; loss: 3.102; l2dist: 1.573\n",
      "    step: 150; loss: 2.332; l2dist: 1.383\n",
      "    step: 200; loss: 2.157; l2dist: 1.334\n",
      "    step: 250; loss: 2.078; l2dist: 1.321\n",
      "    step: 300; loss: 2.048; l2dist: 1.313\n",
      "    step: 350; loss: 2.043; l2dist: 1.301\n",
      "    step: 400; loss: 2.022; l2dist: 1.305\n",
      "    step: 450; loss: 2.014; l2dist: 1.300\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.970; l2dist: 0.000\n",
      "    step: 50; loss: 6.038; l2dist: 1.965\n",
      "    step: 100; loss: 3.103; l2dist: 1.575\n",
      "    step: 150; loss: 2.340; l2dist: 1.391\n",
      "    step: 200; loss: 2.153; l2dist: 1.342\n",
      "    step: 250; loss: 2.083; l2dist: 1.329\n",
      "    step: 300; loss: 2.060; l2dist: 1.314\n",
      "    step: 350; loss: 2.030; l2dist: 1.312\n",
      "    step: 400; loss: 2.028; l2dist: 1.314\n",
      "    step: 450; loss: 2.025; l2dist: 1.302\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 116.750; l2dist: 0.000\n",
      "    step: 50; loss: 12.413; l2dist: 3.270\n",
      "    step: 100; loss: 5.516; l2dist: 2.240\n",
      "    step: 150; loss: 3.679; l2dist: 1.811\n",
      "    step: 200; loss: 2.951; l2dist: 1.617\n",
      "    step: 250; loss: 2.683; l2dist: 1.529\n",
      "    step: 300; loss: 2.515; l2dist: 1.480\n",
      "    step: 350; loss: 2.435; l2dist: 1.457\n",
      "    step: 400; loss: 2.337; l2dist: 1.420\n",
      "    step: 450; loss: 2.325; l2dist: 1.414\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 94.212; l2dist: 0.000\n",
      "    step: 50; loss: 10.423; l2dist: 2.937\n",
      "    step: 100; loss: 4.564; l2dist: 1.992\n",
      "    step: 150; loss: 3.137; l2dist: 1.643\n",
      "    step: 200; loss: 2.617; l2dist: 1.494\n",
      "    step: 250; loss: 2.406; l2dist: 1.442\n",
      "    step: 300; loss: 2.274; l2dist: 1.399\n",
      "    step: 350; loss: 2.207; l2dist: 1.377\n",
      "    step: 400; loss: 2.164; l2dist: 1.376\n",
      "    step: 450; loss: 2.164; l2dist: 1.372\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.099; l2dist: 0.000\n",
      "    step: 50; loss: 8.424; l2dist: 2.568\n",
      "    step: 100; loss: 3.629; l2dist: 1.779\n",
      "    step: 150; loss: 2.645; l2dist: 1.507\n",
      "    step: 200; loss: 2.332; l2dist: 1.424\n",
      "    step: 250; loss: 2.209; l2dist: 1.379\n",
      "    step: 300; loss: 2.113; l2dist: 1.359\n",
      "    step: 350; loss: 2.092; l2dist: 1.352\n",
      "    step: 400; loss: 2.062; l2dist: 1.344\n",
      "    step: 450; loss: 2.064; l2dist: 1.334\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.302; l2dist: 0.000\n",
      "    step: 50; loss: 7.329; l2dist: 2.314\n",
      "    step: 100; loss: 3.287; l2dist: 1.691\n",
      "    step: 150; loss: 2.452; l2dist: 1.459\n",
      "    step: 200; loss: 2.195; l2dist: 1.386\n",
      "    step: 250; loss: 2.108; l2dist: 1.350\n",
      "    step: 300; loss: 2.069; l2dist: 1.335\n",
      "    step: 350; loss: 2.035; l2dist: 1.335\n",
      "    step: 400; loss: 2.013; l2dist: 1.323\n",
      "    step: 450; loss: 2.011; l2dist: 1.322\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.324; l2dist: 0.000\n",
      "    step: 50; loss: 6.500; l2dist: 2.107\n",
      "    step: 100; loss: 3.167; l2dist: 1.611\n",
      "    step: 150; loss: 2.365; l2dist: 1.413\n",
      "    step: 200; loss: 2.143; l2dist: 1.356\n",
      "    step: 250; loss: 2.050; l2dist: 1.326\n",
      "    step: 300; loss: 2.018; l2dist: 1.313\n",
      "    step: 350; loss: 1.992; l2dist: 1.313\n",
      "    step: 400; loss: 1.984; l2dist: 1.306\n",
      "    step: 450; loss: 1.974; l2dist: 1.300\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.188; l2dist: 0.000\n",
      "    step: 50; loss: 6.174; l2dist: 1.995\n",
      "    step: 100; loss: 3.055; l2dist: 1.531\n",
      "    step: 150; loss: 2.307; l2dist: 1.365\n",
      "    step: 200; loss: 2.112; l2dist: 1.309\n",
      "    step: 250; loss: 2.044; l2dist: 1.304\n",
      "    step: 300; loss: 2.002; l2dist: 1.289\n",
      "    step: 350; loss: 1.973; l2dist: 1.289\n",
      "    step: 400; loss: 1.961; l2dist: 1.282\n",
      "    step: 450; loss: 1.952; l2dist: 1.277\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.765; l2dist: 0.000\n",
      "    step: 50; loss: 6.090; l2dist: 1.966\n",
      "    step: 100; loss: 3.029; l2dist: 1.520\n",
      "    step: 150; loss: 2.279; l2dist: 1.346\n",
      "    step: 200; loss: 2.092; l2dist: 1.298\n",
      "    step: 250; loss: 2.020; l2dist: 1.282\n",
      "    step: 300; loss: 1.980; l2dist: 1.272\n",
      "    step: 350; loss: 1.959; l2dist: 1.268\n",
      "    step: 400; loss: 1.958; l2dist: 1.261\n",
      "    step: 450; loss: 1.964; l2dist: 1.272\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.863; l2dist: 0.000\n",
      "    step: 50; loss: 6.019; l2dist: 1.943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 3.015; l2dist: 1.509\n",
      "    step: 150; loss: 2.271; l2dist: 1.336\n",
      "    step: 200; loss: 2.081; l2dist: 1.289\n",
      "    step: 250; loss: 2.014; l2dist: 1.274\n",
      "    step: 300; loss: 1.985; l2dist: 1.266\n",
      "    step: 350; loss: 1.960; l2dist: 1.264\n",
      "    step: 400; loss: 1.949; l2dist: 1.257\n",
      "    step: 450; loss: 1.948; l2dist: 1.258\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.571; l2dist: 0.000\n",
      "    step: 50; loss: 6.010; l2dist: 1.938\n",
      "    step: 100; loss: 3.016; l2dist: 1.511\n",
      "    step: 150; loss: 2.271; l2dist: 1.335\n",
      "    step: 200; loss: 2.081; l2dist: 1.291\n",
      "    step: 250; loss: 2.008; l2dist: 1.274\n",
      "    step: 300; loss: 1.972; l2dist: 1.270\n",
      "    step: 350; loss: 1.957; l2dist: 1.260\n",
      "    step: 400; loss: 1.953; l2dist: 1.260\n",
      "    step: 450; loss: 1.946; l2dist: 1.262\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.735; l2dist: 0.000\n",
      "    step: 50; loss: 6.050; l2dist: 1.952\n",
      "    step: 100; loss: 3.040; l2dist: 1.520\n",
      "    step: 150; loss: 2.286; l2dist: 1.344\n",
      "    step: 200; loss: 2.096; l2dist: 1.303\n",
      "    step: 250; loss: 2.027; l2dist: 1.286\n",
      "    step: 300; loss: 1.993; l2dist: 1.273\n",
      "    step: 350; loss: 1.981; l2dist: 1.269\n",
      "    step: 400; loss: 1.979; l2dist: 1.266\n",
      "    step: 450; loss: 1.955; l2dist: 1.269\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.575; l2dist: 0.000\n",
      "    step: 50; loss: 13.838; l2dist: 3.467\n",
      "    step: 100; loss: 5.820; l2dist: 2.322\n",
      "    step: 150; loss: 3.891; l2dist: 1.876\n",
      "    step: 200; loss: 3.138; l2dist: 1.677\n",
      "    step: 250; loss: 2.764; l2dist: 1.565\n",
      "    step: 300; loss: 2.674; l2dist: 1.533\n",
      "    step: 350; loss: 2.550; l2dist: 1.498\n",
      "    step: 400; loss: 2.502; l2dist: 1.485\n",
      "    step: 450; loss: 2.500; l2dist: 1.465\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 127.143; l2dist: 0.000\n",
      "    step: 50; loss: 13.463; l2dist: 3.316\n",
      "    step: 100; loss: 5.731; l2dist: 2.248\n",
      "    step: 150; loss: 3.966; l2dist: 1.851\n",
      "    step: 200; loss: 3.251; l2dist: 1.671\n",
      "    step: 250; loss: 2.897; l2dist: 1.576\n",
      "    step: 300; loss: 2.686; l2dist: 1.524\n",
      "    step: 350; loss: 2.615; l2dist: 1.493\n",
      "    step: 400; loss: 2.518; l2dist: 1.473\n",
      "    step: 450; loss: 2.488; l2dist: 1.469\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.669; l2dist: 0.000\n",
      "    step: 50; loss: 10.282; l2dist: 2.826\n",
      "    step: 100; loss: 4.331; l2dist: 1.931\n",
      "    step: 150; loss: 3.141; l2dist: 1.640\n",
      "    step: 200; loss: 2.707; l2dist: 1.528\n",
      "    step: 250; loss: 2.535; l2dist: 1.477\n",
      "    step: 300; loss: 2.402; l2dist: 1.446\n",
      "    step: 350; loss: 2.354; l2dist: 1.431\n",
      "    step: 400; loss: 2.313; l2dist: 1.415\n",
      "    step: 450; loss: 2.319; l2dist: 1.418\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.331; l2dist: 0.000\n",
      "    step: 50; loss: 8.582; l2dist: 2.546\n",
      "    step: 100; loss: 3.785; l2dist: 1.810\n",
      "    step: 150; loss: 2.797; l2dist: 1.557\n",
      "    step: 200; loss: 2.544; l2dist: 1.481\n",
      "    step: 250; loss: 2.399; l2dist: 1.437\n",
      "    step: 300; loss: 2.333; l2dist: 1.419\n",
      "    step: 350; loss: 2.300; l2dist: 1.410\n",
      "    step: 400; loss: 2.275; l2dist: 1.404\n",
      "    step: 450; loss: 2.254; l2dist: 1.398\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.311; l2dist: 0.000\n",
      "    step: 50; loss: 7.677; l2dist: 2.318\n",
      "    step: 100; loss: 3.685; l2dist: 1.707\n",
      "    step: 150; loss: 2.736; l2dist: 1.508\n",
      "    step: 200; loss: 2.502; l2dist: 1.445\n",
      "    step: 250; loss: 2.408; l2dist: 1.425\n",
      "    step: 300; loss: 2.353; l2dist: 1.410\n",
      "    step: 350; loss: 2.337; l2dist: 1.400\n",
      "    step: 400; loss: 2.292; l2dist: 1.392\n",
      "    step: 450; loss: 2.273; l2dist: 1.384\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.813; l2dist: 0.000\n",
      "    step: 50; loss: 7.289; l2dist: 2.206\n",
      "    step: 100; loss: 3.558; l2dist: 1.656\n",
      "    step: 150; loss: 2.719; l2dist: 1.469\n",
      "    step: 200; loss: 2.496; l2dist: 1.422\n",
      "    step: 250; loss: 2.402; l2dist: 1.398\n",
      "    step: 300; loss: 2.346; l2dist: 1.385\n",
      "    step: 350; loss: 2.332; l2dist: 1.376\n",
      "    step: 400; loss: 2.301; l2dist: 1.378\n",
      "    step: 450; loss: 2.292; l2dist: 1.368\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.684; l2dist: 0.000\n",
      "    step: 50; loss: 7.166; l2dist: 2.155\n",
      "    step: 100; loss: 3.515; l2dist: 1.642\n",
      "    step: 150; loss: 2.664; l2dist: 1.457\n",
      "    step: 200; loss: 2.452; l2dist: 1.409\n",
      "    step: 250; loss: 2.368; l2dist: 1.390\n",
      "    step: 300; loss: 2.326; l2dist: 1.378\n",
      "    step: 350; loss: 2.297; l2dist: 1.365\n",
      "    step: 400; loss: 2.287; l2dist: 1.365\n",
      "    step: 450; loss: 2.265; l2dist: 1.362\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.110; l2dist: 0.000\n",
      "    step: 50; loss: 7.133; l2dist: 2.145\n",
      "    step: 100; loss: 3.550; l2dist: 1.653\n",
      "    step: 150; loss: 2.686; l2dist: 1.462\n",
      "    step: 200; loss: 2.469; l2dist: 1.417\n",
      "    step: 250; loss: 2.381; l2dist: 1.396\n",
      "    step: 300; loss: 2.344; l2dist: 1.384\n",
      "    step: 350; loss: 2.324; l2dist: 1.377\n",
      "    step: 400; loss: 2.298; l2dist: 1.375\n",
      "    step: 450; loss: 2.299; l2dist: 1.375\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.585; l2dist: 0.000\n",
      "    step: 50; loss: 7.095; l2dist: 2.133\n",
      "    step: 100; loss: 3.532; l2dist: 1.651\n",
      "    step: 150; loss: 2.669; l2dist: 1.463\n",
      "    step: 200; loss: 2.460; l2dist: 1.420\n",
      "    step: 250; loss: 2.377; l2dist: 1.397\n",
      "    step: 300; loss: 2.346; l2dist: 1.384\n",
      "    step: 350; loss: 2.320; l2dist: 1.385\n",
      "    step: 400; loss: 2.314; l2dist: 1.381\n",
      "    step: 450; loss: 2.289; l2dist: 1.378\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.716; l2dist: 0.000\n",
      "    step: 50; loss: 7.118; l2dist: 2.141\n",
      "    step: 100; loss: 3.535; l2dist: 1.655\n",
      "    step: 150; loss: 2.670; l2dist: 1.471\n",
      "    step: 200; loss: 2.469; l2dist: 1.423\n",
      "    step: 250; loss: 2.378; l2dist: 1.399\n",
      "    step: 300; loss: 2.336; l2dist: 1.390\n",
      "    step: 350; loss: 2.319; l2dist: 1.382\n",
      "    step: 400; loss: 2.311; l2dist: 1.384\n",
      "    step: 450; loss: 2.312; l2dist: 1.383\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 105.994; l2dist: 0.000\n",
      "    step: 50; loss: 10.703; l2dist: 3.011\n",
      "    step: 100; loss: 4.562; l2dist: 2.052\n",
      "    step: 150; loss: 2.994; l2dist: 1.640\n",
      "    step: 200; loss: 2.463; l2dist: 1.480\n",
      "    step: 250; loss: 2.230; l2dist: 1.394\n",
      "    step: 300; loss: 2.121; l2dist: 1.361\n",
      "    step: 350; loss: 2.056; l2dist: 1.334\n",
      "    step: 400; loss: 2.038; l2dist: 1.323\n",
      "    step: 450; loss: 2.039; l2dist: 1.331\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 86.769; l2dist: 0.000\n",
      "    step: 50; loss: 9.280; l2dist: 2.743\n",
      "    step: 100; loss: 3.946; l2dist: 1.857\n",
      "    step: 150; loss: 2.760; l2dist: 1.537\n",
      "    step: 200; loss: 2.309; l2dist: 1.400\n",
      "    step: 250; loss: 2.152; l2dist: 1.343\n",
      "    step: 300; loss: 2.054; l2dist: 1.322\n",
      "    step: 350; loss: 1.989; l2dist: 1.298\n",
      "    step: 400; loss: 1.957; l2dist: 1.290\n",
      "    step: 450; loss: 1.928; l2dist: 1.283\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.438; l2dist: 0.000\n",
      "    step: 50; loss: 7.333; l2dist: 2.390\n",
      "    step: 100; loss: 3.228; l2dist: 1.667\n",
      "    step: 150; loss: 2.340; l2dist: 1.416\n",
      "    step: 200; loss: 2.096; l2dist: 1.334\n",
      "    step: 250; loss: 1.987; l2dist: 1.304\n",
      "    step: 300; loss: 1.933; l2dist: 1.279\n",
      "    step: 350; loss: 1.919; l2dist: 1.274\n",
      "    step: 400; loss: 1.906; l2dist: 1.274\n",
      "    step: 450; loss: 1.906; l2dist: 1.269\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.392; l2dist: 0.000\n",
      "    step: 50; loss: 6.325; l2dist: 2.180\n",
      "    step: 100; loss: 2.928; l2dist: 1.582\n",
      "    step: 150; loss: 2.175; l2dist: 1.366\n",
      "    step: 200; loss: 2.012; l2dist: 1.312\n",
      "    step: 250; loss: 1.908; l2dist: 1.274\n",
      "    step: 300; loss: 1.891; l2dist: 1.256\n",
      "    step: 350; loss: 1.865; l2dist: 1.262\n",
      "    step: 400; loss: 1.863; l2dist: 1.264\n",
      "    step: 450; loss: 1.838; l2dist: 1.252\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.051; l2dist: 0.000\n",
      "    step: 50; loss: 5.719; l2dist: 2.018\n",
      "    step: 100; loss: 2.842; l2dist: 1.523\n",
      "    step: 150; loss: 2.142; l2dist: 1.337\n",
      "    step: 200; loss: 1.956; l2dist: 1.286\n",
      "    step: 250; loss: 1.889; l2dist: 1.259\n",
      "    step: 300; loss: 1.858; l2dist: 1.249\n",
      "    step: 350; loss: 1.839; l2dist: 1.239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 1.828; l2dist: 1.232\n",
      "    step: 450; loss: 1.823; l2dist: 1.236\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.722; l2dist: 0.000\n",
      "    step: 50; loss: 5.336; l2dist: 1.895\n",
      "    step: 100; loss: 2.792; l2dist: 1.460\n",
      "    step: 150; loss: 2.097; l2dist: 1.295\n",
      "    step: 200; loss: 1.948; l2dist: 1.251\n",
      "    step: 250; loss: 1.873; l2dist: 1.230\n",
      "    step: 300; loss: 1.835; l2dist: 1.228\n",
      "    step: 350; loss: 1.828; l2dist: 1.218\n",
      "    step: 400; loss: 1.808; l2dist: 1.219\n",
      "    step: 450; loss: 1.803; l2dist: 1.214\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.958; l2dist: 0.000\n",
      "    step: 50; loss: 5.302; l2dist: 1.880\n",
      "    step: 100; loss: 2.764; l2dist: 1.444\n",
      "    step: 150; loss: 2.078; l2dist: 1.274\n",
      "    step: 200; loss: 1.922; l2dist: 1.232\n",
      "    step: 250; loss: 1.861; l2dist: 1.212\n",
      "    step: 300; loss: 1.824; l2dist: 1.212\n",
      "    step: 350; loss: 1.816; l2dist: 1.207\n",
      "    step: 400; loss: 1.803; l2dist: 1.203\n",
      "    step: 450; loss: 1.800; l2dist: 1.203\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.187; l2dist: 0.000\n",
      "    step: 50; loss: 5.281; l2dist: 1.869\n",
      "    step: 100; loss: 2.773; l2dist: 1.445\n",
      "    step: 150; loss: 2.079; l2dist: 1.277\n",
      "    step: 200; loss: 1.922; l2dist: 1.235\n",
      "    step: 250; loss: 1.874; l2dist: 1.221\n",
      "    step: 300; loss: 1.825; l2dist: 1.212\n",
      "    step: 350; loss: 1.817; l2dist: 1.206\n",
      "    step: 400; loss: 1.804; l2dist: 1.206\n",
      "    step: 450; loss: 1.803; l2dist: 1.205\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.165; l2dist: 0.000\n",
      "    step: 50; loss: 5.274; l2dist: 1.867\n",
      "    step: 100; loss: 2.779; l2dist: 1.449\n",
      "    step: 150; loss: 2.092; l2dist: 1.279\n",
      "    step: 200; loss: 1.916; l2dist: 1.234\n",
      "    step: 250; loss: 1.860; l2dist: 1.215\n",
      "    step: 300; loss: 1.827; l2dist: 1.212\n",
      "    step: 350; loss: 1.819; l2dist: 1.211\n",
      "    step: 400; loss: 1.799; l2dist: 1.206\n",
      "    step: 450; loss: 1.796; l2dist: 1.201\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.396; l2dist: 0.000\n",
      "    step: 50; loss: 5.305; l2dist: 1.875\n",
      "    step: 100; loss: 2.775; l2dist: 1.447\n",
      "    step: 150; loss: 2.078; l2dist: 1.277\n",
      "    step: 200; loss: 1.920; l2dist: 1.236\n",
      "    step: 250; loss: 1.868; l2dist: 1.224\n",
      "    step: 300; loss: 1.836; l2dist: 1.208\n",
      "    step: 350; loss: 1.814; l2dist: 1.211\n",
      "    step: 400; loss: 1.800; l2dist: 1.208\n",
      "    step: 450; loss: 1.799; l2dist: 1.202\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.698; l2dist: 0.000\n",
      "    step: 50; loss: 14.194; l2dist: 3.409\n",
      "    step: 100; loss: 5.856; l2dist: 2.332\n",
      "    step: 150; loss: 3.966; l2dist: 1.897\n",
      "    step: 200; loss: 3.159; l2dist: 1.696\n",
      "    step: 250; loss: 2.806; l2dist: 1.594\n",
      "    step: 300; loss: 2.677; l2dist: 1.542\n",
      "    step: 350; loss: 2.595; l2dist: 1.507\n",
      "    step: 400; loss: 2.535; l2dist: 1.501\n",
      "    step: 450; loss: 2.522; l2dist: 1.510\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 97.818; l2dist: 0.000\n",
      "    step: 50; loss: 11.811; l2dist: 3.041\n",
      "    step: 100; loss: 4.964; l2dist: 2.090\n",
      "    step: 150; loss: 3.401; l2dist: 1.730\n",
      "    step: 200; loss: 2.856; l2dist: 1.582\n",
      "    step: 250; loss: 2.651; l2dist: 1.524\n",
      "    step: 300; loss: 2.567; l2dist: 1.507\n",
      "    step: 350; loss: 2.473; l2dist: 1.471\n",
      "    step: 400; loss: 2.447; l2dist: 1.466\n",
      "    step: 450; loss: 2.428; l2dist: 1.459\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.397; l2dist: 0.000\n",
      "    step: 50; loss: 9.109; l2dist: 2.612\n",
      "    step: 100; loss: 3.833; l2dist: 1.842\n",
      "    step: 150; loss: 2.872; l2dist: 1.588\n",
      "    step: 200; loss: 2.583; l2dist: 1.508\n",
      "    step: 250; loss: 2.434; l2dist: 1.463\n",
      "    step: 300; loss: 2.379; l2dist: 1.444\n",
      "    step: 350; loss: 2.333; l2dist: 1.435\n",
      "    step: 400; loss: 2.310; l2dist: 1.426\n",
      "    step: 450; loss: 2.287; l2dist: 1.429\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.935; l2dist: 0.000\n",
      "    step: 50; loss: 7.856; l2dist: 2.348\n",
      "    step: 100; loss: 3.542; l2dist: 1.765\n",
      "    step: 150; loss: 2.701; l2dist: 1.533\n",
      "    step: 200; loss: 2.475; l2dist: 1.469\n",
      "    step: 250; loss: 2.365; l2dist: 1.438\n",
      "    step: 300; loss: 2.317; l2dist: 1.432\n",
      "    step: 350; loss: 2.288; l2dist: 1.419\n",
      "    step: 400; loss: 2.287; l2dist: 1.425\n",
      "    step: 450; loss: 2.263; l2dist: 1.411\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.968; l2dist: 0.000\n",
      "    step: 50; loss: 6.942; l2dist: 2.128\n",
      "    step: 100; loss: 3.425; l2dist: 1.670\n",
      "    step: 150; loss: 2.599; l2dist: 1.489\n",
      "    step: 200; loss: 2.405; l2dist: 1.438\n",
      "    step: 250; loss: 2.315; l2dist: 1.417\n",
      "    step: 300; loss: 2.265; l2dist: 1.403\n",
      "    step: 350; loss: 2.269; l2dist: 1.400\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.162; l2dist: 0.000\n",
      "    step: 50; loss: 6.571; l2dist: 2.035\n",
      "    step: 100; loss: 3.367; l2dist: 1.610\n",
      "    step: 150; loss: 2.568; l2dist: 1.445\n",
      "    step: 200; loss: 2.374; l2dist: 1.402\n",
      "    step: 250; loss: 2.306; l2dist: 1.380\n",
      "    step: 300; loss: 2.258; l2dist: 1.380\n",
      "    step: 350; loss: 2.234; l2dist: 1.370\n",
      "    step: 400; loss: 2.232; l2dist: 1.372\n",
      "    step: 450; loss: 2.217; l2dist: 1.366\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.978; l2dist: 0.000\n",
      "    step: 50; loss: 6.395; l2dist: 1.975\n",
      "    step: 100; loss: 3.352; l2dist: 1.592\n",
      "    step: 150; loss: 2.561; l2dist: 1.440\n",
      "    step: 200; loss: 2.363; l2dist: 1.396\n",
      "    step: 250; loss: 2.279; l2dist: 1.384\n",
      "    step: 300; loss: 2.268; l2dist: 1.375\n",
      "    step: 350; loss: 2.237; l2dist: 1.368\n",
      "    step: 400; loss: 2.218; l2dist: 1.365\n",
      "    step: 450; loss: 2.230; l2dist: 1.365\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.807; l2dist: 0.000\n",
      "    step: 50; loss: 6.437; l2dist: 1.983\n",
      "    step: 100; loss: 3.361; l2dist: 1.604\n",
      "    step: 150; loss: 2.554; l2dist: 1.444\n",
      "    step: 200; loss: 2.361; l2dist: 1.400\n",
      "    step: 250; loss: 2.282; l2dist: 1.386\n",
      "    step: 300; loss: 2.253; l2dist: 1.376\n",
      "    step: 350; loss: 2.229; l2dist: 1.374\n",
      "    step: 400; loss: 2.223; l2dist: 1.369\n",
      "    step: 450; loss: 2.216; l2dist: 1.368\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.451; l2dist: 0.000\n",
      "    step: 50; loss: 6.414; l2dist: 1.980\n",
      "    step: 100; loss: 3.347; l2dist: 1.599\n",
      "    step: 150; loss: 2.546; l2dist: 1.440\n",
      "    step: 200; loss: 2.350; l2dist: 1.393\n",
      "    step: 250; loss: 2.274; l2dist: 1.380\n",
      "    step: 300; loss: 2.253; l2dist: 1.371\n",
      "    step: 350; loss: 2.243; l2dist: 1.367\n",
      "    step: 400; loss: 2.222; l2dist: 1.370\n",
      "    step: 450; loss: 2.211; l2dist: 1.367\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.751; l2dist: 0.000\n",
      "    step: 50; loss: 6.473; l2dist: 1.995\n",
      "    step: 100; loss: 3.355; l2dist: 1.614\n",
      "    step: 150; loss: 2.552; l2dist: 1.446\n",
      "    step: 200; loss: 2.359; l2dist: 1.404\n",
      "    step: 250; loss: 2.286; l2dist: 1.386\n",
      "    step: 300; loss: 2.255; l2dist: 1.377\n",
      "    step: 350; loss: 2.243; l2dist: 1.373\n",
      "    step: 400; loss: 2.228; l2dist: 1.379\n",
      "    step: 450; loss: 2.222; l2dist: 1.374\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 118.228; l2dist: 0.000\n",
      "    step: 50; loss: 12.572; l2dist: 3.292\n",
      "    step: 100; loss: 5.401; l2dist: 2.236\n",
      "    step: 150; loss: 3.643; l2dist: 1.812\n",
      "    step: 200; loss: 2.850; l2dist: 1.600\n",
      "    step: 250; loss: 2.540; l2dist: 1.513\n",
      "    step: 300; loss: 2.429; l2dist: 1.471\n",
      "    step: 350; loss: 2.341; l2dist: 1.445\n",
      "    step: 400; loss: 2.332; l2dist: 1.438\n",
      "    step: 450; loss: 2.259; l2dist: 1.414\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 146.433; l2dist: 0.000\n",
      "    step: 50; loss: 12.534; l2dist: 3.233\n",
      "    step: 100; loss: 5.819; l2dist: 2.255\n",
      "    step: 150; loss: 3.943; l2dist: 1.834\n",
      "    step: 200; loss: 3.126; l2dist: 1.638\n",
      "    step: 250; loss: 2.775; l2dist: 1.546\n",
      "    step: 300; loss: 2.528; l2dist: 1.482\n",
      "    step: 350; loss: 2.421; l2dist: 1.454\n",
      "    step: 400; loss: 2.361; l2dist: 1.444\n",
      "    step: 450; loss: 2.285; l2dist: 1.413\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.718; l2dist: 0.000\n",
      "    step: 50; loss: 9.511; l2dist: 2.746\n",
      "    step: 100; loss: 4.214; l2dist: 1.911\n",
      "    step: 150; loss: 2.963; l2dist: 1.607\n",
      "    step: 200; loss: 2.513; l2dist: 1.481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 250; loss: 2.330; l2dist: 1.431\n",
      "    step: 300; loss: 2.221; l2dist: 1.398\n",
      "    step: 350; loss: 2.131; l2dist: 1.368\n",
      "    step: 400; loss: 2.127; l2dist: 1.376\n",
      "    step: 450; loss: 2.102; l2dist: 1.368\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.329; l2dist: 0.000\n",
      "    step: 50; loss: 7.693; l2dist: 2.356\n",
      "    step: 100; loss: 3.500; l2dist: 1.737\n",
      "    step: 150; loss: 2.508; l2dist: 1.480\n",
      "    step: 200; loss: 2.218; l2dist: 1.396\n",
      "    step: 250; loss: 2.127; l2dist: 1.370\n",
      "    step: 300; loss: 2.062; l2dist: 1.350\n",
      "    step: 350; loss: 2.036; l2dist: 1.336\n",
      "    step: 400; loss: 2.069; l2dist: 1.347\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.059; l2dist: 0.000\n",
      "    step: 50; loss: 6.849; l2dist: 2.167\n",
      "    step: 100; loss: 3.257; l2dist: 1.648\n",
      "    step: 150; loss: 2.363; l2dist: 1.426\n",
      "    step: 200; loss: 2.153; l2dist: 1.369\n",
      "    step: 250; loss: 2.073; l2dist: 1.342\n",
      "    step: 300; loss: 2.027; l2dist: 1.327\n",
      "    step: 350; loss: 2.022; l2dist: 1.324\n",
      "    step: 400; loss: 2.002; l2dist: 1.316\n",
      "    step: 450; loss: 2.002; l2dist: 1.316\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.477; l2dist: 0.000\n",
      "    step: 50; loss: 6.530; l2dist: 2.083\n",
      "    step: 100; loss: 3.131; l2dist: 1.567\n",
      "    step: 150; loss: 2.339; l2dist: 1.389\n",
      "    step: 200; loss: 2.145; l2dist: 1.344\n",
      "    step: 250; loss: 2.070; l2dist: 1.323\n",
      "    step: 300; loss: 2.030; l2dist: 1.311\n",
      "    step: 350; loss: 2.012; l2dist: 1.309\n",
      "    step: 400; loss: 2.003; l2dist: 1.308\n",
      "    step: 450; loss: 2.020; l2dist: 1.303\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.048; l2dist: 0.000\n",
      "    step: 50; loss: 6.455; l2dist: 2.063\n",
      "    step: 100; loss: 3.108; l2dist: 1.550\n",
      "    step: 150; loss: 2.322; l2dist: 1.373\n",
      "    step: 200; loss: 2.129; l2dist: 1.331\n",
      "    step: 250; loss: 2.058; l2dist: 1.310\n",
      "    step: 300; loss: 2.044; l2dist: 1.310\n",
      "    step: 350; loss: 2.011; l2dist: 1.300\n",
      "    step: 400; loss: 2.005; l2dist: 1.294\n",
      "    step: 450; loss: 1.987; l2dist: 1.291\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.033; l2dist: 0.000\n",
      "    step: 50; loss: 6.420; l2dist: 2.057\n",
      "    step: 100; loss: 3.109; l2dist: 1.559\n",
      "    step: 150; loss: 2.337; l2dist: 1.386\n",
      "    step: 200; loss: 2.143; l2dist: 1.335\n",
      "    step: 250; loss: 2.067; l2dist: 1.319\n",
      "    step: 300; loss: 2.029; l2dist: 1.310\n",
      "    step: 350; loss: 2.017; l2dist: 1.305\n",
      "    step: 400; loss: 2.040; l2dist: 1.308\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.307; l2dist: 0.000\n",
      "    step: 50; loss: 6.367; l2dist: 2.047\n",
      "    step: 100; loss: 3.093; l2dist: 1.551\n",
      "    step: 150; loss: 2.321; l2dist: 1.378\n",
      "    step: 200; loss: 2.131; l2dist: 1.332\n",
      "    step: 250; loss: 2.055; l2dist: 1.315\n",
      "    step: 300; loss: 2.031; l2dist: 1.307\n",
      "    step: 350; loss: 2.009; l2dist: 1.302\n",
      "    step: 400; loss: 1.995; l2dist: 1.304\n",
      "    step: 450; loss: 1.997; l2dist: 1.302\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.534; l2dist: 0.000\n",
      "    step: 50; loss: 6.399; l2dist: 2.055\n",
      "    step: 100; loss: 3.102; l2dist: 1.562\n",
      "    step: 150; loss: 2.320; l2dist: 1.383\n",
      "    step: 200; loss: 2.143; l2dist: 1.340\n",
      "    step: 250; loss: 2.074; l2dist: 1.330\n",
      "    step: 300; loss: 2.036; l2dist: 1.314\n",
      "    step: 350; loss: 2.013; l2dist: 1.308\n",
      "    step: 400; loss: 2.009; l2dist: 1.304\n",
      "    step: 450; loss: 1.992; l2dist: 1.306\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.775; l2dist: 0.000\n",
      "    step: 50; loss: 13.413; l2dist: 3.352\n",
      "    step: 100; loss: 5.694; l2dist: 2.304\n",
      "    step: 150; loss: 3.826; l2dist: 1.872\n",
      "    step: 200; loss: 3.104; l2dist: 1.679\n",
      "    step: 250; loss: 2.764; l2dist: 1.567\n",
      "    step: 300; loss: 2.584; l2dist: 1.519\n",
      "    step: 350; loss: 2.503; l2dist: 1.481\n",
      "    step: 400; loss: 2.488; l2dist: 1.475\n",
      "    step: 450; loss: 2.455; l2dist: 1.469\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 69.665; l2dist: 0.000\n",
      "    step: 50; loss: 10.618; l2dist: 2.926\n",
      "    step: 100; loss: 4.293; l2dist: 1.981\n",
      "    step: 150; loss: 3.032; l2dist: 1.647\n",
      "    step: 200; loss: 2.586; l2dist: 1.519\n",
      "    step: 250; loss: 2.429; l2dist: 1.473\n",
      "    step: 300; loss: 2.330; l2dist: 1.448\n",
      "    step: 350; loss: 2.267; l2dist: 1.425\n",
      "    step: 400; loss: 2.285; l2dist: 1.422\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.809; l2dist: 0.000\n",
      "    step: 50; loss: 8.754; l2dist: 2.602\n",
      "    step: 100; loss: 3.615; l2dist: 1.801\n",
      "    step: 150; loss: 2.687; l2dist: 1.552\n",
      "    step: 200; loss: 2.417; l2dist: 1.467\n",
      "    step: 250; loss: 2.299; l2dist: 1.435\n",
      "    step: 300; loss: 2.266; l2dist: 1.420\n",
      "    step: 350; loss: 2.235; l2dist: 1.413\n",
      "    step: 400; loss: 2.214; l2dist: 1.410\n",
      "    step: 450; loss: 2.215; l2dist: 1.392\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.190; l2dist: 0.000\n",
      "    step: 50; loss: 7.879; l2dist: 2.411\n",
      "    step: 100; loss: 3.431; l2dist: 1.740\n",
      "    step: 150; loss: 2.576; l2dist: 1.522\n",
      "    step: 200; loss: 2.363; l2dist: 1.449\n",
      "    step: 250; loss: 2.248; l2dist: 1.415\n",
      "    step: 300; loss: 2.203; l2dist: 1.397\n",
      "    step: 350; loss: 2.167; l2dist: 1.382\n",
      "    step: 400; loss: 2.166; l2dist: 1.385\n",
      "    step: 450; loss: 2.138; l2dist: 1.380\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.283; l2dist: 0.000\n",
      "    step: 50; loss: 7.227; l2dist: 2.254\n",
      "    step: 100; loss: 3.308; l2dist: 1.642\n",
      "    step: 150; loss: 2.481; l2dist: 1.459\n",
      "    step: 200; loss: 2.286; l2dist: 1.403\n",
      "    step: 250; loss: 2.209; l2dist: 1.374\n",
      "    step: 300; loss: 2.152; l2dist: 1.362\n",
      "    step: 350; loss: 2.130; l2dist: 1.357\n",
      "    step: 400; loss: 2.117; l2dist: 1.358\n",
      "    step: 450; loss: 2.108; l2dist: 1.361\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.448; l2dist: 0.000\n",
      "    step: 50; loss: 6.928; l2dist: 2.175\n",
      "    step: 100; loss: 3.224; l2dist: 1.594\n",
      "    step: 150; loss: 2.486; l2dist: 1.422\n",
      "    step: 200; loss: 2.261; l2dist: 1.365\n",
      "    step: 250; loss: 2.180; l2dist: 1.344\n",
      "    step: 300; loss: 2.142; l2dist: 1.342\n",
      "    step: 350; loss: 2.120; l2dist: 1.331\n",
      "    step: 400; loss: 2.107; l2dist: 1.332\n",
      "    step: 450; loss: 2.102; l2dist: 1.325\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.569; l2dist: 0.000\n",
      "    step: 50; loss: 6.927; l2dist: 2.159\n",
      "    step: 100; loss: 3.263; l2dist: 1.593\n",
      "    step: 150; loss: 2.511; l2dist: 1.428\n",
      "    step: 200; loss: 2.279; l2dist: 1.377\n",
      "    step: 250; loss: 2.192; l2dist: 1.352\n",
      "    step: 300; loss: 2.169; l2dist: 1.339\n",
      "    step: 350; loss: 2.171; l2dist: 1.331\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.254; l2dist: 0.000\n",
      "    step: 50; loss: 6.895; l2dist: 2.142\n",
      "    step: 100; loss: 3.240; l2dist: 1.590\n",
      "    step: 150; loss: 2.496; l2dist: 1.420\n",
      "    step: 200; loss: 2.280; l2dist: 1.364\n",
      "    step: 250; loss: 2.206; l2dist: 1.354\n",
      "    step: 300; loss: 2.168; l2dist: 1.347\n",
      "    step: 350; loss: 2.130; l2dist: 1.328\n",
      "    step: 400; loss: 2.133; l2dist: 1.335\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.050; l2dist: 0.000\n",
      "    step: 50; loss: 6.879; l2dist: 2.138\n",
      "    step: 100; loss: 3.258; l2dist: 1.593\n",
      "    step: 150; loss: 2.501; l2dist: 1.431\n",
      "    step: 200; loss: 2.273; l2dist: 1.369\n",
      "    step: 250; loss: 2.205; l2dist: 1.354\n",
      "    step: 300; loss: 2.165; l2dist: 1.344\n",
      "    step: 350; loss: 2.139; l2dist: 1.337\n",
      "    step: 400; loss: 2.122; l2dist: 1.333\n",
      "    step: 450; loss: 2.123; l2dist: 1.337\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.192; l2dist: 0.000\n",
      "    step: 50; loss: 6.906; l2dist: 2.147\n",
      "    step: 100; loss: 3.262; l2dist: 1.607\n",
      "    step: 150; loss: 2.502; l2dist: 1.433\n",
      "    step: 200; loss: 2.285; l2dist: 1.378\n",
      "    step: 250; loss: 2.206; l2dist: 1.361\n",
      "    step: 300; loss: 2.171; l2dist: 1.351\n",
      "    step: 350; loss: 2.156; l2dist: 1.347\n",
      "    step: 400; loss: 2.144; l2dist: 1.342\n",
      "    step: 450; loss: 2.140; l2dist: 1.343\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 109.244; l2dist: 0.000\n",
      "    step: 50; loss: 12.247; l2dist: 3.218\n",
      "    step: 100; loss: 5.017; l2dist: 2.144\n",
      "    step: 150; loss: 3.282; l2dist: 1.724\n",
      "    step: 200; loss: 2.680; l2dist: 1.537\n",
      "    step: 250; loss: 2.458; l2dist: 1.453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 2.285; l2dist: 1.417\n",
      "    step: 350; loss: 2.225; l2dist: 1.390\n",
      "    step: 400; loss: 2.202; l2dist: 1.387\n",
      "    step: 450; loss: 2.186; l2dist: 1.384\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 79.316; l2dist: 0.000\n",
      "    step: 50; loss: 9.887; l2dist: 2.815\n",
      "    step: 100; loss: 4.072; l2dist: 1.890\n",
      "    step: 150; loss: 2.861; l2dist: 1.563\n",
      "    step: 200; loss: 2.448; l2dist: 1.441\n",
      "    step: 250; loss: 2.284; l2dist: 1.387\n",
      "    step: 300; loss: 2.223; l2dist: 1.368\n",
      "    step: 350; loss: 2.135; l2dist: 1.351\n",
      "    step: 400; loss: 2.105; l2dist: 1.348\n",
      "    step: 450; loss: 2.101; l2dist: 1.343\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.344; l2dist: 0.000\n",
      "    step: 50; loss: 8.046; l2dist: 2.435\n",
      "    step: 100; loss: 3.321; l2dist: 1.696\n",
      "    step: 150; loss: 2.476; l2dist: 1.459\n",
      "    step: 200; loss: 2.233; l2dist: 1.386\n",
      "    step: 250; loss: 2.142; l2dist: 1.351\n",
      "    step: 300; loss: 2.052; l2dist: 1.326\n",
      "    step: 350; loss: 2.022; l2dist: 1.318\n",
      "    step: 400; loss: 2.010; l2dist: 1.315\n",
      "    step: 450; loss: 1.992; l2dist: 1.313\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.419; l2dist: 0.000\n",
      "    step: 50; loss: 6.861; l2dist: 2.163\n",
      "    step: 100; loss: 3.080; l2dist: 1.628\n",
      "    step: 150; loss: 2.311; l2dist: 1.410\n",
      "    step: 200; loss: 2.122; l2dist: 1.349\n",
      "    step: 250; loss: 2.040; l2dist: 1.321\n",
      "    step: 300; loss: 2.006; l2dist: 1.313\n",
      "    step: 350; loss: 1.969; l2dist: 1.303\n",
      "    step: 400; loss: 1.959; l2dist: 1.304\n",
      "    step: 450; loss: 1.954; l2dist: 1.300\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.783; l2dist: 0.000\n",
      "    step: 50; loss: 6.284; l2dist: 1.984\n",
      "    step: 100; loss: 3.017; l2dist: 1.549\n",
      "    step: 150; loss: 2.271; l2dist: 1.382\n",
      "    step: 200; loss: 2.080; l2dist: 1.330\n",
      "    step: 250; loss: 2.011; l2dist: 1.310\n",
      "    step: 300; loss: 1.968; l2dist: 1.289\n",
      "    step: 350; loss: 1.952; l2dist: 1.286\n",
      "    step: 400; loss: 1.935; l2dist: 1.283\n",
      "    step: 450; loss: 1.927; l2dist: 1.283\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.679; l2dist: 0.000\n",
      "    step: 50; loss: 5.952; l2dist: 1.882\n",
      "    step: 100; loss: 2.924; l2dist: 1.475\n",
      "    step: 150; loss: 2.216; l2dist: 1.321\n",
      "    step: 200; loss: 2.042; l2dist: 1.282\n",
      "    step: 250; loss: 1.975; l2dist: 1.267\n",
      "    step: 300; loss: 1.936; l2dist: 1.260\n",
      "    step: 350; loss: 1.919; l2dist: 1.254\n",
      "    step: 400; loss: 1.918; l2dist: 1.256\n",
      "    step: 450; loss: 1.911; l2dist: 1.249\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.778; l2dist: 0.000\n",
      "    step: 50; loss: 5.898; l2dist: 1.863\n",
      "    step: 100; loss: 2.926; l2dist: 1.478\n",
      "    step: 150; loss: 2.217; l2dist: 1.322\n",
      "    step: 200; loss: 2.043; l2dist: 1.284\n",
      "    step: 250; loss: 1.971; l2dist: 1.265\n",
      "    step: 300; loss: 1.942; l2dist: 1.257\n",
      "    step: 350; loss: 1.915; l2dist: 1.248\n",
      "    step: 400; loss: 1.908; l2dist: 1.245\n",
      "    step: 450; loss: 1.918; l2dist: 1.249\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.441; l2dist: 0.000\n",
      "    step: 50; loss: 5.917; l2dist: 1.863\n",
      "    step: 100; loss: 2.924; l2dist: 1.478\n",
      "    step: 150; loss: 2.228; l2dist: 1.329\n",
      "    step: 200; loss: 2.049; l2dist: 1.285\n",
      "    step: 250; loss: 1.977; l2dist: 1.263\n",
      "    step: 300; loss: 1.945; l2dist: 1.262\n",
      "    step: 350; loss: 1.927; l2dist: 1.256\n",
      "    step: 400; loss: 1.920; l2dist: 1.256\n",
      "    step: 450; loss: 1.917; l2dist: 1.258\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.135; l2dist: 0.000\n",
      "    step: 50; loss: 5.887; l2dist: 1.855\n",
      "    step: 100; loss: 2.924; l2dist: 1.475\n",
      "    step: 150; loss: 2.218; l2dist: 1.327\n",
      "    step: 200; loss: 2.046; l2dist: 1.289\n",
      "    step: 250; loss: 1.982; l2dist: 1.273\n",
      "    step: 300; loss: 1.952; l2dist: 1.262\n",
      "    step: 350; loss: 1.943; l2dist: 1.256\n",
      "    step: 400; loss: 1.933; l2dist: 1.258\n",
      "    step: 450; loss: 1.926; l2dist: 1.250\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.299; l2dist: 0.000\n",
      "    step: 50; loss: 5.927; l2dist: 1.868\n",
      "    step: 100; loss: 2.933; l2dist: 1.488\n",
      "    step: 150; loss: 2.225; l2dist: 1.335\n",
      "    step: 200; loss: 2.056; l2dist: 1.293\n",
      "    step: 250; loss: 1.986; l2dist: 1.273\n",
      "    step: 300; loss: 1.949; l2dist: 1.269\n",
      "    step: 350; loss: 1.933; l2dist: 1.273\n",
      "    step: 400; loss: 1.926; l2dist: 1.261\n",
      "    step: 450; loss: 1.916; l2dist: 1.259\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.666; l2dist: 0.000\n",
      "    step: 50; loss: 13.269; l2dist: 3.248\n",
      "    step: 100; loss: 5.320; l2dist: 2.206\n",
      "    step: 150; loss: 3.572; l2dist: 1.774\n",
      "    step: 200; loss: 2.873; l2dist: 1.605\n",
      "    step: 250; loss: 2.538; l2dist: 1.502\n",
      "    step: 300; loss: 2.413; l2dist: 1.451\n",
      "    step: 350; loss: 2.293; l2dist: 1.425\n",
      "    step: 400; loss: 2.255; l2dist: 1.391\n",
      "    step: 450; loss: 2.282; l2dist: 1.408\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 103.862; l2dist: 0.000\n",
      "    step: 50; loss: 11.309; l2dist: 2.930\n",
      "    step: 100; loss: 4.717; l2dist: 2.014\n",
      "    step: 150; loss: 3.266; l2dist: 1.670\n",
      "    step: 200; loss: 2.793; l2dist: 1.531\n",
      "    step: 250; loss: 2.480; l2dist: 1.457\n",
      "    step: 300; loss: 2.352; l2dist: 1.422\n",
      "    step: 350; loss: 2.275; l2dist: 1.402\n",
      "    step: 400; loss: 2.248; l2dist: 1.398\n",
      "    step: 450; loss: 2.213; l2dist: 1.373\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.925; l2dist: 0.000\n",
      "    step: 50; loss: 8.602; l2dist: 2.504\n",
      "    step: 100; loss: 3.664; l2dist: 1.777\n",
      "    step: 150; loss: 2.677; l2dist: 1.514\n",
      "    step: 200; loss: 2.372; l2dist: 1.430\n",
      "    step: 250; loss: 2.207; l2dist: 1.387\n",
      "    step: 300; loss: 2.137; l2dist: 1.364\n",
      "    step: 350; loss: 2.105; l2dist: 1.354\n",
      "    step: 400; loss: 2.098; l2dist: 1.348\n",
      "    step: 450; loss: 2.090; l2dist: 1.344\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.019; l2dist: 0.000\n",
      "    step: 50; loss: 7.489; l2dist: 2.249\n",
      "    step: 100; loss: 3.396; l2dist: 1.696\n",
      "    step: 150; loss: 2.481; l2dist: 1.465\n",
      "    step: 200; loss: 2.244; l2dist: 1.392\n",
      "    step: 250; loss: 2.140; l2dist: 1.361\n",
      "    step: 300; loss: 2.105; l2dist: 1.347\n",
      "    step: 350; loss: 2.081; l2dist: 1.334\n",
      "    step: 400; loss: 2.051; l2dist: 1.329\n",
      "    step: 450; loss: 2.056; l2dist: 1.332\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.473; l2dist: 0.000\n",
      "    step: 50; loss: 6.630; l2dist: 2.046\n",
      "    step: 100; loss: 3.276; l2dist: 1.593\n",
      "    step: 150; loss: 2.406; l2dist: 1.414\n",
      "    step: 200; loss: 2.195; l2dist: 1.364\n",
      "    step: 250; loss: 2.120; l2dist: 1.339\n",
      "    step: 300; loss: 2.064; l2dist: 1.325\n",
      "    step: 350; loss: 2.072; l2dist: 1.328\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.185; l2dist: 0.000\n",
      "    step: 50; loss: 6.317; l2dist: 1.965\n",
      "    step: 100; loss: 3.159; l2dist: 1.540\n",
      "    step: 150; loss: 2.350; l2dist: 1.371\n",
      "    step: 200; loss: 2.167; l2dist: 1.328\n",
      "    step: 250; loss: 2.090; l2dist: 1.314\n",
      "    step: 300; loss: 2.058; l2dist: 1.302\n",
      "    step: 350; loss: 2.030; l2dist: 1.292\n",
      "    step: 400; loss: 2.015; l2dist: 1.300\n",
      "    step: 450; loss: 2.024; l2dist: 1.291\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.650; l2dist: 0.000\n",
      "    step: 50; loss: 6.200; l2dist: 1.943\n",
      "    step: 100; loss: 3.104; l2dist: 1.519\n",
      "    step: 150; loss: 2.312; l2dist: 1.353\n",
      "    step: 200; loss: 2.134; l2dist: 1.308\n",
      "    step: 250; loss: 2.080; l2dist: 1.293\n",
      "    step: 300; loss: 2.037; l2dist: 1.284\n",
      "    step: 350; loss: 2.020; l2dist: 1.282\n",
      "    step: 400; loss: 2.007; l2dist: 1.281\n",
      "    step: 450; loss: 2.002; l2dist: 1.276\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.102; l2dist: 0.000\n",
      "    step: 50; loss: 6.183; l2dist: 1.933\n",
      "    step: 100; loss: 3.106; l2dist: 1.515\n",
      "    step: 150; loss: 2.317; l2dist: 1.348\n",
      "    step: 200; loss: 2.143; l2dist: 1.308\n",
      "    step: 250; loss: 2.073; l2dist: 1.294\n",
      "    step: 300; loss: 2.040; l2dist: 1.286\n",
      "    step: 350; loss: 2.027; l2dist: 1.283\n",
      "    step: 400; loss: 2.015; l2dist: 1.279\n",
      "    step: 450; loss: 2.018; l2dist: 1.279\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.646; l2dist: 0.000\n",
      "    step: 50; loss: 6.146; l2dist: 1.925\n",
      "    step: 100; loss: 3.109; l2dist: 1.515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 2.316; l2dist: 1.347\n",
      "    step: 200; loss: 2.136; l2dist: 1.306\n",
      "    step: 250; loss: 2.082; l2dist: 1.300\n",
      "    step: 300; loss: 2.040; l2dist: 1.285\n",
      "    step: 350; loss: 2.028; l2dist: 1.285\n",
      "    step: 400; loss: 2.013; l2dist: 1.282\n",
      "    step: 450; loss: 2.008; l2dist: 1.278\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.792; l2dist: 0.000\n",
      "    step: 50; loss: 6.168; l2dist: 1.934\n",
      "    step: 100; loss: 3.120; l2dist: 1.522\n",
      "    step: 150; loss: 2.328; l2dist: 1.356\n",
      "    step: 200; loss: 2.163; l2dist: 1.316\n",
      "    step: 250; loss: 2.093; l2dist: 1.297\n",
      "    step: 300; loss: 2.044; l2dist: 1.291\n",
      "    step: 350; loss: 2.025; l2dist: 1.288\n",
      "    step: 400; loss: 2.017; l2dist: 1.286\n",
      "    step: 450; loss: 2.013; l2dist: 1.280\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.726; l2dist: 0.000\n",
      "    step: 50; loss: 10.885; l2dist: 3.137\n",
      "    step: 100; loss: 4.882; l2dist: 2.108\n",
      "    step: 150; loss: 3.176; l2dist: 1.680\n",
      "    step: 200; loss: 2.588; l2dist: 1.505\n",
      "    step: 250; loss: 2.301; l2dist: 1.422\n",
      "    step: 300; loss: 2.178; l2dist: 1.372\n",
      "    step: 350; loss: 2.092; l2dist: 1.351\n",
      "    step: 400; loss: 2.098; l2dist: 1.346\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 100.649; l2dist: 0.000\n",
      "    step: 50; loss: 9.965; l2dist: 2.924\n",
      "    step: 100; loss: 4.392; l2dist: 1.943\n",
      "    step: 150; loss: 2.934; l2dist: 1.578\n",
      "    step: 200; loss: 2.443; l2dist: 1.444\n",
      "    step: 250; loss: 2.179; l2dist: 1.361\n",
      "    step: 300; loss: 2.102; l2dist: 1.340\n",
      "    step: 350; loss: 2.019; l2dist: 1.309\n",
      "    step: 400; loss: 1.998; l2dist: 1.305\n",
      "    step: 450; loss: 1.980; l2dist: 1.296\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.093; l2dist: 0.000\n",
      "    step: 50; loss: 7.709; l2dist: 2.459\n",
      "    step: 100; loss: 3.336; l2dist: 1.697\n",
      "    step: 150; loss: 2.393; l2dist: 1.434\n",
      "    step: 200; loss: 2.109; l2dist: 1.344\n",
      "    step: 250; loss: 2.002; l2dist: 1.303\n",
      "    step: 300; loss: 1.940; l2dist: 1.289\n",
      "    step: 350; loss: 1.933; l2dist: 1.283\n",
      "    step: 400; loss: 1.925; l2dist: 1.281\n",
      "    step: 450; loss: 1.883; l2dist: 1.265\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.039; l2dist: 0.000\n",
      "    step: 50; loss: 6.304; l2dist: 2.152\n",
      "    step: 100; loss: 2.961; l2dist: 1.583\n",
      "    step: 150; loss: 2.180; l2dist: 1.366\n",
      "    step: 200; loss: 1.974; l2dist: 1.298\n",
      "    step: 250; loss: 1.888; l2dist: 1.263\n",
      "    step: 300; loss: 1.874; l2dist: 1.271\n",
      "    step: 350; loss: 1.833; l2dist: 1.242\n",
      "    step: 400; loss: 1.826; l2dist: 1.248\n",
      "    step: 450; loss: 1.807; l2dist: 1.240\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.290; l2dist: 0.000\n",
      "    step: 50; loss: 5.706; l2dist: 1.987\n",
      "    step: 100; loss: 2.877; l2dist: 1.525\n",
      "    step: 150; loss: 2.151; l2dist: 1.336\n",
      "    step: 200; loss: 1.966; l2dist: 1.284\n",
      "    step: 250; loss: 1.879; l2dist: 1.259\n",
      "    step: 300; loss: 1.853; l2dist: 1.247\n",
      "    step: 350; loss: 1.822; l2dist: 1.243\n",
      "    step: 400; loss: 1.820; l2dist: 1.236\n",
      "    step: 450; loss: 1.829; l2dist: 1.238\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.891; l2dist: 0.000\n",
      "    step: 50; loss: 5.409; l2dist: 1.868\n",
      "    step: 100; loss: 2.830; l2dist: 1.458\n",
      "    step: 150; loss: 2.121; l2dist: 1.293\n",
      "    step: 200; loss: 1.955; l2dist: 1.253\n",
      "    step: 250; loss: 1.883; l2dist: 1.237\n",
      "    step: 300; loss: 1.855; l2dist: 1.220\n",
      "    step: 350; loss: 1.824; l2dist: 1.222\n",
      "    step: 400; loss: 1.814; l2dist: 1.225\n",
      "    step: 450; loss: 1.811; l2dist: 1.215\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.986; l2dist: 0.000\n",
      "    step: 50; loss: 5.309; l2dist: 1.831\n",
      "    step: 100; loss: 2.820; l2dist: 1.454\n",
      "    step: 150; loss: 2.112; l2dist: 1.290\n",
      "    step: 200; loss: 1.924; l2dist: 1.244\n",
      "    step: 250; loss: 1.864; l2dist: 1.227\n",
      "    step: 300; loss: 1.844; l2dist: 1.219\n",
      "    step: 350; loss: 1.815; l2dist: 1.214\n",
      "    step: 400; loss: 1.802; l2dist: 1.211\n",
      "    step: 450; loss: 1.797; l2dist: 1.213\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.192; l2dist: 0.000\n",
      "    step: 50; loss: 5.294; l2dist: 1.820\n",
      "    step: 100; loss: 2.829; l2dist: 1.458\n",
      "    step: 150; loss: 2.109; l2dist: 1.294\n",
      "    step: 200; loss: 1.936; l2dist: 1.246\n",
      "    step: 250; loss: 1.871; l2dist: 1.232\n",
      "    step: 300; loss: 1.831; l2dist: 1.227\n",
      "    step: 350; loss: 1.822; l2dist: 1.218\n",
      "    step: 400; loss: 1.829; l2dist: 1.223\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.819; l2dist: 0.000\n",
      "    step: 50; loss: 5.284; l2dist: 1.817\n",
      "    step: 100; loss: 2.835; l2dist: 1.462\n",
      "    step: 150; loss: 2.108; l2dist: 1.294\n",
      "    step: 200; loss: 1.935; l2dist: 1.249\n",
      "    step: 250; loss: 1.870; l2dist: 1.234\n",
      "    step: 300; loss: 1.842; l2dist: 1.232\n",
      "    step: 350; loss: 1.816; l2dist: 1.224\n",
      "    step: 400; loss: 1.807; l2dist: 1.219\n",
      "    step: 450; loss: 1.804; l2dist: 1.220\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.964; l2dist: 0.000\n",
      "    step: 50; loss: 5.312; l2dist: 1.828\n",
      "    step: 100; loss: 2.841; l2dist: 1.470\n",
      "    step: 150; loss: 2.116; l2dist: 1.296\n",
      "    step: 200; loss: 1.951; l2dist: 1.254\n",
      "    step: 250; loss: 1.871; l2dist: 1.240\n",
      "    step: 300; loss: 1.839; l2dist: 1.234\n",
      "    step: 350; loss: 1.817; l2dist: 1.229\n",
      "    step: 400; loss: 1.807; l2dist: 1.225\n",
      "    step: 450; loss: 1.805; l2dist: 1.228\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 119.458; l2dist: 0.000\n",
      "    step: 50; loss: 13.644; l2dist: 3.262\n",
      "    step: 100; loss: 5.533; l2dist: 2.267\n",
      "    step: 150; loss: 3.695; l2dist: 1.830\n",
      "    step: 200; loss: 3.018; l2dist: 1.630\n",
      "    step: 250; loss: 2.696; l2dist: 1.543\n",
      "    step: 300; loss: 2.513; l2dist: 1.485\n",
      "    step: 350; loss: 2.490; l2dist: 1.470\n",
      "    step: 400; loss: 2.392; l2dist: 1.437\n",
      "    step: 450; loss: 2.328; l2dist: 1.422\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 94.836; l2dist: 0.000\n",
      "    step: 50; loss: 11.025; l2dist: 2.932\n",
      "    step: 100; loss: 4.476; l2dist: 2.006\n",
      "    step: 150; loss: 3.162; l2dist: 1.662\n",
      "    step: 200; loss: 2.670; l2dist: 1.524\n",
      "    step: 250; loss: 2.456; l2dist: 1.456\n",
      "    step: 300; loss: 2.344; l2dist: 1.427\n",
      "    step: 350; loss: 2.281; l2dist: 1.412\n",
      "    step: 400; loss: 2.223; l2dist: 1.387\n",
      "    step: 450; loss: 2.215; l2dist: 1.389\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.745; l2dist: 0.000\n",
      "    step: 50; loss: 8.586; l2dist: 2.501\n",
      "    step: 100; loss: 3.590; l2dist: 1.783\n",
      "    step: 150; loss: 2.702; l2dist: 1.534\n",
      "    step: 200; loss: 2.391; l2dist: 1.442\n",
      "    step: 250; loss: 2.247; l2dist: 1.400\n",
      "    step: 300; loss: 2.195; l2dist: 1.385\n",
      "    step: 350; loss: 2.169; l2dist: 1.378\n",
      "    step: 400; loss: 2.152; l2dist: 1.356\n",
      "    step: 450; loss: 2.165; l2dist: 1.353\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.160; l2dist: 0.000\n",
      "    step: 50; loss: 7.447; l2dist: 2.254\n",
      "    step: 100; loss: 3.385; l2dist: 1.701\n",
      "    step: 150; loss: 2.518; l2dist: 1.474\n",
      "    step: 200; loss: 2.263; l2dist: 1.395\n",
      "    step: 250; loss: 2.153; l2dist: 1.360\n",
      "    step: 300; loss: 2.097; l2dist: 1.348\n",
      "    step: 350; loss: 2.084; l2dist: 1.333\n",
      "    step: 400; loss: 2.058; l2dist: 1.334\n",
      "    step: 450; loss: 2.046; l2dist: 1.330\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.862; l2dist: 0.000\n",
      "    step: 50; loss: 6.862; l2dist: 2.091\n",
      "    step: 100; loss: 3.326; l2dist: 1.618\n",
      "    step: 150; loss: 2.505; l2dist: 1.430\n",
      "    step: 200; loss: 2.269; l2dist: 1.369\n",
      "    step: 250; loss: 2.166; l2dist: 1.337\n",
      "    step: 300; loss: 2.125; l2dist: 1.325\n",
      "    step: 350; loss: 2.107; l2dist: 1.323\n",
      "    step: 400; loss: 2.103; l2dist: 1.317\n",
      "    step: 450; loss: 2.093; l2dist: 1.312\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.579; l2dist: 0.000\n",
      "    step: 50; loss: 6.543; l2dist: 1.970\n",
      "    step: 100; loss: 3.222; l2dist: 1.558\n",
      "    step: 150; loss: 2.453; l2dist: 1.388\n",
      "    step: 200; loss: 2.259; l2dist: 1.334\n",
      "    step: 250; loss: 2.142; l2dist: 1.314\n",
      "    step: 300; loss: 2.094; l2dist: 1.305\n",
      "    step: 350; loss: 2.071; l2dist: 1.295\n",
      "    step: 400; loss: 2.083; l2dist: 1.299\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.828; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 6.436; l2dist: 1.942\n",
      "    step: 100; loss: 3.238; l2dist: 1.543\n",
      "    step: 150; loss: 2.444; l2dist: 1.387\n",
      "    step: 200; loss: 2.236; l2dist: 1.335\n",
      "    step: 250; loss: 2.143; l2dist: 1.317\n",
      "    step: 300; loss: 2.095; l2dist: 1.307\n",
      "    step: 350; loss: 2.081; l2dist: 1.300\n",
      "    step: 400; loss: 2.047; l2dist: 1.296\n",
      "    step: 450; loss: 2.050; l2dist: 1.296\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.952; l2dist: 0.000\n",
      "    step: 50; loss: 6.391; l2dist: 1.926\n",
      "    step: 100; loss: 3.228; l2dist: 1.543\n",
      "    step: 150; loss: 2.435; l2dist: 1.383\n",
      "    step: 200; loss: 2.223; l2dist: 1.333\n",
      "    step: 250; loss: 2.139; l2dist: 1.314\n",
      "    step: 300; loss: 2.092; l2dist: 1.303\n",
      "    step: 350; loss: 2.063; l2dist: 1.299\n",
      "    step: 400; loss: 2.048; l2dist: 1.292\n",
      "    step: 450; loss: 2.053; l2dist: 1.297\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.520; l2dist: 0.000\n",
      "    step: 50; loss: 6.351; l2dist: 1.915\n",
      "    step: 100; loss: 3.224; l2dist: 1.539\n",
      "    step: 150; loss: 2.436; l2dist: 1.386\n",
      "    step: 200; loss: 2.222; l2dist: 1.333\n",
      "    step: 250; loss: 2.127; l2dist: 1.315\n",
      "    step: 300; loss: 2.093; l2dist: 1.303\n",
      "    step: 350; loss: 2.059; l2dist: 1.295\n",
      "    step: 400; loss: 2.054; l2dist: 1.294\n",
      "    step: 450; loss: 2.046; l2dist: 1.292\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.620; l2dist: 0.000\n",
      "    step: 50; loss: 6.378; l2dist: 1.924\n",
      "    step: 100; loss: 3.231; l2dist: 1.545\n",
      "    step: 150; loss: 2.439; l2dist: 1.386\n",
      "    step: 200; loss: 2.221; l2dist: 1.336\n",
      "    step: 250; loss: 2.132; l2dist: 1.325\n",
      "    step: 300; loss: 2.094; l2dist: 1.312\n",
      "    step: 350; loss: 2.080; l2dist: 1.305\n",
      "    step: 400; loss: 2.049; l2dist: 1.297\n",
      "    step: 450; loss: 2.053; l2dist: 1.298\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.533; l2dist: 0.000\n",
      "    step: 50; loss: 12.457; l2dist: 3.267\n",
      "    step: 100; loss: 5.332; l2dist: 2.206\n",
      "    step: 150; loss: 3.538; l2dist: 1.763\n",
      "    step: 200; loss: 2.868; l2dist: 1.579\n",
      "    step: 250; loss: 2.538; l2dist: 1.485\n",
      "    step: 300; loss: 2.363; l2dist: 1.423\n",
      "    step: 350; loss: 2.322; l2dist: 1.408\n",
      "    step: 400; loss: 2.289; l2dist: 1.406\n",
      "    step: 450; loss: 2.289; l2dist: 1.407\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 70.290; l2dist: 0.000\n",
      "    step: 50; loss: 9.975; l2dist: 2.862\n",
      "    step: 100; loss: 4.030; l2dist: 1.883\n",
      "    step: 150; loss: 2.827; l2dist: 1.568\n",
      "    step: 200; loss: 2.431; l2dist: 1.444\n",
      "    step: 250; loss: 2.286; l2dist: 1.400\n",
      "    step: 300; loss: 2.206; l2dist: 1.379\n",
      "    step: 350; loss: 2.159; l2dist: 1.355\n",
      "    step: 400; loss: 2.111; l2dist: 1.351\n",
      "    step: 450; loss: 2.140; l2dist: 1.356\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.837; l2dist: 0.000\n",
      "    step: 50; loss: 8.020; l2dist: 2.469\n",
      "    step: 100; loss: 3.295; l2dist: 1.699\n",
      "    step: 150; loss: 2.457; l2dist: 1.461\n",
      "    step: 200; loss: 2.237; l2dist: 1.382\n",
      "    step: 250; loss: 2.144; l2dist: 1.359\n",
      "    step: 300; loss: 2.102; l2dist: 1.341\n",
      "    step: 350; loss: 2.070; l2dist: 1.333\n",
      "    step: 400; loss: 2.064; l2dist: 1.323\n",
      "    step: 450; loss: 2.045; l2dist: 1.318\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.154; l2dist: 0.000\n",
      "    step: 50; loss: 6.817; l2dist: 2.174\n",
      "    step: 100; loss: 3.132; l2dist: 1.632\n",
      "    step: 150; loss: 2.357; l2dist: 1.417\n",
      "    step: 200; loss: 2.169; l2dist: 1.358\n",
      "    step: 250; loss: 2.081; l2dist: 1.333\n",
      "    step: 300; loss: 2.047; l2dist: 1.324\n",
      "    step: 350; loss: 2.031; l2dist: 1.317\n",
      "    step: 400; loss: 2.009; l2dist: 1.314\n",
      "    step: 450; loss: 2.012; l2dist: 1.313\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.727; l2dist: 0.000\n",
      "    step: 50; loss: 6.220; l2dist: 1.938\n",
      "    step: 100; loss: 3.170; l2dist: 1.562\n",
      "    step: 150; loss: 2.354; l2dist: 1.382\n",
      "    step: 200; loss: 2.159; l2dist: 1.333\n",
      "    step: 250; loss: 2.080; l2dist: 1.318\n",
      "    step: 300; loss: 2.044; l2dist: 1.303\n",
      "    step: 350; loss: 2.026; l2dist: 1.297\n",
      "    step: 400; loss: 2.016; l2dist: 1.296\n",
      "    step: 450; loss: 2.008; l2dist: 1.296\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 13.767; l2dist: 0.000\n",
      "    step: 50; loss: 5.892; l2dist: 1.810\n",
      "    step: 100; loss: 3.102; l2dist: 1.478\n",
      "    step: 150; loss: 2.333; l2dist: 1.340\n",
      "    step: 200; loss: 2.150; l2dist: 1.308\n",
      "    step: 250; loss: 2.073; l2dist: 1.297\n",
      "    step: 300; loss: 2.042; l2dist: 1.283\n",
      "    step: 350; loss: 2.014; l2dist: 1.281\n",
      "    step: 400; loss: 2.006; l2dist: 1.277\n",
      "    step: 450; loss: 1.999; l2dist: 1.284\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 13.091; l2dist: 0.000\n",
      "    step: 50; loss: 5.796; l2dist: 1.785\n",
      "    step: 100; loss: 3.085; l2dist: 1.478\n",
      "    step: 150; loss: 2.320; l2dist: 1.330\n",
      "    step: 200; loss: 2.136; l2dist: 1.293\n",
      "    step: 250; loss: 2.059; l2dist: 1.277\n",
      "    step: 300; loss: 2.033; l2dist: 1.271\n",
      "    step: 350; loss: 2.011; l2dist: 1.263\n",
      "    step: 400; loss: 1.996; l2dist: 1.267\n",
      "    step: 450; loss: 1.989; l2dist: 1.263\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.832; l2dist: 0.000\n",
      "    step: 50; loss: 5.776; l2dist: 1.773\n",
      "    step: 100; loss: 3.084; l2dist: 1.476\n",
      "    step: 150; loss: 2.319; l2dist: 1.332\n",
      "    step: 200; loss: 2.136; l2dist: 1.291\n",
      "    step: 250; loss: 2.062; l2dist: 1.282\n",
      "    step: 300; loss: 2.035; l2dist: 1.269\n",
      "    step: 350; loss: 2.008; l2dist: 1.269\n",
      "    step: 400; loss: 2.001; l2dist: 1.269\n",
      "    step: 450; loss: 1.993; l2dist: 1.268\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.727; l2dist: 0.000\n",
      "    step: 50; loss: 5.771; l2dist: 1.772\n",
      "    step: 100; loss: 3.089; l2dist: 1.479\n",
      "    step: 150; loss: 2.314; l2dist: 1.331\n",
      "    step: 200; loss: 2.132; l2dist: 1.302\n",
      "    step: 250; loss: 2.062; l2dist: 1.279\n",
      "    step: 300; loss: 2.031; l2dist: 1.279\n",
      "    step: 350; loss: 2.011; l2dist: 1.273\n",
      "    step: 400; loss: 2.001; l2dist: 1.268\n",
      "    step: 450; loss: 1.988; l2dist: 1.265\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.921; l2dist: 0.000\n",
      "    step: 50; loss: 5.825; l2dist: 1.789\n",
      "    step: 100; loss: 3.098; l2dist: 1.490\n",
      "    step: 150; loss: 2.319; l2dist: 1.342\n",
      "    step: 200; loss: 2.135; l2dist: 1.304\n",
      "    step: 250; loss: 2.072; l2dist: 1.295\n",
      "    step: 300; loss: 2.039; l2dist: 1.279\n",
      "    step: 350; loss: 2.016; l2dist: 1.276\n",
      "    step: 400; loss: 2.003; l2dist: 1.275\n",
      "    step: 450; loss: 1.997; l2dist: 1.271\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 124.295; l2dist: 0.000\n",
      "    step: 50; loss: 13.726; l2dist: 3.414\n",
      "    step: 100; loss: 5.970; l2dist: 2.365\n",
      "    step: 150; loss: 3.985; l2dist: 1.919\n",
      "    step: 200; loss: 3.219; l2dist: 1.706\n",
      "    step: 250; loss: 2.826; l2dist: 1.600\n",
      "    step: 300; loss: 2.628; l2dist: 1.545\n",
      "    step: 350; loss: 2.514; l2dist: 1.509\n",
      "    step: 400; loss: 2.485; l2dist: 1.501\n",
      "    step: 450; loss: 2.436; l2dist: 1.471\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 104.628; l2dist: 0.000\n",
      "    step: 50; loss: 12.033; l2dist: 3.130\n",
      "    step: 100; loss: 5.178; l2dist: 2.142\n",
      "    step: 150; loss: 3.564; l2dist: 1.765\n",
      "    step: 200; loss: 3.002; l2dist: 1.615\n",
      "    step: 250; loss: 2.692; l2dist: 1.533\n",
      "    step: 300; loss: 2.559; l2dist: 1.492\n",
      "    step: 350; loss: 2.482; l2dist: 1.478\n",
      "    step: 400; loss: 2.425; l2dist: 1.459\n",
      "    step: 450; loss: 2.433; l2dist: 1.465\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.839; l2dist: 0.000\n",
      "    step: 50; loss: 9.414; l2dist: 2.681\n",
      "    step: 100; loss: 4.072; l2dist: 1.893\n",
      "    step: 150; loss: 2.997; l2dist: 1.622\n",
      "    step: 200; loss: 2.634; l2dist: 1.524\n",
      "    step: 250; loss: 2.456; l2dist: 1.480\n",
      "    step: 300; loss: 2.366; l2dist: 1.437\n",
      "    step: 350; loss: 2.351; l2dist: 1.438\n",
      "    step: 400; loss: 2.294; l2dist: 1.425\n",
      "    step: 450; loss: 2.327; l2dist: 1.439\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.741; l2dist: 0.000\n",
      "    step: 50; loss: 8.233; l2dist: 2.451\n",
      "    step: 100; loss: 3.826; l2dist: 1.816\n",
      "    step: 150; loss: 2.850; l2dist: 1.568\n",
      "    step: 200; loss: 2.535; l2dist: 1.483\n",
      "    step: 250; loss: 2.428; l2dist: 1.453\n",
      "    step: 300; loss: 2.332; l2dist: 1.430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 2.318; l2dist: 1.436\n",
      "    step: 400; loss: 2.282; l2dist: 1.415\n",
      "    step: 450; loss: 2.254; l2dist: 1.412\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.662; l2dist: 0.000\n",
      "    step: 50; loss: 7.392; l2dist: 2.284\n",
      "    step: 100; loss: 3.659; l2dist: 1.725\n",
      "    step: 150; loss: 2.752; l2dist: 1.523\n",
      "    step: 200; loss: 2.465; l2dist: 1.451\n",
      "    step: 250; loss: 2.380; l2dist: 1.427\n",
      "    step: 300; loss: 2.296; l2dist: 1.408\n",
      "    step: 350; loss: 2.253; l2dist: 1.398\n",
      "    step: 400; loss: 2.248; l2dist: 1.392\n",
      "    step: 450; loss: 2.218; l2dist: 1.391\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.982; l2dist: 0.000\n",
      "    step: 50; loss: 6.967; l2dist: 2.187\n",
      "    step: 100; loss: 3.561; l2dist: 1.665\n",
      "    step: 150; loss: 2.699; l2dist: 1.466\n",
      "    step: 200; loss: 2.436; l2dist: 1.415\n",
      "    step: 250; loss: 2.345; l2dist: 1.388\n",
      "    step: 300; loss: 2.306; l2dist: 1.388\n",
      "    step: 350; loss: 2.357; l2dist: 1.389\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.080; l2dist: 0.000\n",
      "    step: 50; loss: 6.884; l2dist: 2.173\n",
      "    step: 100; loss: 3.535; l2dist: 1.655\n",
      "    step: 150; loss: 2.676; l2dist: 1.468\n",
      "    step: 200; loss: 2.432; l2dist: 1.416\n",
      "    step: 250; loss: 2.327; l2dist: 1.389\n",
      "    step: 300; loss: 2.268; l2dist: 1.385\n",
      "    step: 350; loss: 2.253; l2dist: 1.370\n",
      "    step: 400; loss: 2.213; l2dist: 1.366\n",
      "    step: 450; loss: 2.201; l2dist: 1.363\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.575; l2dist: 0.000\n",
      "    step: 50; loss: 6.891; l2dist: 2.171\n",
      "    step: 100; loss: 3.544; l2dist: 1.656\n",
      "    step: 150; loss: 2.691; l2dist: 1.472\n",
      "    step: 200; loss: 2.430; l2dist: 1.418\n",
      "    step: 250; loss: 2.341; l2dist: 1.397\n",
      "    step: 300; loss: 2.282; l2dist: 1.389\n",
      "    step: 350; loss: 2.243; l2dist: 1.390\n",
      "    step: 400; loss: 2.208; l2dist: 1.378\n",
      "    step: 450; loss: 2.197; l2dist: 1.371\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.340; l2dist: 0.000\n",
      "    step: 50; loss: 6.885; l2dist: 2.169\n",
      "    step: 100; loss: 3.537; l2dist: 1.659\n",
      "    step: 150; loss: 2.686; l2dist: 1.477\n",
      "    step: 200; loss: 2.444; l2dist: 1.425\n",
      "    step: 250; loss: 2.335; l2dist: 1.400\n",
      "    step: 300; loss: 2.265; l2dist: 1.388\n",
      "    step: 350; loss: 2.223; l2dist: 1.377\n",
      "    step: 400; loss: 2.211; l2dist: 1.379\n",
      "    step: 450; loss: 2.191; l2dist: 1.374\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.474; l2dist: 0.000\n",
      "    step: 50; loss: 6.916; l2dist: 2.176\n",
      "    step: 100; loss: 3.550; l2dist: 1.665\n",
      "    step: 150; loss: 2.695; l2dist: 1.481\n",
      "    step: 200; loss: 2.443; l2dist: 1.432\n",
      "    step: 250; loss: 2.325; l2dist: 1.403\n",
      "    step: 300; loss: 2.268; l2dist: 1.392\n",
      "    step: 350; loss: 2.233; l2dist: 1.380\n",
      "    step: 400; loss: 2.210; l2dist: 1.385\n",
      "    step: 450; loss: 2.202; l2dist: 1.377\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 127.153; l2dist: 0.000\n",
      "    step: 50; loss: 13.390; l2dist: 3.388\n",
      "    step: 100; loss: 5.996; l2dist: 2.369\n",
      "    step: 150; loss: 3.959; l2dist: 1.908\n",
      "    step: 200; loss: 3.067; l2dist: 1.664\n",
      "    step: 250; loss: 2.691; l2dist: 1.551\n",
      "    step: 300; loss: 2.526; l2dist: 1.500\n",
      "    step: 350; loss: 2.411; l2dist: 1.466\n",
      "    step: 400; loss: 2.334; l2dist: 1.446\n",
      "    step: 450; loss: 2.296; l2dist: 1.431\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 115.364; l2dist: 0.000\n",
      "    step: 50; loss: 12.027; l2dist: 3.181\n",
      "    step: 100; loss: 5.197; l2dist: 2.165\n",
      "    step: 150; loss: 3.536; l2dist: 1.762\n",
      "    step: 200; loss: 2.863; l2dist: 1.585\n",
      "    step: 250; loss: 2.556; l2dist: 1.504\n",
      "    step: 300; loss: 2.410; l2dist: 1.455\n",
      "    step: 350; loss: 2.340; l2dist: 1.432\n",
      "    step: 400; loss: 2.297; l2dist: 1.427\n",
      "    step: 450; loss: 2.248; l2dist: 1.403\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.063; l2dist: 0.000\n",
      "    step: 50; loss: 9.324; l2dist: 2.712\n",
      "    step: 100; loss: 3.871; l2dist: 1.861\n",
      "    step: 150; loss: 2.776; l2dist: 1.566\n",
      "    step: 200; loss: 2.422; l2dist: 1.464\n",
      "    step: 250; loss: 2.268; l2dist: 1.421\n",
      "    step: 300; loss: 2.201; l2dist: 1.400\n",
      "    step: 350; loss: 2.175; l2dist: 1.388\n",
      "    step: 400; loss: 2.154; l2dist: 1.383\n",
      "    step: 450; loss: 2.149; l2dist: 1.383\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.039; l2dist: 0.000\n",
      "    step: 50; loss: 8.025; l2dist: 2.444\n",
      "    step: 100; loss: 3.490; l2dist: 1.757\n",
      "    step: 150; loss: 2.548; l2dist: 1.506\n",
      "    step: 200; loss: 2.309; l2dist: 1.421\n",
      "    step: 250; loss: 2.198; l2dist: 1.402\n",
      "    step: 300; loss: 2.146; l2dist: 1.377\n",
      "    step: 350; loss: 2.138; l2dist: 1.370\n",
      "    step: 400; loss: 2.112; l2dist: 1.368\n",
      "    step: 450; loss: 2.094; l2dist: 1.365\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.336; l2dist: 0.000\n",
      "    step: 50; loss: 7.227; l2dist: 2.276\n",
      "    step: 100; loss: 3.361; l2dist: 1.679\n",
      "    step: 150; loss: 2.494; l2dist: 1.476\n",
      "    step: 200; loss: 2.284; l2dist: 1.412\n",
      "    step: 250; loss: 2.182; l2dist: 1.381\n",
      "    step: 300; loss: 2.148; l2dist: 1.368\n",
      "    step: 350; loss: 2.122; l2dist: 1.364\n",
      "    step: 400; loss: 2.120; l2dist: 1.358\n",
      "    step: 450; loss: 2.115; l2dist: 1.362\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.965; l2dist: 0.000\n",
      "    step: 50; loss: 6.987; l2dist: 2.215\n",
      "    step: 100; loss: 3.283; l2dist: 1.644\n",
      "    step: 150; loss: 2.450; l2dist: 1.439\n",
      "    step: 200; loss: 2.246; l2dist: 1.383\n",
      "    step: 250; loss: 2.181; l2dist: 1.370\n",
      "    step: 300; loss: 2.143; l2dist: 1.353\n",
      "    step: 350; loss: 2.113; l2dist: 1.350\n",
      "    step: 400; loss: 2.106; l2dist: 1.347\n",
      "    step: 450; loss: 2.103; l2dist: 1.347\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.547; l2dist: 0.000\n",
      "    step: 50; loss: 6.869; l2dist: 2.188\n",
      "    step: 100; loss: 3.231; l2dist: 1.631\n",
      "    step: 150; loss: 2.442; l2dist: 1.446\n",
      "    step: 200; loss: 2.244; l2dist: 1.382\n",
      "    step: 250; loss: 2.160; l2dist: 1.367\n",
      "    step: 300; loss: 2.141; l2dist: 1.364\n",
      "    step: 350; loss: 2.109; l2dist: 1.349\n",
      "    step: 400; loss: 2.111; l2dist: 1.341\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.356; l2dist: 0.000\n",
      "    step: 50; loss: 6.845; l2dist: 2.175\n",
      "    step: 100; loss: 3.229; l2dist: 1.616\n",
      "    step: 150; loss: 2.427; l2dist: 1.423\n",
      "    step: 200; loss: 2.240; l2dist: 1.374\n",
      "    step: 250; loss: 2.166; l2dist: 1.362\n",
      "    step: 300; loss: 2.133; l2dist: 1.349\n",
      "    step: 350; loss: 2.123; l2dist: 1.338\n",
      "    step: 400; loss: 2.108; l2dist: 1.345\n",
      "    step: 450; loss: 2.094; l2dist: 1.338\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.213; l2dist: 0.000\n",
      "    step: 50; loss: 6.830; l2dist: 2.174\n",
      "    step: 100; loss: 3.226; l2dist: 1.618\n",
      "    step: 150; loss: 2.430; l2dist: 1.424\n",
      "    step: 200; loss: 2.260; l2dist: 1.375\n",
      "    step: 250; loss: 2.162; l2dist: 1.357\n",
      "    step: 300; loss: 2.142; l2dist: 1.349\n",
      "    step: 350; loss: 2.118; l2dist: 1.348\n",
      "    step: 400; loss: 2.102; l2dist: 1.343\n",
      "    step: 450; loss: 2.104; l2dist: 1.335\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.393; l2dist: 0.000\n",
      "    step: 50; loss: 6.864; l2dist: 2.185\n",
      "    step: 100; loss: 3.233; l2dist: 1.622\n",
      "    step: 150; loss: 2.446; l2dist: 1.441\n",
      "    step: 200; loss: 2.241; l2dist: 1.379\n",
      "    step: 250; loss: 2.176; l2dist: 1.365\n",
      "    step: 300; loss: 2.137; l2dist: 1.350\n",
      "    step: 350; loss: 2.110; l2dist: 1.347\n",
      "    step: 400; loss: 2.113; l2dist: 1.348\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.549; l2dist: 0.000\n",
      "    step: 50; loss: 13.175; l2dist: 3.204\n",
      "    step: 100; loss: 5.215; l2dist: 2.179\n",
      "    step: 150; loss: 3.515; l2dist: 1.761\n",
      "    step: 200; loss: 2.847; l2dist: 1.569\n",
      "    step: 250; loss: 2.540; l2dist: 1.473\n",
      "    step: 300; loss: 2.359; l2dist: 1.421\n",
      "    step: 350; loss: 2.252; l2dist: 1.400\n",
      "    step: 400; loss: 2.262; l2dist: 1.392\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 123.623; l2dist: 0.000\n",
      "    step: 50; loss: 13.157; l2dist: 3.165\n",
      "    step: 100; loss: 5.440; l2dist: 2.145\n",
      "    step: 150; loss: 3.616; l2dist: 1.729\n",
      "    step: 200; loss: 2.894; l2dist: 1.544\n",
      "    step: 250; loss: 2.555; l2dist: 1.456\n",
      "    step: 300; loss: 2.458; l2dist: 1.430\n",
      "    step: 350; loss: 2.283; l2dist: 1.385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 2.221; l2dist: 1.372\n",
      "    step: 450; loss: 2.175; l2dist: 1.360\n",
      "binary step: 1; number of successful adv: 99/100\n",
      "    step: 0; loss: 215.070; l2dist: 0.000\n",
      "    step: 50; loss: 17.873; l2dist: 3.688\n",
      "    step: 100; loss: 8.082; l2dist: 2.658\n",
      "    step: 150; loss: 5.299; l2dist: 2.127\n",
      "    step: 200; loss: 3.905; l2dist: 1.814\n",
      "    step: 250; loss: 3.144; l2dist: 1.632\n",
      "    step: 300; loss: 2.728; l2dist: 1.518\n",
      "    step: 350; loss: 2.583; l2dist: 1.474\n",
      "    step: 400; loss: 2.423; l2dist: 1.438\n",
      "    step: 450; loss: 2.356; l2dist: 1.418\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 119.180; l2dist: 0.000\n",
      "    step: 50; loss: 12.968; l2dist: 3.087\n",
      "    step: 100; loss: 5.390; l2dist: 2.147\n",
      "    step: 150; loss: 3.513; l2dist: 1.716\n",
      "    step: 200; loss: 2.784; l2dist: 1.530\n",
      "    step: 250; loss: 2.422; l2dist: 1.432\n",
      "    step: 300; loss: 2.317; l2dist: 1.399\n",
      "    step: 350; loss: 2.187; l2dist: 1.361\n",
      "    step: 400; loss: 2.155; l2dist: 1.357\n",
      "    step: 450; loss: 2.110; l2dist: 1.339\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.691; l2dist: 0.000\n",
      "    step: 50; loss: 10.359; l2dist: 2.748\n",
      "    step: 100; loss: 4.104; l2dist: 1.830\n",
      "    step: 150; loss: 2.798; l2dist: 1.517\n",
      "    step: 200; loss: 2.388; l2dist: 1.404\n",
      "    step: 250; loss: 2.232; l2dist: 1.360\n",
      "    step: 300; loss: 2.119; l2dist: 1.324\n",
      "    step: 350; loss: 2.092; l2dist: 1.320\n",
      "    step: 400; loss: 2.063; l2dist: 1.311\n",
      "    step: 450; loss: 2.077; l2dist: 1.309\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.235; l2dist: 0.000\n",
      "    step: 50; loss: 8.994; l2dist: 2.535\n",
      "    step: 100; loss: 3.449; l2dist: 1.638\n",
      "    step: 150; loss: 2.474; l2dist: 1.387\n",
      "    step: 200; loss: 2.184; l2dist: 1.305\n",
      "    step: 250; loss: 2.070; l2dist: 1.271\n",
      "    step: 300; loss: 2.003; l2dist: 1.254\n",
      "    step: 350; loss: 1.970; l2dist: 1.250\n",
      "    step: 400; loss: 1.975; l2dist: 1.249\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.576; l2dist: 0.000\n",
      "    step: 50; loss: 8.362; l2dist: 2.431\n",
      "    step: 100; loss: 3.248; l2dist: 1.575\n",
      "    step: 150; loss: 2.409; l2dist: 1.369\n",
      "    step: 200; loss: 2.168; l2dist: 1.298\n",
      "    step: 250; loss: 2.121; l2dist: 1.280\n",
      "    step: 300; loss: 2.021; l2dist: 1.261\n",
      "    step: 350; loss: 1.995; l2dist: 1.257\n",
      "    step: 400; loss: 1.976; l2dist: 1.251\n",
      "    step: 450; loss: 1.958; l2dist: 1.252\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.163; l2dist: 0.000\n",
      "    step: 50; loss: 8.082; l2dist: 2.383\n",
      "    step: 100; loss: 3.140; l2dist: 1.535\n",
      "    step: 150; loss: 2.345; l2dist: 1.339\n",
      "    step: 200; loss: 2.140; l2dist: 1.283\n",
      "    step: 250; loss: 2.057; l2dist: 1.263\n",
      "    step: 300; loss: 1.992; l2dist: 1.251\n",
      "    step: 350; loss: 1.980; l2dist: 1.240\n",
      "    step: 400; loss: 1.957; l2dist: 1.242\n",
      "    step: 450; loss: 1.933; l2dist: 1.235\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.440; l2dist: 0.000\n",
      "    step: 50; loss: 7.944; l2dist: 2.347\n",
      "    step: 100; loss: 3.110; l2dist: 1.520\n",
      "    step: 150; loss: 2.331; l2dist: 1.333\n",
      "    step: 200; loss: 2.122; l2dist: 1.277\n",
      "    step: 250; loss: 2.101; l2dist: 1.266\n",
      "    step: 300; loss: 2.004; l2dist: 1.250\n",
      "    step: 350; loss: 1.981; l2dist: 1.245\n",
      "    step: 400; loss: 1.967; l2dist: 1.245\n",
      "    step: 450; loss: 1.958; l2dist: 1.248\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.631; l2dist: 0.000\n",
      "    step: 50; loss: 7.965; l2dist: 2.350\n",
      "    step: 100; loss: 3.128; l2dist: 1.530\n",
      "    step: 150; loss: 2.339; l2dist: 1.345\n",
      "    step: 200; loss: 2.134; l2dist: 1.289\n",
      "    step: 250; loss: 2.042; l2dist: 1.268\n",
      "    step: 300; loss: 2.003; l2dist: 1.255\n",
      "    step: 350; loss: 1.976; l2dist: 1.250\n",
      "    step: 400; loss: 1.968; l2dist: 1.249\n",
      "    step: 450; loss: 1.943; l2dist: 1.243\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 124.077; l2dist: 0.000\n",
      "    step: 50; loss: 13.976; l2dist: 3.454\n",
      "    step: 100; loss: 5.923; l2dist: 2.336\n",
      "    step: 150; loss: 3.915; l2dist: 1.879\n",
      "    step: 200; loss: 3.195; l2dist: 1.667\n",
      "    step: 250; loss: 2.782; l2dist: 1.560\n",
      "    step: 300; loss: 2.679; l2dist: 1.518\n",
      "    step: 350; loss: 2.621; l2dist: 1.496\n",
      "    step: 400; loss: 2.563; l2dist: 1.484\n",
      "    step: 450; loss: 2.499; l2dist: 1.466\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 97.366; l2dist: 0.000\n",
      "    step: 50; loss: 12.026; l2dist: 3.175\n",
      "    step: 100; loss: 4.948; l2dist: 2.087\n",
      "    step: 150; loss: 3.445; l2dist: 1.715\n",
      "    step: 200; loss: 2.962; l2dist: 1.576\n",
      "    step: 250; loss: 2.656; l2dist: 1.489\n",
      "    step: 300; loss: 2.571; l2dist: 1.479\n",
      "    step: 350; loss: 2.479; l2dist: 1.444\n",
      "    step: 400; loss: 2.440; l2dist: 1.434\n",
      "    step: 450; loss: 2.468; l2dist: 1.445\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.493; l2dist: 0.000\n",
      "    step: 50; loss: 9.412; l2dist: 2.734\n",
      "    step: 100; loss: 3.861; l2dist: 1.824\n",
      "    step: 150; loss: 2.883; l2dist: 1.557\n",
      "    step: 200; loss: 2.600; l2dist: 1.476\n",
      "    step: 250; loss: 2.465; l2dist: 1.440\n",
      "    step: 300; loss: 2.420; l2dist: 1.420\n",
      "    step: 350; loss: 2.355; l2dist: 1.409\n",
      "    step: 400; loss: 2.340; l2dist: 1.414\n",
      "    step: 450; loss: 2.310; l2dist: 1.400\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.191; l2dist: 0.000\n",
      "    step: 50; loss: 8.276; l2dist: 2.449\n",
      "    step: 100; loss: 3.549; l2dist: 1.736\n",
      "    step: 150; loss: 2.688; l2dist: 1.501\n",
      "    step: 200; loss: 2.478; l2dist: 1.444\n",
      "    step: 250; loss: 2.387; l2dist: 1.403\n",
      "    step: 300; loss: 2.333; l2dist: 1.399\n",
      "    step: 350; loss: 2.303; l2dist: 1.399\n",
      "    step: 400; loss: 2.297; l2dist: 1.387\n",
      "    step: 450; loss: 2.274; l2dist: 1.383\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.507; l2dist: 0.000\n",
      "    step: 50; loss: 7.329; l2dist: 2.207\n",
      "    step: 100; loss: 3.472; l2dist: 1.671\n",
      "    step: 150; loss: 2.632; l2dist: 1.477\n",
      "    step: 200; loss: 2.428; l2dist: 1.416\n",
      "    step: 250; loss: 2.343; l2dist: 1.396\n",
      "    step: 300; loss: 2.299; l2dist: 1.387\n",
      "    step: 350; loss: 2.281; l2dist: 1.377\n",
      "    step: 400; loss: 2.266; l2dist: 1.376\n",
      "    step: 450; loss: 2.262; l2dist: 1.370\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.467; l2dist: 0.000\n",
      "    step: 50; loss: 6.809; l2dist: 2.052\n",
      "    step: 100; loss: 3.387; l2dist: 1.566\n",
      "    step: 150; loss: 2.591; l2dist: 1.414\n",
      "    step: 200; loss: 2.385; l2dist: 1.378\n",
      "    step: 250; loss: 2.308; l2dist: 1.354\n",
      "    step: 300; loss: 2.273; l2dist: 1.347\n",
      "    step: 350; loss: 2.248; l2dist: 1.339\n",
      "    step: 400; loss: 2.234; l2dist: 1.341\n",
      "    step: 450; loss: 2.239; l2dist: 1.337\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.066; l2dist: 0.000\n",
      "    step: 50; loss: 6.957; l2dist: 2.069\n",
      "    step: 100; loss: 3.397; l2dist: 1.571\n",
      "    step: 150; loss: 2.596; l2dist: 1.411\n",
      "    step: 200; loss: 2.404; l2dist: 1.365\n",
      "    step: 250; loss: 2.326; l2dist: 1.354\n",
      "    step: 300; loss: 2.285; l2dist: 1.351\n",
      "    step: 350; loss: 2.260; l2dist: 1.338\n",
      "    step: 400; loss: 2.251; l2dist: 1.338\n",
      "    step: 450; loss: 2.240; l2dist: 1.335\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.598; l2dist: 0.000\n",
      "    step: 50; loss: 6.939; l2dist: 2.066\n",
      "    step: 100; loss: 3.417; l2dist: 1.584\n",
      "    step: 150; loss: 2.593; l2dist: 1.421\n",
      "    step: 200; loss: 2.401; l2dist: 1.376\n",
      "    step: 250; loss: 2.330; l2dist: 1.360\n",
      "    step: 300; loss: 2.291; l2dist: 1.347\n",
      "    step: 350; loss: 2.256; l2dist: 1.350\n",
      "    step: 400; loss: 2.260; l2dist: 1.346\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.419; l2dist: 0.000\n",
      "    step: 50; loss: 6.931; l2dist: 2.066\n",
      "    step: 100; loss: 3.412; l2dist: 1.580\n",
      "    step: 150; loss: 2.598; l2dist: 1.420\n",
      "    step: 200; loss: 2.406; l2dist: 1.382\n",
      "    step: 250; loss: 2.328; l2dist: 1.366\n",
      "    step: 300; loss: 2.279; l2dist: 1.353\n",
      "    step: 350; loss: 2.259; l2dist: 1.345\n",
      "    step: 400; loss: 2.259; l2dist: 1.350\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.601; l2dist: 0.000\n",
      "    step: 50; loss: 6.968; l2dist: 2.075\n",
      "    step: 100; loss: 3.418; l2dist: 1.591\n",
      "    step: 150; loss: 2.609; l2dist: 1.424\n",
      "    step: 200; loss: 2.406; l2dist: 1.387\n",
      "    step: 250; loss: 2.325; l2dist: 1.369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 2.285; l2dist: 1.359\n",
      "    step: 350; loss: 2.270; l2dist: 1.357\n",
      "    step: 400; loss: 2.254; l2dist: 1.356\n",
      "    step: 450; loss: 2.245; l2dist: 1.345\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 151.598; l2dist: 0.000\n",
      "    step: 50; loss: 16.331; l2dist: 3.816\n",
      "    step: 100; loss: 7.644; l2dist: 2.711\n",
      "    step: 150; loss: 5.117; l2dist: 2.202\n",
      "    step: 200; loss: 4.082; l2dist: 1.956\n",
      "    step: 250; loss: 3.647; l2dist: 1.841\n",
      "    step: 300; loss: 3.481; l2dist: 1.795\n",
      "    step: 350; loss: 3.411; l2dist: 1.772\n",
      "    step: 400; loss: 3.243; l2dist: 1.731\n",
      "    step: 450; loss: 3.222; l2dist: 1.724\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 179.844; l2dist: 0.000\n",
      "    step: 50; loss: 15.547; l2dist: 3.708\n",
      "    step: 100; loss: 7.336; l2dist: 2.613\n",
      "    step: 150; loss: 5.048; l2dist: 2.143\n",
      "    step: 200; loss: 4.191; l2dist: 1.942\n",
      "    step: 250; loss: 3.711; l2dist: 1.840\n",
      "    step: 300; loss: 3.475; l2dist: 1.783\n",
      "    step: 350; loss: 3.331; l2dist: 1.745\n",
      "    step: 400; loss: 3.211; l2dist: 1.723\n",
      "    step: 450; loss: 3.152; l2dist: 1.705\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 107.238; l2dist: 0.000\n",
      "    step: 50; loss: 12.553; l2dist: 3.299\n",
      "    step: 100; loss: 5.665; l2dist: 2.279\n",
      "    step: 150; loss: 4.158; l2dist: 1.944\n",
      "    step: 200; loss: 3.591; l2dist: 1.808\n",
      "    step: 250; loss: 3.290; l2dist: 1.746\n",
      "    step: 300; loss: 3.151; l2dist: 1.702\n",
      "    step: 350; loss: 3.056; l2dist: 1.678\n",
      "    step: 400; loss: 3.043; l2dist: 1.676\n",
      "    step: 450; loss: 2.998; l2dist: 1.660\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.333; l2dist: 0.000\n",
      "    step: 50; loss: 11.025; l2dist: 3.015\n",
      "    step: 100; loss: 5.102; l2dist: 2.155\n",
      "    step: 150; loss: 3.798; l2dist: 1.856\n",
      "    step: 200; loss: 3.339; l2dist: 1.748\n",
      "    step: 250; loss: 3.128; l2dist: 1.694\n",
      "    step: 300; loss: 3.036; l2dist: 1.674\n",
      "    step: 350; loss: 3.017; l2dist: 1.658\n",
      "    step: 400; loss: 2.963; l2dist: 1.647\n",
      "    step: 450; loss: 2.950; l2dist: 1.646\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.178; l2dist: 0.000\n",
      "    step: 50; loss: 10.109; l2dist: 2.852\n",
      "    step: 100; loss: 4.797; l2dist: 2.069\n",
      "    step: 150; loss: 3.639; l2dist: 1.814\n",
      "    step: 200; loss: 3.236; l2dist: 1.720\n",
      "    step: 250; loss: 3.088; l2dist: 1.672\n",
      "    step: 300; loss: 2.990; l2dist: 1.658\n",
      "    step: 350; loss: 2.945; l2dist: 1.644\n",
      "    step: 400; loss: 2.921; l2dist: 1.638\n",
      "    step: 450; loss: 2.920; l2dist: 1.641\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.706; l2dist: 0.000\n",
      "    step: 50; loss: 9.689; l2dist: 2.796\n",
      "    step: 100; loss: 4.585; l2dist: 2.032\n",
      "    step: 150; loss: 3.502; l2dist: 1.779\n",
      "    step: 200; loss: 3.163; l2dist: 1.694\n",
      "    step: 250; loss: 3.076; l2dist: 1.664\n",
      "    step: 300; loss: 2.991; l2dist: 1.642\n",
      "    step: 350; loss: 2.942; l2dist: 1.632\n",
      "    step: 400; loss: 2.915; l2dist: 1.629\n",
      "    step: 450; loss: 2.907; l2dist: 1.625\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.770; l2dist: 0.000\n",
      "    step: 50; loss: 9.536; l2dist: 2.764\n",
      "    step: 100; loss: 4.531; l2dist: 2.016\n",
      "    step: 150; loss: 3.490; l2dist: 1.776\n",
      "    step: 200; loss: 3.153; l2dist: 1.689\n",
      "    step: 250; loss: 3.021; l2dist: 1.662\n",
      "    step: 300; loss: 2.954; l2dist: 1.642\n",
      "    step: 350; loss: 2.920; l2dist: 1.630\n",
      "    step: 400; loss: 2.900; l2dist: 1.626\n",
      "    step: 450; loss: 2.889; l2dist: 1.623\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.567; l2dist: 0.000\n",
      "    step: 50; loss: 9.488; l2dist: 2.762\n",
      "    step: 100; loss: 4.523; l2dist: 2.013\n",
      "    step: 150; loss: 3.460; l2dist: 1.772\n",
      "    step: 200; loss: 3.124; l2dist: 1.690\n",
      "    step: 250; loss: 3.016; l2dist: 1.661\n",
      "    step: 300; loss: 2.957; l2dist: 1.647\n",
      "    step: 350; loss: 2.912; l2dist: 1.631\n",
      "    step: 400; loss: 2.905; l2dist: 1.628\n",
      "    step: 450; loss: 2.894; l2dist: 1.625\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.472; l2dist: 0.000\n",
      "    step: 50; loss: 9.428; l2dist: 2.747\n",
      "    step: 100; loss: 4.496; l2dist: 2.005\n",
      "    step: 150; loss: 3.477; l2dist: 1.771\n",
      "    step: 200; loss: 3.141; l2dist: 1.693\n",
      "    step: 250; loss: 3.022; l2dist: 1.654\n",
      "    step: 300; loss: 2.938; l2dist: 1.642\n",
      "    step: 350; loss: 2.916; l2dist: 1.639\n",
      "    step: 400; loss: 2.906; l2dist: 1.630\n",
      "    step: 450; loss: 2.900; l2dist: 1.629\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.616; l2dist: 0.000\n",
      "    step: 50; loss: 9.449; l2dist: 2.753\n",
      "    step: 100; loss: 4.511; l2dist: 2.008\n",
      "    step: 150; loss: 3.444; l2dist: 1.776\n",
      "    step: 200; loss: 3.142; l2dist: 1.691\n",
      "    step: 250; loss: 3.027; l2dist: 1.660\n",
      "    step: 300; loss: 2.956; l2dist: 1.649\n",
      "    step: 350; loss: 2.937; l2dist: 1.630\n",
      "    step: 400; loss: 2.907; l2dist: 1.629\n",
      "    step: 450; loss: 2.918; l2dist: 1.627\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 142.969; l2dist: 0.000\n",
      "    step: 50; loss: 16.328; l2dist: 3.742\n",
      "    step: 100; loss: 6.977; l2dist: 2.579\n",
      "    step: 150; loss: 4.699; l2dist: 2.088\n",
      "    step: 200; loss: 3.840; l2dist: 1.866\n",
      "    step: 250; loss: 3.386; l2dist: 1.752\n",
      "    step: 300; loss: 3.161; l2dist: 1.686\n",
      "    step: 350; loss: 3.065; l2dist: 1.652\n",
      "    step: 400; loss: 3.013; l2dist: 1.642\n",
      "    step: 450; loss: 3.022; l2dist: 1.622\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 169.547; l2dist: 0.000\n",
      "    step: 50; loss: 16.445; l2dist: 3.682\n",
      "    step: 100; loss: 7.282; l2dist: 2.560\n",
      "    step: 150; loss: 5.038; l2dist: 2.099\n",
      "    step: 200; loss: 4.021; l2dist: 1.875\n",
      "    step: 250; loss: 3.636; l2dist: 1.775\n",
      "    step: 300; loss: 3.321; l2dist: 1.708\n",
      "    step: 350; loss: 3.257; l2dist: 1.688\n",
      "    step: 400; loss: 3.058; l2dist: 1.641\n",
      "    step: 450; loss: 3.014; l2dist: 1.629\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 93.507; l2dist: 0.000\n",
      "    step: 50; loss: 12.312; l2dist: 3.134\n",
      "    step: 100; loss: 5.326; l2dist: 2.171\n",
      "    step: 150; loss: 3.907; l2dist: 1.851\n",
      "    step: 200; loss: 3.349; l2dist: 1.716\n",
      "    step: 250; loss: 3.103; l2dist: 1.652\n",
      "    step: 300; loss: 2.994; l2dist: 1.633\n",
      "    step: 350; loss: 2.882; l2dist: 1.595\n",
      "    step: 400; loss: 2.843; l2dist: 1.589\n",
      "    step: 450; loss: 2.844; l2dist: 1.585\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.471; l2dist: 0.000\n",
      "    step: 50; loss: 10.093; l2dist: 2.734\n",
      "    step: 100; loss: 4.609; l2dist: 2.005\n",
      "    step: 150; loss: 3.486; l2dist: 1.748\n",
      "    step: 200; loss: 3.087; l2dist: 1.647\n",
      "    step: 250; loss: 2.946; l2dist: 1.607\n",
      "    step: 300; loss: 2.845; l2dist: 1.587\n",
      "    step: 350; loss: 2.805; l2dist: 1.576\n",
      "    step: 400; loss: 2.805; l2dist: 1.577\n",
      "    step: 450; loss: 2.779; l2dist: 1.574\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.106; l2dist: 0.000\n",
      "    step: 50; loss: 9.048; l2dist: 2.541\n",
      "    step: 100; loss: 4.357; l2dist: 1.913\n",
      "    step: 150; loss: 3.313; l2dist: 1.679\n",
      "    step: 200; loss: 3.002; l2dist: 1.605\n",
      "    step: 250; loss: 2.862; l2dist: 1.578\n",
      "    step: 300; loss: 2.795; l2dist: 1.563\n",
      "    step: 350; loss: 2.769; l2dist: 1.550\n",
      "    step: 400; loss: 2.790; l2dist: 1.565\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.275; l2dist: 0.000\n",
      "    step: 50; loss: 8.576; l2dist: 2.442\n",
      "    step: 100; loss: 4.244; l2dist: 1.877\n",
      "    step: 150; loss: 3.199; l2dist: 1.653\n",
      "    step: 200; loss: 2.927; l2dist: 1.585\n",
      "    step: 250; loss: 2.797; l2dist: 1.557\n",
      "    step: 300; loss: 2.794; l2dist: 1.553\n",
      "    step: 350; loss: 2.733; l2dist: 1.537\n",
      "    step: 400; loss: 2.722; l2dist: 1.540\n",
      "    step: 450; loss: 2.719; l2dist: 1.534\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.677; l2dist: 0.000\n",
      "    step: 50; loss: 8.385; l2dist: 2.407\n",
      "    step: 100; loss: 4.153; l2dist: 1.856\n",
      "    step: 150; loss: 3.154; l2dist: 1.640\n",
      "    step: 200; loss: 2.887; l2dist: 1.570\n",
      "    step: 250; loss: 2.781; l2dist: 1.542\n",
      "    step: 300; loss: 2.737; l2dist: 1.532\n",
      "    step: 350; loss: 2.699; l2dist: 1.522\n",
      "    step: 400; loss: 2.676; l2dist: 1.517\n",
      "    step: 450; loss: 2.698; l2dist: 1.515\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.549; l2dist: 0.000\n",
      "    step: 50; loss: 8.375; l2dist: 2.404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 4.163; l2dist: 1.860\n",
      "    step: 150; loss: 3.172; l2dist: 1.640\n",
      "    step: 200; loss: 2.909; l2dist: 1.575\n",
      "    step: 250; loss: 2.791; l2dist: 1.550\n",
      "    step: 300; loss: 2.758; l2dist: 1.539\n",
      "    step: 350; loss: 2.728; l2dist: 1.534\n",
      "    step: 400; loss: 2.692; l2dist: 1.530\n",
      "    step: 450; loss: 2.683; l2dist: 1.525\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.896; l2dist: 0.000\n",
      "    step: 50; loss: 8.369; l2dist: 2.394\n",
      "    step: 100; loss: 4.145; l2dist: 1.857\n",
      "    step: 150; loss: 3.169; l2dist: 1.639\n",
      "    step: 200; loss: 2.916; l2dist: 1.577\n",
      "    step: 250; loss: 2.805; l2dist: 1.559\n",
      "    step: 300; loss: 2.754; l2dist: 1.544\n",
      "    step: 350; loss: 2.731; l2dist: 1.534\n",
      "    step: 400; loss: 2.705; l2dist: 1.530\n",
      "    step: 450; loss: 2.691; l2dist: 1.526\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.112; l2dist: 0.000\n",
      "    step: 50; loss: 8.411; l2dist: 2.405\n",
      "    step: 100; loss: 4.134; l2dist: 1.856\n",
      "    step: 150; loss: 3.146; l2dist: 1.643\n",
      "    step: 200; loss: 2.896; l2dist: 1.583\n",
      "    step: 250; loss: 2.807; l2dist: 1.567\n",
      "    step: 300; loss: 2.752; l2dist: 1.544\n",
      "    step: 350; loss: 2.731; l2dist: 1.542\n",
      "    step: 400; loss: 2.712; l2dist: 1.540\n",
      "    step: 450; loss: 2.699; l2dist: 1.529\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 138.363; l2dist: 0.000\n",
      "    step: 50; loss: 16.318; l2dist: 3.668\n",
      "    step: 100; loss: 6.683; l2dist: 2.495\n",
      "    step: 150; loss: 4.466; l2dist: 2.021\n",
      "    step: 200; loss: 3.582; l2dist: 1.796\n",
      "    step: 250; loss: 3.225; l2dist: 1.691\n",
      "    step: 300; loss: 2.970; l2dist: 1.624\n",
      "    step: 350; loss: 2.871; l2dist: 1.600\n",
      "    step: 400; loss: 2.787; l2dist: 1.571\n",
      "    step: 450; loss: 2.787; l2dist: 1.576\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 93.593; l2dist: 0.000\n",
      "    step: 50; loss: 13.073; l2dist: 3.235\n",
      "    step: 100; loss: 5.192; l2dist: 2.165\n",
      "    step: 150; loss: 3.666; l2dist: 1.801\n",
      "    step: 200; loss: 3.151; l2dist: 1.670\n",
      "    step: 250; loss: 2.879; l2dist: 1.594\n",
      "    step: 300; loss: 2.772; l2dist: 1.563\n",
      "    step: 350; loss: 2.733; l2dist: 1.546\n",
      "    step: 400; loss: 2.707; l2dist: 1.547\n",
      "    step: 450; loss: 2.674; l2dist: 1.534\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.037; l2dist: 0.000\n",
      "    step: 50; loss: 10.281; l2dist: 2.780\n",
      "    step: 100; loss: 4.193; l2dist: 1.932\n",
      "    step: 150; loss: 3.109; l2dist: 1.653\n",
      "    step: 200; loss: 2.857; l2dist: 1.581\n",
      "    step: 250; loss: 2.713; l2dist: 1.543\n",
      "    step: 300; loss: 2.647; l2dist: 1.530\n",
      "    step: 350; loss: 2.637; l2dist: 1.526\n",
      "    step: 400; loss: 2.628; l2dist: 1.518\n",
      "    step: 450; loss: 2.600; l2dist: 1.514\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.874; l2dist: 0.000\n",
      "    step: 50; loss: 9.061; l2dist: 2.514\n",
      "    step: 100; loss: 3.939; l2dist: 1.856\n",
      "    step: 150; loss: 2.970; l2dist: 1.621\n",
      "    step: 200; loss: 2.724; l2dist: 1.545\n",
      "    step: 250; loss: 2.606; l2dist: 1.514\n",
      "    step: 300; loss: 2.583; l2dist: 1.498\n",
      "    step: 350; loss: 2.545; l2dist: 1.495\n",
      "    step: 400; loss: 2.541; l2dist: 1.488\n",
      "    step: 450; loss: 2.531; l2dist: 1.488\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.626; l2dist: 0.000\n",
      "    step: 50; loss: 8.250; l2dist: 2.318\n",
      "    step: 100; loss: 3.890; l2dist: 1.778\n",
      "    step: 150; loss: 2.925; l2dist: 1.584\n",
      "    step: 200; loss: 2.700; l2dist: 1.523\n",
      "    step: 250; loss: 2.611; l2dist: 1.504\n",
      "    step: 300; loss: 2.564; l2dist: 1.488\n",
      "    step: 350; loss: 2.535; l2dist: 1.485\n",
      "    step: 400; loss: 2.528; l2dist: 1.479\n",
      "    step: 450; loss: 2.521; l2dist: 1.473\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.362; l2dist: 0.000\n",
      "    step: 50; loss: 7.939; l2dist: 2.232\n",
      "    step: 100; loss: 3.738; l2dist: 1.722\n",
      "    step: 150; loss: 2.868; l2dist: 1.539\n",
      "    step: 200; loss: 2.666; l2dist: 1.489\n",
      "    step: 250; loss: 2.573; l2dist: 1.469\n",
      "    step: 300; loss: 2.530; l2dist: 1.464\n",
      "    step: 350; loss: 2.507; l2dist: 1.460\n",
      "    step: 400; loss: 2.502; l2dist: 1.462\n",
      "    step: 450; loss: 2.511; l2dist: 1.457\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.442; l2dist: 0.000\n",
      "    step: 50; loss: 7.916; l2dist: 2.214\n",
      "    step: 100; loss: 3.758; l2dist: 1.728\n",
      "    step: 150; loss: 2.874; l2dist: 1.554\n",
      "    step: 200; loss: 2.648; l2dist: 1.500\n",
      "    step: 250; loss: 2.571; l2dist: 1.478\n",
      "    step: 300; loss: 2.530; l2dist: 1.471\n",
      "    step: 350; loss: 2.519; l2dist: 1.477\n",
      "    step: 400; loss: 2.505; l2dist: 1.472\n",
      "    step: 450; loss: 2.488; l2dist: 1.461\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.799; l2dist: 0.000\n",
      "    step: 50; loss: 7.852; l2dist: 2.200\n",
      "    step: 100; loss: 3.779; l2dist: 1.728\n",
      "    step: 150; loss: 2.873; l2dist: 1.556\n",
      "    step: 200; loss: 2.658; l2dist: 1.511\n",
      "    step: 250; loss: 2.573; l2dist: 1.489\n",
      "    step: 300; loss: 2.546; l2dist: 1.470\n",
      "    step: 350; loss: 2.522; l2dist: 1.470\n",
      "    step: 400; loss: 2.505; l2dist: 1.468\n",
      "    step: 450; loss: 2.502; l2dist: 1.470\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.446; l2dist: 0.000\n",
      "    step: 50; loss: 7.835; l2dist: 2.185\n",
      "    step: 100; loss: 3.758; l2dist: 1.721\n",
      "    step: 150; loss: 2.866; l2dist: 1.550\n",
      "    step: 200; loss: 2.644; l2dist: 1.501\n",
      "    step: 250; loss: 2.573; l2dist: 1.478\n",
      "    step: 300; loss: 2.533; l2dist: 1.476\n",
      "    step: 350; loss: 2.506; l2dist: 1.468\n",
      "    step: 400; loss: 2.506; l2dist: 1.467\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.672; l2dist: 0.000\n",
      "    step: 50; loss: 7.878; l2dist: 2.197\n",
      "    step: 100; loss: 3.759; l2dist: 1.728\n",
      "    step: 150; loss: 2.877; l2dist: 1.554\n",
      "    step: 200; loss: 2.647; l2dist: 1.504\n",
      "    step: 250; loss: 2.568; l2dist: 1.487\n",
      "    step: 300; loss: 2.533; l2dist: 1.483\n",
      "    step: 350; loss: 2.516; l2dist: 1.469\n",
      "    step: 400; loss: 2.500; l2dist: 1.470\n",
      "    step: 450; loss: 2.495; l2dist: 1.467\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 159.874; l2dist: 0.000\n",
      "    step: 50; loss: 16.258; l2dist: 3.900\n",
      "    step: 100; loss: 8.104; l2dist: 2.809\n",
      "    step: 150; loss: 5.427; l2dist: 2.294\n",
      "    step: 200; loss: 4.378; l2dist: 2.047\n",
      "    step: 250; loss: 3.876; l2dist: 1.918\n",
      "    step: 300; loss: 3.610; l2dist: 1.851\n",
      "    step: 350; loss: 3.517; l2dist: 1.830\n",
      "    step: 400; loss: 3.392; l2dist: 1.797\n",
      "    step: 450; loss: 3.442; l2dist: 1.790\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 183.371; l2dist: 0.000\n",
      "    step: 50; loss: 15.998; l2dist: 3.778\n",
      "    step: 100; loss: 7.775; l2dist: 2.691\n",
      "    step: 150; loss: 5.382; l2dist: 2.223\n",
      "    step: 200; loss: 4.444; l2dist: 2.003\n",
      "    step: 250; loss: 3.894; l2dist: 1.901\n",
      "    step: 300; loss: 3.641; l2dist: 1.835\n",
      "    step: 350; loss: 3.473; l2dist: 1.800\n",
      "    step: 400; loss: 3.426; l2dist: 1.790\n",
      "    step: 450; loss: 3.400; l2dist: 1.788\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 100.596; l2dist: 0.000\n",
      "    step: 50; loss: 12.567; l2dist: 3.313\n",
      "    step: 100; loss: 5.716; l2dist: 2.301\n",
      "    step: 150; loss: 4.255; l2dist: 1.974\n",
      "    step: 200; loss: 3.694; l2dist: 1.851\n",
      "    step: 250; loss: 3.447; l2dist: 1.786\n",
      "    step: 300; loss: 3.351; l2dist: 1.769\n",
      "    step: 350; loss: 3.318; l2dist: 1.751\n",
      "    step: 400; loss: 3.253; l2dist: 1.743\n",
      "    step: 450; loss: 3.221; l2dist: 1.728\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.421; l2dist: 0.000\n",
      "    step: 50; loss: 10.503; l2dist: 2.905\n",
      "    step: 100; loss: 4.922; l2dist: 2.132\n",
      "    step: 150; loss: 3.704; l2dist: 1.857\n",
      "    step: 200; loss: 3.376; l2dist: 1.776\n",
      "    step: 250; loss: 3.239; l2dist: 1.740\n",
      "    step: 300; loss: 3.161; l2dist: 1.718\n",
      "    step: 350; loss: 3.160; l2dist: 1.714\n",
      "    step: 400; loss: 3.098; l2dist: 1.701\n",
      "    step: 450; loss: 3.134; l2dist: 1.711\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.669; l2dist: 0.000\n",
      "    step: 50; loss: 9.128; l2dist: 2.630\n",
      "    step: 100; loss: 4.540; l2dist: 1.974\n",
      "    step: 150; loss: 3.538; l2dist: 1.786\n",
      "    step: 200; loss: 3.269; l2dist: 1.721\n",
      "    step: 250; loss: 3.145; l2dist: 1.703\n",
      "    step: 300; loss: 3.079; l2dist: 1.684\n",
      "    step: 350; loss: 3.056; l2dist: 1.678\n",
      "    step: 400; loss: 3.063; l2dist: 1.679\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.349; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 8.835; l2dist: 2.569\n",
      "    step: 100; loss: 4.438; l2dist: 1.956\n",
      "    step: 150; loss: 3.471; l2dist: 1.767\n",
      "    step: 200; loss: 3.220; l2dist: 1.706\n",
      "    step: 250; loss: 3.132; l2dist: 1.684\n",
      "    step: 300; loss: 3.127; l2dist: 1.681\n",
      "    step: 350; loss: 3.078; l2dist: 1.679\n",
      "    step: 400; loss: 3.040; l2dist: 1.669\n",
      "    step: 450; loss: 3.039; l2dist: 1.674\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.431; l2dist: 0.000\n",
      "    step: 50; loss: 8.655; l2dist: 2.541\n",
      "    step: 100; loss: 4.425; l2dist: 1.947\n",
      "    step: 150; loss: 3.443; l2dist: 1.763\n",
      "    step: 200; loss: 3.206; l2dist: 1.716\n",
      "    step: 250; loss: 3.108; l2dist: 1.690\n",
      "    step: 300; loss: 3.090; l2dist: 1.692\n",
      "    step: 350; loss: 3.046; l2dist: 1.679\n",
      "    step: 400; loss: 3.044; l2dist: 1.668\n",
      "    step: 450; loss: 3.034; l2dist: 1.667\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.547; l2dist: 0.000\n",
      "    step: 50; loss: 8.593; l2dist: 2.527\n",
      "    step: 100; loss: 4.424; l2dist: 1.941\n",
      "    step: 150; loss: 3.440; l2dist: 1.761\n",
      "    step: 200; loss: 3.201; l2dist: 1.711\n",
      "    step: 250; loss: 3.111; l2dist: 1.685\n",
      "    step: 300; loss: 3.063; l2dist: 1.672\n",
      "    step: 350; loss: 3.050; l2dist: 1.670\n",
      "    step: 400; loss: 3.037; l2dist: 1.673\n",
      "    step: 450; loss: 3.016; l2dist: 1.664\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.698; l2dist: 0.000\n",
      "    step: 50; loss: 8.538; l2dist: 2.518\n",
      "    step: 100; loss: 4.420; l2dist: 1.935\n",
      "    step: 150; loss: 3.442; l2dist: 1.762\n",
      "    step: 200; loss: 3.200; l2dist: 1.703\n",
      "    step: 250; loss: 3.101; l2dist: 1.690\n",
      "    step: 300; loss: 3.068; l2dist: 1.680\n",
      "    step: 350; loss: 3.038; l2dist: 1.670\n",
      "    step: 400; loss: 3.043; l2dist: 1.676\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.881; l2dist: 0.000\n",
      "    step: 50; loss: 8.574; l2dist: 2.528\n",
      "    step: 100; loss: 4.424; l2dist: 1.944\n",
      "    step: 150; loss: 3.430; l2dist: 1.765\n",
      "    step: 200; loss: 3.200; l2dist: 1.713\n",
      "    step: 250; loss: 3.102; l2dist: 1.692\n",
      "    step: 300; loss: 3.071; l2dist: 1.682\n",
      "    step: 350; loss: 3.057; l2dist: 1.684\n",
      "    step: 400; loss: 3.026; l2dist: 1.675\n",
      "    step: 450; loss: 3.018; l2dist: 1.672\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 153.649; l2dist: 0.000\n",
      "    step: 50; loss: 16.917; l2dist: 3.873\n",
      "    step: 100; loss: 7.497; l2dist: 2.684\n",
      "    step: 150; loss: 5.107; l2dist: 2.194\n",
      "    step: 200; loss: 4.121; l2dist: 1.959\n",
      "    step: 250; loss: 3.706; l2dist: 1.852\n",
      "    step: 300; loss: 3.478; l2dist: 1.788\n",
      "    step: 350; loss: 3.365; l2dist: 1.751\n",
      "    step: 400; loss: 3.275; l2dist: 1.726\n",
      "    step: 450; loss: 3.266; l2dist: 1.725\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 90.283; l2dist: 0.000\n",
      "    step: 50; loss: 13.002; l2dist: 3.332\n",
      "    step: 100; loss: 5.504; l2dist: 2.266\n",
      "    step: 150; loss: 4.014; l2dist: 1.926\n",
      "    step: 200; loss: 3.533; l2dist: 1.790\n",
      "    step: 250; loss: 3.315; l2dist: 1.737\n",
      "    step: 300; loss: 3.218; l2dist: 1.704\n",
      "    step: 350; loss: 3.130; l2dist: 1.689\n",
      "    step: 400; loss: 3.119; l2dist: 1.679\n",
      "    step: 450; loss: 3.112; l2dist: 1.682\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.119; l2dist: 0.000\n",
      "    step: 50; loss: 10.524; l2dist: 2.904\n",
      "    step: 100; loss: 4.612; l2dist: 2.060\n",
      "    step: 150; loss: 3.564; l2dist: 1.806\n",
      "    step: 200; loss: 3.245; l2dist: 1.719\n",
      "    step: 250; loss: 3.117; l2dist: 1.676\n",
      "    step: 300; loss: 3.073; l2dist: 1.670\n",
      "    step: 350; loss: 3.044; l2dist: 1.669\n",
      "    step: 400; loss: 3.026; l2dist: 1.665\n",
      "    step: 450; loss: 3.032; l2dist: 1.656\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.717; l2dist: 0.000\n",
      "    step: 50; loss: 9.457; l2dist: 2.678\n",
      "    step: 100; loss: 4.456; l2dist: 2.008\n",
      "    step: 150; loss: 3.476; l2dist: 1.775\n",
      "    step: 200; loss: 3.189; l2dist: 1.701\n",
      "    step: 250; loss: 3.093; l2dist: 1.673\n",
      "    step: 300; loss: 3.052; l2dist: 1.657\n",
      "    step: 350; loss: 3.021; l2dist: 1.657\n",
      "    step: 400; loss: 3.009; l2dist: 1.642\n",
      "    step: 450; loss: 2.981; l2dist: 1.639\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.402; l2dist: 0.000\n",
      "    step: 50; loss: 8.732; l2dist: 2.532\n",
      "    step: 100; loss: 4.390; l2dist: 1.926\n",
      "    step: 150; loss: 3.429; l2dist: 1.734\n",
      "    step: 200; loss: 3.158; l2dist: 1.664\n",
      "    step: 250; loss: 3.051; l2dist: 1.641\n",
      "    step: 300; loss: 3.013; l2dist: 1.630\n",
      "    step: 350; loss: 2.988; l2dist: 1.627\n",
      "    step: 400; loss: 2.955; l2dist: 1.622\n",
      "    step: 450; loss: 2.947; l2dist: 1.623\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.067; l2dist: 0.000\n",
      "    step: 50; loss: 8.472; l2dist: 2.467\n",
      "    step: 100; loss: 4.321; l2dist: 1.898\n",
      "    step: 150; loss: 3.395; l2dist: 1.703\n",
      "    step: 200; loss: 3.133; l2dist: 1.642\n",
      "    step: 250; loss: 3.026; l2dist: 1.625\n",
      "    step: 300; loss: 2.985; l2dist: 1.611\n",
      "    step: 350; loss: 2.968; l2dist: 1.609\n",
      "    step: 400; loss: 2.961; l2dist: 1.610\n",
      "    step: 450; loss: 2.943; l2dist: 1.609\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.764; l2dist: 0.000\n",
      "    step: 50; loss: 8.422; l2dist: 2.445\n",
      "    step: 100; loss: 4.341; l2dist: 1.894\n",
      "    step: 150; loss: 3.394; l2dist: 1.704\n",
      "    step: 200; loss: 3.126; l2dist: 1.648\n",
      "    step: 250; loss: 3.033; l2dist: 1.633\n",
      "    step: 300; loss: 2.999; l2dist: 1.619\n",
      "    step: 350; loss: 2.972; l2dist: 1.618\n",
      "    step: 400; loss: 2.946; l2dist: 1.613\n",
      "    step: 450; loss: 2.947; l2dist: 1.618\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.062; l2dist: 0.000\n",
      "    step: 50; loss: 8.369; l2dist: 2.430\n",
      "    step: 100; loss: 4.349; l2dist: 1.898\n",
      "    step: 150; loss: 3.405; l2dist: 1.712\n",
      "    step: 200; loss: 3.124; l2dist: 1.653\n",
      "    step: 250; loss: 3.033; l2dist: 1.634\n",
      "    step: 300; loss: 2.982; l2dist: 1.621\n",
      "    step: 350; loss: 2.969; l2dist: 1.624\n",
      "    step: 400; loss: 2.959; l2dist: 1.617\n",
      "    step: 450; loss: 2.949; l2dist: 1.614\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.917; l2dist: 0.000\n",
      "    step: 50; loss: 8.379; l2dist: 2.432\n",
      "    step: 100; loss: 4.354; l2dist: 1.901\n",
      "    step: 150; loss: 3.396; l2dist: 1.719\n",
      "    step: 200; loss: 3.118; l2dist: 1.649\n",
      "    step: 250; loss: 3.035; l2dist: 1.631\n",
      "    step: 300; loss: 3.011; l2dist: 1.626\n",
      "    step: 350; loss: 2.965; l2dist: 1.627\n",
      "    step: 400; loss: 2.948; l2dist: 1.616\n",
      "    step: 450; loss: 2.937; l2dist: 1.619\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.091; l2dist: 0.000\n",
      "    step: 50; loss: 8.412; l2dist: 2.441\n",
      "    step: 100; loss: 4.357; l2dist: 1.905\n",
      "    step: 150; loss: 3.404; l2dist: 1.720\n",
      "    step: 200; loss: 3.138; l2dist: 1.659\n",
      "    step: 250; loss: 3.021; l2dist: 1.635\n",
      "    step: 300; loss: 2.988; l2dist: 1.625\n",
      "    step: 350; loss: 2.961; l2dist: 1.623\n",
      "    step: 400; loss: 2.960; l2dist: 1.619\n",
      "    step: 450; loss: 2.946; l2dist: 1.623\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 137.989; l2dist: 0.000\n",
      "    step: 50; loss: 17.420; l2dist: 3.640\n",
      "    step: 100; loss: 6.885; l2dist: 2.554\n",
      "    step: 150; loss: 4.662; l2dist: 2.088\n",
      "    step: 200; loss: 3.813; l2dist: 1.872\n",
      "    step: 250; loss: 3.396; l2dist: 1.768\n",
      "    step: 300; loss: 3.187; l2dist: 1.711\n",
      "    step: 350; loss: 3.105; l2dist: 1.675\n",
      "    step: 400; loss: 3.090; l2dist: 1.683\n",
      "    step: 450; loss: 3.013; l2dist: 1.656\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 127.663; l2dist: 0.000\n",
      "    step: 50; loss: 15.004; l2dist: 3.361\n",
      "    step: 100; loss: 6.071; l2dist: 2.339\n",
      "    step: 150; loss: 4.261; l2dist: 1.951\n",
      "    step: 200; loss: 3.590; l2dist: 1.788\n",
      "    step: 250; loss: 3.251; l2dist: 1.701\n",
      "    step: 300; loss: 3.092; l2dist: 1.668\n",
      "    step: 350; loss: 2.966; l2dist: 1.646\n",
      "    step: 400; loss: 2.918; l2dist: 1.625\n",
      "    step: 450; loss: 2.831; l2dist: 1.606\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.739; l2dist: 0.000\n",
      "    step: 50; loss: 11.707; l2dist: 2.962\n",
      "    step: 100; loss: 4.782; l2dist: 2.076\n",
      "    step: 150; loss: 3.544; l2dist: 1.782\n",
      "    step: 200; loss: 3.103; l2dist: 1.671\n",
      "    step: 250; loss: 2.926; l2dist: 1.627\n",
      "    step: 300; loss: 2.884; l2dist: 1.611\n",
      "    step: 350; loss: 2.839; l2dist: 1.605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 2.812; l2dist: 1.575\n",
      "    step: 450; loss: 2.755; l2dist: 1.578\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.332; l2dist: 0.000\n",
      "    step: 50; loss: 10.042; l2dist: 2.665\n",
      "    step: 100; loss: 4.396; l2dist: 1.967\n",
      "    step: 150; loss: 3.280; l2dist: 1.712\n",
      "    step: 200; loss: 2.937; l2dist: 1.625\n",
      "    step: 250; loss: 2.832; l2dist: 1.589\n",
      "    step: 300; loss: 2.766; l2dist: 1.568\n",
      "    step: 350; loss: 2.718; l2dist: 1.563\n",
      "    step: 400; loss: 2.672; l2dist: 1.554\n",
      "    step: 450; loss: 2.669; l2dist: 1.553\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.372; l2dist: 0.000\n",
      "    step: 50; loss: 9.250; l2dist: 2.509\n",
      "    step: 100; loss: 4.161; l2dist: 1.891\n",
      "    step: 150; loss: 3.149; l2dist: 1.660\n",
      "    step: 200; loss: 2.886; l2dist: 1.598\n",
      "    step: 250; loss: 2.776; l2dist: 1.570\n",
      "    step: 300; loss: 2.739; l2dist: 1.563\n",
      "    step: 350; loss: 2.683; l2dist: 1.552\n",
      "    step: 400; loss: 2.690; l2dist: 1.549\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.022; l2dist: 0.000\n",
      "    step: 50; loss: 9.018; l2dist: 2.456\n",
      "    step: 100; loss: 4.098; l2dist: 1.859\n",
      "    step: 150; loss: 3.106; l2dist: 1.644\n",
      "    step: 200; loss: 2.861; l2dist: 1.587\n",
      "    step: 250; loss: 2.766; l2dist: 1.559\n",
      "    step: 300; loss: 2.716; l2dist: 1.552\n",
      "    step: 350; loss: 2.691; l2dist: 1.549\n",
      "    step: 400; loss: 2.688; l2dist: 1.540\n",
      "    step: 450; loss: 2.662; l2dist: 1.540\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.754; l2dist: 0.000\n",
      "    step: 50; loss: 8.870; l2dist: 2.439\n",
      "    step: 100; loss: 4.047; l2dist: 1.853\n",
      "    step: 150; loss: 3.088; l2dist: 1.641\n",
      "    step: 200; loss: 2.861; l2dist: 1.587\n",
      "    step: 250; loss: 2.762; l2dist: 1.566\n",
      "    step: 300; loss: 2.709; l2dist: 1.547\n",
      "    step: 350; loss: 2.677; l2dist: 1.540\n",
      "    step: 400; loss: 2.664; l2dist: 1.541\n",
      "    step: 450; loss: 2.668; l2dist: 1.537\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.117; l2dist: 0.000\n",
      "    step: 50; loss: 8.830; l2dist: 2.431\n",
      "    step: 100; loss: 4.059; l2dist: 1.860\n",
      "    step: 150; loss: 3.104; l2dist: 1.657\n",
      "    step: 200; loss: 2.845; l2dist: 1.592\n",
      "    step: 250; loss: 2.754; l2dist: 1.567\n",
      "    step: 300; loss: 2.719; l2dist: 1.559\n",
      "    step: 350; loss: 2.700; l2dist: 1.547\n",
      "    step: 400; loss: 2.691; l2dist: 1.557\n",
      "    step: 450; loss: 2.678; l2dist: 1.551\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.438; l2dist: 0.000\n",
      "    step: 50; loss: 8.780; l2dist: 2.420\n",
      "    step: 100; loss: 4.043; l2dist: 1.858\n",
      "    step: 150; loss: 3.078; l2dist: 1.649\n",
      "    step: 200; loss: 2.841; l2dist: 1.591\n",
      "    step: 250; loss: 2.742; l2dist: 1.566\n",
      "    step: 300; loss: 2.706; l2dist: 1.553\n",
      "    step: 350; loss: 2.691; l2dist: 1.547\n",
      "    step: 400; loss: 2.665; l2dist: 1.543\n",
      "    step: 450; loss: 2.673; l2dist: 1.545\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.629; l2dist: 0.000\n",
      "    step: 50; loss: 8.806; l2dist: 2.428\n",
      "    step: 100; loss: 4.052; l2dist: 1.862\n",
      "    step: 150; loss: 3.086; l2dist: 1.654\n",
      "    step: 200; loss: 2.829; l2dist: 1.586\n",
      "    step: 250; loss: 2.750; l2dist: 1.565\n",
      "    step: 300; loss: 2.710; l2dist: 1.555\n",
      "    step: 350; loss: 2.679; l2dist: 1.549\n",
      "    step: 400; loss: 2.691; l2dist: 1.549\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 125.501; l2dist: 0.000\n",
      "    step: 50; loss: 14.720; l2dist: 3.439\n",
      "    step: 100; loss: 6.200; l2dist: 2.397\n",
      "    step: 150; loss: 4.097; l2dist: 1.925\n",
      "    step: 200; loss: 3.262; l2dist: 1.691\n",
      "    step: 250; loss: 2.906; l2dist: 1.592\n",
      "    step: 300; loss: 2.712; l2dist: 1.537\n",
      "    step: 350; loss: 2.639; l2dist: 1.512\n",
      "    step: 400; loss: 2.590; l2dist: 1.493\n",
      "    step: 450; loss: 2.568; l2dist: 1.489\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 88.230; l2dist: 0.000\n",
      "    step: 50; loss: 11.680; l2dist: 3.038\n",
      "    step: 100; loss: 4.751; l2dist: 2.046\n",
      "    step: 150; loss: 3.301; l2dist: 1.676\n",
      "    step: 200; loss: 2.821; l2dist: 1.552\n",
      "    step: 250; loss: 2.632; l2dist: 1.497\n",
      "    step: 300; loss: 2.503; l2dist: 1.464\n",
      "    step: 350; loss: 2.411; l2dist: 1.435\n",
      "    step: 400; loss: 2.364; l2dist: 1.423\n",
      "    step: 450; loss: 2.378; l2dist: 1.420\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.862; l2dist: 0.000\n",
      "    step: 50; loss: 9.346; l2dist: 2.648\n",
      "    step: 100; loss: 3.763; l2dist: 1.808\n",
      "    step: 150; loss: 2.831; l2dist: 1.548\n",
      "    step: 200; loss: 2.559; l2dist: 1.465\n",
      "    step: 250; loss: 2.426; l2dist: 1.440\n",
      "    step: 300; loss: 2.379; l2dist: 1.424\n",
      "    step: 350; loss: 2.342; l2dist: 1.419\n",
      "    step: 400; loss: 2.339; l2dist: 1.415\n",
      "    step: 450; loss: 2.301; l2dist: 1.407\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.197; l2dist: 0.000\n",
      "    step: 50; loss: 7.918; l2dist: 2.371\n",
      "    step: 100; loss: 3.421; l2dist: 1.719\n",
      "    step: 150; loss: 2.646; l2dist: 1.493\n",
      "    step: 200; loss: 2.430; l2dist: 1.445\n",
      "    step: 250; loss: 2.351; l2dist: 1.420\n",
      "    step: 300; loss: 2.291; l2dist: 1.399\n",
      "    step: 350; loss: 2.287; l2dist: 1.392\n",
      "    step: 400; loss: 2.268; l2dist: 1.389\n",
      "    step: 450; loss: 2.246; l2dist: 1.385\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.581; l2dist: 0.000\n",
      "    step: 50; loss: 7.514; l2dist: 2.267\n",
      "    step: 100; loss: 3.380; l2dist: 1.670\n",
      "    step: 150; loss: 2.616; l2dist: 1.484\n",
      "    step: 200; loss: 2.411; l2dist: 1.430\n",
      "    step: 250; loss: 2.356; l2dist: 1.407\n",
      "    step: 300; loss: 2.310; l2dist: 1.396\n",
      "    step: 350; loss: 2.278; l2dist: 1.387\n",
      "    step: 400; loss: 2.266; l2dist: 1.390\n",
      "    step: 450; loss: 2.283; l2dist: 1.395\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.479; l2dist: 0.000\n",
      "    step: 50; loss: 7.189; l2dist: 2.127\n",
      "    step: 100; loss: 3.307; l2dist: 1.607\n",
      "    step: 150; loss: 2.573; l2dist: 1.444\n",
      "    step: 200; loss: 2.381; l2dist: 1.393\n",
      "    step: 250; loss: 2.312; l2dist: 1.377\n",
      "    step: 300; loss: 2.272; l2dist: 1.368\n",
      "    step: 350; loss: 2.256; l2dist: 1.360\n",
      "    step: 400; loss: 2.242; l2dist: 1.358\n",
      "    step: 450; loss: 2.228; l2dist: 1.357\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.696; l2dist: 0.000\n",
      "    step: 50; loss: 7.173; l2dist: 2.132\n",
      "    step: 100; loss: 3.314; l2dist: 1.620\n",
      "    step: 150; loss: 2.546; l2dist: 1.445\n",
      "    step: 200; loss: 2.374; l2dist: 1.405\n",
      "    step: 250; loss: 2.295; l2dist: 1.379\n",
      "    step: 300; loss: 2.265; l2dist: 1.371\n",
      "    step: 350; loss: 2.244; l2dist: 1.361\n",
      "    step: 400; loss: 2.244; l2dist: 1.364\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.013; l2dist: 0.000\n",
      "    step: 50; loss: 7.119; l2dist: 2.117\n",
      "    step: 100; loss: 3.302; l2dist: 1.617\n",
      "    step: 150; loss: 2.548; l2dist: 1.438\n",
      "    step: 200; loss: 2.371; l2dist: 1.390\n",
      "    step: 250; loss: 2.296; l2dist: 1.376\n",
      "    step: 300; loss: 2.272; l2dist: 1.367\n",
      "    step: 350; loss: 2.247; l2dist: 1.366\n",
      "    step: 400; loss: 2.247; l2dist: 1.364\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.653; l2dist: 0.000\n",
      "    step: 50; loss: 7.083; l2dist: 2.106\n",
      "    step: 100; loss: 3.299; l2dist: 1.613\n",
      "    step: 150; loss: 2.539; l2dist: 1.434\n",
      "    step: 200; loss: 2.366; l2dist: 1.387\n",
      "    step: 250; loss: 2.295; l2dist: 1.379\n",
      "    step: 300; loss: 2.273; l2dist: 1.372\n",
      "    step: 350; loss: 2.234; l2dist: 1.361\n",
      "    step: 400; loss: 2.239; l2dist: 1.358\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.833; l2dist: 0.000\n",
      "    step: 50; loss: 7.118; l2dist: 2.119\n",
      "    step: 100; loss: 3.298; l2dist: 1.618\n",
      "    step: 150; loss: 2.541; l2dist: 1.443\n",
      "    step: 200; loss: 2.369; l2dist: 1.406\n",
      "    step: 250; loss: 2.299; l2dist: 1.378\n",
      "    step: 300; loss: 2.261; l2dist: 1.374\n",
      "    step: 350; loss: 2.236; l2dist: 1.365\n",
      "    step: 400; loss: 2.248; l2dist: 1.363\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 136.738; l2dist: 0.000\n",
      "    step: 50; loss: 16.602; l2dist: 3.550\n",
      "    step: 100; loss: 6.531; l2dist: 2.464\n",
      "    step: 150; loss: 4.411; l2dist: 2.008\n",
      "    step: 200; loss: 3.543; l2dist: 1.786\n",
      "    step: 250; loss: 3.141; l2dist: 1.678\n",
      "    step: 300; loss: 2.908; l2dist: 1.607\n",
      "    step: 350; loss: 2.879; l2dist: 1.595\n",
      "    step: 400; loss: 2.753; l2dist: 1.557\n",
      "    step: 450; loss: 2.775; l2dist: 1.565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 121.991; l2dist: 0.000\n",
      "    step: 50; loss: 14.683; l2dist: 3.265\n",
      "    step: 100; loss: 5.679; l2dist: 2.231\n",
      "    step: 150; loss: 3.988; l2dist: 1.847\n",
      "    step: 200; loss: 3.302; l2dist: 1.687\n",
      "    step: 250; loss: 2.998; l2dist: 1.615\n",
      "    step: 300; loss: 2.854; l2dist: 1.577\n",
      "    step: 350; loss: 2.750; l2dist: 1.545\n",
      "    step: 400; loss: 2.668; l2dist: 1.534\n",
      "    step: 450; loss: 2.684; l2dist: 1.526\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.156; l2dist: 0.000\n",
      "    step: 50; loss: 11.680; l2dist: 2.872\n",
      "    step: 100; loss: 4.588; l2dist: 1.996\n",
      "    step: 150; loss: 3.344; l2dist: 1.709\n",
      "    step: 200; loss: 2.917; l2dist: 1.601\n",
      "    step: 250; loss: 2.764; l2dist: 1.554\n",
      "    step: 300; loss: 2.679; l2dist: 1.536\n",
      "    step: 350; loss: 2.609; l2dist: 1.514\n",
      "    step: 400; loss: 2.602; l2dist: 1.513\n",
      "    step: 450; loss: 2.598; l2dist: 1.513\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.747; l2dist: 0.000\n",
      "    step: 50; loss: 9.783; l2dist: 2.575\n",
      "    step: 100; loss: 4.066; l2dist: 1.873\n",
      "    step: 150; loss: 3.049; l2dist: 1.634\n",
      "    step: 200; loss: 2.762; l2dist: 1.555\n",
      "    step: 250; loss: 2.661; l2dist: 1.523\n",
      "    step: 300; loss: 2.604; l2dist: 1.511\n",
      "    step: 350; loss: 2.558; l2dist: 1.495\n",
      "    step: 400; loss: 2.549; l2dist: 1.494\n",
      "    step: 450; loss: 2.526; l2dist: 1.495\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.801; l2dist: 0.000\n",
      "    step: 50; loss: 8.847; l2dist: 2.409\n",
      "    step: 100; loss: 3.921; l2dist: 1.798\n",
      "    step: 150; loss: 2.969; l2dist: 1.590\n",
      "    step: 200; loss: 2.706; l2dist: 1.526\n",
      "    step: 250; loss: 2.612; l2dist: 1.502\n",
      "    step: 300; loss: 2.542; l2dist: 1.483\n",
      "    step: 350; loss: 2.550; l2dist: 1.483\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.020; l2dist: 0.000\n",
      "    step: 50; loss: 8.586; l2dist: 2.325\n",
      "    step: 100; loss: 3.826; l2dist: 1.765\n",
      "    step: 150; loss: 2.945; l2dist: 1.571\n",
      "    step: 200; loss: 2.710; l2dist: 1.513\n",
      "    step: 250; loss: 2.614; l2dist: 1.483\n",
      "    step: 300; loss: 2.554; l2dist: 1.471\n",
      "    step: 350; loss: 2.520; l2dist: 1.462\n",
      "    step: 400; loss: 2.507; l2dist: 1.466\n",
      "    step: 450; loss: 2.505; l2dist: 1.465\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.974; l2dist: 0.000\n",
      "    step: 50; loss: 8.453; l2dist: 2.315\n",
      "    step: 100; loss: 3.795; l2dist: 1.764\n",
      "    step: 150; loss: 2.905; l2dist: 1.567\n",
      "    step: 200; loss: 2.660; l2dist: 1.507\n",
      "    step: 250; loss: 2.573; l2dist: 1.480\n",
      "    step: 300; loss: 2.536; l2dist: 1.469\n",
      "    step: 350; loss: 2.524; l2dist: 1.469\n",
      "    step: 400; loss: 2.502; l2dist: 1.461\n",
      "    step: 450; loss: 2.486; l2dist: 1.454\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.801; l2dist: 0.000\n",
      "    step: 50; loss: 8.360; l2dist: 2.301\n",
      "    step: 100; loss: 3.771; l2dist: 1.762\n",
      "    step: 150; loss: 2.908; l2dist: 1.558\n",
      "    step: 200; loss: 2.680; l2dist: 1.504\n",
      "    step: 250; loss: 2.591; l2dist: 1.484\n",
      "    step: 300; loss: 2.547; l2dist: 1.472\n",
      "    step: 350; loss: 2.520; l2dist: 1.463\n",
      "    step: 400; loss: 2.505; l2dist: 1.458\n",
      "    step: 450; loss: 2.495; l2dist: 1.462\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.061; l2dist: 0.000\n",
      "    step: 50; loss: 8.284; l2dist: 2.289\n",
      "    step: 100; loss: 3.771; l2dist: 1.755\n",
      "    step: 150; loss: 2.897; l2dist: 1.557\n",
      "    step: 200; loss: 2.670; l2dist: 1.497\n",
      "    step: 250; loss: 2.588; l2dist: 1.481\n",
      "    step: 300; loss: 2.527; l2dist: 1.468\n",
      "    step: 350; loss: 2.529; l2dist: 1.466\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.318; l2dist: 0.000\n",
      "    step: 50; loss: 8.329; l2dist: 2.299\n",
      "    step: 100; loss: 3.784; l2dist: 1.762\n",
      "    step: 150; loss: 2.909; l2dist: 1.563\n",
      "    step: 200; loss: 2.681; l2dist: 1.513\n",
      "    step: 250; loss: 2.578; l2dist: 1.485\n",
      "    step: 300; loss: 2.541; l2dist: 1.471\n",
      "    step: 350; loss: 2.517; l2dist: 1.472\n",
      "    step: 400; loss: 2.495; l2dist: 1.460\n",
      "    step: 450; loss: 2.520; l2dist: 1.470\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 134.058; l2dist: 0.000\n",
      "    step: 50; loss: 14.105; l2dist: 3.522\n",
      "    step: 100; loss: 6.442; l2dist: 2.452\n",
      "    step: 150; loss: 4.303; l2dist: 1.988\n",
      "    step: 200; loss: 3.443; l2dist: 1.771\n",
      "    step: 250; loss: 3.050; l2dist: 1.662\n",
      "    step: 300; loss: 2.892; l2dist: 1.606\n",
      "    step: 350; loss: 2.804; l2dist: 1.597\n",
      "    step: 400; loss: 2.836; l2dist: 1.598\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 118.534; l2dist: 0.000\n",
      "    step: 50; loss: 12.694; l2dist: 3.290\n",
      "    step: 100; loss: 5.705; l2dist: 2.263\n",
      "    step: 150; loss: 3.912; l2dist: 1.861\n",
      "    step: 200; loss: 3.268; l2dist: 1.700\n",
      "    step: 250; loss: 2.987; l2dist: 1.631\n",
      "    step: 300; loss: 2.864; l2dist: 1.597\n",
      "    step: 350; loss: 2.748; l2dist: 1.552\n",
      "    step: 400; loss: 2.665; l2dist: 1.533\n",
      "    step: 450; loss: 2.622; l2dist: 1.530\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.121; l2dist: 0.000\n",
      "    step: 50; loss: 9.917; l2dist: 2.822\n",
      "    step: 100; loss: 4.343; l2dist: 1.980\n",
      "    step: 150; loss: 3.215; l2dist: 1.687\n",
      "    step: 200; loss: 2.881; l2dist: 1.598\n",
      "    step: 250; loss: 2.680; l2dist: 1.540\n",
      "    step: 300; loss: 2.576; l2dist: 1.516\n",
      "    step: 350; loss: 2.555; l2dist: 1.502\n",
      "    step: 400; loss: 2.506; l2dist: 1.490\n",
      "    step: 450; loss: 2.542; l2dist: 1.506\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.886; l2dist: 0.000\n",
      "    step: 50; loss: 8.353; l2dist: 2.532\n",
      "    step: 100; loss: 3.894; l2dist: 1.861\n",
      "    step: 150; loss: 2.953; l2dist: 1.616\n",
      "    step: 200; loss: 2.689; l2dist: 1.550\n",
      "    step: 250; loss: 2.577; l2dist: 1.513\n",
      "    step: 300; loss: 2.513; l2dist: 1.493\n",
      "    step: 350; loss: 2.521; l2dist: 1.494\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.524; l2dist: 0.000\n",
      "    step: 50; loss: 7.460; l2dist: 2.355\n",
      "    step: 100; loss: 3.783; l2dist: 1.776\n",
      "    step: 150; loss: 2.832; l2dist: 1.569\n",
      "    step: 200; loss: 2.595; l2dist: 1.510\n",
      "    step: 250; loss: 2.497; l2dist: 1.477\n",
      "    step: 300; loss: 2.440; l2dist: 1.469\n",
      "    step: 350; loss: 2.482; l2dist: 1.464\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.736; l2dist: 0.000\n",
      "    step: 50; loss: 7.251; l2dist: 2.329\n",
      "    step: 100; loss: 3.705; l2dist: 1.762\n",
      "    step: 150; loss: 2.811; l2dist: 1.563\n",
      "    step: 200; loss: 2.585; l2dist: 1.503\n",
      "    step: 250; loss: 2.506; l2dist: 1.474\n",
      "    step: 300; loss: 2.459; l2dist: 1.460\n",
      "    step: 350; loss: 2.450; l2dist: 1.468\n",
      "    step: 400; loss: 2.435; l2dist: 1.453\n",
      "    step: 450; loss: 2.426; l2dist: 1.452\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.443; l2dist: 0.000\n",
      "    step: 50; loss: 7.142; l2dist: 2.299\n",
      "    step: 100; loss: 3.691; l2dist: 1.751\n",
      "    step: 150; loss: 2.793; l2dist: 1.549\n",
      "    step: 200; loss: 2.592; l2dist: 1.500\n",
      "    step: 250; loss: 2.499; l2dist: 1.478\n",
      "    step: 300; loss: 2.474; l2dist: 1.469\n",
      "    step: 350; loss: 2.442; l2dist: 1.461\n",
      "    step: 400; loss: 2.434; l2dist: 1.453\n",
      "    step: 450; loss: 2.449; l2dist: 1.456\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.568; l2dist: 0.000\n",
      "    step: 50; loss: 7.117; l2dist: 2.295\n",
      "    step: 100; loss: 3.682; l2dist: 1.751\n",
      "    step: 150; loss: 2.790; l2dist: 1.553\n",
      "    step: 200; loss: 2.573; l2dist: 1.496\n",
      "    step: 250; loss: 2.492; l2dist: 1.478\n",
      "    step: 300; loss: 2.480; l2dist: 1.462\n",
      "    step: 350; loss: 2.447; l2dist: 1.466\n",
      "    step: 400; loss: 2.432; l2dist: 1.458\n",
      "    step: 450; loss: 2.430; l2dist: 1.452\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.069; l2dist: 0.000\n",
      "    step: 50; loss: 7.102; l2dist: 2.290\n",
      "    step: 100; loss: 3.676; l2dist: 1.750\n",
      "    step: 150; loss: 2.787; l2dist: 1.556\n",
      "    step: 200; loss: 2.578; l2dist: 1.505\n",
      "    step: 250; loss: 2.514; l2dist: 1.474\n",
      "    step: 300; loss: 2.465; l2dist: 1.466\n",
      "    step: 350; loss: 2.451; l2dist: 1.467\n",
      "    step: 400; loss: 2.455; l2dist: 1.460\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.292; l2dist: 0.000\n",
      "    step: 50; loss: 7.134; l2dist: 2.298\n",
      "    step: 100; loss: 3.676; l2dist: 1.754\n",
      "    step: 150; loss: 2.792; l2dist: 1.554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 200; loss: 2.574; l2dist: 1.506\n",
      "    step: 250; loss: 2.502; l2dist: 1.479\n",
      "    step: 300; loss: 2.460; l2dist: 1.466\n",
      "    step: 350; loss: 2.460; l2dist: 1.464\n",
      "    step: 400; loss: 2.438; l2dist: 1.464\n",
      "    step: 450; loss: 2.442; l2dist: 1.460\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 104.523; l2dist: 0.000\n",
      "    step: 50; loss: 10.339; l2dist: 3.045\n",
      "    step: 100; loss: 4.607; l2dist: 2.048\n",
      "    step: 150; loss: 3.169; l2dist: 1.679\n",
      "    step: 200; loss: 2.554; l2dist: 1.500\n",
      "    step: 250; loss: 2.287; l2dist: 1.411\n",
      "    step: 300; loss: 2.168; l2dist: 1.374\n",
      "    step: 350; loss: 2.066; l2dist: 1.346\n",
      "    step: 400; loss: 2.065; l2dist: 1.340\n",
      "    step: 450; loss: 2.096; l2dist: 1.339\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 116.862; l2dist: 0.000\n",
      "    step: 50; loss: 10.312; l2dist: 2.989\n",
      "    step: 100; loss: 4.617; l2dist: 2.020\n",
      "    step: 150; loss: 3.180; l2dist: 1.651\n",
      "    step: 200; loss: 2.610; l2dist: 1.497\n",
      "    step: 250; loss: 2.372; l2dist: 1.425\n",
      "    step: 300; loss: 2.276; l2dist: 1.395\n",
      "    step: 350; loss: 2.145; l2dist: 1.358\n",
      "    step: 400; loss: 2.110; l2dist: 1.342\n",
      "    step: 450; loss: 2.065; l2dist: 1.328\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.423; l2dist: 0.000\n",
      "    step: 50; loss: 8.128; l2dist: 2.578\n",
      "    step: 100; loss: 3.568; l2dist: 1.757\n",
      "    step: 150; loss: 2.608; l2dist: 1.495\n",
      "    step: 200; loss: 2.268; l2dist: 1.399\n",
      "    step: 250; loss: 2.136; l2dist: 1.355\n",
      "    step: 300; loss: 2.014; l2dist: 1.320\n",
      "    step: 350; loss: 1.986; l2dist: 1.308\n",
      "    step: 400; loss: 1.993; l2dist: 1.310\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.051; l2dist: 0.000\n",
      "    step: 50; loss: 6.854; l2dist: 2.273\n",
      "    step: 100; loss: 3.084; l2dist: 1.630\n",
      "    step: 150; loss: 2.327; l2dist: 1.422\n",
      "    step: 200; loss: 2.078; l2dist: 1.341\n",
      "    step: 250; loss: 1.966; l2dist: 1.306\n",
      "    step: 300; loss: 1.935; l2dist: 1.287\n",
      "    step: 350; loss: 1.864; l2dist: 1.271\n",
      "    step: 400; loss: 1.859; l2dist: 1.277\n",
      "    step: 450; loss: 1.847; l2dist: 1.266\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.118; l2dist: 0.000\n",
      "    step: 50; loss: 6.086; l2dist: 2.082\n",
      "    step: 100; loss: 2.944; l2dist: 1.559\n",
      "    step: 150; loss: 2.245; l2dist: 1.380\n",
      "    step: 200; loss: 2.020; l2dist: 1.306\n",
      "    step: 250; loss: 1.925; l2dist: 1.280\n",
      "    step: 300; loss: 1.866; l2dist: 1.264\n",
      "    step: 350; loss: 1.848; l2dist: 1.254\n",
      "    step: 400; loss: 1.843; l2dist: 1.257\n",
      "    step: 450; loss: 1.841; l2dist: 1.254\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.714; l2dist: 0.000\n",
      "    step: 50; loss: 5.675; l2dist: 1.986\n",
      "    step: 100; loss: 2.846; l2dist: 1.503\n",
      "    step: 150; loss: 2.171; l2dist: 1.332\n",
      "    step: 200; loss: 1.970; l2dist: 1.279\n",
      "    step: 250; loss: 1.870; l2dist: 1.249\n",
      "    step: 300; loss: 1.833; l2dist: 1.248\n",
      "    step: 350; loss: 1.808; l2dist: 1.234\n",
      "    step: 400; loss: 1.809; l2dist: 1.234\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.424; l2dist: 0.000\n",
      "    step: 50; loss: 5.577; l2dist: 1.957\n",
      "    step: 100; loss: 2.809; l2dist: 1.496\n",
      "    step: 150; loss: 2.140; l2dist: 1.325\n",
      "    step: 200; loss: 1.954; l2dist: 1.269\n",
      "    step: 250; loss: 1.859; l2dist: 1.245\n",
      "    step: 300; loss: 1.816; l2dist: 1.239\n",
      "    step: 350; loss: 1.819; l2dist: 1.236\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.354; l2dist: 0.000\n",
      "    step: 50; loss: 5.541; l2dist: 1.950\n",
      "    step: 100; loss: 2.803; l2dist: 1.487\n",
      "    step: 150; loss: 2.144; l2dist: 1.323\n",
      "    step: 200; loss: 1.937; l2dist: 1.265\n",
      "    step: 250; loss: 1.858; l2dist: 1.246\n",
      "    step: 300; loss: 1.836; l2dist: 1.237\n",
      "    step: 350; loss: 1.812; l2dist: 1.229\n",
      "    step: 400; loss: 1.801; l2dist: 1.224\n",
      "    step: 450; loss: 1.788; l2dist: 1.226\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.788; l2dist: 0.000\n",
      "    step: 50; loss: 5.523; l2dist: 1.942\n",
      "    step: 100; loss: 2.799; l2dist: 1.487\n",
      "    step: 150; loss: 2.131; l2dist: 1.326\n",
      "    step: 200; loss: 1.931; l2dist: 1.270\n",
      "    step: 250; loss: 1.855; l2dist: 1.250\n",
      "    step: 300; loss: 1.822; l2dist: 1.235\n",
      "    step: 350; loss: 1.817; l2dist: 1.237\n",
      "    step: 400; loss: 1.793; l2dist: 1.232\n",
      "    step: 450; loss: 1.796; l2dist: 1.229\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.946; l2dist: 0.000\n",
      "    step: 50; loss: 5.539; l2dist: 1.952\n",
      "    step: 100; loss: 2.797; l2dist: 1.498\n",
      "    step: 150; loss: 2.120; l2dist: 1.327\n",
      "    step: 200; loss: 1.924; l2dist: 1.276\n",
      "    step: 250; loss: 1.871; l2dist: 1.259\n",
      "    step: 300; loss: 1.821; l2dist: 1.240\n",
      "    step: 350; loss: 1.815; l2dist: 1.237\n",
      "    step: 400; loss: 1.794; l2dist: 1.232\n",
      "    step: 450; loss: 1.802; l2dist: 1.238\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 112.517; l2dist: 0.000\n",
      "    step: 50; loss: 13.808; l2dist: 3.117\n",
      "    step: 100; loss: 5.133; l2dist: 2.161\n",
      "    step: 150; loss: 3.551; l2dist: 1.776\n",
      "    step: 200; loss: 2.985; l2dist: 1.608\n",
      "    step: 250; loss: 2.687; l2dist: 1.515\n",
      "    step: 300; loss: 2.583; l2dist: 1.490\n",
      "    step: 350; loss: 2.450; l2dist: 1.450\n",
      "    step: 400; loss: 2.388; l2dist: 1.435\n",
      "    step: 450; loss: 2.418; l2dist: 1.432\n",
      "binary step: 0; number of successful adv: 91/100\n",
      "    step: 0; loss: 158.186; l2dist: 0.000\n",
      "    step: 50; loss: 14.116; l2dist: 3.193\n",
      "    step: 100; loss: 5.905; l2dist: 2.265\n",
      "    step: 150; loss: 4.149; l2dist: 1.871\n",
      "    step: 200; loss: 3.342; l2dist: 1.671\n",
      "    step: 250; loss: 2.968; l2dist: 1.582\n",
      "    step: 300; loss: 2.707; l2dist: 1.511\n",
      "    step: 350; loss: 2.612; l2dist: 1.483\n",
      "    step: 400; loss: 2.515; l2dist: 1.453\n",
      "    step: 450; loss: 2.512; l2dist: 1.448\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.907; l2dist: 0.000\n",
      "    step: 50; loss: 10.789; l2dist: 2.737\n",
      "    step: 100; loss: 4.401; l2dist: 1.941\n",
      "    step: 150; loss: 3.183; l2dist: 1.639\n",
      "    step: 200; loss: 2.717; l2dist: 1.515\n",
      "    step: 250; loss: 2.492; l2dist: 1.453\n",
      "    step: 300; loss: 2.382; l2dist: 1.422\n",
      "    step: 350; loss: 2.363; l2dist: 1.409\n",
      "    step: 400; loss: 2.308; l2dist: 1.405\n",
      "    step: 450; loss: 2.289; l2dist: 1.395\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.469; l2dist: 0.000\n",
      "    step: 50; loss: 8.763; l2dist: 2.413\n",
      "    step: 100; loss: 3.733; l2dist: 1.774\n",
      "    step: 150; loss: 2.775; l2dist: 1.531\n",
      "    step: 200; loss: 2.517; l2dist: 1.457\n",
      "    step: 250; loss: 2.386; l2dist: 1.421\n",
      "    step: 300; loss: 2.320; l2dist: 1.397\n",
      "    step: 350; loss: 2.293; l2dist: 1.391\n",
      "    step: 400; loss: 2.263; l2dist: 1.379\n",
      "    step: 450; loss: 2.265; l2dist: 1.383\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.307; l2dist: 0.000\n",
      "    step: 50; loss: 7.385; l2dist: 2.110\n",
      "    step: 100; loss: 3.396; l2dist: 1.626\n",
      "    step: 150; loss: 2.617; l2dist: 1.454\n",
      "    step: 200; loss: 2.380; l2dist: 1.395\n",
      "    step: 250; loss: 2.290; l2dist: 1.369\n",
      "    step: 300; loss: 2.249; l2dist: 1.362\n",
      "    step: 350; loss: 2.222; l2dist: 1.348\n",
      "    step: 400; loss: 2.200; l2dist: 1.353\n",
      "    step: 450; loss: 2.186; l2dist: 1.343\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.333; l2dist: 0.000\n",
      "    step: 50; loss: 6.857; l2dist: 2.000\n",
      "    step: 100; loss: 3.298; l2dist: 1.572\n",
      "    step: 150; loss: 2.528; l2dist: 1.414\n",
      "    step: 200; loss: 2.335; l2dist: 1.361\n",
      "    step: 250; loss: 2.248; l2dist: 1.344\n",
      "    step: 300; loss: 2.198; l2dist: 1.337\n",
      "    step: 350; loss: 2.183; l2dist: 1.325\n",
      "    step: 400; loss: 2.175; l2dist: 1.324\n",
      "    step: 450; loss: 2.151; l2dist: 1.325\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.853; l2dist: 0.000\n",
      "    step: 50; loss: 6.596; l2dist: 1.944\n",
      "    step: 100; loss: 3.256; l2dist: 1.555\n",
      "    step: 150; loss: 2.511; l2dist: 1.396\n",
      "    step: 200; loss: 2.305; l2dist: 1.359\n",
      "    step: 250; loss: 2.226; l2dist: 1.337\n",
      "    step: 300; loss: 2.184; l2dist: 1.330\n",
      "    step: 350; loss: 2.174; l2dist: 1.327\n",
      "    step: 400; loss: 2.153; l2dist: 1.320\n",
      "    step: 450; loss: 2.147; l2dist: 1.318\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.342; l2dist: 0.000\n",
      "    step: 50; loss: 6.514; l2dist: 1.927\n",
      "    step: 100; loss: 3.245; l2dist: 1.554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 2.494; l2dist: 1.401\n",
      "    step: 200; loss: 2.302; l2dist: 1.357\n",
      "    step: 250; loss: 2.248; l2dist: 1.342\n",
      "    step: 300; loss: 2.187; l2dist: 1.330\n",
      "    step: 350; loss: 2.167; l2dist: 1.324\n",
      "    step: 400; loss: 2.150; l2dist: 1.316\n",
      "    step: 450; loss: 2.143; l2dist: 1.318\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.506; l2dist: 0.000\n",
      "    step: 50; loss: 6.450; l2dist: 1.919\n",
      "    step: 100; loss: 3.246; l2dist: 1.549\n",
      "    step: 150; loss: 2.495; l2dist: 1.405\n",
      "    step: 200; loss: 2.301; l2dist: 1.362\n",
      "    step: 250; loss: 2.231; l2dist: 1.340\n",
      "    step: 300; loss: 2.194; l2dist: 1.333\n",
      "    step: 350; loss: 2.164; l2dist: 1.326\n",
      "    step: 400; loss: 2.154; l2dist: 1.326\n",
      "    step: 450; loss: 2.140; l2dist: 1.318\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.700; l2dist: 0.000\n",
      "    step: 50; loss: 6.492; l2dist: 1.930\n",
      "    step: 100; loss: 3.252; l2dist: 1.558\n",
      "    step: 150; loss: 2.496; l2dist: 1.409\n",
      "    step: 200; loss: 2.303; l2dist: 1.363\n",
      "    step: 250; loss: 2.228; l2dist: 1.343\n",
      "    step: 300; loss: 2.196; l2dist: 1.329\n",
      "    step: 350; loss: 2.163; l2dist: 1.328\n",
      "    step: 400; loss: 2.163; l2dist: 1.327\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 150.636; l2dist: 0.000\n",
      "    step: 50; loss: 17.404; l2dist: 3.771\n",
      "    step: 100; loss: 7.665; l2dist: 2.703\n",
      "    step: 150; loss: 5.419; l2dist: 2.250\n",
      "    step: 200; loss: 4.511; l2dist: 2.052\n",
      "    step: 250; loss: 4.086; l2dist: 1.950\n",
      "    step: 300; loss: 3.886; l2dist: 1.894\n",
      "    step: 350; loss: 3.775; l2dist: 1.864\n",
      "    step: 400; loss: 3.715; l2dist: 1.843\n",
      "    step: 450; loss: 3.725; l2dist: 1.847\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 137.971; l2dist: 0.000\n",
      "    step: 50; loss: 15.307; l2dist: 3.544\n",
      "    step: 100; loss: 6.870; l2dist: 2.520\n",
      "    step: 150; loss: 5.024; l2dist: 2.140\n",
      "    step: 200; loss: 4.274; l2dist: 1.964\n",
      "    step: 250; loss: 3.965; l2dist: 1.892\n",
      "    step: 300; loss: 3.760; l2dist: 1.846\n",
      "    step: 350; loss: 3.597; l2dist: 1.814\n",
      "    step: 400; loss: 3.528; l2dist: 1.790\n",
      "    step: 450; loss: 3.537; l2dist: 1.792\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.218; l2dist: 0.000\n",
      "    step: 50; loss: 11.853; l2dist: 3.064\n",
      "    step: 100; loss: 5.457; l2dist: 2.236\n",
      "    step: 150; loss: 4.227; l2dist: 1.952\n",
      "    step: 200; loss: 3.819; l2dist: 1.859\n",
      "    step: 250; loss: 3.588; l2dist: 1.803\n",
      "    step: 300; loss: 3.490; l2dist: 1.787\n",
      "    step: 350; loss: 3.442; l2dist: 1.778\n",
      "    step: 400; loss: 3.417; l2dist: 1.765\n",
      "    step: 450; loss: 3.438; l2dist: 1.764\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.869; l2dist: 0.000\n",
      "    step: 50; loss: 10.442; l2dist: 2.815\n",
      "    step: 100; loss: 5.062; l2dist: 2.129\n",
      "    step: 150; loss: 3.994; l2dist: 1.902\n",
      "    step: 200; loss: 3.653; l2dist: 1.818\n",
      "    step: 250; loss: 3.531; l2dist: 1.790\n",
      "    step: 300; loss: 3.471; l2dist: 1.769\n",
      "    step: 350; loss: 3.448; l2dist: 1.764\n",
      "    step: 400; loss: 3.429; l2dist: 1.758\n",
      "    step: 450; loss: 3.437; l2dist: 1.765\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.783; l2dist: 0.000\n",
      "    step: 50; loss: 9.801; l2dist: 2.695\n",
      "    step: 100; loss: 4.869; l2dist: 2.057\n",
      "    step: 150; loss: 3.853; l2dist: 1.848\n",
      "    step: 200; loss: 3.581; l2dist: 1.795\n",
      "    step: 250; loss: 3.464; l2dist: 1.768\n",
      "    step: 300; loss: 3.422; l2dist: 1.755\n",
      "    step: 350; loss: 3.406; l2dist: 1.746\n",
      "    step: 400; loss: 3.389; l2dist: 1.749\n",
      "    step: 450; loss: 3.397; l2dist: 1.744\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.979; l2dist: 0.000\n",
      "    step: 50; loss: 9.490; l2dist: 2.650\n",
      "    step: 100; loss: 4.780; l2dist: 2.056\n",
      "    step: 150; loss: 3.810; l2dist: 1.844\n",
      "    step: 200; loss: 3.540; l2dist: 1.786\n",
      "    step: 250; loss: 3.445; l2dist: 1.766\n",
      "    step: 300; loss: 3.412; l2dist: 1.751\n",
      "    step: 350; loss: 3.388; l2dist: 1.750\n",
      "    step: 400; loss: 3.374; l2dist: 1.745\n",
      "    step: 450; loss: 3.355; l2dist: 1.748\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.504; l2dist: 0.000\n",
      "    step: 50; loss: 9.409; l2dist: 2.645\n",
      "    step: 100; loss: 4.751; l2dist: 2.041\n",
      "    step: 150; loss: 3.793; l2dist: 1.846\n",
      "    step: 200; loss: 3.550; l2dist: 1.790\n",
      "    step: 250; loss: 3.467; l2dist: 1.770\n",
      "    step: 300; loss: 3.405; l2dist: 1.751\n",
      "    step: 350; loss: 3.379; l2dist: 1.751\n",
      "    step: 400; loss: 3.359; l2dist: 1.741\n",
      "    step: 450; loss: 3.369; l2dist: 1.737\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.488; l2dist: 0.000\n",
      "    step: 50; loss: 9.346; l2dist: 2.640\n",
      "    step: 100; loss: 4.737; l2dist: 2.046\n",
      "    step: 150; loss: 3.783; l2dist: 1.838\n",
      "    step: 200; loss: 3.521; l2dist: 1.779\n",
      "    step: 250; loss: 3.438; l2dist: 1.758\n",
      "    step: 300; loss: 3.412; l2dist: 1.748\n",
      "    step: 350; loss: 3.383; l2dist: 1.742\n",
      "    step: 400; loss: 3.395; l2dist: 1.745\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.876; l2dist: 0.000\n",
      "    step: 50; loss: 9.315; l2dist: 2.632\n",
      "    step: 100; loss: 4.721; l2dist: 2.038\n",
      "    step: 150; loss: 3.778; l2dist: 1.837\n",
      "    step: 200; loss: 3.530; l2dist: 1.783\n",
      "    step: 250; loss: 3.460; l2dist: 1.757\n",
      "    step: 300; loss: 3.438; l2dist: 1.746\n",
      "    step: 350; loss: 3.387; l2dist: 1.742\n",
      "    step: 400; loss: 3.374; l2dist: 1.744\n",
      "    step: 450; loss: 3.370; l2dist: 1.740\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.037; l2dist: 0.000\n",
      "    step: 50; loss: 9.337; l2dist: 2.639\n",
      "    step: 100; loss: 4.731; l2dist: 2.045\n",
      "    step: 150; loss: 3.785; l2dist: 1.841\n",
      "    step: 200; loss: 3.541; l2dist: 1.781\n",
      "    step: 250; loss: 3.444; l2dist: 1.758\n",
      "    step: 300; loss: 3.408; l2dist: 1.758\n",
      "    step: 350; loss: 3.379; l2dist: 1.751\n",
      "    step: 400; loss: 3.384; l2dist: 1.743\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 168.486; l2dist: 0.000\n",
      "    step: 50; loss: 19.911; l2dist: 4.031\n",
      "    step: 100; loss: 8.519; l2dist: 2.886\n",
      "    step: 150; loss: 5.684; l2dist: 2.344\n",
      "    step: 200; loss: 4.578; l2dist: 2.084\n",
      "    step: 250; loss: 4.069; l2dist: 1.951\n",
      "    step: 300; loss: 3.830; l2dist: 1.898\n",
      "    step: 350; loss: 3.672; l2dist: 1.847\n",
      "    step: 400; loss: 3.517; l2dist: 1.819\n",
      "    step: 450; loss: 3.501; l2dist: 1.802\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 100.782; l2dist: 0.000\n",
      "    step: 50; loss: 15.445; l2dist: 3.575\n",
      "    step: 100; loss: 6.113; l2dist: 2.411\n",
      "    step: 150; loss: 4.444; l2dist: 2.033\n",
      "    step: 200; loss: 3.854; l2dist: 1.891\n",
      "    step: 250; loss: 3.616; l2dist: 1.827\n",
      "    step: 300; loss: 3.458; l2dist: 1.787\n",
      "    step: 350; loss: 3.425; l2dist: 1.775\n",
      "    step: 400; loss: 3.336; l2dist: 1.765\n",
      "    step: 450; loss: 3.352; l2dist: 1.756\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.301; l2dist: 0.000\n",
      "    step: 50; loss: 12.289; l2dist: 3.117\n",
      "    step: 100; loss: 4.987; l2dist: 2.168\n",
      "    step: 150; loss: 3.813; l2dist: 1.885\n",
      "    step: 200; loss: 3.460; l2dist: 1.786\n",
      "    step: 250; loss: 3.295; l2dist: 1.746\n",
      "    step: 300; loss: 3.261; l2dist: 1.733\n",
      "    step: 350; loss: 3.201; l2dist: 1.728\n",
      "    step: 400; loss: 3.194; l2dist: 1.722\n",
      "    step: 450; loss: 3.166; l2dist: 1.719\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.638; l2dist: 0.000\n",
      "    step: 50; loss: 10.993; l2dist: 2.865\n",
      "    step: 100; loss: 4.729; l2dist: 2.077\n",
      "    step: 150; loss: 3.677; l2dist: 1.844\n",
      "    step: 200; loss: 3.382; l2dist: 1.771\n",
      "    step: 250; loss: 3.262; l2dist: 1.733\n",
      "    step: 300; loss: 3.228; l2dist: 1.732\n",
      "    step: 350; loss: 3.216; l2dist: 1.728\n",
      "    step: 400; loss: 3.177; l2dist: 1.720\n",
      "    step: 450; loss: 3.184; l2dist: 1.720\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.941; l2dist: 0.000\n",
      "    step: 50; loss: 9.985; l2dist: 2.668\n",
      "    step: 100; loss: 4.584; l2dist: 1.993\n",
      "    step: 150; loss: 3.539; l2dist: 1.786\n",
      "    step: 200; loss: 3.280; l2dist: 1.727\n",
      "    step: 250; loss: 3.190; l2dist: 1.700\n",
      "    step: 300; loss: 3.157; l2dist: 1.690\n",
      "    step: 350; loss: 3.118; l2dist: 1.682\n",
      "    step: 400; loss: 3.101; l2dist: 1.685\n",
      "    step: 450; loss: 3.105; l2dist: 1.687\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.095; l2dist: 0.000\n",
      "    step: 50; loss: 9.693; l2dist: 2.609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 4.524; l2dist: 1.967\n",
      "    step: 150; loss: 3.529; l2dist: 1.777\n",
      "    step: 200; loss: 3.267; l2dist: 1.716\n",
      "    step: 250; loss: 3.159; l2dist: 1.691\n",
      "    step: 300; loss: 3.122; l2dist: 1.682\n",
      "    step: 350; loss: 3.100; l2dist: 1.684\n",
      "    step: 400; loss: 3.088; l2dist: 1.680\n",
      "    step: 450; loss: 3.082; l2dist: 1.676\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.066; l2dist: 0.000\n",
      "    step: 50; loss: 9.664; l2dist: 2.609\n",
      "    step: 100; loss: 4.523; l2dist: 1.975\n",
      "    step: 150; loss: 3.528; l2dist: 1.781\n",
      "    step: 200; loss: 3.272; l2dist: 1.728\n",
      "    step: 250; loss: 3.192; l2dist: 1.696\n",
      "    step: 300; loss: 3.152; l2dist: 1.693\n",
      "    step: 350; loss: 3.110; l2dist: 1.685\n",
      "    step: 400; loss: 3.097; l2dist: 1.683\n",
      "    step: 450; loss: 3.089; l2dist: 1.681\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.575; l2dist: 0.000\n",
      "    step: 50; loss: 9.659; l2dist: 2.608\n",
      "    step: 100; loss: 4.532; l2dist: 1.979\n",
      "    step: 150; loss: 3.533; l2dist: 1.784\n",
      "    step: 200; loss: 3.286; l2dist: 1.724\n",
      "    step: 250; loss: 3.184; l2dist: 1.710\n",
      "    step: 300; loss: 3.142; l2dist: 1.698\n",
      "    step: 350; loss: 3.124; l2dist: 1.690\n",
      "    step: 400; loss: 3.113; l2dist: 1.692\n",
      "    step: 450; loss: 3.118; l2dist: 1.689\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.322; l2dist: 0.000\n",
      "    step: 50; loss: 9.640; l2dist: 2.605\n",
      "    step: 100; loss: 4.560; l2dist: 1.984\n",
      "    step: 150; loss: 3.551; l2dist: 1.787\n",
      "    step: 200; loss: 3.300; l2dist: 1.733\n",
      "    step: 250; loss: 3.201; l2dist: 1.718\n",
      "    step: 300; loss: 3.160; l2dist: 1.699\n",
      "    step: 350; loss: 3.145; l2dist: 1.700\n",
      "    step: 400; loss: 3.123; l2dist: 1.697\n",
      "    step: 450; loss: 3.124; l2dist: 1.691\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.626; l2dist: 0.000\n",
      "    step: 50; loss: 9.692; l2dist: 2.620\n",
      "    step: 100; loss: 4.565; l2dist: 1.994\n",
      "    step: 150; loss: 3.551; l2dist: 1.794\n",
      "    step: 200; loss: 3.297; l2dist: 1.735\n",
      "    step: 250; loss: 3.207; l2dist: 1.714\n",
      "    step: 300; loss: 3.168; l2dist: 1.710\n",
      "    step: 350; loss: 3.142; l2dist: 1.704\n",
      "    step: 400; loss: 3.135; l2dist: 1.693\n",
      "    step: 450; loss: 3.119; l2dist: 1.699\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 162.691; l2dist: 0.000\n",
      "    step: 50; loss: 20.302; l2dist: 4.086\n",
      "    step: 100; loss: 8.655; l2dist: 2.880\n",
      "    step: 150; loss: 5.919; l2dist: 2.364\n",
      "    step: 200; loss: 4.740; l2dist: 2.103\n",
      "    step: 250; loss: 4.318; l2dist: 1.991\n",
      "    step: 300; loss: 4.006; l2dist: 1.925\n",
      "    step: 350; loss: 3.851; l2dist: 1.882\n",
      "    step: 400; loss: 3.806; l2dist: 1.863\n",
      "    step: 450; loss: 3.708; l2dist: 1.834\n",
      "binary step: 0; number of successful adv: 91/100\n",
      "    step: 0; loss: 225.794; l2dist: 0.000\n",
      "    step: 50; loss: 21.403; l2dist: 4.099\n",
      "    step: 100; loss: 9.906; l2dist: 3.004\n",
      "    step: 150; loss: 6.911; l2dist: 2.479\n",
      "    step: 200; loss: 5.547; l2dist: 2.207\n",
      "    step: 250; loss: 4.693; l2dist: 2.040\n",
      "    step: 300; loss: 4.301; l2dist: 1.964\n",
      "    step: 350; loss: 4.081; l2dist: 1.913\n",
      "    step: 400; loss: 3.966; l2dist: 1.881\n",
      "    step: 450; loss: 3.900; l2dist: 1.865\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 128.435; l2dist: 0.000\n",
      "    step: 50; loss: 16.861; l2dist: 3.598\n",
      "    step: 100; loss: 7.274; l2dist: 2.553\n",
      "    step: 150; loss: 5.208; l2dist: 2.152\n",
      "    step: 200; loss: 4.339; l2dist: 1.969\n",
      "    step: 250; loss: 4.004; l2dist: 1.897\n",
      "    step: 300; loss: 3.817; l2dist: 1.858\n",
      "    step: 350; loss: 3.664; l2dist: 1.822\n",
      "    step: 400; loss: 3.639; l2dist: 1.814\n",
      "    step: 450; loss: 3.608; l2dist: 1.810\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 85.377; l2dist: 0.000\n",
      "    step: 50; loss: 13.808; l2dist: 3.203\n",
      "    step: 100; loss: 6.092; l2dist: 2.292\n",
      "    step: 150; loss: 4.422; l2dist: 1.976\n",
      "    step: 200; loss: 3.936; l2dist: 1.878\n",
      "    step: 250; loss: 3.694; l2dist: 1.824\n",
      "    step: 300; loss: 3.582; l2dist: 1.790\n",
      "    step: 350; loss: 3.523; l2dist: 1.784\n",
      "    step: 400; loss: 3.511; l2dist: 1.779\n",
      "    step: 450; loss: 3.471; l2dist: 1.759\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.980; l2dist: 0.000\n",
      "    step: 50; loss: 12.141; l2dist: 2.914\n",
      "    step: 100; loss: 5.372; l2dist: 2.123\n",
      "    step: 150; loss: 4.094; l2dist: 1.874\n",
      "    step: 200; loss: 3.732; l2dist: 1.795\n",
      "    step: 250; loss: 3.561; l2dist: 1.762\n",
      "    step: 300; loss: 3.465; l2dist: 1.745\n",
      "    step: 350; loss: 3.408; l2dist: 1.734\n",
      "    step: 400; loss: 3.393; l2dist: 1.728\n",
      "    step: 450; loss: 3.395; l2dist: 1.730\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.095; l2dist: 0.000\n",
      "    step: 50; loss: 11.546; l2dist: 2.816\n",
      "    step: 100; loss: 5.186; l2dist: 2.075\n",
      "    step: 150; loss: 3.987; l2dist: 1.851\n",
      "    step: 200; loss: 3.643; l2dist: 1.787\n",
      "    step: 250; loss: 3.499; l2dist: 1.757\n",
      "    step: 300; loss: 3.426; l2dist: 1.743\n",
      "    step: 350; loss: 3.394; l2dist: 1.735\n",
      "    step: 400; loss: 3.341; l2dist: 1.723\n",
      "    step: 450; loss: 3.349; l2dist: 1.725\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.581; l2dist: 0.000\n",
      "    step: 50; loss: 11.345; l2dist: 2.786\n",
      "    step: 100; loss: 5.093; l2dist: 2.061\n",
      "    step: 150; loss: 3.929; l2dist: 1.846\n",
      "    step: 200; loss: 3.624; l2dist: 1.777\n",
      "    step: 250; loss: 3.462; l2dist: 1.753\n",
      "    step: 300; loss: 3.431; l2dist: 1.735\n",
      "    step: 350; loss: 3.363; l2dist: 1.728\n",
      "    step: 400; loss: 3.338; l2dist: 1.726\n",
      "    step: 450; loss: 3.334; l2dist: 1.722\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.197; l2dist: 0.000\n",
      "    step: 50; loss: 11.194; l2dist: 2.763\n",
      "    step: 100; loss: 5.047; l2dist: 2.053\n",
      "    step: 150; loss: 3.913; l2dist: 1.834\n",
      "    step: 200; loss: 3.594; l2dist: 1.775\n",
      "    step: 250; loss: 3.464; l2dist: 1.747\n",
      "    step: 300; loss: 3.411; l2dist: 1.732\n",
      "    step: 350; loss: 3.386; l2dist: 1.733\n",
      "    step: 400; loss: 3.363; l2dist: 1.721\n",
      "    step: 450; loss: 3.322; l2dist: 1.714\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.266; l2dist: 0.000\n",
      "    step: 50; loss: 11.133; l2dist: 2.753\n",
      "    step: 100; loss: 5.027; l2dist: 2.054\n",
      "    step: 150; loss: 3.888; l2dist: 1.837\n",
      "    step: 200; loss: 3.585; l2dist: 1.776\n",
      "    step: 250; loss: 3.465; l2dist: 1.754\n",
      "    step: 300; loss: 3.392; l2dist: 1.734\n",
      "    step: 350; loss: 3.352; l2dist: 1.724\n",
      "    step: 400; loss: 3.352; l2dist: 1.724\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.708; l2dist: 0.000\n",
      "    step: 50; loss: 11.192; l2dist: 2.763\n",
      "    step: 100; loss: 5.023; l2dist: 2.056\n",
      "    step: 150; loss: 3.912; l2dist: 1.840\n",
      "    step: 200; loss: 3.581; l2dist: 1.776\n",
      "    step: 250; loss: 3.452; l2dist: 1.751\n",
      "    step: 300; loss: 3.383; l2dist: 1.735\n",
      "    step: 350; loss: 3.387; l2dist: 1.742\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 157.596; l2dist: 0.000\n",
      "    step: 50; loss: 18.342; l2dist: 3.890\n",
      "    step: 100; loss: 8.096; l2dist: 2.768\n",
      "    step: 150; loss: 5.483; l2dist: 2.262\n",
      "    step: 200; loss: 4.460; l2dist: 2.025\n",
      "    step: 250; loss: 3.978; l2dist: 1.905\n",
      "    step: 300; loss: 3.683; l2dist: 1.828\n",
      "    step: 350; loss: 3.671; l2dist: 1.814\n",
      "    step: 400; loss: 3.578; l2dist: 1.791\n",
      "    step: 450; loss: 3.529; l2dist: 1.790\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 172.794; l2dist: 0.000\n",
      "    step: 50; loss: 17.964; l2dist: 3.830\n",
      "    step: 100; loss: 8.234; l2dist: 2.722\n",
      "    step: 150; loss: 5.760; l2dist: 2.244\n",
      "    step: 200; loss: 4.628; l2dist: 2.010\n",
      "    step: 250; loss: 4.175; l2dist: 1.914\n",
      "    step: 300; loss: 3.877; l2dist: 1.849\n",
      "    step: 350; loss: 3.691; l2dist: 1.805\n",
      "    step: 400; loss: 3.691; l2dist: 1.798\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 102.248; l2dist: 0.000\n",
      "    step: 50; loss: 14.121; l2dist: 3.350\n",
      "    step: 100; loss: 6.174; l2dist: 2.335\n",
      "    step: 150; loss: 4.489; l2dist: 1.990\n",
      "    step: 200; loss: 3.903; l2dist: 1.856\n",
      "    step: 250; loss: 3.611; l2dist: 1.787\n",
      "    step: 300; loss: 3.453; l2dist: 1.750\n",
      "    step: 350; loss: 3.409; l2dist: 1.744\n",
      "    step: 400; loss: 3.401; l2dist: 1.735\n",
      "    step: 450; loss: 3.383; l2dist: 1.738\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.699; l2dist: 0.000\n",
      "    step: 50; loss: 11.721; l2dist: 2.919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 5.278; l2dist: 2.148\n",
      "    step: 150; loss: 4.001; l2dist: 1.874\n",
      "    step: 200; loss: 3.579; l2dist: 1.784\n",
      "    step: 250; loss: 3.413; l2dist: 1.739\n",
      "    step: 300; loss: 3.320; l2dist: 1.727\n",
      "    step: 350; loss: 3.305; l2dist: 1.720\n",
      "    step: 400; loss: 3.264; l2dist: 1.704\n",
      "    step: 450; loss: 3.272; l2dist: 1.706\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.007; l2dist: 0.000\n",
      "    step: 50; loss: 10.655; l2dist: 2.726\n",
      "    step: 100; loss: 4.984; l2dist: 2.070\n",
      "    step: 150; loss: 3.835; l2dist: 1.843\n",
      "    step: 200; loss: 3.484; l2dist: 1.748\n",
      "    step: 250; loss: 3.376; l2dist: 1.718\n",
      "    step: 300; loss: 3.331; l2dist: 1.724\n",
      "    step: 350; loss: 3.270; l2dist: 1.702\n",
      "    step: 400; loss: 3.255; l2dist: 1.698\n",
      "    step: 450; loss: 3.242; l2dist: 1.701\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.037; l2dist: 0.000\n",
      "    step: 50; loss: 10.103; l2dist: 2.641\n",
      "    step: 100; loss: 4.765; l2dist: 1.999\n",
      "    step: 150; loss: 3.692; l2dist: 1.785\n",
      "    step: 200; loss: 3.413; l2dist: 1.721\n",
      "    step: 250; loss: 3.301; l2dist: 1.692\n",
      "    step: 300; loss: 3.245; l2dist: 1.678\n",
      "    step: 350; loss: 3.223; l2dist: 1.671\n",
      "    step: 400; loss: 3.209; l2dist: 1.667\n",
      "    step: 450; loss: 3.194; l2dist: 1.674\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.584; l2dist: 0.000\n",
      "    step: 50; loss: 9.842; l2dist: 2.589\n",
      "    step: 100; loss: 4.743; l2dist: 1.986\n",
      "    step: 150; loss: 3.683; l2dist: 1.780\n",
      "    step: 200; loss: 3.419; l2dist: 1.721\n",
      "    step: 250; loss: 3.288; l2dist: 1.696\n",
      "    step: 300; loss: 3.245; l2dist: 1.686\n",
      "    step: 350; loss: 3.211; l2dist: 1.681\n",
      "    step: 400; loss: 3.203; l2dist: 1.682\n",
      "    step: 450; loss: 3.202; l2dist: 1.668\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.572; l2dist: 0.000\n",
      "    step: 50; loss: 9.799; l2dist: 2.589\n",
      "    step: 100; loss: 4.740; l2dist: 1.994\n",
      "    step: 150; loss: 3.685; l2dist: 1.783\n",
      "    step: 200; loss: 3.415; l2dist: 1.723\n",
      "    step: 250; loss: 3.293; l2dist: 1.701\n",
      "    step: 300; loss: 3.242; l2dist: 1.684\n",
      "    step: 350; loss: 3.215; l2dist: 1.684\n",
      "    step: 400; loss: 3.202; l2dist: 1.680\n",
      "    step: 450; loss: 3.201; l2dist: 1.680\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.769; l2dist: 0.000\n",
      "    step: 50; loss: 9.742; l2dist: 2.584\n",
      "    step: 100; loss: 4.728; l2dist: 1.991\n",
      "    step: 150; loss: 3.662; l2dist: 1.786\n",
      "    step: 200; loss: 3.385; l2dist: 1.724\n",
      "    step: 250; loss: 3.280; l2dist: 1.700\n",
      "    step: 300; loss: 3.236; l2dist: 1.696\n",
      "    step: 350; loss: 3.205; l2dist: 1.685\n",
      "    step: 400; loss: 3.191; l2dist: 1.678\n",
      "    step: 450; loss: 3.182; l2dist: 1.678\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.973; l2dist: 0.000\n",
      "    step: 50; loss: 9.784; l2dist: 2.595\n",
      "    step: 100; loss: 4.727; l2dist: 2.003\n",
      "    step: 150; loss: 3.684; l2dist: 1.793\n",
      "    step: 200; loss: 3.389; l2dist: 1.730\n",
      "    step: 250; loss: 3.292; l2dist: 1.712\n",
      "    step: 300; loss: 3.245; l2dist: 1.704\n",
      "    step: 350; loss: 3.222; l2dist: 1.693\n",
      "    step: 400; loss: 3.193; l2dist: 1.687\n",
      "    step: 450; loss: 3.197; l2dist: 1.692\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.523; l2dist: 0.000\n",
      "    step: 50; loss: 13.740; l2dist: 3.240\n",
      "    step: 100; loss: 5.343; l2dist: 2.180\n",
      "    step: 150; loss: 3.667; l2dist: 1.779\n",
      "    step: 200; loss: 3.009; l2dist: 1.603\n",
      "    step: 250; loss: 2.686; l2dist: 1.506\n",
      "    step: 300; loss: 2.549; l2dist: 1.470\n",
      "    step: 350; loss: 2.446; l2dist: 1.425\n",
      "    step: 400; loss: 2.431; l2dist: 1.425\n",
      "    step: 450; loss: 2.396; l2dist: 1.419\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 109.355; l2dist: 0.000\n",
      "    step: 50; loss: 12.205; l2dist: 2.994\n",
      "    step: 100; loss: 4.976; l2dist: 2.043\n",
      "    step: 150; loss: 3.466; l2dist: 1.688\n",
      "    step: 200; loss: 2.861; l2dist: 1.527\n",
      "    step: 250; loss: 2.576; l2dist: 1.455\n",
      "    step: 300; loss: 2.463; l2dist: 1.431\n",
      "    step: 350; loss: 2.361; l2dist: 1.390\n",
      "    step: 400; loss: 2.329; l2dist: 1.382\n",
      "    step: 450; loss: 2.256; l2dist: 1.373\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.852; l2dist: 0.000\n",
      "    step: 50; loss: 9.476; l2dist: 2.586\n",
      "    step: 100; loss: 3.832; l2dist: 1.790\n",
      "    step: 150; loss: 2.798; l2dist: 1.523\n",
      "    step: 200; loss: 2.449; l2dist: 1.423\n",
      "    step: 250; loss: 2.303; l2dist: 1.387\n",
      "    step: 300; loss: 2.217; l2dist: 1.354\n",
      "    step: 350; loss: 2.187; l2dist: 1.350\n",
      "    step: 400; loss: 2.168; l2dist: 1.346\n",
      "    step: 450; loss: 2.204; l2dist: 1.357\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.055; l2dist: 0.000\n",
      "    step: 50; loss: 7.893; l2dist: 2.279\n",
      "    step: 100; loss: 3.351; l2dist: 1.665\n",
      "    step: 150; loss: 2.499; l2dist: 1.445\n",
      "    step: 200; loss: 2.281; l2dist: 1.378\n",
      "    step: 250; loss: 2.164; l2dist: 1.343\n",
      "    step: 300; loss: 2.159; l2dist: 1.331\n",
      "    step: 350; loss: 2.133; l2dist: 1.325\n",
      "    step: 400; loss: 2.092; l2dist: 1.315\n",
      "    step: 450; loss: 2.074; l2dist: 1.316\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.527; l2dist: 0.000\n",
      "    step: 50; loss: 6.984; l2dist: 2.058\n",
      "    step: 100; loss: 3.252; l2dist: 1.583\n",
      "    step: 150; loss: 2.456; l2dist: 1.413\n",
      "    step: 200; loss: 2.246; l2dist: 1.354\n",
      "    step: 250; loss: 2.154; l2dist: 1.326\n",
      "    step: 300; loss: 2.116; l2dist: 1.319\n",
      "    step: 350; loss: 2.096; l2dist: 1.309\n",
      "    step: 400; loss: 2.101; l2dist: 1.304\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.361; l2dist: 0.000\n",
      "    step: 50; loss: 6.685; l2dist: 1.979\n",
      "    step: 100; loss: 3.104; l2dist: 1.514\n",
      "    step: 150; loss: 2.388; l2dist: 1.357\n",
      "    step: 200; loss: 2.197; l2dist: 1.312\n",
      "    step: 250; loss: 2.105; l2dist: 1.295\n",
      "    step: 300; loss: 2.085; l2dist: 1.284\n",
      "    step: 350; loss: 2.058; l2dist: 1.282\n",
      "    step: 400; loss: 2.055; l2dist: 1.276\n",
      "    step: 450; loss: 2.053; l2dist: 1.274\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.259; l2dist: 0.000\n",
      "    step: 50; loss: 6.503; l2dist: 1.923\n",
      "    step: 100; loss: 3.085; l2dist: 1.486\n",
      "    step: 150; loss: 2.366; l2dist: 1.330\n",
      "    step: 200; loss: 2.168; l2dist: 1.283\n",
      "    step: 250; loss: 2.092; l2dist: 1.263\n",
      "    step: 300; loss: 2.073; l2dist: 1.262\n",
      "    step: 350; loss: 2.050; l2dist: 1.255\n",
      "    step: 400; loss: 2.039; l2dist: 1.253\n",
      "    step: 450; loss: 2.033; l2dist: 1.253\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.337; l2dist: 0.000\n",
      "    step: 50; loss: 6.459; l2dist: 1.908\n",
      "    step: 100; loss: 3.098; l2dist: 1.485\n",
      "    step: 150; loss: 2.366; l2dist: 1.333\n",
      "    step: 200; loss: 2.178; l2dist: 1.291\n",
      "    step: 250; loss: 2.100; l2dist: 1.272\n",
      "    step: 300; loss: 2.066; l2dist: 1.263\n",
      "    step: 350; loss: 2.045; l2dist: 1.263\n",
      "    step: 400; loss: 2.043; l2dist: 1.259\n",
      "    step: 450; loss: 2.031; l2dist: 1.255\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.795; l2dist: 0.000\n",
      "    step: 50; loss: 6.414; l2dist: 1.895\n",
      "    step: 100; loss: 3.084; l2dist: 1.481\n",
      "    step: 150; loss: 2.351; l2dist: 1.324\n",
      "    step: 200; loss: 2.173; l2dist: 1.279\n",
      "    step: 250; loss: 2.094; l2dist: 1.266\n",
      "    step: 300; loss: 2.071; l2dist: 1.253\n",
      "    step: 350; loss: 2.047; l2dist: 1.250\n",
      "    step: 400; loss: 2.040; l2dist: 1.250\n",
      "    step: 450; loss: 2.034; l2dist: 1.252\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.024; l2dist: 0.000\n",
      "    step: 50; loss: 6.449; l2dist: 1.905\n",
      "    step: 100; loss: 3.091; l2dist: 1.489\n",
      "    step: 150; loss: 2.369; l2dist: 1.328\n",
      "    step: 200; loss: 2.174; l2dist: 1.285\n",
      "    step: 250; loss: 2.091; l2dist: 1.266\n",
      "    step: 300; loss: 2.068; l2dist: 1.264\n",
      "    step: 350; loss: 2.048; l2dist: 1.257\n",
      "    step: 400; loss: 2.042; l2dist: 1.256\n",
      "    step: 450; loss: 2.033; l2dist: 1.252\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 125.112; l2dist: 0.000\n",
      "    step: 50; loss: 13.109; l2dist: 3.440\n",
      "    step: 100; loss: 5.891; l2dist: 2.342\n",
      "    step: 150; loss: 3.924; l2dist: 1.894\n",
      "    step: 200; loss: 3.045; l2dist: 1.664\n",
      "    step: 250; loss: 2.746; l2dist: 1.570\n",
      "    step: 300; loss: 2.573; l2dist: 1.517\n",
      "    step: 350; loss: 2.506; l2dist: 1.499\n",
      "    step: 400; loss: 2.420; l2dist: 1.473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 450; loss: 2.436; l2dist: 1.474\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 134.265; l2dist: 0.000\n",
      "    step: 50; loss: 12.922; l2dist: 3.261\n",
      "    step: 100; loss: 5.963; l2dist: 2.262\n",
      "    step: 150; loss: 3.959; l2dist: 1.838\n",
      "    step: 200; loss: 3.117; l2dist: 1.638\n",
      "    step: 250; loss: 2.781; l2dist: 1.548\n",
      "    step: 300; loss: 2.557; l2dist: 1.498\n",
      "    step: 350; loss: 2.492; l2dist: 1.485\n",
      "    step: 400; loss: 2.399; l2dist: 1.449\n",
      "    step: 450; loss: 2.370; l2dist: 1.447\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.690; l2dist: 0.000\n",
      "    step: 50; loss: 9.981; l2dist: 2.812\n",
      "    step: 100; loss: 4.358; l2dist: 1.943\n",
      "    step: 150; loss: 3.068; l2dist: 1.630\n",
      "    step: 200; loss: 2.641; l2dist: 1.524\n",
      "    step: 250; loss: 2.444; l2dist: 1.471\n",
      "    step: 300; loss: 2.351; l2dist: 1.448\n",
      "    step: 350; loss: 2.284; l2dist: 1.427\n",
      "    step: 400; loss: 2.313; l2dist: 1.424\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.888; l2dist: 0.000\n",
      "    step: 50; loss: 8.333; l2dist: 2.490\n",
      "    step: 100; loss: 3.750; l2dist: 1.810\n",
      "    step: 150; loss: 2.782; l2dist: 1.563\n",
      "    step: 200; loss: 2.462; l2dist: 1.475\n",
      "    step: 250; loss: 2.314; l2dist: 1.432\n",
      "    step: 300; loss: 2.292; l2dist: 1.421\n",
      "    step: 350; loss: 2.244; l2dist: 1.416\n",
      "    step: 400; loss: 2.210; l2dist: 1.401\n",
      "    step: 450; loss: 2.210; l2dist: 1.401\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.215; l2dist: 0.000\n",
      "    step: 50; loss: 7.393; l2dist: 2.262\n",
      "    step: 100; loss: 3.543; l2dist: 1.726\n",
      "    step: 150; loss: 2.660; l2dist: 1.515\n",
      "    step: 200; loss: 2.368; l2dist: 1.443\n",
      "    step: 250; loss: 2.267; l2dist: 1.419\n",
      "    step: 300; loss: 2.209; l2dist: 1.399\n",
      "    step: 350; loss: 2.196; l2dist: 1.391\n",
      "    step: 400; loss: 2.182; l2dist: 1.384\n",
      "    step: 450; loss: 2.165; l2dist: 1.386\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.140; l2dist: 0.000\n",
      "    step: 50; loss: 6.870; l2dist: 2.121\n",
      "    step: 100; loss: 3.435; l2dist: 1.642\n",
      "    step: 150; loss: 2.560; l2dist: 1.464\n",
      "    step: 200; loss: 2.318; l2dist: 1.405\n",
      "    step: 250; loss: 2.218; l2dist: 1.379\n",
      "    step: 300; loss: 2.174; l2dist: 1.367\n",
      "    step: 350; loss: 2.158; l2dist: 1.366\n",
      "    step: 400; loss: 2.144; l2dist: 1.358\n",
      "    step: 450; loss: 2.143; l2dist: 1.359\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.419; l2dist: 0.000\n",
      "    step: 50; loss: 6.702; l2dist: 2.081\n",
      "    step: 100; loss: 3.413; l2dist: 1.627\n",
      "    step: 150; loss: 2.534; l2dist: 1.445\n",
      "    step: 200; loss: 2.314; l2dist: 1.386\n",
      "    step: 250; loss: 2.211; l2dist: 1.367\n",
      "    step: 300; loss: 2.179; l2dist: 1.357\n",
      "    step: 350; loss: 2.164; l2dist: 1.349\n",
      "    step: 400; loss: 2.158; l2dist: 1.358\n",
      "    step: 450; loss: 2.127; l2dist: 1.347\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.290; l2dist: 0.000\n",
      "    step: 50; loss: 6.650; l2dist: 2.069\n",
      "    step: 100; loss: 3.417; l2dist: 1.628\n",
      "    step: 150; loss: 2.535; l2dist: 1.447\n",
      "    step: 200; loss: 2.307; l2dist: 1.389\n",
      "    step: 250; loss: 2.225; l2dist: 1.364\n",
      "    step: 300; loss: 2.169; l2dist: 1.360\n",
      "    step: 350; loss: 2.150; l2dist: 1.353\n",
      "    step: 400; loss: 2.147; l2dist: 1.354\n",
      "    step: 450; loss: 2.130; l2dist: 1.350\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.584; l2dist: 0.000\n",
      "    step: 50; loss: 6.581; l2dist: 2.055\n",
      "    step: 100; loss: 3.398; l2dist: 1.624\n",
      "    step: 150; loss: 2.513; l2dist: 1.436\n",
      "    step: 200; loss: 2.299; l2dist: 1.385\n",
      "    step: 250; loss: 2.205; l2dist: 1.368\n",
      "    step: 300; loss: 2.158; l2dist: 1.359\n",
      "    step: 350; loss: 2.141; l2dist: 1.349\n",
      "    step: 400; loss: 2.123; l2dist: 1.346\n",
      "    step: 450; loss: 2.118; l2dist: 1.349\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.978; l2dist: 0.000\n",
      "    step: 50; loss: 6.627; l2dist: 2.069\n",
      "    step: 100; loss: 3.403; l2dist: 1.630\n",
      "    step: 150; loss: 2.530; l2dist: 1.450\n",
      "    step: 200; loss: 2.295; l2dist: 1.393\n",
      "    step: 250; loss: 2.203; l2dist: 1.374\n",
      "    step: 300; loss: 2.169; l2dist: 1.362\n",
      "    step: 350; loss: 2.152; l2dist: 1.352\n",
      "    step: 400; loss: 2.126; l2dist: 1.353\n",
      "    step: 450; loss: 2.126; l2dist: 1.351\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 128.264; l2dist: 0.000\n",
      "    step: 50; loss: 17.676; l2dist: 3.601\n",
      "    step: 100; loss: 6.686; l2dist: 2.505\n",
      "    step: 150; loss: 4.610; l2dist: 2.053\n",
      "    step: 200; loss: 3.753; l2dist: 1.841\n",
      "    step: 250; loss: 3.385; l2dist: 1.748\n",
      "    step: 300; loss: 3.183; l2dist: 1.688\n",
      "    step: 350; loss: 3.049; l2dist: 1.649\n",
      "    step: 400; loss: 2.985; l2dist: 1.629\n",
      "    step: 450; loss: 2.925; l2dist: 1.622\n",
      "binary step: 0; number of successful adv: 92/100\n",
      "    step: 0; loss: 160.836; l2dist: 0.000\n",
      "    step: 50; loss: 16.317; l2dist: 3.472\n",
      "    step: 100; loss: 6.786; l2dist: 2.462\n",
      "    step: 150; loss: 4.879; l2dist: 2.060\n",
      "    step: 200; loss: 3.973; l2dist: 1.858\n",
      "    step: 250; loss: 3.500; l2dist: 1.751\n",
      "    step: 300; loss: 3.294; l2dist: 1.693\n",
      "    step: 350; loss: 3.112; l2dist: 1.661\n",
      "    step: 400; loss: 3.007; l2dist: 1.624\n",
      "    step: 450; loss: 2.907; l2dist: 1.611\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.417; l2dist: 0.000\n",
      "    step: 50; loss: 12.243; l2dist: 2.965\n",
      "    step: 100; loss: 5.175; l2dist: 2.131\n",
      "    step: 150; loss: 3.844; l2dist: 1.829\n",
      "    step: 200; loss: 3.266; l2dist: 1.696\n",
      "    step: 250; loss: 3.020; l2dist: 1.637\n",
      "    step: 300; loss: 2.939; l2dist: 1.604\n",
      "    step: 350; loss: 2.855; l2dist: 1.592\n",
      "    step: 400; loss: 2.794; l2dist: 1.578\n",
      "    step: 450; loss: 2.773; l2dist: 1.568\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.074; l2dist: 0.000\n",
      "    step: 50; loss: 9.863; l2dist: 2.590\n",
      "    step: 100; loss: 4.503; l2dist: 1.971\n",
      "    step: 150; loss: 3.369; l2dist: 1.722\n",
      "    step: 200; loss: 3.006; l2dist: 1.632\n",
      "    step: 250; loss: 2.864; l2dist: 1.591\n",
      "    step: 300; loss: 2.785; l2dist: 1.569\n",
      "    step: 350; loss: 2.736; l2dist: 1.560\n",
      "    step: 400; loss: 2.727; l2dist: 1.547\n",
      "    step: 450; loss: 2.681; l2dist: 1.535\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.872; l2dist: 0.000\n",
      "    step: 50; loss: 8.816; l2dist: 2.381\n",
      "    step: 100; loss: 4.242; l2dist: 1.868\n",
      "    step: 150; loss: 3.203; l2dist: 1.658\n",
      "    step: 200; loss: 2.949; l2dist: 1.594\n",
      "    step: 250; loss: 2.815; l2dist: 1.559\n",
      "    step: 300; loss: 2.754; l2dist: 1.548\n",
      "    step: 350; loss: 2.737; l2dist: 1.537\n",
      "    step: 400; loss: 2.705; l2dist: 1.534\n",
      "    step: 450; loss: 2.689; l2dist: 1.529\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.575; l2dist: 0.000\n",
      "    step: 50; loss: 8.273; l2dist: 2.252\n",
      "    step: 100; loss: 4.042; l2dist: 1.785\n",
      "    step: 150; loss: 3.103; l2dist: 1.600\n",
      "    step: 200; loss: 2.876; l2dist: 1.546\n",
      "    step: 250; loss: 2.765; l2dist: 1.524\n",
      "    step: 300; loss: 2.729; l2dist: 1.519\n",
      "    step: 350; loss: 2.677; l2dist: 1.505\n",
      "    step: 400; loss: 2.663; l2dist: 1.501\n",
      "    step: 450; loss: 2.645; l2dist: 1.500\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.566; l2dist: 0.000\n",
      "    step: 50; loss: 8.192; l2dist: 2.237\n",
      "    step: 100; loss: 4.046; l2dist: 1.795\n",
      "    step: 150; loss: 3.098; l2dist: 1.613\n",
      "    step: 200; loss: 2.882; l2dist: 1.564\n",
      "    step: 250; loss: 2.774; l2dist: 1.537\n",
      "    step: 300; loss: 2.723; l2dist: 1.537\n",
      "    step: 350; loss: 2.686; l2dist: 1.523\n",
      "    step: 400; loss: 2.671; l2dist: 1.516\n",
      "    step: 450; loss: 2.674; l2dist: 1.516\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.141; l2dist: 0.000\n",
      "    step: 50; loss: 8.124; l2dist: 2.228\n",
      "    step: 100; loss: 4.035; l2dist: 1.798\n",
      "    step: 150; loss: 3.084; l2dist: 1.616\n",
      "    step: 200; loss: 2.883; l2dist: 1.565\n",
      "    step: 250; loss: 2.768; l2dist: 1.550\n",
      "    step: 300; loss: 2.733; l2dist: 1.536\n",
      "    step: 350; loss: 2.674; l2dist: 1.524\n",
      "    step: 400; loss: 2.687; l2dist: 1.524\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.505; l2dist: 0.000\n",
      "    step: 50; loss: 8.071; l2dist: 2.218\n",
      "    step: 100; loss: 4.031; l2dist: 1.799\n",
      "    step: 150; loss: 3.080; l2dist: 1.612\n",
      "    step: 200; loss: 2.856; l2dist: 1.561\n",
      "    step: 250; loss: 2.759; l2dist: 1.536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 2.707; l2dist: 1.536\n",
      "    step: 350; loss: 2.691; l2dist: 1.526\n",
      "    step: 400; loss: 2.673; l2dist: 1.519\n",
      "    step: 450; loss: 2.653; l2dist: 1.517\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.698; l2dist: 0.000\n",
      "    step: 50; loss: 8.127; l2dist: 2.229\n",
      "    step: 100; loss: 4.038; l2dist: 1.803\n",
      "    step: 150; loss: 3.105; l2dist: 1.620\n",
      "    step: 200; loss: 2.862; l2dist: 1.569\n",
      "    step: 250; loss: 2.767; l2dist: 1.548\n",
      "    step: 300; loss: 2.717; l2dist: 1.534\n",
      "    step: 350; loss: 2.706; l2dist: 1.528\n",
      "    step: 400; loss: 2.691; l2dist: 1.522\n",
      "    step: 450; loss: 2.679; l2dist: 1.529\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 140.648; l2dist: 0.000\n",
      "    step: 50; loss: 15.508; l2dist: 3.734\n",
      "    step: 100; loss: 7.026; l2dist: 2.585\n",
      "    step: 150; loss: 4.568; l2dist: 2.075\n",
      "    step: 200; loss: 3.566; l2dist: 1.820\n",
      "    step: 250; loss: 3.146; l2dist: 1.703\n",
      "    step: 300; loss: 2.919; l2dist: 1.644\n",
      "    step: 350; loss: 2.798; l2dist: 1.600\n",
      "    step: 400; loss: 2.768; l2dist: 1.586\n",
      "    step: 450; loss: 2.683; l2dist: 1.571\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 140.698; l2dist: 0.000\n",
      "    step: 50; loss: 14.725; l2dist: 3.619\n",
      "    step: 100; loss: 6.889; l2dist: 2.503\n",
      "    step: 150; loss: 4.570; l2dist: 2.023\n",
      "    step: 200; loss: 3.614; l2dist: 1.798\n",
      "    step: 250; loss: 3.149; l2dist: 1.686\n",
      "    step: 300; loss: 2.926; l2dist: 1.626\n",
      "    step: 350; loss: 2.803; l2dist: 1.591\n",
      "    step: 400; loss: 2.729; l2dist: 1.579\n",
      "    step: 450; loss: 2.676; l2dist: 1.556\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 77.749; l2dist: 0.000\n",
      "    step: 50; loss: 11.550; l2dist: 3.137\n",
      "    step: 100; loss: 4.923; l2dist: 2.104\n",
      "    step: 150; loss: 3.465; l2dist: 1.760\n",
      "    step: 200; loss: 2.982; l2dist: 1.640\n",
      "    step: 250; loss: 2.797; l2dist: 1.594\n",
      "    step: 300; loss: 2.661; l2dist: 1.552\n",
      "    step: 350; loss: 2.620; l2dist: 1.543\n",
      "    step: 400; loss: 2.595; l2dist: 1.531\n",
      "    step: 450; loss: 2.567; l2dist: 1.528\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.553; l2dist: 0.000\n",
      "    step: 50; loss: 9.716; l2dist: 2.761\n",
      "    step: 100; loss: 4.194; l2dist: 1.937\n",
      "    step: 150; loss: 3.081; l2dist: 1.664\n",
      "    step: 200; loss: 2.770; l2dist: 1.580\n",
      "    step: 250; loss: 2.652; l2dist: 1.549\n",
      "    step: 300; loss: 2.572; l2dist: 1.524\n",
      "    step: 350; loss: 2.506; l2dist: 1.508\n",
      "    step: 400; loss: 2.476; l2dist: 1.503\n",
      "    step: 450; loss: 2.479; l2dist: 1.504\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.260; l2dist: 0.000\n",
      "    step: 50; loss: 8.678; l2dist: 2.504\n",
      "    step: 100; loss: 3.902; l2dist: 1.822\n",
      "    step: 150; loss: 2.930; l2dist: 1.619\n",
      "    step: 200; loss: 2.644; l2dist: 1.544\n",
      "    step: 250; loss: 2.543; l2dist: 1.513\n",
      "    step: 300; loss: 2.481; l2dist: 1.501\n",
      "    step: 350; loss: 2.464; l2dist: 1.497\n",
      "    step: 400; loss: 2.463; l2dist: 1.493\n",
      "    step: 450; loss: 2.438; l2dist: 1.483\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.398; l2dist: 0.000\n",
      "    step: 50; loss: 8.256; l2dist: 2.428\n",
      "    step: 100; loss: 3.833; l2dist: 1.790\n",
      "    step: 150; loss: 2.881; l2dist: 1.586\n",
      "    step: 200; loss: 2.632; l2dist: 1.528\n",
      "    step: 250; loss: 2.529; l2dist: 1.502\n",
      "    step: 300; loss: 2.472; l2dist: 1.483\n",
      "    step: 350; loss: 2.464; l2dist: 1.480\n",
      "    step: 400; loss: 2.444; l2dist: 1.482\n",
      "    step: 450; loss: 2.432; l2dist: 1.473\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.302; l2dist: 0.000\n",
      "    step: 50; loss: 8.140; l2dist: 2.391\n",
      "    step: 100; loss: 3.826; l2dist: 1.782\n",
      "    step: 150; loss: 2.864; l2dist: 1.592\n",
      "    step: 200; loss: 2.624; l2dist: 1.524\n",
      "    step: 250; loss: 2.516; l2dist: 1.504\n",
      "    step: 300; loss: 2.478; l2dist: 1.497\n",
      "    step: 350; loss: 2.457; l2dist: 1.487\n",
      "    step: 400; loss: 2.441; l2dist: 1.472\n",
      "    step: 450; loss: 2.426; l2dist: 1.476\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.618; l2dist: 0.000\n",
      "    step: 50; loss: 8.103; l2dist: 2.386\n",
      "    step: 100; loss: 3.818; l2dist: 1.791\n",
      "    step: 150; loss: 2.850; l2dist: 1.595\n",
      "    step: 200; loss: 2.631; l2dist: 1.525\n",
      "    step: 250; loss: 2.515; l2dist: 1.504\n",
      "    step: 300; loss: 2.464; l2dist: 1.494\n",
      "    step: 350; loss: 2.459; l2dist: 1.490\n",
      "    step: 400; loss: 2.426; l2dist: 1.486\n",
      "    step: 450; loss: 2.434; l2dist: 1.489\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.901; l2dist: 0.000\n",
      "    step: 50; loss: 8.044; l2dist: 2.370\n",
      "    step: 100; loss: 3.805; l2dist: 1.788\n",
      "    step: 150; loss: 2.853; l2dist: 1.594\n",
      "    step: 200; loss: 2.617; l2dist: 1.533\n",
      "    step: 250; loss: 2.525; l2dist: 1.508\n",
      "    step: 300; loss: 2.476; l2dist: 1.497\n",
      "    step: 350; loss: 2.445; l2dist: 1.488\n",
      "    step: 400; loss: 2.438; l2dist: 1.484\n",
      "    step: 450; loss: 2.427; l2dist: 1.483\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.157; l2dist: 0.000\n",
      "    step: 50; loss: 8.069; l2dist: 2.379\n",
      "    step: 100; loss: 3.810; l2dist: 1.798\n",
      "    step: 150; loss: 2.849; l2dist: 1.593\n",
      "    step: 200; loss: 2.618; l2dist: 1.538\n",
      "    step: 250; loss: 2.519; l2dist: 1.506\n",
      "    step: 300; loss: 2.477; l2dist: 1.494\n",
      "    step: 350; loss: 2.447; l2dist: 1.486\n",
      "    step: 400; loss: 2.441; l2dist: 1.492\n",
      "    step: 450; loss: 2.430; l2dist: 1.481\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 146.501; l2dist: 0.000\n",
      "    step: 50; loss: 20.939; l2dist: 3.780\n",
      "    step: 100; loss: 7.487; l2dist: 2.680\n",
      "    step: 150; loss: 5.057; l2dist: 2.183\n",
      "    step: 200; loss: 3.959; l2dist: 1.913\n",
      "    step: 250; loss: 3.483; l2dist: 1.774\n",
      "    step: 300; loss: 3.228; l2dist: 1.721\n",
      "    step: 350; loss: 3.032; l2dist: 1.657\n",
      "    step: 400; loss: 2.981; l2dist: 1.635\n",
      "    step: 450; loss: 2.959; l2dist: 1.643\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 104.889; l2dist: 0.000\n",
      "    step: 50; loss: 17.433; l2dist: 3.471\n",
      "    step: 100; loss: 5.974; l2dist: 2.337\n",
      "    step: 150; loss: 4.164; l2dist: 1.929\n",
      "    step: 200; loss: 3.485; l2dist: 1.766\n",
      "    step: 250; loss: 3.193; l2dist: 1.686\n",
      "    step: 300; loss: 3.071; l2dist: 1.654\n",
      "    step: 350; loss: 2.927; l2dist: 1.614\n",
      "    step: 400; loss: 2.860; l2dist: 1.603\n",
      "    step: 450; loss: 2.832; l2dist: 1.598\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.170; l2dist: 0.000\n",
      "    step: 50; loss: 14.011; l2dist: 3.090\n",
      "    step: 100; loss: 4.731; l2dist: 2.063\n",
      "    step: 150; loss: 3.544; l2dist: 1.764\n",
      "    step: 200; loss: 3.138; l2dist: 1.667\n",
      "    step: 250; loss: 2.955; l2dist: 1.617\n",
      "    step: 300; loss: 2.837; l2dist: 1.594\n",
      "    step: 350; loss: 2.811; l2dist: 1.584\n",
      "    step: 400; loss: 2.722; l2dist: 1.571\n",
      "    step: 450; loss: 2.687; l2dist: 1.556\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.464; l2dist: 0.000\n",
      "    step: 50; loss: 11.993; l2dist: 2.756\n",
      "    step: 100; loss: 4.253; l2dist: 1.928\n",
      "    step: 150; loss: 3.280; l2dist: 1.700\n",
      "    step: 200; loss: 2.976; l2dist: 1.621\n",
      "    step: 250; loss: 2.823; l2dist: 1.584\n",
      "    step: 300; loss: 2.739; l2dist: 1.566\n",
      "    step: 350; loss: 2.738; l2dist: 1.559\n",
      "    step: 400; loss: 2.669; l2dist: 1.551\n",
      "    step: 450; loss: 2.646; l2dist: 1.543\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.548; l2dist: 0.000\n",
      "    step: 50; loss: 10.472; l2dist: 2.495\n",
      "    step: 100; loss: 4.016; l2dist: 1.821\n",
      "    step: 150; loss: 3.126; l2dist: 1.633\n",
      "    step: 200; loss: 2.854; l2dist: 1.575\n",
      "    step: 250; loss: 2.742; l2dist: 1.546\n",
      "    step: 300; loss: 2.670; l2dist: 1.532\n",
      "    step: 350; loss: 2.628; l2dist: 1.521\n",
      "    step: 400; loss: 2.643; l2dist: 1.520\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.044; l2dist: 0.000\n",
      "    step: 50; loss: 10.271; l2dist: 2.450\n",
      "    step: 100; loss: 4.012; l2dist: 1.815\n",
      "    step: 150; loss: 3.116; l2dist: 1.629\n",
      "    step: 200; loss: 2.853; l2dist: 1.572\n",
      "    step: 250; loss: 2.741; l2dist: 1.546\n",
      "    step: 300; loss: 2.674; l2dist: 1.534\n",
      "    step: 350; loss: 2.638; l2dist: 1.525\n",
      "    step: 400; loss: 2.633; l2dist: 1.519\n",
      "    step: 450; loss: 2.617; l2dist: 1.516\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.063; l2dist: 0.000\n",
      "    step: 50; loss: 10.239; l2dist: 2.446\n",
      "    step: 100; loss: 4.024; l2dist: 1.817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 3.125; l2dist: 1.630\n",
      "    step: 200; loss: 2.866; l2dist: 1.578\n",
      "    step: 250; loss: 2.761; l2dist: 1.554\n",
      "    step: 300; loss: 2.686; l2dist: 1.537\n",
      "    step: 350; loss: 2.653; l2dist: 1.532\n",
      "    step: 400; loss: 2.634; l2dist: 1.527\n",
      "    step: 450; loss: 2.639; l2dist: 1.526\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.724; l2dist: 0.000\n",
      "    step: 50; loss: 10.223; l2dist: 2.447\n",
      "    step: 100; loss: 4.023; l2dist: 1.822\n",
      "    step: 150; loss: 3.109; l2dist: 1.631\n",
      "    step: 200; loss: 2.857; l2dist: 1.571\n",
      "    step: 250; loss: 2.742; l2dist: 1.550\n",
      "    step: 300; loss: 2.683; l2dist: 1.536\n",
      "    step: 350; loss: 2.646; l2dist: 1.526\n",
      "    step: 400; loss: 2.622; l2dist: 1.526\n",
      "    step: 450; loss: 2.620; l2dist: 1.520\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.512; l2dist: 0.000\n",
      "    step: 50; loss: 10.197; l2dist: 2.446\n",
      "    step: 100; loss: 4.036; l2dist: 1.823\n",
      "    step: 150; loss: 3.119; l2dist: 1.637\n",
      "    step: 200; loss: 2.862; l2dist: 1.577\n",
      "    step: 250; loss: 2.746; l2dist: 1.559\n",
      "    step: 300; loss: 2.690; l2dist: 1.548\n",
      "    step: 350; loss: 2.658; l2dist: 1.528\n",
      "    step: 400; loss: 2.640; l2dist: 1.534\n",
      "    step: 450; loss: 2.614; l2dist: 1.525\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.770; l2dist: 0.000\n",
      "    step: 50; loss: 10.246; l2dist: 2.455\n",
      "    step: 100; loss: 4.041; l2dist: 1.827\n",
      "    step: 150; loss: 3.110; l2dist: 1.640\n",
      "    step: 200; loss: 2.865; l2dist: 1.581\n",
      "    step: 250; loss: 2.746; l2dist: 1.558\n",
      "    step: 300; loss: 2.696; l2dist: 1.544\n",
      "    step: 350; loss: 2.657; l2dist: 1.538\n",
      "    step: 400; loss: 2.645; l2dist: 1.534\n",
      "    step: 450; loss: 2.626; l2dist: 1.529\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 154.033; l2dist: 0.000\n",
      "    step: 50; loss: 17.603; l2dist: 3.870\n",
      "    step: 100; loss: 7.841; l2dist: 2.728\n",
      "    step: 150; loss: 5.217; l2dist: 2.208\n",
      "    step: 200; loss: 4.097; l2dist: 1.947\n",
      "    step: 250; loss: 3.648; l2dist: 1.824\n",
      "    step: 300; loss: 3.408; l2dist: 1.747\n",
      "    step: 350; loss: 3.313; l2dist: 1.735\n",
      "    step: 400; loss: 3.237; l2dist: 1.706\n",
      "    step: 450; loss: 3.207; l2dist: 1.697\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 107.313; l2dist: 0.000\n",
      "    step: 50; loss: 13.856; l2dist: 3.435\n",
      "    step: 100; loss: 5.760; l2dist: 2.313\n",
      "    step: 150; loss: 4.131; l2dist: 1.942\n",
      "    step: 200; loss: 3.581; l2dist: 1.794\n",
      "    step: 250; loss: 3.300; l2dist: 1.728\n",
      "    step: 300; loss: 3.141; l2dist: 1.689\n",
      "    step: 350; loss: 3.108; l2dist: 1.677\n",
      "    step: 400; loss: 3.081; l2dist: 1.670\n",
      "    step: 450; loss: 3.020; l2dist: 1.657\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.204; l2dist: 0.000\n",
      "    step: 50; loss: 11.090; l2dist: 2.980\n",
      "    step: 100; loss: 4.655; l2dist: 2.062\n",
      "    step: 150; loss: 3.536; l2dist: 1.791\n",
      "    step: 200; loss: 3.221; l2dist: 1.713\n",
      "    step: 250; loss: 3.069; l2dist: 1.665\n",
      "    step: 300; loss: 3.008; l2dist: 1.652\n",
      "    step: 350; loss: 2.965; l2dist: 1.641\n",
      "    step: 400; loss: 2.957; l2dist: 1.629\n",
      "    step: 450; loss: 2.919; l2dist: 1.623\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.947; l2dist: 0.000\n",
      "    step: 50; loss: 9.262; l2dist: 2.583\n",
      "    step: 100; loss: 4.275; l2dist: 1.950\n",
      "    step: 150; loss: 3.301; l2dist: 1.726\n",
      "    step: 200; loss: 3.077; l2dist: 1.661\n",
      "    step: 250; loss: 2.980; l2dist: 1.632\n",
      "    step: 300; loss: 2.911; l2dist: 1.627\n",
      "    step: 350; loss: 2.893; l2dist: 1.613\n",
      "    step: 400; loss: 2.877; l2dist: 1.612\n",
      "    step: 450; loss: 2.867; l2dist: 1.605\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.226; l2dist: 0.000\n",
      "    step: 50; loss: 8.893; l2dist: 2.477\n",
      "    step: 100; loss: 4.274; l2dist: 1.915\n",
      "    step: 150; loss: 3.275; l2dist: 1.706\n",
      "    step: 200; loss: 3.053; l2dist: 1.650\n",
      "    step: 250; loss: 2.959; l2dist: 1.623\n",
      "    step: 300; loss: 2.918; l2dist: 1.613\n",
      "    step: 350; loss: 2.891; l2dist: 1.608\n",
      "    step: 400; loss: 2.882; l2dist: 1.607\n",
      "    step: 450; loss: 2.877; l2dist: 1.600\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.589; l2dist: 0.000\n",
      "    step: 50; loss: 8.428; l2dist: 2.341\n",
      "    step: 100; loss: 4.197; l2dist: 1.848\n",
      "    step: 150; loss: 3.235; l2dist: 1.662\n",
      "    step: 200; loss: 3.018; l2dist: 1.614\n",
      "    step: 250; loss: 2.925; l2dist: 1.596\n",
      "    step: 300; loss: 2.887; l2dist: 1.588\n",
      "    step: 350; loss: 2.859; l2dist: 1.580\n",
      "    step: 400; loss: 2.866; l2dist: 1.578\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.459; l2dist: 0.000\n",
      "    step: 50; loss: 8.407; l2dist: 2.344\n",
      "    step: 100; loss: 4.223; l2dist: 1.864\n",
      "    step: 150; loss: 3.243; l2dist: 1.675\n",
      "    step: 200; loss: 3.034; l2dist: 1.626\n",
      "    step: 250; loss: 2.947; l2dist: 1.612\n",
      "    step: 300; loss: 2.918; l2dist: 1.608\n",
      "    step: 350; loss: 2.885; l2dist: 1.592\n",
      "    step: 400; loss: 2.891; l2dist: 1.597\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.752; l2dist: 0.000\n",
      "    step: 50; loss: 8.380; l2dist: 2.340\n",
      "    step: 100; loss: 4.205; l2dist: 1.862\n",
      "    step: 150; loss: 3.236; l2dist: 1.676\n",
      "    step: 200; loss: 3.034; l2dist: 1.623\n",
      "    step: 250; loss: 2.935; l2dist: 1.611\n",
      "    step: 300; loss: 2.924; l2dist: 1.597\n",
      "    step: 350; loss: 2.893; l2dist: 1.596\n",
      "    step: 400; loss: 2.880; l2dist: 1.596\n",
      "    step: 450; loss: 2.863; l2dist: 1.592\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.563; l2dist: 0.000\n",
      "    step: 50; loss: 8.400; l2dist: 2.340\n",
      "    step: 100; loss: 4.216; l2dist: 1.866\n",
      "    step: 150; loss: 3.238; l2dist: 1.676\n",
      "    step: 200; loss: 3.025; l2dist: 1.627\n",
      "    step: 250; loss: 2.944; l2dist: 1.612\n",
      "    step: 300; loss: 2.912; l2dist: 1.604\n",
      "    step: 350; loss: 2.893; l2dist: 1.594\n",
      "    step: 400; loss: 2.872; l2dist: 1.591\n",
      "    step: 450; loss: 2.872; l2dist: 1.595\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.807; l2dist: 0.000\n",
      "    step: 50; loss: 8.440; l2dist: 2.356\n",
      "    step: 100; loss: 4.207; l2dist: 1.876\n",
      "    step: 150; loss: 3.232; l2dist: 1.678\n",
      "    step: 200; loss: 3.039; l2dist: 1.637\n",
      "    step: 250; loss: 2.945; l2dist: 1.617\n",
      "    step: 300; loss: 2.901; l2dist: 1.603\n",
      "    step: 350; loss: 2.893; l2dist: 1.611\n",
      "    step: 400; loss: 2.878; l2dist: 1.596\n",
      "    step: 450; loss: 2.863; l2dist: 1.595\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 155.293; l2dist: 0.000\n",
      "    step: 50; loss: 17.182; l2dist: 3.924\n",
      "    step: 100; loss: 7.841; l2dist: 2.734\n",
      "    step: 150; loss: 5.337; l2dist: 2.240\n",
      "    step: 200; loss: 4.264; l2dist: 1.985\n",
      "    step: 250; loss: 3.819; l2dist: 1.875\n",
      "    step: 300; loss: 3.584; l2dist: 1.809\n",
      "    step: 350; loss: 3.425; l2dist: 1.773\n",
      "    step: 400; loss: 3.335; l2dist: 1.753\n",
      "    step: 450; loss: 3.254; l2dist: 1.732\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 189.429; l2dist: 0.000\n",
      "    step: 50; loss: 17.273; l2dist: 3.859\n",
      "    step: 100; loss: 8.089; l2dist: 2.711\n",
      "    step: 150; loss: 5.644; l2dist: 2.240\n",
      "    step: 200; loss: 4.536; l2dist: 2.010\n",
      "    step: 250; loss: 3.975; l2dist: 1.891\n",
      "    step: 300; loss: 3.696; l2dist: 1.829\n",
      "    step: 350; loss: 3.521; l2dist: 1.784\n",
      "    step: 400; loss: 3.414; l2dist: 1.757\n",
      "    step: 450; loss: 3.302; l2dist: 1.739\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 104.059; l2dist: 0.000\n",
      "    step: 50; loss: 13.299; l2dist: 3.297\n",
      "    step: 100; loss: 5.900; l2dist: 2.300\n",
      "    step: 150; loss: 4.289; l2dist: 1.967\n",
      "    step: 200; loss: 3.690; l2dist: 1.824\n",
      "    step: 250; loss: 3.414; l2dist: 1.764\n",
      "    step: 300; loss: 3.251; l2dist: 1.720\n",
      "    step: 350; loss: 3.218; l2dist: 1.708\n",
      "    step: 400; loss: 3.164; l2dist: 1.697\n",
      "    step: 450; loss: 3.114; l2dist: 1.685\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.148; l2dist: 0.000\n",
      "    step: 50; loss: 10.980; l2dist: 2.909\n",
      "    step: 100; loss: 5.039; l2dist: 2.136\n",
      "    step: 150; loss: 3.789; l2dist: 1.851\n",
      "    step: 200; loss: 3.376; l2dist: 1.756\n",
      "    step: 250; loss: 3.210; l2dist: 1.716\n",
      "    step: 300; loss: 3.131; l2dist: 1.686\n",
      "    step: 350; loss: 3.080; l2dist: 1.672\n",
      "    step: 400; loss: 3.091; l2dist: 1.679\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.069; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 9.723; l2dist: 2.641\n",
      "    step: 100; loss: 4.668; l2dist: 1.991\n",
      "    step: 150; loss: 3.541; l2dist: 1.768\n",
      "    step: 200; loss: 3.221; l2dist: 1.689\n",
      "    step: 250; loss: 3.098; l2dist: 1.658\n",
      "    step: 300; loss: 3.028; l2dist: 1.654\n",
      "    step: 350; loss: 3.005; l2dist: 1.638\n",
      "    step: 400; loss: 2.969; l2dist: 1.635\n",
      "    step: 450; loss: 2.972; l2dist: 1.627\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.160; l2dist: 0.000\n",
      "    step: 50; loss: 9.425; l2dist: 2.592\n",
      "    step: 100; loss: 4.471; l2dist: 1.939\n",
      "    step: 150; loss: 3.447; l2dist: 1.724\n",
      "    step: 200; loss: 3.165; l2dist: 1.660\n",
      "    step: 250; loss: 3.041; l2dist: 1.630\n",
      "    step: 300; loss: 2.980; l2dist: 1.617\n",
      "    step: 350; loss: 2.959; l2dist: 1.607\n",
      "    step: 400; loss: 2.941; l2dist: 1.604\n",
      "    step: 450; loss: 2.944; l2dist: 1.612\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.022; l2dist: 0.000\n",
      "    step: 50; loss: 9.259; l2dist: 2.550\n",
      "    step: 100; loss: 4.448; l2dist: 1.933\n",
      "    step: 150; loss: 3.405; l2dist: 1.712\n",
      "    step: 200; loss: 3.153; l2dist: 1.657\n",
      "    step: 250; loss: 3.026; l2dist: 1.628\n",
      "    step: 300; loss: 3.017; l2dist: 1.612\n",
      "    step: 350; loss: 2.957; l2dist: 1.611\n",
      "    step: 400; loss: 2.956; l2dist: 1.618\n",
      "    step: 450; loss: 2.933; l2dist: 1.612\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.798; l2dist: 0.000\n",
      "    step: 50; loss: 9.242; l2dist: 2.545\n",
      "    step: 100; loss: 4.432; l2dist: 1.925\n",
      "    step: 150; loss: 3.411; l2dist: 1.724\n",
      "    step: 200; loss: 3.147; l2dist: 1.667\n",
      "    step: 250; loss: 3.048; l2dist: 1.645\n",
      "    step: 300; loss: 2.996; l2dist: 1.624\n",
      "    step: 350; loss: 2.970; l2dist: 1.618\n",
      "    step: 400; loss: 2.948; l2dist: 1.619\n",
      "    step: 450; loss: 2.944; l2dist: 1.623\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.130; l2dist: 0.000\n",
      "    step: 50; loss: 9.205; l2dist: 2.540\n",
      "    step: 100; loss: 4.424; l2dist: 1.927\n",
      "    step: 150; loss: 3.396; l2dist: 1.717\n",
      "    step: 200; loss: 3.132; l2dist: 1.662\n",
      "    step: 250; loss: 3.034; l2dist: 1.634\n",
      "    step: 300; loss: 2.994; l2dist: 1.625\n",
      "    step: 350; loss: 2.956; l2dist: 1.624\n",
      "    step: 400; loss: 2.953; l2dist: 1.621\n",
      "    step: 450; loss: 2.930; l2dist: 1.618\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.310; l2dist: 0.000\n",
      "    step: 50; loss: 9.243; l2dist: 2.550\n",
      "    step: 100; loss: 4.427; l2dist: 1.932\n",
      "    step: 150; loss: 3.392; l2dist: 1.729\n",
      "    step: 200; loss: 3.133; l2dist: 1.661\n",
      "    step: 250; loss: 3.040; l2dist: 1.638\n",
      "    step: 300; loss: 2.991; l2dist: 1.638\n",
      "    step: 350; loss: 2.957; l2dist: 1.628\n",
      "    step: 400; loss: 2.949; l2dist: 1.626\n",
      "    step: 450; loss: 2.946; l2dist: 1.621\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 134.890; l2dist: 0.000\n",
      "    step: 50; loss: 14.088; l2dist: 3.531\n",
      "    step: 100; loss: 6.168; l2dist: 2.419\n",
      "    step: 150; loss: 4.016; l2dist: 1.930\n",
      "    step: 200; loss: 3.153; l2dist: 1.701\n",
      "    step: 250; loss: 2.782; l2dist: 1.592\n",
      "    step: 300; loss: 2.608; l2dist: 1.527\n",
      "    step: 350; loss: 2.465; l2dist: 1.501\n",
      "    step: 400; loss: 2.443; l2dist: 1.479\n",
      "    step: 450; loss: 2.429; l2dist: 1.475\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 77.023; l2dist: 0.000\n",
      "    step: 50; loss: 10.746; l2dist: 3.025\n",
      "    step: 100; loss: 4.333; l2dist: 2.005\n",
      "    step: 150; loss: 3.086; l2dist: 1.680\n",
      "    step: 200; loss: 2.604; l2dist: 1.540\n",
      "    step: 250; loss: 2.432; l2dist: 1.484\n",
      "    step: 300; loss: 2.364; l2dist: 1.466\n",
      "    step: 350; loss: 2.360; l2dist: 1.454\n",
      "    step: 400; loss: 2.288; l2dist: 1.437\n",
      "    step: 450; loss: 2.297; l2dist: 1.439\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.104; l2dist: 0.000\n",
      "    step: 50; loss: 8.828; l2dist: 2.656\n",
      "    step: 100; loss: 3.591; l2dist: 1.824\n",
      "    step: 150; loss: 2.647; l2dist: 1.552\n",
      "    step: 200; loss: 2.414; l2dist: 1.479\n",
      "    step: 250; loss: 2.313; l2dist: 1.450\n",
      "    step: 300; loss: 2.266; l2dist: 1.432\n",
      "    step: 350; loss: 2.244; l2dist: 1.417\n",
      "    step: 400; loss: 2.212; l2dist: 1.410\n",
      "    step: 450; loss: 2.213; l2dist: 1.412\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.735; l2dist: 0.000\n",
      "    step: 50; loss: 7.800; l2dist: 2.403\n",
      "    step: 100; loss: 3.383; l2dist: 1.745\n",
      "    step: 150; loss: 2.554; l2dist: 1.518\n",
      "    step: 200; loss: 2.345; l2dist: 1.455\n",
      "    step: 250; loss: 2.270; l2dist: 1.433\n",
      "    step: 300; loss: 2.224; l2dist: 1.418\n",
      "    step: 350; loss: 2.199; l2dist: 1.406\n",
      "    step: 400; loss: 2.190; l2dist: 1.402\n",
      "    step: 450; loss: 2.175; l2dist: 1.400\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.291; l2dist: 0.000\n",
      "    step: 50; loss: 7.264; l2dist: 2.236\n",
      "    step: 100; loss: 3.328; l2dist: 1.678\n",
      "    step: 150; loss: 2.520; l2dist: 1.484\n",
      "    step: 200; loss: 2.333; l2dist: 1.428\n",
      "    step: 250; loss: 2.249; l2dist: 1.407\n",
      "    step: 300; loss: 2.213; l2dist: 1.393\n",
      "    step: 350; loss: 2.197; l2dist: 1.386\n",
      "    step: 400; loss: 2.178; l2dist: 1.383\n",
      "    step: 450; loss: 2.169; l2dist: 1.385\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.729; l2dist: 0.000\n",
      "    step: 50; loss: 7.094; l2dist: 2.184\n",
      "    step: 100; loss: 3.281; l2dist: 1.634\n",
      "    step: 150; loss: 2.505; l2dist: 1.441\n",
      "    step: 200; loss: 2.318; l2dist: 1.396\n",
      "    step: 250; loss: 2.234; l2dist: 1.375\n",
      "    step: 300; loss: 2.200; l2dist: 1.368\n",
      "    step: 350; loss: 2.178; l2dist: 1.365\n",
      "    step: 400; loss: 2.174; l2dist: 1.370\n",
      "    step: 450; loss: 2.159; l2dist: 1.360\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.630; l2dist: 0.000\n",
      "    step: 50; loss: 7.025; l2dist: 2.164\n",
      "    step: 100; loss: 3.328; l2dist: 1.640\n",
      "    step: 150; loss: 2.529; l2dist: 1.463\n",
      "    step: 200; loss: 2.330; l2dist: 1.418\n",
      "    step: 250; loss: 2.249; l2dist: 1.397\n",
      "    step: 300; loss: 2.214; l2dist: 1.382\n",
      "    step: 350; loss: 2.210; l2dist: 1.377\n",
      "    step: 400; loss: 2.177; l2dist: 1.377\n",
      "    step: 450; loss: 2.178; l2dist: 1.372\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.216; l2dist: 0.000\n",
      "    step: 50; loss: 7.031; l2dist: 2.150\n",
      "    step: 100; loss: 3.303; l2dist: 1.640\n",
      "    step: 150; loss: 2.513; l2dist: 1.454\n",
      "    step: 200; loss: 2.318; l2dist: 1.419\n",
      "    step: 250; loss: 2.242; l2dist: 1.396\n",
      "    step: 300; loss: 2.208; l2dist: 1.381\n",
      "    step: 350; loss: 2.186; l2dist: 1.377\n",
      "    step: 400; loss: 2.174; l2dist: 1.377\n",
      "    step: 450; loss: 2.162; l2dist: 1.375\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.978; l2dist: 0.000\n",
      "    step: 50; loss: 7.008; l2dist: 2.149\n",
      "    step: 100; loss: 3.316; l2dist: 1.638\n",
      "    step: 150; loss: 2.488; l2dist: 1.460\n",
      "    step: 200; loss: 2.317; l2dist: 1.417\n",
      "    step: 250; loss: 2.239; l2dist: 1.390\n",
      "    step: 300; loss: 2.197; l2dist: 1.386\n",
      "    step: 350; loss: 2.175; l2dist: 1.377\n",
      "    step: 400; loss: 2.169; l2dist: 1.374\n",
      "    step: 450; loss: 2.174; l2dist: 1.375\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.167; l2dist: 0.000\n",
      "    step: 50; loss: 7.043; l2dist: 2.161\n",
      "    step: 100; loss: 3.317; l2dist: 1.648\n",
      "    step: 150; loss: 2.503; l2dist: 1.460\n",
      "    step: 200; loss: 2.324; l2dist: 1.419\n",
      "    step: 250; loss: 2.239; l2dist: 1.398\n",
      "    step: 300; loss: 2.208; l2dist: 1.393\n",
      "    step: 350; loss: 2.189; l2dist: 1.381\n",
      "    step: 400; loss: 2.175; l2dist: 1.380\n",
      "    step: 450; loss: 2.167; l2dist: 1.373\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 154.749; l2dist: 0.000\n",
      "    step: 50; loss: 16.723; l2dist: 3.696\n",
      "    step: 100; loss: 7.460; l2dist: 2.687\n",
      "    step: 150; loss: 5.184; l2dist: 2.219\n",
      "    step: 200; loss: 4.262; l2dist: 2.008\n",
      "    step: 250; loss: 3.779; l2dist: 1.883\n",
      "    step: 300; loss: 3.550; l2dist: 1.818\n",
      "    step: 350; loss: 3.425; l2dist: 1.776\n",
      "    step: 400; loss: 3.330; l2dist: 1.752\n",
      "    step: 450; loss: 3.305; l2dist: 1.747\n",
      "binary step: 0; number of successful adv: 90/100\n",
      "    step: 0; loss: 238.166; l2dist: 0.000\n",
      "    step: 50; loss: 18.506; l2dist: 3.908\n",
      "    step: 100; loss: 9.043; l2dist: 2.894\n",
      "    step: 150; loss: 6.226; l2dist: 2.372\n",
      "    step: 200; loss: 5.007; l2dist: 2.124\n",
      "    step: 250; loss: 4.337; l2dist: 1.973\n",
      "    step: 300; loss: 3.919; l2dist: 1.883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 3.656; l2dist: 1.828\n",
      "    step: 400; loss: 3.489; l2dist: 1.792\n",
      "    step: 450; loss: 3.423; l2dist: 1.777\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 130.740; l2dist: 0.000\n",
      "    step: 50; loss: 14.087; l2dist: 3.406\n",
      "    step: 100; loss: 6.431; l2dist: 2.417\n",
      "    step: 150; loss: 4.630; l2dist: 2.041\n",
      "    step: 200; loss: 3.922; l2dist: 1.887\n",
      "    step: 250; loss: 3.571; l2dist: 1.808\n",
      "    step: 300; loss: 3.374; l2dist: 1.756\n",
      "    step: 350; loss: 3.234; l2dist: 1.731\n",
      "    step: 400; loss: 3.204; l2dist: 1.723\n",
      "    step: 450; loss: 3.136; l2dist: 1.702\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 85.301; l2dist: 0.000\n",
      "    step: 50; loss: 11.404; l2dist: 3.022\n",
      "    step: 100; loss: 5.256; l2dist: 2.169\n",
      "    step: 150; loss: 3.938; l2dist: 1.889\n",
      "    step: 200; loss: 3.492; l2dist: 1.779\n",
      "    step: 250; loss: 3.266; l2dist: 1.726\n",
      "    step: 300; loss: 3.159; l2dist: 1.703\n",
      "    step: 350; loss: 3.099; l2dist: 1.680\n",
      "    step: 400; loss: 3.071; l2dist: 1.678\n",
      "    step: 450; loss: 3.025; l2dist: 1.667\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.247; l2dist: 0.000\n",
      "    step: 50; loss: 10.265; l2dist: 2.820\n",
      "    step: 100; loss: 4.772; l2dist: 2.054\n",
      "    step: 150; loss: 3.681; l2dist: 1.820\n",
      "    step: 200; loss: 3.324; l2dist: 1.728\n",
      "    step: 250; loss: 3.151; l2dist: 1.693\n",
      "    step: 300; loss: 3.089; l2dist: 1.675\n",
      "    step: 350; loss: 3.039; l2dist: 1.662\n",
      "    step: 400; loss: 3.002; l2dist: 1.654\n",
      "    step: 450; loss: 2.988; l2dist: 1.650\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.573; l2dist: 0.000\n",
      "    step: 50; loss: 9.592; l2dist: 2.694\n",
      "    step: 100; loss: 4.569; l2dist: 2.017\n",
      "    step: 150; loss: 3.562; l2dist: 1.790\n",
      "    step: 200; loss: 3.271; l2dist: 1.712\n",
      "    step: 250; loss: 3.144; l2dist: 1.685\n",
      "    step: 300; loss: 3.089; l2dist: 1.674\n",
      "    step: 350; loss: 3.024; l2dist: 1.657\n",
      "    step: 400; loss: 2.994; l2dist: 1.650\n",
      "    step: 450; loss: 3.004; l2dist: 1.645\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.444; l2dist: 0.000\n",
      "    step: 50; loss: 9.390; l2dist: 2.675\n",
      "    step: 100; loss: 4.547; l2dist: 2.011\n",
      "    step: 150; loss: 3.531; l2dist: 1.785\n",
      "    step: 200; loss: 3.274; l2dist: 1.715\n",
      "    step: 250; loss: 3.157; l2dist: 1.688\n",
      "    step: 300; loss: 3.083; l2dist: 1.671\n",
      "    step: 350; loss: 3.060; l2dist: 1.660\n",
      "    step: 400; loss: 3.019; l2dist: 1.655\n",
      "    step: 450; loss: 2.999; l2dist: 1.651\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.618; l2dist: 0.000\n",
      "    step: 50; loss: 9.206; l2dist: 2.649\n",
      "    step: 100; loss: 4.489; l2dist: 2.009\n",
      "    step: 150; loss: 3.510; l2dist: 1.786\n",
      "    step: 200; loss: 3.257; l2dist: 1.720\n",
      "    step: 250; loss: 3.157; l2dist: 1.690\n",
      "    step: 300; loss: 3.099; l2dist: 1.674\n",
      "    step: 350; loss: 3.041; l2dist: 1.666\n",
      "    step: 400; loss: 3.024; l2dist: 1.660\n",
      "    step: 450; loss: 3.011; l2dist: 1.654\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.248; l2dist: 0.000\n",
      "    step: 50; loss: 9.114; l2dist: 2.635\n",
      "    step: 100; loss: 4.445; l2dist: 1.991\n",
      "    step: 150; loss: 3.503; l2dist: 1.779\n",
      "    step: 200; loss: 3.245; l2dist: 1.718\n",
      "    step: 250; loss: 3.135; l2dist: 1.689\n",
      "    step: 300; loss: 3.081; l2dist: 1.671\n",
      "    step: 350; loss: 3.045; l2dist: 1.662\n",
      "    step: 400; loss: 3.034; l2dist: 1.671\n",
      "    step: 450; loss: 3.017; l2dist: 1.657\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.465; l2dist: 0.000\n",
      "    step: 50; loss: 9.135; l2dist: 2.643\n",
      "    step: 100; loss: 4.450; l2dist: 1.990\n",
      "    step: 150; loss: 3.492; l2dist: 1.780\n",
      "    step: 200; loss: 3.257; l2dist: 1.717\n",
      "    step: 250; loss: 3.140; l2dist: 1.686\n",
      "    step: 300; loss: 3.074; l2dist: 1.676\n",
      "    step: 350; loss: 3.033; l2dist: 1.665\n",
      "    step: 400; loss: 3.028; l2dist: 1.656\n",
      "    step: 450; loss: 3.003; l2dist: 1.649\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 127.149; l2dist: 0.000\n",
      "    step: 50; loss: 13.761; l2dist: 3.517\n",
      "    step: 100; loss: 6.087; l2dist: 2.392\n",
      "    step: 150; loss: 4.000; l2dist: 1.915\n",
      "    step: 200; loss: 3.192; l2dist: 1.694\n",
      "    step: 250; loss: 2.916; l2dist: 1.618\n",
      "    step: 300; loss: 2.718; l2dist: 1.554\n",
      "    step: 350; loss: 2.673; l2dist: 1.539\n",
      "    step: 400; loss: 2.603; l2dist: 1.498\n",
      "    step: 450; loss: 2.498; l2dist: 1.486\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 121.193; l2dist: 0.000\n",
      "    step: 50; loss: 13.127; l2dist: 3.345\n",
      "    step: 100; loss: 5.960; l2dist: 2.298\n",
      "    step: 150; loss: 3.976; l2dist: 1.861\n",
      "    step: 200; loss: 3.237; l2dist: 1.679\n",
      "    step: 250; loss: 2.910; l2dist: 1.588\n",
      "    step: 300; loss: 2.718; l2dist: 1.538\n",
      "    step: 350; loss: 2.605; l2dist: 1.501\n",
      "    step: 400; loss: 2.528; l2dist: 1.479\n",
      "    step: 450; loss: 2.539; l2dist: 1.487\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.476; l2dist: 0.000\n",
      "    step: 50; loss: 10.388; l2dist: 2.920\n",
      "    step: 100; loss: 4.434; l2dist: 1.971\n",
      "    step: 150; loss: 3.194; l2dist: 1.668\n",
      "    step: 200; loss: 2.833; l2dist: 1.564\n",
      "    step: 250; loss: 2.563; l2dist: 1.497\n",
      "    step: 300; loss: 2.457; l2dist: 1.463\n",
      "    step: 350; loss: 2.426; l2dist: 1.452\n",
      "    step: 400; loss: 2.399; l2dist: 1.447\n",
      "    step: 450; loss: 2.372; l2dist: 1.436\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.786; l2dist: 0.000\n",
      "    step: 50; loss: 8.917; l2dist: 2.616\n",
      "    step: 100; loss: 3.912; l2dist: 1.843\n",
      "    step: 150; loss: 2.905; l2dist: 1.583\n",
      "    step: 200; loss: 2.591; l2dist: 1.497\n",
      "    step: 250; loss: 2.429; l2dist: 1.452\n",
      "    step: 300; loss: 2.344; l2dist: 1.429\n",
      "    step: 350; loss: 2.299; l2dist: 1.418\n",
      "    step: 400; loss: 2.264; l2dist: 1.408\n",
      "    step: 450; loss: 2.261; l2dist: 1.400\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.915; l2dist: 0.000\n",
      "    step: 50; loss: 7.955; l2dist: 2.374\n",
      "    step: 100; loss: 3.744; l2dist: 1.756\n",
      "    step: 150; loss: 2.844; l2dist: 1.547\n",
      "    step: 200; loss: 2.513; l2dist: 1.469\n",
      "    step: 250; loss: 2.379; l2dist: 1.432\n",
      "    step: 300; loss: 2.334; l2dist: 1.420\n",
      "    step: 350; loss: 2.300; l2dist: 1.408\n",
      "    step: 400; loss: 2.288; l2dist: 1.409\n",
      "    step: 450; loss: 2.278; l2dist: 1.402\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.967; l2dist: 0.000\n",
      "    step: 50; loss: 7.468; l2dist: 2.256\n",
      "    step: 100; loss: 3.661; l2dist: 1.682\n",
      "    step: 150; loss: 2.789; l2dist: 1.493\n",
      "    step: 200; loss: 2.485; l2dist: 1.429\n",
      "    step: 250; loss: 2.386; l2dist: 1.402\n",
      "    step: 300; loss: 2.330; l2dist: 1.391\n",
      "    step: 350; loss: 2.289; l2dist: 1.382\n",
      "    step: 400; loss: 2.291; l2dist: 1.376\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.873; l2dist: 0.000\n",
      "    step: 50; loss: 7.390; l2dist: 2.221\n",
      "    step: 100; loss: 3.624; l2dist: 1.669\n",
      "    step: 150; loss: 2.760; l2dist: 1.483\n",
      "    step: 200; loss: 2.487; l2dist: 1.430\n",
      "    step: 250; loss: 2.382; l2dist: 1.403\n",
      "    step: 300; loss: 2.314; l2dist: 1.390\n",
      "    step: 350; loss: 2.296; l2dist: 1.388\n",
      "    step: 400; loss: 2.249; l2dist: 1.375\n",
      "    step: 450; loss: 2.278; l2dist: 1.377\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.274; l2dist: 0.000\n",
      "    step: 50; loss: 7.353; l2dist: 2.213\n",
      "    step: 100; loss: 3.617; l2dist: 1.667\n",
      "    step: 150; loss: 2.740; l2dist: 1.480\n",
      "    step: 200; loss: 2.470; l2dist: 1.425\n",
      "    step: 250; loss: 2.372; l2dist: 1.403\n",
      "    step: 300; loss: 2.329; l2dist: 1.393\n",
      "    step: 350; loss: 2.287; l2dist: 1.382\n",
      "    step: 400; loss: 2.273; l2dist: 1.381\n",
      "    step: 450; loss: 2.274; l2dist: 1.384\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.700; l2dist: 0.000\n",
      "    step: 50; loss: 7.321; l2dist: 2.209\n",
      "    step: 100; loss: 3.628; l2dist: 1.674\n",
      "    step: 150; loss: 2.744; l2dist: 1.484\n",
      "    step: 200; loss: 2.474; l2dist: 1.433\n",
      "    step: 250; loss: 2.372; l2dist: 1.404\n",
      "    step: 300; loss: 2.323; l2dist: 1.393\n",
      "    step: 350; loss: 2.305; l2dist: 1.394\n",
      "    step: 400; loss: 2.278; l2dist: 1.385\n",
      "    step: 450; loss: 2.258; l2dist: 1.382\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.919; l2dist: 0.000\n",
      "    step: 50; loss: 7.373; l2dist: 2.221\n",
      "    step: 100; loss: 3.641; l2dist: 1.685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 2.742; l2dist: 1.498\n",
      "    step: 200; loss: 2.486; l2dist: 1.436\n",
      "    step: 250; loss: 2.366; l2dist: 1.409\n",
      "    step: 300; loss: 2.309; l2dist: 1.392\n",
      "    step: 350; loss: 2.282; l2dist: 1.389\n",
      "    step: 400; loss: 2.264; l2dist: 1.392\n",
      "    step: 450; loss: 2.256; l2dist: 1.386\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 135.294; l2dist: 0.000\n",
      "    step: 50; loss: 15.036; l2dist: 3.616\n",
      "    step: 100; loss: 6.657; l2dist: 2.516\n",
      "    step: 150; loss: 4.491; l2dist: 2.042\n",
      "    step: 200; loss: 3.584; l2dist: 1.821\n",
      "    step: 250; loss: 3.215; l2dist: 1.709\n",
      "    step: 300; loss: 3.025; l2dist: 1.660\n",
      "    step: 350; loss: 2.924; l2dist: 1.624\n",
      "    step: 400; loss: 2.961; l2dist: 1.640\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 97.289; l2dist: 0.000\n",
      "    step: 50; loss: 12.987; l2dist: 3.360\n",
      "    step: 100; loss: 5.373; l2dist: 2.231\n",
      "    step: 150; loss: 3.750; l2dist: 1.844\n",
      "    step: 200; loss: 3.169; l2dist: 1.692\n",
      "    step: 250; loss: 2.974; l2dist: 1.626\n",
      "    step: 300; loss: 2.862; l2dist: 1.589\n",
      "    step: 350; loss: 2.817; l2dist: 1.571\n",
      "    step: 400; loss: 2.765; l2dist: 1.575\n",
      "    step: 450; loss: 2.724; l2dist: 1.561\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.525; l2dist: 0.000\n",
      "    step: 50; loss: 10.432; l2dist: 2.924\n",
      "    step: 100; loss: 4.330; l2dist: 1.991\n",
      "    step: 150; loss: 3.294; l2dist: 1.713\n",
      "    step: 200; loss: 2.946; l2dist: 1.621\n",
      "    step: 250; loss: 2.806; l2dist: 1.576\n",
      "    step: 300; loss: 2.719; l2dist: 1.553\n",
      "    step: 350; loss: 2.667; l2dist: 1.544\n",
      "    step: 400; loss: 2.640; l2dist: 1.537\n",
      "    step: 450; loss: 2.663; l2dist: 1.544\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.766; l2dist: 0.000\n",
      "    step: 50; loss: 9.343; l2dist: 2.686\n",
      "    step: 100; loss: 3.983; l2dist: 1.894\n",
      "    step: 150; loss: 3.059; l2dist: 1.651\n",
      "    step: 200; loss: 2.782; l2dist: 1.576\n",
      "    step: 250; loss: 2.680; l2dist: 1.544\n",
      "    step: 300; loss: 2.619; l2dist: 1.527\n",
      "    step: 350; loss: 2.582; l2dist: 1.522\n",
      "    step: 400; loss: 2.546; l2dist: 1.507\n",
      "    step: 450; loss: 2.521; l2dist: 1.503\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.512; l2dist: 0.000\n",
      "    step: 50; loss: 8.499; l2dist: 2.481\n",
      "    step: 100; loss: 3.859; l2dist: 1.826\n",
      "    step: 150; loss: 2.962; l2dist: 1.612\n",
      "    step: 200; loss: 2.729; l2dist: 1.550\n",
      "    step: 250; loss: 2.622; l2dist: 1.517\n",
      "    step: 300; loss: 2.581; l2dist: 1.504\n",
      "    step: 350; loss: 2.542; l2dist: 1.497\n",
      "    step: 400; loss: 2.513; l2dist: 1.490\n",
      "    step: 450; loss: 2.505; l2dist: 1.489\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.279; l2dist: 0.000\n",
      "    step: 50; loss: 8.210; l2dist: 2.413\n",
      "    step: 100; loss: 3.791; l2dist: 1.787\n",
      "    step: 150; loss: 2.943; l2dist: 1.581\n",
      "    step: 200; loss: 2.693; l2dist: 1.520\n",
      "    step: 250; loss: 2.588; l2dist: 1.500\n",
      "    step: 300; loss: 2.539; l2dist: 1.492\n",
      "    step: 350; loss: 2.507; l2dist: 1.477\n",
      "    step: 400; loss: 2.514; l2dist: 1.480\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.982; l2dist: 0.000\n",
      "    step: 50; loss: 8.054; l2dist: 2.396\n",
      "    step: 100; loss: 3.776; l2dist: 1.794\n",
      "    step: 150; loss: 2.925; l2dist: 1.589\n",
      "    step: 200; loss: 2.671; l2dist: 1.515\n",
      "    step: 250; loss: 2.581; l2dist: 1.495\n",
      "    step: 300; loss: 2.547; l2dist: 1.484\n",
      "    step: 350; loss: 2.507; l2dist: 1.472\n",
      "    step: 400; loss: 2.489; l2dist: 1.470\n",
      "    step: 450; loss: 2.489; l2dist: 1.473\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.396; l2dist: 0.000\n",
      "    step: 50; loss: 8.015; l2dist: 2.387\n",
      "    step: 100; loss: 3.800; l2dist: 1.794\n",
      "    step: 150; loss: 2.949; l2dist: 1.590\n",
      "    step: 200; loss: 2.673; l2dist: 1.524\n",
      "    step: 250; loss: 2.597; l2dist: 1.492\n",
      "    step: 300; loss: 2.543; l2dist: 1.492\n",
      "    step: 350; loss: 2.540; l2dist: 1.478\n",
      "    step: 400; loss: 2.505; l2dist: 1.480\n",
      "    step: 450; loss: 2.481; l2dist: 1.472\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.007; l2dist: 0.000\n",
      "    step: 50; loss: 7.977; l2dist: 2.381\n",
      "    step: 100; loss: 3.792; l2dist: 1.787\n",
      "    step: 150; loss: 2.920; l2dist: 1.581\n",
      "    step: 200; loss: 2.677; l2dist: 1.515\n",
      "    step: 250; loss: 2.585; l2dist: 1.504\n",
      "    step: 300; loss: 2.531; l2dist: 1.487\n",
      "    step: 350; loss: 2.520; l2dist: 1.478\n",
      "    step: 400; loss: 2.500; l2dist: 1.479\n",
      "    step: 450; loss: 2.490; l2dist: 1.477\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.139; l2dist: 0.000\n",
      "    step: 50; loss: 7.998; l2dist: 2.390\n",
      "    step: 100; loss: 3.806; l2dist: 1.796\n",
      "    step: 150; loss: 2.925; l2dist: 1.585\n",
      "    step: 200; loss: 2.692; l2dist: 1.522\n",
      "    step: 250; loss: 2.583; l2dist: 1.505\n",
      "    step: 300; loss: 2.550; l2dist: 1.489\n",
      "    step: 350; loss: 2.498; l2dist: 1.486\n",
      "    step: 400; loss: 2.516; l2dist: 1.480\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 139.619; l2dist: 0.000\n",
      "    step: 50; loss: 14.453; l2dist: 3.627\n",
      "    step: 100; loss: 6.653; l2dist: 2.517\n",
      "    step: 150; loss: 4.354; l2dist: 2.019\n",
      "    step: 200; loss: 3.444; l2dist: 1.786\n",
      "    step: 250; loss: 3.034; l2dist: 1.673\n",
      "    step: 300; loss: 2.862; l2dist: 1.612\n",
      "    step: 350; loss: 2.789; l2dist: 1.594\n",
      "    step: 400; loss: 2.690; l2dist: 1.564\n",
      "    step: 450; loss: 2.659; l2dist: 1.550\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 151.357; l2dist: 0.000\n",
      "    step: 50; loss: 13.345; l2dist: 3.461\n",
      "    step: 100; loss: 6.336; l2dist: 2.397\n",
      "    step: 150; loss: 4.292; l2dist: 1.951\n",
      "    step: 200; loss: 3.470; l2dist: 1.746\n",
      "    step: 250; loss: 3.067; l2dist: 1.650\n",
      "    step: 300; loss: 2.834; l2dist: 1.594\n",
      "    step: 350; loss: 2.737; l2dist: 1.551\n",
      "    step: 400; loss: 2.685; l2dist: 1.547\n",
      "    step: 450; loss: 2.688; l2dist: 1.543\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.552; l2dist: 0.000\n",
      "    step: 50; loss: 10.564; l2dist: 3.011\n",
      "    step: 100; loss: 4.800; l2dist: 2.065\n",
      "    step: 150; loss: 3.408; l2dist: 1.742\n",
      "    step: 200; loss: 2.936; l2dist: 1.614\n",
      "    step: 250; loss: 2.734; l2dist: 1.569\n",
      "    step: 300; loss: 2.619; l2dist: 1.534\n",
      "    step: 350; loss: 2.566; l2dist: 1.521\n",
      "    step: 400; loss: 2.555; l2dist: 1.519\n",
      "    step: 450; loss: 2.559; l2dist: 1.521\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.935; l2dist: 0.000\n",
      "    step: 50; loss: 8.968; l2dist: 2.672\n",
      "    step: 100; loss: 4.084; l2dist: 1.901\n",
      "    step: 150; loss: 3.073; l2dist: 1.641\n",
      "    step: 200; loss: 2.738; l2dist: 1.556\n",
      "    step: 250; loss: 2.605; l2dist: 1.522\n",
      "    step: 300; loss: 2.538; l2dist: 1.503\n",
      "    step: 350; loss: 2.503; l2dist: 1.495\n",
      "    step: 400; loss: 2.479; l2dist: 1.491\n",
      "    step: 450; loss: 2.491; l2dist: 1.483\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.072; l2dist: 0.000\n",
      "    step: 50; loss: 8.560; l2dist: 2.595\n",
      "    step: 100; loss: 3.937; l2dist: 1.853\n",
      "    step: 150; loss: 2.969; l2dist: 1.621\n",
      "    step: 200; loss: 2.699; l2dist: 1.545\n",
      "    step: 250; loss: 2.583; l2dist: 1.517\n",
      "    step: 300; loss: 2.523; l2dist: 1.502\n",
      "    step: 350; loss: 2.487; l2dist: 1.495\n",
      "    step: 400; loss: 2.459; l2dist: 1.484\n",
      "    step: 450; loss: 2.473; l2dist: 1.490\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.177; l2dist: 0.000\n",
      "    step: 50; loss: 8.106; l2dist: 2.496\n",
      "    step: 100; loss: 3.790; l2dist: 1.793\n",
      "    step: 150; loss: 2.911; l2dist: 1.582\n",
      "    step: 200; loss: 2.663; l2dist: 1.521\n",
      "    step: 250; loss: 2.566; l2dist: 1.494\n",
      "    step: 300; loss: 2.504; l2dist: 1.483\n",
      "    step: 350; loss: 2.473; l2dist: 1.470\n",
      "    step: 400; loss: 2.436; l2dist: 1.465\n",
      "    step: 450; loss: 2.426; l2dist: 1.462\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.784; l2dist: 0.000\n",
      "    step: 50; loss: 8.016; l2dist: 2.475\n",
      "    step: 100; loss: 3.783; l2dist: 1.796\n",
      "    step: 150; loss: 2.889; l2dist: 1.591\n",
      "    step: 200; loss: 2.640; l2dist: 1.525\n",
      "    step: 250; loss: 2.528; l2dist: 1.500\n",
      "    step: 300; loss: 2.479; l2dist: 1.489\n",
      "    step: 350; loss: 2.463; l2dist: 1.482\n",
      "    step: 400; loss: 2.449; l2dist: 1.464\n",
      "    step: 450; loss: 2.428; l2dist: 1.467\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.076; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 7.927; l2dist: 2.450\n",
      "    step: 100; loss: 3.766; l2dist: 1.783\n",
      "    step: 150; loss: 2.899; l2dist: 1.588\n",
      "    step: 200; loss: 2.638; l2dist: 1.521\n",
      "    step: 250; loss: 2.533; l2dist: 1.496\n",
      "    step: 300; loss: 2.500; l2dist: 1.489\n",
      "    step: 350; loss: 2.474; l2dist: 1.474\n",
      "    step: 400; loss: 2.443; l2dist: 1.476\n",
      "    step: 450; loss: 2.456; l2dist: 1.477\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.294; l2dist: 0.000\n",
      "    step: 50; loss: 7.862; l2dist: 2.439\n",
      "    step: 100; loss: 3.746; l2dist: 1.786\n",
      "    step: 150; loss: 2.867; l2dist: 1.579\n",
      "    step: 200; loss: 2.622; l2dist: 1.518\n",
      "    step: 250; loss: 2.544; l2dist: 1.496\n",
      "    step: 300; loss: 2.505; l2dist: 1.489\n",
      "    step: 350; loss: 2.444; l2dist: 1.474\n",
      "    step: 400; loss: 2.432; l2dist: 1.465\n",
      "    step: 450; loss: 2.446; l2dist: 1.475\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.456; l2dist: 0.000\n",
      "    step: 50; loss: 7.889; l2dist: 2.446\n",
      "    step: 100; loss: 3.740; l2dist: 1.784\n",
      "    step: 150; loss: 2.871; l2dist: 1.582\n",
      "    step: 200; loss: 2.635; l2dist: 1.521\n",
      "    step: 250; loss: 2.527; l2dist: 1.501\n",
      "    step: 300; loss: 2.477; l2dist: 1.482\n",
      "    step: 350; loss: 2.468; l2dist: 1.477\n",
      "    step: 400; loss: 2.433; l2dist: 1.475\n",
      "    step: 450; loss: 2.421; l2dist: 1.467\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 146.792; l2dist: 0.000\n",
      "    step: 50; loss: 18.560; l2dist: 3.742\n",
      "    step: 100; loss: 7.411; l2dist: 2.666\n",
      "    step: 150; loss: 5.111; l2dist: 2.192\n",
      "    step: 200; loss: 4.220; l2dist: 1.983\n",
      "    step: 250; loss: 3.702; l2dist: 1.849\n",
      "    step: 300; loss: 3.539; l2dist: 1.808\n",
      "    step: 350; loss: 3.438; l2dist: 1.769\n",
      "    step: 400; loss: 3.360; l2dist: 1.761\n",
      "    step: 450; loss: 3.297; l2dist: 1.739\n",
      "binary step: 0; number of successful adv: 92/100\n",
      "    step: 0; loss: 183.420; l2dist: 0.000\n",
      "    step: 50; loss: 18.756; l2dist: 3.730\n",
      "    step: 100; loss: 8.087; l2dist: 2.701\n",
      "    step: 150; loss: 5.590; l2dist: 2.225\n",
      "    step: 200; loss: 4.565; l2dist: 2.011\n",
      "    step: 250; loss: 4.072; l2dist: 1.905\n",
      "    step: 300; loss: 3.742; l2dist: 1.831\n",
      "    step: 350; loss: 3.553; l2dist: 1.789\n",
      "    step: 400; loss: 3.400; l2dist: 1.753\n",
      "    step: 450; loss: 3.368; l2dist: 1.750\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 103.044; l2dist: 0.000\n",
      "    step: 50; loss: 14.192; l2dist: 3.259\n",
      "    step: 100; loss: 5.910; l2dist: 2.300\n",
      "    step: 150; loss: 4.348; l2dist: 1.965\n",
      "    step: 200; loss: 3.768; l2dist: 1.834\n",
      "    step: 250; loss: 3.452; l2dist: 1.766\n",
      "    step: 300; loss: 3.293; l2dist: 1.730\n",
      "    step: 350; loss: 3.203; l2dist: 1.711\n",
      "    step: 400; loss: 3.273; l2dist: 1.723\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.963; l2dist: 0.000\n",
      "    step: 50; loss: 11.613; l2dist: 2.869\n",
      "    step: 100; loss: 5.003; l2dist: 2.111\n",
      "    step: 150; loss: 3.808; l2dist: 1.844\n",
      "    step: 200; loss: 3.389; l2dist: 1.751\n",
      "    step: 250; loss: 3.259; l2dist: 1.717\n",
      "    step: 300; loss: 3.169; l2dist: 1.693\n",
      "    step: 350; loss: 3.100; l2dist: 1.682\n",
      "    step: 400; loss: 3.070; l2dist: 1.666\n",
      "    step: 450; loss: 3.091; l2dist: 1.676\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.814; l2dist: 0.000\n",
      "    step: 50; loss: 10.234; l2dist: 2.638\n",
      "    step: 100; loss: 4.609; l2dist: 2.008\n",
      "    step: 150; loss: 3.598; l2dist: 1.784\n",
      "    step: 200; loss: 3.302; l2dist: 1.720\n",
      "    step: 250; loss: 3.147; l2dist: 1.689\n",
      "    step: 300; loss: 3.094; l2dist: 1.673\n",
      "    step: 350; loss: 3.067; l2dist: 1.660\n",
      "    step: 400; loss: 3.057; l2dist: 1.661\n",
      "    step: 450; loss: 3.051; l2dist: 1.657\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.881; l2dist: 0.000\n",
      "    step: 50; loss: 9.565; l2dist: 2.540\n",
      "    step: 100; loss: 4.449; l2dist: 1.955\n",
      "    step: 150; loss: 3.442; l2dist: 1.738\n",
      "    step: 200; loss: 3.199; l2dist: 1.679\n",
      "    step: 250; loss: 3.132; l2dist: 1.667\n",
      "    step: 300; loss: 3.058; l2dist: 1.650\n",
      "    step: 350; loss: 3.035; l2dist: 1.640\n",
      "    step: 400; loss: 3.020; l2dist: 1.641\n",
      "    step: 450; loss: 3.012; l2dist: 1.638\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.431; l2dist: 0.000\n",
      "    step: 50; loss: 9.352; l2dist: 2.518\n",
      "    step: 100; loss: 4.367; l2dist: 1.953\n",
      "    step: 150; loss: 3.422; l2dist: 1.736\n",
      "    step: 200; loss: 3.199; l2dist: 1.681\n",
      "    step: 250; loss: 3.079; l2dist: 1.662\n",
      "    step: 300; loss: 3.051; l2dist: 1.659\n",
      "    step: 350; loss: 3.015; l2dist: 1.644\n",
      "    step: 400; loss: 3.005; l2dist: 1.647\n",
      "    step: 450; loss: 2.999; l2dist: 1.635\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.950; l2dist: 0.000\n",
      "    step: 50; loss: 9.259; l2dist: 2.508\n",
      "    step: 100; loss: 4.360; l2dist: 1.943\n",
      "    step: 150; loss: 3.423; l2dist: 1.744\n",
      "    step: 200; loss: 3.201; l2dist: 1.696\n",
      "    step: 250; loss: 3.102; l2dist: 1.661\n",
      "    step: 300; loss: 3.053; l2dist: 1.659\n",
      "    step: 350; loss: 3.058; l2dist: 1.650\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.500; l2dist: 0.000\n",
      "    step: 50; loss: 9.232; l2dist: 2.506\n",
      "    step: 100; loss: 4.345; l2dist: 1.946\n",
      "    step: 150; loss: 3.388; l2dist: 1.741\n",
      "    step: 200; loss: 3.165; l2dist: 1.687\n",
      "    step: 250; loss: 3.098; l2dist: 1.664\n",
      "    step: 300; loss: 3.057; l2dist: 1.651\n",
      "    step: 350; loss: 3.010; l2dist: 1.641\n",
      "    step: 400; loss: 3.026; l2dist: 1.638\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.870; l2dist: 0.000\n",
      "    step: 50; loss: 9.277; l2dist: 2.515\n",
      "    step: 100; loss: 4.359; l2dist: 1.952\n",
      "    step: 150; loss: 3.399; l2dist: 1.741\n",
      "    step: 200; loss: 3.185; l2dist: 1.693\n",
      "    step: 250; loss: 3.087; l2dist: 1.669\n",
      "    step: 300; loss: 3.034; l2dist: 1.655\n",
      "    step: 350; loss: 3.022; l2dist: 1.651\n",
      "    step: 400; loss: 3.017; l2dist: 1.649\n",
      "    step: 450; loss: 2.994; l2dist: 1.650\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 119.864; l2dist: 0.000\n",
      "    step: 50; loss: 13.200; l2dist: 3.288\n",
      "    step: 100; loss: 5.775; l2dist: 2.306\n",
      "    step: 150; loss: 4.004; l2dist: 1.912\n",
      "    step: 200; loss: 3.279; l2dist: 1.724\n",
      "    step: 250; loss: 3.010; l2dist: 1.644\n",
      "    step: 300; loss: 2.838; l2dist: 1.597\n",
      "    step: 350; loss: 2.772; l2dist: 1.580\n",
      "    step: 400; loss: 2.731; l2dist: 1.569\n",
      "    step: 450; loss: 2.736; l2dist: 1.555\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 145.921; l2dist: 0.000\n",
      "    step: 50; loss: 13.249; l2dist: 3.323\n",
      "    step: 100; loss: 5.977; l2dist: 2.306\n",
      "    step: 150; loss: 4.194; l2dist: 1.916\n",
      "    step: 200; loss: 3.493; l2dist: 1.741\n",
      "    step: 250; loss: 3.145; l2dist: 1.656\n",
      "    step: 300; loss: 2.960; l2dist: 1.611\n",
      "    step: 350; loss: 2.864; l2dist: 1.582\n",
      "    step: 400; loss: 2.868; l2dist: 1.579\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.632; l2dist: 0.000\n",
      "    step: 50; loss: 10.747; l2dist: 2.912\n",
      "    step: 100; loss: 4.682; l2dist: 2.027\n",
      "    step: 150; loss: 3.450; l2dist: 1.735\n",
      "    step: 200; loss: 3.054; l2dist: 1.631\n",
      "    step: 250; loss: 2.863; l2dist: 1.586\n",
      "    step: 300; loss: 2.737; l2dist: 1.557\n",
      "    step: 350; loss: 2.686; l2dist: 1.542\n",
      "    step: 400; loss: 2.640; l2dist: 1.532\n",
      "    step: 450; loss: 2.641; l2dist: 1.523\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.392; l2dist: 0.000\n",
      "    step: 50; loss: 9.069; l2dist: 2.566\n",
      "    step: 100; loss: 4.083; l2dist: 1.879\n",
      "    step: 150; loss: 3.190; l2dist: 1.664\n",
      "    step: 200; loss: 2.882; l2dist: 1.593\n",
      "    step: 250; loss: 2.755; l2dist: 1.555\n",
      "    step: 300; loss: 2.683; l2dist: 1.543\n",
      "    step: 350; loss: 2.631; l2dist: 1.528\n",
      "    step: 400; loss: 2.600; l2dist: 1.514\n",
      "    step: 450; loss: 2.572; l2dist: 1.513\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.886; l2dist: 0.000\n",
      "    step: 50; loss: 8.150; l2dist: 2.360\n",
      "    step: 100; loss: 3.840; l2dist: 1.788\n",
      "    step: 150; loss: 2.996; l2dist: 1.597\n",
      "    step: 200; loss: 2.752; l2dist: 1.541\n",
      "    step: 250; loss: 2.650; l2dist: 1.514\n",
      "    step: 300; loss: 2.583; l2dist: 1.497\n",
      "    step: 350; loss: 2.541; l2dist: 1.493\n",
      "    step: 400; loss: 2.531; l2dist: 1.489\n",
      "    step: 450; loss: 2.531; l2dist: 1.481\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.277; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 7.853; l2dist: 2.295\n",
      "    step: 100; loss: 3.747; l2dist: 1.754\n",
      "    step: 150; loss: 2.965; l2dist: 1.570\n",
      "    step: 200; loss: 2.734; l2dist: 1.511\n",
      "    step: 250; loss: 2.632; l2dist: 1.495\n",
      "    step: 300; loss: 2.573; l2dist: 1.490\n",
      "    step: 350; loss: 2.536; l2dist: 1.480\n",
      "    step: 400; loss: 2.522; l2dist: 1.473\n",
      "    step: 450; loss: 2.506; l2dist: 1.474\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.172; l2dist: 0.000\n",
      "    step: 50; loss: 7.667; l2dist: 2.259\n",
      "    step: 100; loss: 3.700; l2dist: 1.738\n",
      "    step: 150; loss: 2.907; l2dist: 1.566\n",
      "    step: 200; loss: 2.656; l2dist: 1.496\n",
      "    step: 250; loss: 2.579; l2dist: 1.482\n",
      "    step: 300; loss: 2.531; l2dist: 1.471\n",
      "    step: 350; loss: 2.480; l2dist: 1.460\n",
      "    step: 400; loss: 2.492; l2dist: 1.462\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.759; l2dist: 0.000\n",
      "    step: 50; loss: 7.567; l2dist: 2.242\n",
      "    step: 100; loss: 3.688; l2dist: 1.736\n",
      "    step: 150; loss: 2.865; l2dist: 1.551\n",
      "    step: 200; loss: 2.656; l2dist: 1.498\n",
      "    step: 250; loss: 2.556; l2dist: 1.478\n",
      "    step: 300; loss: 2.503; l2dist: 1.470\n",
      "    step: 350; loss: 2.489; l2dist: 1.462\n",
      "    step: 400; loss: 2.489; l2dist: 1.458\n",
      "    step: 450; loss: 2.468; l2dist: 1.463\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.172; l2dist: 0.000\n",
      "    step: 50; loss: 7.558; l2dist: 2.239\n",
      "    step: 100; loss: 3.682; l2dist: 1.734\n",
      "    step: 150; loss: 2.879; l2dist: 1.551\n",
      "    step: 200; loss: 2.666; l2dist: 1.499\n",
      "    step: 250; loss: 2.553; l2dist: 1.479\n",
      "    step: 300; loss: 2.520; l2dist: 1.471\n",
      "    step: 350; loss: 2.505; l2dist: 1.472\n",
      "    step: 400; loss: 2.485; l2dist: 1.458\n",
      "    step: 450; loss: 2.480; l2dist: 1.467\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.502; l2dist: 0.000\n",
      "    step: 50; loss: 7.610; l2dist: 2.254\n",
      "    step: 100; loss: 3.699; l2dist: 1.740\n",
      "    step: 150; loss: 2.879; l2dist: 1.552\n",
      "    step: 200; loss: 2.664; l2dist: 1.506\n",
      "    step: 250; loss: 2.570; l2dist: 1.492\n",
      "    step: 300; loss: 2.524; l2dist: 1.479\n",
      "    step: 350; loss: 2.494; l2dist: 1.473\n",
      "    step: 400; loss: 2.480; l2dist: 1.464\n",
      "    step: 450; loss: 2.480; l2dist: 1.462\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 130.965; l2dist: 0.000\n",
      "    step: 50; loss: 14.711; l2dist: 3.537\n",
      "    step: 100; loss: 6.596; l2dist: 2.471\n",
      "    step: 150; loss: 4.621; l2dist: 2.039\n",
      "    step: 200; loss: 3.868; l2dist: 1.860\n",
      "    step: 250; loss: 3.571; l2dist: 1.783\n",
      "    step: 300; loss: 3.379; l2dist: 1.736\n",
      "    step: 350; loss: 3.290; l2dist: 1.704\n",
      "    step: 400; loss: 3.301; l2dist: 1.718\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 76.970; l2dist: 0.000\n",
      "    step: 50; loss: 11.467; l2dist: 3.071\n",
      "    step: 100; loss: 5.001; l2dist: 2.120\n",
      "    step: 150; loss: 3.801; l2dist: 1.835\n",
      "    step: 200; loss: 3.408; l2dist: 1.743\n",
      "    step: 250; loss: 3.235; l2dist: 1.683\n",
      "    step: 300; loss: 3.168; l2dist: 1.673\n",
      "    step: 350; loss: 3.115; l2dist: 1.658\n",
      "    step: 400; loss: 3.093; l2dist: 1.654\n",
      "    step: 450; loss: 3.062; l2dist: 1.651\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.023; l2dist: 0.000\n",
      "    step: 50; loss: 9.331; l2dist: 2.724\n",
      "    step: 100; loss: 4.377; l2dist: 1.970\n",
      "    step: 150; loss: 3.456; l2dist: 1.745\n",
      "    step: 200; loss: 3.226; l2dist: 1.679\n",
      "    step: 250; loss: 3.094; l2dist: 1.647\n",
      "    step: 300; loss: 3.038; l2dist: 1.632\n",
      "    step: 350; loss: 3.007; l2dist: 1.627\n",
      "    step: 400; loss: 3.002; l2dist: 1.625\n",
      "    step: 450; loss: 2.986; l2dist: 1.619\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.444; l2dist: 0.000\n",
      "    step: 50; loss: 8.291; l2dist: 2.458\n",
      "    step: 100; loss: 4.268; l2dist: 1.893\n",
      "    step: 150; loss: 3.354; l2dist: 1.697\n",
      "    step: 200; loss: 3.128; l2dist: 1.646\n",
      "    step: 250; loss: 3.040; l2dist: 1.629\n",
      "    step: 300; loss: 2.991; l2dist: 1.610\n",
      "    step: 350; loss: 2.980; l2dist: 1.611\n",
      "    step: 400; loss: 2.963; l2dist: 1.608\n",
      "    step: 450; loss: 2.962; l2dist: 1.604\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.388; l2dist: 0.000\n",
      "    step: 50; loss: 7.706; l2dist: 2.299\n",
      "    step: 100; loss: 4.115; l2dist: 1.813\n",
      "    step: 150; loss: 3.273; l2dist: 1.637\n",
      "    step: 200; loss: 3.069; l2dist: 1.606\n",
      "    step: 250; loss: 2.993; l2dist: 1.588\n",
      "    step: 300; loss: 2.960; l2dist: 1.580\n",
      "    step: 350; loss: 2.942; l2dist: 1.574\n",
      "    step: 400; loss: 2.927; l2dist: 1.569\n",
      "    step: 450; loss: 2.928; l2dist: 1.569\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.870; l2dist: 0.000\n",
      "    step: 50; loss: 7.609; l2dist: 2.242\n",
      "    step: 100; loss: 4.140; l2dist: 1.798\n",
      "    step: 150; loss: 3.257; l2dist: 1.634\n",
      "    step: 200; loss: 3.059; l2dist: 1.596\n",
      "    step: 250; loss: 2.995; l2dist: 1.587\n",
      "    step: 300; loss: 2.963; l2dist: 1.581\n",
      "    step: 350; loss: 2.940; l2dist: 1.570\n",
      "    step: 400; loss: 2.924; l2dist: 1.569\n",
      "    step: 450; loss: 2.928; l2dist: 1.569\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.931; l2dist: 0.000\n",
      "    step: 50; loss: 7.498; l2dist: 2.206\n",
      "    step: 100; loss: 4.098; l2dist: 1.774\n",
      "    step: 150; loss: 3.249; l2dist: 1.622\n",
      "    step: 200; loss: 3.059; l2dist: 1.585\n",
      "    step: 250; loss: 2.988; l2dist: 1.565\n",
      "    step: 300; loss: 2.949; l2dist: 1.562\n",
      "    step: 350; loss: 2.925; l2dist: 1.562\n",
      "    step: 400; loss: 2.921; l2dist: 1.556\n",
      "    step: 450; loss: 2.911; l2dist: 1.554\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.453; l2dist: 0.000\n",
      "    step: 50; loss: 7.478; l2dist: 2.193\n",
      "    step: 100; loss: 4.108; l2dist: 1.779\n",
      "    step: 150; loss: 3.240; l2dist: 1.628\n",
      "    step: 200; loss: 3.055; l2dist: 1.591\n",
      "    step: 250; loss: 2.991; l2dist: 1.571\n",
      "    step: 300; loss: 2.952; l2dist: 1.562\n",
      "    step: 350; loss: 2.936; l2dist: 1.561\n",
      "    step: 400; loss: 2.921; l2dist: 1.561\n",
      "    step: 450; loss: 2.916; l2dist: 1.555\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.266; l2dist: 0.000\n",
      "    step: 50; loss: 7.463; l2dist: 2.188\n",
      "    step: 100; loss: 4.109; l2dist: 1.787\n",
      "    step: 150; loss: 3.246; l2dist: 1.621\n",
      "    step: 200; loss: 3.064; l2dist: 1.582\n",
      "    step: 250; loss: 2.987; l2dist: 1.577\n",
      "    step: 300; loss: 2.954; l2dist: 1.569\n",
      "    step: 350; loss: 2.926; l2dist: 1.562\n",
      "    step: 400; loss: 2.928; l2dist: 1.558\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.537; l2dist: 0.000\n",
      "    step: 50; loss: 7.506; l2dist: 2.203\n",
      "    step: 100; loss: 4.111; l2dist: 1.783\n",
      "    step: 150; loss: 3.242; l2dist: 1.626\n",
      "    step: 200; loss: 3.061; l2dist: 1.591\n",
      "    step: 250; loss: 3.002; l2dist: 1.576\n",
      "    step: 300; loss: 2.952; l2dist: 1.569\n",
      "    step: 350; loss: 2.933; l2dist: 1.566\n",
      "    step: 400; loss: 2.929; l2dist: 1.562\n",
      "    step: 450; loss: 2.919; l2dist: 1.563\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 129.372; l2dist: 0.000\n",
      "    step: 50; loss: 14.266; l2dist: 3.415\n",
      "    step: 100; loss: 6.327; l2dist: 2.402\n",
      "    step: 150; loss: 4.384; l2dist: 1.985\n",
      "    step: 200; loss: 3.609; l2dist: 1.789\n",
      "    step: 250; loss: 3.297; l2dist: 1.700\n",
      "    step: 300; loss: 3.150; l2dist: 1.660\n",
      "    step: 350; loss: 3.032; l2dist: 1.628\n",
      "    step: 400; loss: 2.978; l2dist: 1.609\n",
      "    step: 450; loss: 2.954; l2dist: 1.606\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 133.737; l2dist: 0.000\n",
      "    step: 50; loss: 13.136; l2dist: 3.280\n",
      "    step: 100; loss: 5.924; l2dist: 2.270\n",
      "    step: 150; loss: 4.229; l2dist: 1.900\n",
      "    step: 200; loss: 3.573; l2dist: 1.739\n",
      "    step: 250; loss: 3.254; l2dist: 1.660\n",
      "    step: 300; loss: 3.091; l2dist: 1.614\n",
      "    step: 350; loss: 2.969; l2dist: 1.589\n",
      "    step: 400; loss: 2.896; l2dist: 1.582\n",
      "    step: 450; loss: 2.904; l2dist: 1.576\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.721; l2dist: 0.000\n",
      "    step: 50; loss: 10.223; l2dist: 2.825\n",
      "    step: 100; loss: 4.660; l2dist: 1.995\n",
      "    step: 150; loss: 3.505; l2dist: 1.733\n",
      "    step: 200; loss: 3.113; l2dist: 1.629\n",
      "    step: 250; loss: 2.942; l2dist: 1.589\n",
      "    step: 300; loss: 2.851; l2dist: 1.565\n",
      "    step: 350; loss: 2.783; l2dist: 1.551\n",
      "    step: 400; loss: 2.770; l2dist: 1.546\n",
      "    step: 450; loss: 2.739; l2dist: 1.541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.948; l2dist: 0.000\n",
      "    step: 50; loss: 8.614; l2dist: 2.495\n",
      "    step: 100; loss: 4.116; l2dist: 1.871\n",
      "    step: 150; loss: 3.199; l2dist: 1.648\n",
      "    step: 200; loss: 2.906; l2dist: 1.579\n",
      "    step: 250; loss: 2.802; l2dist: 1.557\n",
      "    step: 300; loss: 2.735; l2dist: 1.539\n",
      "    step: 350; loss: 2.713; l2dist: 1.534\n",
      "    step: 400; loss: 2.703; l2dist: 1.527\n",
      "    step: 450; loss: 2.685; l2dist: 1.521\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.760; l2dist: 0.000\n",
      "    step: 50; loss: 7.881; l2dist: 2.329\n",
      "    step: 100; loss: 3.962; l2dist: 1.798\n",
      "    step: 150; loss: 3.112; l2dist: 1.620\n",
      "    step: 200; loss: 2.876; l2dist: 1.571\n",
      "    step: 250; loss: 2.778; l2dist: 1.549\n",
      "    step: 300; loss: 2.743; l2dist: 1.533\n",
      "    step: 350; loss: 2.709; l2dist: 1.529\n",
      "    step: 400; loss: 2.699; l2dist: 1.518\n",
      "    step: 450; loss: 2.697; l2dist: 1.526\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.930; l2dist: 0.000\n",
      "    step: 50; loss: 7.756; l2dist: 2.310\n",
      "    step: 100; loss: 3.850; l2dist: 1.781\n",
      "    step: 150; loss: 3.007; l2dist: 1.585\n",
      "    step: 200; loss: 2.809; l2dist: 1.536\n",
      "    step: 250; loss: 2.722; l2dist: 1.516\n",
      "    step: 300; loss: 2.680; l2dist: 1.500\n",
      "    step: 350; loss: 2.704; l2dist: 1.503\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.534; l2dist: 0.000\n",
      "    step: 50; loss: 7.680; l2dist: 2.293\n",
      "    step: 100; loss: 3.862; l2dist: 1.780\n",
      "    step: 150; loss: 3.042; l2dist: 1.597\n",
      "    step: 200; loss: 2.827; l2dist: 1.545\n",
      "    step: 250; loss: 2.743; l2dist: 1.523\n",
      "    step: 300; loss: 2.729; l2dist: 1.517\n",
      "    step: 350; loss: 2.693; l2dist: 1.511\n",
      "    step: 400; loss: 2.689; l2dist: 1.505\n",
      "    step: 450; loss: 2.685; l2dist: 1.507\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.025; l2dist: 0.000\n",
      "    step: 50; loss: 7.632; l2dist: 2.281\n",
      "    step: 100; loss: 3.855; l2dist: 1.779\n",
      "    step: 150; loss: 3.031; l2dist: 1.596\n",
      "    step: 200; loss: 2.822; l2dist: 1.545\n",
      "    step: 250; loss: 2.741; l2dist: 1.524\n",
      "    step: 300; loss: 2.718; l2dist: 1.516\n",
      "    step: 350; loss: 2.685; l2dist: 1.516\n",
      "    step: 400; loss: 2.690; l2dist: 1.511\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.525; l2dist: 0.000\n",
      "    step: 50; loss: 7.593; l2dist: 2.272\n",
      "    step: 100; loss: 3.846; l2dist: 1.779\n",
      "    step: 150; loss: 3.021; l2dist: 1.593\n",
      "    step: 200; loss: 2.813; l2dist: 1.550\n",
      "    step: 250; loss: 2.753; l2dist: 1.524\n",
      "    step: 300; loss: 2.704; l2dist: 1.518\n",
      "    step: 350; loss: 2.695; l2dist: 1.514\n",
      "    step: 400; loss: 2.669; l2dist: 1.506\n",
      "    step: 450; loss: 2.668; l2dist: 1.505\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.691; l2dist: 0.000\n",
      "    step: 50; loss: 7.617; l2dist: 2.279\n",
      "    step: 100; loss: 3.850; l2dist: 1.777\n",
      "    step: 150; loss: 3.029; l2dist: 1.598\n",
      "    step: 200; loss: 2.821; l2dist: 1.547\n",
      "    step: 250; loss: 2.757; l2dist: 1.531\n",
      "    step: 300; loss: 2.712; l2dist: 1.512\n",
      "    step: 350; loss: 2.697; l2dist: 1.511\n",
      "    step: 400; loss: 2.672; l2dist: 1.511\n",
      "    step: 450; loss: 2.669; l2dist: 1.513\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 145.148; l2dist: 0.000\n",
      "    step: 50; loss: 18.470; l2dist: 3.811\n",
      "    step: 100; loss: 7.670; l2dist: 2.680\n",
      "    step: 150; loss: 5.264; l2dist: 2.180\n",
      "    step: 200; loss: 4.279; l2dist: 1.949\n",
      "    step: 250; loss: 3.802; l2dist: 1.838\n",
      "    step: 300; loss: 3.663; l2dist: 1.802\n",
      "    step: 350; loss: 3.507; l2dist: 1.755\n",
      "    step: 400; loss: 3.412; l2dist: 1.733\n",
      "    step: 450; loss: 3.386; l2dist: 1.728\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 168.340; l2dist: 0.000\n",
      "    step: 50; loss: 17.259; l2dist: 3.703\n",
      "    step: 100; loss: 7.696; l2dist: 2.614\n",
      "    step: 150; loss: 5.347; l2dist: 2.156\n",
      "    step: 200; loss: 4.363; l2dist: 1.945\n",
      "    step: 250; loss: 3.864; l2dist: 1.835\n",
      "    step: 300; loss: 3.621; l2dist: 1.770\n",
      "    step: 350; loss: 3.464; l2dist: 1.733\n",
      "    step: 400; loss: 3.373; l2dist: 1.717\n",
      "    step: 450; loss: 3.317; l2dist: 1.705\n",
      "binary step: 1; number of successful adv: 99/100\n",
      "    step: 0; loss: 253.010; l2dist: 0.000\n",
      "    step: 50; loss: 18.050; l2dist: 3.858\n",
      "    step: 100; loss: 8.923; l2dist: 2.817\n",
      "    step: 150; loss: 6.110; l2dist: 2.299\n",
      "    step: 200; loss: 4.826; l2dist: 2.036\n",
      "    step: 250; loss: 4.163; l2dist: 1.891\n",
      "    step: 300; loss: 3.817; l2dist: 1.811\n",
      "    step: 350; loss: 3.592; l2dist: 1.759\n",
      "    step: 400; loss: 3.515; l2dist: 1.744\n",
      "    step: 450; loss: 3.401; l2dist: 1.715\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 146.640; l2dist: 0.000\n",
      "    step: 50; loss: 14.266; l2dist: 3.342\n",
      "    step: 100; loss: 6.530; l2dist: 2.384\n",
      "    step: 150; loss: 4.657; l2dist: 1.998\n",
      "    step: 200; loss: 3.916; l2dist: 1.831\n",
      "    step: 250; loss: 3.548; l2dist: 1.751\n",
      "    step: 300; loss: 3.401; l2dist: 1.718\n",
      "    step: 350; loss: 3.318; l2dist: 1.696\n",
      "    step: 400; loss: 3.282; l2dist: 1.682\n",
      "    step: 450; loss: 3.230; l2dist: 1.679\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 94.534; l2dist: 0.000\n",
      "    step: 50; loss: 11.817; l2dist: 2.986\n",
      "    step: 100; loss: 5.385; l2dist: 2.143\n",
      "    step: 150; loss: 3.941; l2dist: 1.834\n",
      "    step: 200; loss: 3.484; l2dist: 1.724\n",
      "    step: 250; loss: 3.290; l2dist: 1.676\n",
      "    step: 300; loss: 3.215; l2dist: 1.652\n",
      "    step: 350; loss: 3.154; l2dist: 1.636\n",
      "    step: 400; loss: 3.130; l2dist: 1.640\n",
      "    step: 450; loss: 3.109; l2dist: 1.633\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.955; l2dist: 0.000\n",
      "    step: 50; loss: 10.780; l2dist: 2.795\n",
      "    step: 100; loss: 4.911; l2dist: 2.029\n",
      "    step: 150; loss: 3.719; l2dist: 1.772\n",
      "    step: 200; loss: 3.377; l2dist: 1.688\n",
      "    step: 250; loss: 3.268; l2dist: 1.661\n",
      "    step: 300; loss: 3.185; l2dist: 1.645\n",
      "    step: 350; loss: 3.142; l2dist: 1.634\n",
      "    step: 400; loss: 3.111; l2dist: 1.638\n",
      "    step: 450; loss: 3.098; l2dist: 1.623\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 57.396; l2dist: 0.000\n",
      "    step: 50; loss: 10.237; l2dist: 2.680\n",
      "    step: 100; loss: 4.702; l2dist: 1.969\n",
      "    step: 150; loss: 3.591; l2dist: 1.737\n",
      "    step: 200; loss: 3.284; l2dist: 1.668\n",
      "    step: 250; loss: 3.182; l2dist: 1.633\n",
      "    step: 300; loss: 3.120; l2dist: 1.621\n",
      "    step: 350; loss: 3.068; l2dist: 1.618\n",
      "    step: 400; loss: 3.063; l2dist: 1.619\n",
      "    step: 450; loss: 3.062; l2dist: 1.620\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.287; l2dist: 0.000\n",
      "    step: 50; loss: 10.032; l2dist: 2.634\n",
      "    step: 100; loss: 4.574; l2dist: 1.951\n",
      "    step: 150; loss: 3.547; l2dist: 1.724\n",
      "    step: 200; loss: 3.255; l2dist: 1.662\n",
      "    step: 250; loss: 3.175; l2dist: 1.632\n",
      "    step: 300; loss: 3.072; l2dist: 1.618\n",
      "    step: 350; loss: 3.093; l2dist: 1.621\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.223; l2dist: 0.000\n",
      "    step: 50; loss: 9.931; l2dist: 2.612\n",
      "    step: 100; loss: 4.544; l2dist: 1.940\n",
      "    step: 150; loss: 3.516; l2dist: 1.724\n",
      "    step: 200; loss: 3.249; l2dist: 1.661\n",
      "    step: 250; loss: 3.128; l2dist: 1.631\n",
      "    step: 300; loss: 3.082; l2dist: 1.623\n",
      "    step: 350; loss: 3.074; l2dist: 1.617\n",
      "    step: 400; loss: 3.031; l2dist: 1.610\n",
      "    step: 450; loss: 3.034; l2dist: 1.610\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.462; l2dist: 0.000\n",
      "    step: 50; loss: 9.985; l2dist: 2.623\n",
      "    step: 100; loss: 4.557; l2dist: 1.952\n",
      "    step: 150; loss: 3.522; l2dist: 1.730\n",
      "    step: 200; loss: 3.239; l2dist: 1.668\n",
      "    step: 250; loss: 3.131; l2dist: 1.639\n",
      "    step: 300; loss: 3.105; l2dist: 1.633\n",
      "    step: 350; loss: 3.078; l2dist: 1.621\n",
      "    step: 400; loss: 3.048; l2dist: 1.619\n",
      "    step: 450; loss: 3.045; l2dist: 1.614\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 135.869; l2dist: 0.000\n",
      "    step: 50; loss: 14.284; l2dist: 3.580\n",
      "    step: 100; loss: 6.466; l2dist: 2.472\n",
      "    step: 150; loss: 4.314; l2dist: 1.999\n",
      "    step: 200; loss: 3.465; l2dist: 1.786\n",
      "    step: 250; loss: 3.170; l2dist: 1.695\n",
      "    step: 300; loss: 2.936; l2dist: 1.638\n",
      "    step: 350; loss: 2.876; l2dist: 1.619\n",
      "    step: 400; loss: 2.847; l2dist: 1.607\n",
      "    step: 450; loss: 2.811; l2dist: 1.590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 120.554; l2dist: 0.000\n",
      "    step: 50; loss: 12.345; l2dist: 3.276\n",
      "    step: 100; loss: 5.314; l2dist: 2.205\n",
      "    step: 150; loss: 3.758; l2dist: 1.832\n",
      "    step: 200; loss: 3.221; l2dist: 1.696\n",
      "    step: 250; loss: 2.975; l2dist: 1.638\n",
      "    step: 300; loss: 2.863; l2dist: 1.606\n",
      "    step: 350; loss: 2.765; l2dist: 1.578\n",
      "    step: 400; loss: 2.705; l2dist: 1.568\n",
      "    step: 450; loss: 2.671; l2dist: 1.560\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.938; l2dist: 0.000\n",
      "    step: 50; loss: 9.887; l2dist: 2.845\n",
      "    step: 100; loss: 4.265; l2dist: 1.961\n",
      "    step: 150; loss: 3.213; l2dist: 1.695\n",
      "    step: 200; loss: 2.877; l2dist: 1.610\n",
      "    step: 250; loss: 2.749; l2dist: 1.575\n",
      "    step: 300; loss: 2.675; l2dist: 1.559\n",
      "    step: 350; loss: 2.623; l2dist: 1.540\n",
      "    step: 400; loss: 2.592; l2dist: 1.538\n",
      "    step: 450; loss: 2.615; l2dist: 1.538\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.177; l2dist: 0.000\n",
      "    step: 50; loss: 8.529; l2dist: 2.566\n",
      "    step: 100; loss: 3.902; l2dist: 1.865\n",
      "    step: 150; loss: 2.998; l2dist: 1.641\n",
      "    step: 200; loss: 2.745; l2dist: 1.575\n",
      "    step: 250; loss: 2.641; l2dist: 1.550\n",
      "    step: 300; loss: 2.586; l2dist: 1.533\n",
      "    step: 350; loss: 2.543; l2dist: 1.519\n",
      "    step: 400; loss: 2.523; l2dist: 1.517\n",
      "    step: 450; loss: 2.540; l2dist: 1.518\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.617; l2dist: 0.000\n",
      "    step: 50; loss: 7.667; l2dist: 2.339\n",
      "    step: 100; loss: 3.789; l2dist: 1.783\n",
      "    step: 150; loss: 2.906; l2dist: 1.588\n",
      "    step: 200; loss: 2.683; l2dist: 1.532\n",
      "    step: 250; loss: 2.578; l2dist: 1.511\n",
      "    step: 300; loss: 2.535; l2dist: 1.491\n",
      "    step: 350; loss: 2.487; l2dist: 1.484\n",
      "    step: 400; loss: 2.484; l2dist: 1.487\n",
      "    step: 450; loss: 2.477; l2dist: 1.478\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.789; l2dist: 0.000\n",
      "    step: 50; loss: 7.164; l2dist: 2.191\n",
      "    step: 100; loss: 3.705; l2dist: 1.700\n",
      "    step: 150; loss: 2.879; l2dist: 1.537\n",
      "    step: 200; loss: 2.667; l2dist: 1.497\n",
      "    step: 250; loss: 2.565; l2dist: 1.479\n",
      "    step: 300; loss: 2.517; l2dist: 1.462\n",
      "    step: 350; loss: 2.491; l2dist: 1.462\n",
      "    step: 400; loss: 2.493; l2dist: 1.462\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.897; l2dist: 0.000\n",
      "    step: 50; loss: 7.082; l2dist: 2.178\n",
      "    step: 100; loss: 3.685; l2dist: 1.698\n",
      "    step: 150; loss: 2.855; l2dist: 1.533\n",
      "    step: 200; loss: 2.625; l2dist: 1.497\n",
      "    step: 250; loss: 2.548; l2dist: 1.470\n",
      "    step: 300; loss: 2.504; l2dist: 1.472\n",
      "    step: 350; loss: 2.488; l2dist: 1.458\n",
      "    step: 400; loss: 2.471; l2dist: 1.461\n",
      "    step: 450; loss: 2.450; l2dist: 1.461\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.890; l2dist: 0.000\n",
      "    step: 50; loss: 7.038; l2dist: 2.167\n",
      "    step: 100; loss: 3.696; l2dist: 1.692\n",
      "    step: 150; loss: 2.855; l2dist: 1.545\n",
      "    step: 200; loss: 2.634; l2dist: 1.500\n",
      "    step: 250; loss: 2.543; l2dist: 1.486\n",
      "    step: 300; loss: 2.499; l2dist: 1.470\n",
      "    step: 350; loss: 2.491; l2dist: 1.463\n",
      "    step: 400; loss: 2.469; l2dist: 1.460\n",
      "    step: 450; loss: 2.474; l2dist: 1.467\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.413; l2dist: 0.000\n",
      "    step: 50; loss: 7.012; l2dist: 2.161\n",
      "    step: 100; loss: 3.697; l2dist: 1.687\n",
      "    step: 150; loss: 2.844; l2dist: 1.543\n",
      "    step: 200; loss: 2.627; l2dist: 1.504\n",
      "    step: 250; loss: 2.539; l2dist: 1.484\n",
      "    step: 300; loss: 2.517; l2dist: 1.485\n",
      "    step: 350; loss: 2.493; l2dist: 1.481\n",
      "    step: 400; loss: 2.471; l2dist: 1.467\n",
      "    step: 450; loss: 2.460; l2dist: 1.473\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.647; l2dist: 0.000\n",
      "    step: 50; loss: 7.072; l2dist: 2.176\n",
      "    step: 100; loss: 3.713; l2dist: 1.701\n",
      "    step: 150; loss: 2.853; l2dist: 1.559\n",
      "    step: 200; loss: 2.630; l2dist: 1.507\n",
      "    step: 250; loss: 2.546; l2dist: 1.495\n",
      "    step: 300; loss: 2.527; l2dist: 1.490\n",
      "    step: 350; loss: 2.485; l2dist: 1.479\n",
      "    step: 400; loss: 2.483; l2dist: 1.476\n",
      "    step: 450; loss: 2.459; l2dist: 1.474\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 140.690; l2dist: 0.000\n",
      "    step: 50; loss: 17.048; l2dist: 3.696\n",
      "    step: 100; loss: 7.246; l2dist: 2.614\n",
      "    step: 150; loss: 4.984; l2dist: 2.148\n",
      "    step: 200; loss: 4.018; l2dist: 1.923\n",
      "    step: 250; loss: 3.548; l2dist: 1.808\n",
      "    step: 300; loss: 3.337; l2dist: 1.750\n",
      "    step: 350; loss: 3.218; l2dist: 1.716\n",
      "    step: 400; loss: 3.134; l2dist: 1.698\n",
      "    step: 450; loss: 3.106; l2dist: 1.687\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 84.165; l2dist: 0.000\n",
      "    step: 50; loss: 13.089; l2dist: 3.215\n",
      "    step: 100; loss: 5.411; l2dist: 2.217\n",
      "    step: 150; loss: 3.919; l2dist: 1.879\n",
      "    step: 200; loss: 3.407; l2dist: 1.757\n",
      "    step: 250; loss: 3.181; l2dist: 1.699\n",
      "    step: 300; loss: 3.059; l2dist: 1.665\n",
      "    step: 350; loss: 2.988; l2dist: 1.654\n",
      "    step: 400; loss: 2.970; l2dist: 1.644\n",
      "    step: 450; loss: 2.948; l2dist: 1.643\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.561; l2dist: 0.000\n",
      "    step: 50; loss: 10.498; l2dist: 2.839\n",
      "    step: 100; loss: 4.427; l2dist: 2.013\n",
      "    step: 150; loss: 3.387; l2dist: 1.749\n",
      "    step: 200; loss: 3.095; l2dist: 1.672\n",
      "    step: 250; loss: 2.967; l2dist: 1.646\n",
      "    step: 300; loss: 2.907; l2dist: 1.623\n",
      "    step: 350; loss: 2.864; l2dist: 1.617\n",
      "    step: 400; loss: 2.860; l2dist: 1.615\n",
      "    step: 450; loss: 2.828; l2dist: 1.605\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.881; l2dist: 0.000\n",
      "    step: 50; loss: 9.110; l2dist: 2.547\n",
      "    step: 100; loss: 4.150; l2dist: 1.930\n",
      "    step: 150; loss: 3.159; l2dist: 1.700\n",
      "    step: 200; loss: 2.938; l2dist: 1.626\n",
      "    step: 250; loss: 2.838; l2dist: 1.614\n",
      "    step: 300; loss: 2.803; l2dist: 1.597\n",
      "    step: 350; loss: 2.795; l2dist: 1.603\n",
      "    step: 400; loss: 2.773; l2dist: 1.585\n",
      "    step: 450; loss: 2.766; l2dist: 1.593\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.406; l2dist: 0.000\n",
      "    step: 50; loss: 8.407; l2dist: 2.353\n",
      "    step: 100; loss: 4.076; l2dist: 1.848\n",
      "    step: 150; loss: 3.126; l2dist: 1.662\n",
      "    step: 200; loss: 2.910; l2dist: 1.611\n",
      "    step: 250; loss: 2.836; l2dist: 1.596\n",
      "    step: 300; loss: 2.788; l2dist: 1.586\n",
      "    step: 350; loss: 2.767; l2dist: 1.579\n",
      "    step: 400; loss: 2.758; l2dist: 1.573\n",
      "    step: 450; loss: 2.751; l2dist: 1.575\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.521; l2dist: 0.000\n",
      "    step: 50; loss: 8.184; l2dist: 2.314\n",
      "    step: 100; loss: 4.070; l2dist: 1.832\n",
      "    step: 150; loss: 3.114; l2dist: 1.639\n",
      "    step: 200; loss: 2.920; l2dist: 1.599\n",
      "    step: 250; loss: 2.844; l2dist: 1.584\n",
      "    step: 300; loss: 2.807; l2dist: 1.578\n",
      "    step: 350; loss: 2.800; l2dist: 1.565\n",
      "    step: 400; loss: 2.778; l2dist: 1.566\n",
      "    step: 450; loss: 2.782; l2dist: 1.559\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.573; l2dist: 0.000\n",
      "    step: 50; loss: 8.099; l2dist: 2.292\n",
      "    step: 100; loss: 4.064; l2dist: 1.834\n",
      "    step: 150; loss: 3.092; l2dist: 1.637\n",
      "    step: 200; loss: 2.892; l2dist: 1.597\n",
      "    step: 250; loss: 2.816; l2dist: 1.580\n",
      "    step: 300; loss: 2.785; l2dist: 1.569\n",
      "    step: 350; loss: 2.768; l2dist: 1.562\n",
      "    step: 400; loss: 2.763; l2dist: 1.560\n",
      "    step: 450; loss: 2.747; l2dist: 1.561\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.052; l2dist: 0.000\n",
      "    step: 50; loss: 8.037; l2dist: 2.270\n",
      "    step: 100; loss: 4.052; l2dist: 1.826\n",
      "    step: 150; loss: 3.092; l2dist: 1.641\n",
      "    step: 200; loss: 2.897; l2dist: 1.593\n",
      "    step: 250; loss: 2.829; l2dist: 1.568\n",
      "    step: 300; loss: 2.790; l2dist: 1.562\n",
      "    step: 350; loss: 2.777; l2dist: 1.559\n",
      "    step: 400; loss: 2.769; l2dist: 1.569\n",
      "    step: 450; loss: 2.745; l2dist: 1.562\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.060; l2dist: 0.000\n",
      "    step: 50; loss: 8.035; l2dist: 2.269\n",
      "    step: 100; loss: 4.057; l2dist: 1.830\n",
      "    step: 150; loss: 3.094; l2dist: 1.632\n",
      "    step: 200; loss: 2.898; l2dist: 1.590\n",
      "    step: 250; loss: 2.827; l2dist: 1.574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 2.787; l2dist: 1.566\n",
      "    step: 350; loss: 2.760; l2dist: 1.566\n",
      "    step: 400; loss: 2.755; l2dist: 1.569\n",
      "    step: 450; loss: 2.759; l2dist: 1.559\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.262; l2dist: 0.000\n",
      "    step: 50; loss: 8.081; l2dist: 2.279\n",
      "    step: 100; loss: 4.060; l2dist: 1.836\n",
      "    step: 150; loss: 3.096; l2dist: 1.640\n",
      "    step: 200; loss: 2.900; l2dist: 1.593\n",
      "    step: 250; loss: 2.833; l2dist: 1.577\n",
      "    step: 300; loss: 2.799; l2dist: 1.571\n",
      "    step: 350; loss: 2.765; l2dist: 1.569\n",
      "    step: 400; loss: 2.752; l2dist: 1.565\n",
      "    step: 450; loss: 2.761; l2dist: 1.559\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 144.595; l2dist: 0.000\n",
      "    step: 50; loss: 16.906; l2dist: 3.706\n",
      "    step: 100; loss: 7.138; l2dist: 2.590\n",
      "    step: 150; loss: 4.933; l2dist: 2.133\n",
      "    step: 200; loss: 4.080; l2dist: 1.927\n",
      "    step: 250; loss: 3.687; l2dist: 1.819\n",
      "    step: 300; loss: 3.452; l2dist: 1.761\n",
      "    step: 350; loss: 3.358; l2dist: 1.736\n",
      "    step: 400; loss: 3.241; l2dist: 1.708\n",
      "    step: 450; loss: 3.286; l2dist: 1.709\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 160.166; l2dist: 0.000\n",
      "    step: 50; loss: 16.188; l2dist: 3.603\n",
      "    step: 100; loss: 6.917; l2dist: 2.491\n",
      "    step: 150; loss: 4.891; l2dist: 2.074\n",
      "    step: 200; loss: 4.067; l2dist: 1.881\n",
      "    step: 250; loss: 3.656; l2dist: 1.786\n",
      "    step: 300; loss: 3.492; l2dist: 1.734\n",
      "    step: 350; loss: 3.336; l2dist: 1.711\n",
      "    step: 400; loss: 3.301; l2dist: 1.697\n",
      "    step: 450; loss: 3.210; l2dist: 1.687\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.089; l2dist: 0.000\n",
      "    step: 50; loss: 12.600; l2dist: 3.136\n",
      "    step: 100; loss: 5.342; l2dist: 2.172\n",
      "    step: 150; loss: 3.985; l2dist: 1.859\n",
      "    step: 200; loss: 3.516; l2dist: 1.761\n",
      "    step: 250; loss: 3.324; l2dist: 1.713\n",
      "    step: 300; loss: 3.210; l2dist: 1.680\n",
      "    step: 350; loss: 3.154; l2dist: 1.668\n",
      "    step: 400; loss: 3.095; l2dist: 1.657\n",
      "    step: 450; loss: 3.076; l2dist: 1.653\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.877; l2dist: 0.000\n",
      "    step: 50; loss: 10.596; l2dist: 2.776\n",
      "    step: 100; loss: 4.654; l2dist: 2.014\n",
      "    step: 150; loss: 3.605; l2dist: 1.777\n",
      "    step: 200; loss: 3.295; l2dist: 1.698\n",
      "    step: 250; loss: 3.141; l2dist: 1.671\n",
      "    step: 300; loss: 3.060; l2dist: 1.650\n",
      "    step: 350; loss: 3.015; l2dist: 1.640\n",
      "    step: 400; loss: 2.978; l2dist: 1.620\n",
      "    step: 450; loss: 2.945; l2dist: 1.622\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.004; l2dist: 0.000\n",
      "    step: 50; loss: 9.913; l2dist: 2.636\n",
      "    step: 100; loss: 4.525; l2dist: 1.948\n",
      "    step: 150; loss: 3.558; l2dist: 1.740\n",
      "    step: 200; loss: 3.277; l2dist: 1.673\n",
      "    step: 250; loss: 3.154; l2dist: 1.646\n",
      "    step: 300; loss: 3.072; l2dist: 1.627\n",
      "    step: 350; loss: 3.041; l2dist: 1.618\n",
      "    step: 400; loss: 2.994; l2dist: 1.619\n",
      "    step: 450; loss: 2.985; l2dist: 1.610\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.181; l2dist: 0.000\n",
      "    step: 50; loss: 9.527; l2dist: 2.555\n",
      "    step: 100; loss: 4.432; l2dist: 1.920\n",
      "    step: 150; loss: 3.510; l2dist: 1.712\n",
      "    step: 200; loss: 3.227; l2dist: 1.662\n",
      "    step: 250; loss: 3.145; l2dist: 1.638\n",
      "    step: 300; loss: 3.052; l2dist: 1.624\n",
      "    step: 350; loss: 3.016; l2dist: 1.607\n",
      "    step: 400; loss: 3.006; l2dist: 1.617\n",
      "    step: 450; loss: 2.988; l2dist: 1.605\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.687; l2dist: 0.000\n",
      "    step: 50; loss: 9.294; l2dist: 2.506\n",
      "    step: 100; loss: 4.382; l2dist: 1.902\n",
      "    step: 150; loss: 3.465; l2dist: 1.707\n",
      "    step: 200; loss: 3.199; l2dist: 1.653\n",
      "    step: 250; loss: 3.097; l2dist: 1.626\n",
      "    step: 300; loss: 3.047; l2dist: 1.625\n",
      "    step: 350; loss: 3.021; l2dist: 1.608\n",
      "    step: 400; loss: 3.000; l2dist: 1.605\n",
      "    step: 450; loss: 2.991; l2dist: 1.606\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.619; l2dist: 0.000\n",
      "    step: 50; loss: 9.266; l2dist: 2.500\n",
      "    step: 100; loss: 4.381; l2dist: 1.900\n",
      "    step: 150; loss: 3.455; l2dist: 1.712\n",
      "    step: 200; loss: 3.204; l2dist: 1.663\n",
      "    step: 250; loss: 3.099; l2dist: 1.632\n",
      "    step: 300; loss: 3.033; l2dist: 1.619\n",
      "    step: 350; loss: 3.011; l2dist: 1.611\n",
      "    step: 400; loss: 2.991; l2dist: 1.609\n",
      "    step: 450; loss: 2.990; l2dist: 1.609\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.826; l2dist: 0.000\n",
      "    step: 50; loss: 9.209; l2dist: 2.489\n",
      "    step: 100; loss: 4.403; l2dist: 1.902\n",
      "    step: 150; loss: 3.475; l2dist: 1.716\n",
      "    step: 200; loss: 3.240; l2dist: 1.668\n",
      "    step: 250; loss: 3.107; l2dist: 1.642\n",
      "    step: 300; loss: 3.057; l2dist: 1.625\n",
      "    step: 350; loss: 3.034; l2dist: 1.620\n",
      "    step: 400; loss: 3.012; l2dist: 1.617\n",
      "    step: 450; loss: 3.016; l2dist: 1.611\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.277; l2dist: 0.000\n",
      "    step: 50; loss: 9.272; l2dist: 2.505\n",
      "    step: 100; loss: 4.411; l2dist: 1.916\n",
      "    step: 150; loss: 3.478; l2dist: 1.720\n",
      "    step: 200; loss: 3.226; l2dist: 1.669\n",
      "    step: 250; loss: 3.160; l2dist: 1.649\n",
      "    step: 300; loss: 3.060; l2dist: 1.625\n",
      "    step: 350; loss: 3.018; l2dist: 1.620\n",
      "    step: 400; loss: 3.011; l2dist: 1.617\n",
      "    step: 450; loss: 3.018; l2dist: 1.619\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 127.703; l2dist: 0.000\n",
      "    step: 50; loss: 16.087; l2dist: 3.471\n",
      "    step: 100; loss: 6.198; l2dist: 2.414\n",
      "    step: 150; loss: 4.333; l2dist: 1.990\n",
      "    step: 200; loss: 3.558; l2dist: 1.796\n",
      "    step: 250; loss: 3.156; l2dist: 1.685\n",
      "    step: 300; loss: 3.011; l2dist: 1.639\n",
      "    step: 350; loss: 2.889; l2dist: 1.608\n",
      "    step: 400; loss: 2.833; l2dist: 1.587\n",
      "    step: 450; loss: 2.819; l2dist: 1.586\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 115.951; l2dist: 0.000\n",
      "    step: 50; loss: 13.470; l2dist: 3.179\n",
      "    step: 100; loss: 5.531; l2dist: 2.213\n",
      "    step: 150; loss: 3.994; l2dist: 1.867\n",
      "    step: 200; loss: 3.390; l2dist: 1.714\n",
      "    step: 250; loss: 3.089; l2dist: 1.639\n",
      "    step: 300; loss: 2.952; l2dist: 1.609\n",
      "    step: 350; loss: 2.843; l2dist: 1.574\n",
      "    step: 400; loss: 2.742; l2dist: 1.565\n",
      "    step: 450; loss: 2.737; l2dist: 1.556\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.297; l2dist: 0.000\n",
      "    step: 50; loss: 10.285; l2dist: 2.719\n",
      "    step: 100; loss: 4.358; l2dist: 1.946\n",
      "    step: 150; loss: 3.302; l2dist: 1.698\n",
      "    step: 200; loss: 2.940; l2dist: 1.609\n",
      "    step: 250; loss: 2.772; l2dist: 1.569\n",
      "    step: 300; loss: 2.689; l2dist: 1.537\n",
      "    step: 350; loss: 2.618; l2dist: 1.528\n",
      "    step: 400; loss: 2.575; l2dist: 1.511\n",
      "    step: 450; loss: 2.583; l2dist: 1.519\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.944; l2dist: 0.000\n",
      "    step: 50; loss: 8.474; l2dist: 2.413\n",
      "    step: 100; loss: 3.926; l2dist: 1.839\n",
      "    step: 150; loss: 3.043; l2dist: 1.629\n",
      "    step: 200; loss: 2.768; l2dist: 1.563\n",
      "    step: 250; loss: 2.641; l2dist: 1.527\n",
      "    step: 300; loss: 2.600; l2dist: 1.518\n",
      "    step: 350; loss: 2.563; l2dist: 1.507\n",
      "    step: 400; loss: 2.533; l2dist: 1.499\n",
      "    step: 450; loss: 2.532; l2dist: 1.491\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.568; l2dist: 0.000\n",
      "    step: 50; loss: 7.664; l2dist: 2.223\n",
      "    step: 100; loss: 3.821; l2dist: 1.790\n",
      "    step: 150; loss: 2.920; l2dist: 1.574\n",
      "    step: 200; loss: 2.689; l2dist: 1.527\n",
      "    step: 250; loss: 2.575; l2dist: 1.495\n",
      "    step: 300; loss: 2.548; l2dist: 1.481\n",
      "    step: 350; loss: 2.518; l2dist: 1.473\n",
      "    step: 400; loss: 2.518; l2dist: 1.478\n",
      "    step: 450; loss: 2.494; l2dist: 1.469\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.361; l2dist: 0.000\n",
      "    step: 50; loss: 7.369; l2dist: 2.135\n",
      "    step: 100; loss: 3.760; l2dist: 1.734\n",
      "    step: 150; loss: 2.857; l2dist: 1.550\n",
      "    step: 200; loss: 2.657; l2dist: 1.495\n",
      "    step: 250; loss: 2.553; l2dist: 1.478\n",
      "    step: 300; loss: 2.549; l2dist: 1.480\n",
      "    step: 350; loss: 2.495; l2dist: 1.466\n",
      "    step: 400; loss: 2.527; l2dist: 1.473\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.797; l2dist: 0.000\n",
      "    step: 50; loss: 7.339; l2dist: 2.123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 3.746; l2dist: 1.739\n",
      "    step: 150; loss: 2.855; l2dist: 1.557\n",
      "    step: 200; loss: 2.629; l2dist: 1.501\n",
      "    step: 250; loss: 2.558; l2dist: 1.480\n",
      "    step: 300; loss: 2.520; l2dist: 1.477\n",
      "    step: 350; loss: 2.503; l2dist: 1.472\n",
      "    step: 400; loss: 2.484; l2dist: 1.464\n",
      "    step: 450; loss: 2.485; l2dist: 1.466\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.722; l2dist: 0.000\n",
      "    step: 50; loss: 7.241; l2dist: 2.105\n",
      "    step: 100; loss: 3.726; l2dist: 1.733\n",
      "    step: 150; loss: 2.830; l2dist: 1.552\n",
      "    step: 200; loss: 2.620; l2dist: 1.498\n",
      "    step: 250; loss: 2.556; l2dist: 1.481\n",
      "    step: 300; loss: 2.506; l2dist: 1.471\n",
      "    step: 350; loss: 2.516; l2dist: 1.469\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.276; l2dist: 0.000\n",
      "    step: 50; loss: 7.239; l2dist: 2.098\n",
      "    step: 100; loss: 3.726; l2dist: 1.729\n",
      "    step: 150; loss: 2.829; l2dist: 1.549\n",
      "    step: 200; loss: 2.638; l2dist: 1.502\n",
      "    step: 250; loss: 2.542; l2dist: 1.481\n",
      "    step: 300; loss: 2.526; l2dist: 1.470\n",
      "    step: 350; loss: 2.502; l2dist: 1.470\n",
      "    step: 400; loss: 2.506; l2dist: 1.472\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.494; l2dist: 0.000\n",
      "    step: 50; loss: 7.290; l2dist: 2.114\n",
      "    step: 100; loss: 3.720; l2dist: 1.740\n",
      "    step: 150; loss: 2.827; l2dist: 1.550\n",
      "    step: 200; loss: 2.615; l2dist: 1.506\n",
      "    step: 250; loss: 2.557; l2dist: 1.480\n",
      "    step: 300; loss: 2.535; l2dist: 1.475\n",
      "    step: 350; loss: 2.508; l2dist: 1.476\n",
      "    step: 400; loss: 2.492; l2dist: 1.476\n",
      "    step: 450; loss: 2.489; l2dist: 1.470\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 148.207; l2dist: 0.000\n",
      "    step: 50; loss: 15.764; l2dist: 3.764\n",
      "    step: 100; loss: 7.000; l2dist: 2.608\n",
      "    step: 150; loss: 4.682; l2dist: 2.121\n",
      "    step: 200; loss: 3.755; l2dist: 1.890\n",
      "    step: 250; loss: 3.311; l2dist: 1.770\n",
      "    step: 300; loss: 3.088; l2dist: 1.708\n",
      "    step: 350; loss: 3.009; l2dist: 1.679\n",
      "    step: 400; loss: 2.934; l2dist: 1.658\n",
      "    step: 450; loss: 2.938; l2dist: 1.662\n",
      "binary step: 0; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.103; l2dist: 0.000\n",
      "    step: 50; loss: 11.726; l2dist: 3.202\n",
      "    step: 100; loss: 4.801; l2dist: 2.144\n",
      "    step: 150; loss: 3.521; l2dist: 1.818\n",
      "    step: 200; loss: 3.107; l2dist: 1.708\n",
      "    step: 250; loss: 2.954; l2dist: 1.653\n",
      "    step: 300; loss: 2.908; l2dist: 1.643\n",
      "    step: 350; loss: 2.836; l2dist: 1.618\n",
      "    step: 400; loss: 2.796; l2dist: 1.615\n",
      "    step: 450; loss: 2.792; l2dist: 1.613\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.071; l2dist: 0.000\n",
      "    step: 50; loss: 9.651; l2dist: 2.856\n",
      "    step: 100; loss: 4.056; l2dist: 1.959\n",
      "    step: 150; loss: 3.171; l2dist: 1.712\n",
      "    step: 200; loss: 2.924; l2dist: 1.646\n",
      "    step: 250; loss: 2.812; l2dist: 1.615\n",
      "    step: 300; loss: 2.762; l2dist: 1.605\n",
      "    step: 350; loss: 2.729; l2dist: 1.599\n",
      "    step: 400; loss: 2.724; l2dist: 1.598\n",
      "    step: 450; loss: 2.734; l2dist: 1.600\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.638; l2dist: 0.000\n",
      "    step: 50; loss: 8.405; l2dist: 2.547\n",
      "    step: 100; loss: 3.919; l2dist: 1.910\n",
      "    step: 150; loss: 3.074; l2dist: 1.685\n",
      "    step: 200; loss: 2.817; l2dist: 1.623\n",
      "    step: 250; loss: 2.752; l2dist: 1.602\n",
      "    step: 300; loss: 2.711; l2dist: 1.588\n",
      "    step: 350; loss: 2.708; l2dist: 1.583\n",
      "    step: 400; loss: 2.691; l2dist: 1.583\n",
      "    step: 450; loss: 2.685; l2dist: 1.578\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.136; l2dist: 0.000\n",
      "    step: 50; loss: 7.786; l2dist: 2.384\n",
      "    step: 100; loss: 3.962; l2dist: 1.858\n",
      "    step: 150; loss: 3.061; l2dist: 1.668\n",
      "    step: 200; loss: 2.837; l2dist: 1.621\n",
      "    step: 250; loss: 2.773; l2dist: 1.591\n",
      "    step: 300; loss: 2.730; l2dist: 1.596\n",
      "    step: 350; loss: 2.704; l2dist: 1.573\n",
      "    step: 400; loss: 2.686; l2dist: 1.578\n",
      "    step: 450; loss: 2.680; l2dist: 1.573\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.767; l2dist: 0.000\n",
      "    step: 50; loss: 7.568; l2dist: 2.325\n",
      "    step: 100; loss: 3.927; l2dist: 1.842\n",
      "    step: 150; loss: 3.020; l2dist: 1.636\n",
      "    step: 200; loss: 2.822; l2dist: 1.584\n",
      "    step: 250; loss: 2.752; l2dist: 1.564\n",
      "    step: 300; loss: 2.715; l2dist: 1.556\n",
      "    step: 350; loss: 2.711; l2dist: 1.550\n",
      "    step: 400; loss: 2.684; l2dist: 1.546\n",
      "    step: 450; loss: 2.685; l2dist: 1.547\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.483; l2dist: 0.000\n",
      "    step: 50; loss: 7.629; l2dist: 2.327\n",
      "    step: 100; loss: 3.937; l2dist: 1.850\n",
      "    step: 150; loss: 3.033; l2dist: 1.651\n",
      "    step: 200; loss: 2.811; l2dist: 1.600\n",
      "    step: 250; loss: 2.763; l2dist: 1.579\n",
      "    step: 300; loss: 2.706; l2dist: 1.575\n",
      "    step: 350; loss: 2.694; l2dist: 1.569\n",
      "    step: 400; loss: 2.683; l2dist: 1.563\n",
      "    step: 450; loss: 2.668; l2dist: 1.561\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.960; l2dist: 0.000\n",
      "    step: 50; loss: 7.578; l2dist: 2.316\n",
      "    step: 100; loss: 3.967; l2dist: 1.850\n",
      "    step: 150; loss: 3.051; l2dist: 1.652\n",
      "    step: 200; loss: 2.845; l2dist: 1.607\n",
      "    step: 250; loss: 2.763; l2dist: 1.587\n",
      "    step: 300; loss: 2.719; l2dist: 1.580\n",
      "    step: 350; loss: 2.708; l2dist: 1.570\n",
      "    step: 400; loss: 2.698; l2dist: 1.568\n",
      "    step: 450; loss: 2.686; l2dist: 1.564\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.942; l2dist: 0.000\n",
      "    step: 50; loss: 7.606; l2dist: 2.321\n",
      "    step: 100; loss: 3.955; l2dist: 1.861\n",
      "    step: 150; loss: 3.042; l2dist: 1.654\n",
      "    step: 200; loss: 2.840; l2dist: 1.606\n",
      "    step: 250; loss: 2.762; l2dist: 1.587\n",
      "    step: 300; loss: 2.725; l2dist: 1.584\n",
      "    step: 350; loss: 2.710; l2dist: 1.578\n",
      "    step: 400; loss: 2.691; l2dist: 1.575\n",
      "    step: 450; loss: 2.676; l2dist: 1.571\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.047; l2dist: 0.000\n",
      "    step: 50; loss: 7.623; l2dist: 2.330\n",
      "    step: 100; loss: 3.941; l2dist: 1.865\n",
      "    step: 150; loss: 3.023; l2dist: 1.657\n",
      "    step: 200; loss: 2.822; l2dist: 1.604\n",
      "    step: 250; loss: 2.746; l2dist: 1.593\n",
      "    step: 300; loss: 2.705; l2dist: 1.584\n",
      "    step: 350; loss: 2.693; l2dist: 1.570\n",
      "    step: 400; loss: 2.675; l2dist: 1.567\n",
      "    step: 450; loss: 2.663; l2dist: 1.567\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 167.189; l2dist: 0.000\n",
      "    step: 50; loss: 18.739; l2dist: 4.053\n",
      "    step: 100; loss: 8.899; l2dist: 2.930\n",
      "    step: 150; loss: 6.010; l2dist: 2.392\n",
      "    step: 200; loss: 4.865; l2dist: 2.143\n",
      "    step: 250; loss: 4.309; l2dist: 2.010\n",
      "    step: 300; loss: 4.117; l2dist: 1.948\n",
      "    step: 350; loss: 3.973; l2dist: 1.915\n",
      "    step: 400; loss: 3.877; l2dist: 1.891\n",
      "    step: 450; loss: 3.862; l2dist: 1.881\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 162.479; l2dist: 0.000\n",
      "    step: 50; loss: 17.381; l2dist: 3.874\n",
      "    step: 100; loss: 8.137; l2dist: 2.741\n",
      "    step: 150; loss: 5.620; l2dist: 2.258\n",
      "    step: 200; loss: 4.628; l2dist: 2.050\n",
      "    step: 250; loss: 4.216; l2dist: 1.947\n",
      "    step: 300; loss: 4.015; l2dist: 1.888\n",
      "    step: 350; loss: 3.998; l2dist: 1.897\n",
      "    step: 400; loss: 3.746; l2dist: 1.849\n",
      "    step: 450; loss: 3.718; l2dist: 1.832\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 94.947; l2dist: 0.000\n",
      "    step: 50; loss: 13.589; l2dist: 3.408\n",
      "    step: 100; loss: 6.081; l2dist: 2.356\n",
      "    step: 150; loss: 4.534; l2dist: 2.030\n",
      "    step: 200; loss: 3.985; l2dist: 1.900\n",
      "    step: 250; loss: 3.770; l2dist: 1.858\n",
      "    step: 300; loss: 3.643; l2dist: 1.822\n",
      "    step: 350; loss: 3.567; l2dist: 1.802\n",
      "    step: 400; loss: 3.543; l2dist: 1.800\n",
      "    step: 450; loss: 3.554; l2dist: 1.802\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.676; l2dist: 0.000\n",
      "    step: 50; loss: 11.270; l2dist: 2.965\n",
      "    step: 100; loss: 5.336; l2dist: 2.187\n",
      "    step: 150; loss: 4.151; l2dist: 1.935\n",
      "    step: 200; loss: 3.780; l2dist: 1.849\n",
      "    step: 250; loss: 3.644; l2dist: 1.817\n",
      "    step: 300; loss: 3.560; l2dist: 1.807\n",
      "    step: 350; loss: 3.532; l2dist: 1.800\n",
      "    step: 400; loss: 3.521; l2dist: 1.778\n",
      "    step: 450; loss: 3.496; l2dist: 1.782\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.655; l2dist: 0.000\n",
      "    step: 50; loss: 10.423; l2dist: 2.768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 5.168; l2dist: 2.102\n",
      "    step: 150; loss: 4.019; l2dist: 1.887\n",
      "    step: 200; loss: 3.702; l2dist: 1.822\n",
      "    step: 250; loss: 3.577; l2dist: 1.792\n",
      "    step: 300; loss: 3.486; l2dist: 1.780\n",
      "    step: 350; loss: 3.459; l2dist: 1.765\n",
      "    step: 400; loss: 3.408; l2dist: 1.759\n",
      "    step: 450; loss: 3.409; l2dist: 1.755\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.913; l2dist: 0.000\n",
      "    step: 50; loss: 10.066; l2dist: 2.726\n",
      "    step: 100; loss: 5.034; l2dist: 2.088\n",
      "    step: 150; loss: 3.935; l2dist: 1.859\n",
      "    step: 200; loss: 3.645; l2dist: 1.793\n",
      "    step: 250; loss: 3.519; l2dist: 1.770\n",
      "    step: 300; loss: 3.445; l2dist: 1.748\n",
      "    step: 350; loss: 3.414; l2dist: 1.748\n",
      "    step: 400; loss: 3.426; l2dist: 1.746\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.366; l2dist: 0.000\n",
      "    step: 50; loss: 10.005; l2dist: 2.711\n",
      "    step: 100; loss: 5.022; l2dist: 2.081\n",
      "    step: 150; loss: 3.921; l2dist: 1.866\n",
      "    step: 200; loss: 3.635; l2dist: 1.801\n",
      "    step: 250; loss: 3.503; l2dist: 1.774\n",
      "    step: 300; loss: 3.448; l2dist: 1.761\n",
      "    step: 350; loss: 3.416; l2dist: 1.756\n",
      "    step: 400; loss: 3.405; l2dist: 1.755\n",
      "    step: 450; loss: 3.395; l2dist: 1.752\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.965; l2dist: 0.000\n",
      "    step: 50; loss: 9.983; l2dist: 2.701\n",
      "    step: 100; loss: 5.031; l2dist: 2.084\n",
      "    step: 150; loss: 3.943; l2dist: 1.875\n",
      "    step: 200; loss: 3.635; l2dist: 1.810\n",
      "    step: 250; loss: 3.513; l2dist: 1.780\n",
      "    step: 300; loss: 3.442; l2dist: 1.770\n",
      "    step: 350; loss: 3.421; l2dist: 1.763\n",
      "    step: 400; loss: 3.426; l2dist: 1.763\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.177; l2dist: 0.000\n",
      "    step: 50; loss: 9.943; l2dist: 2.694\n",
      "    step: 100; loss: 5.025; l2dist: 2.081\n",
      "    step: 150; loss: 3.924; l2dist: 1.871\n",
      "    step: 200; loss: 3.639; l2dist: 1.811\n",
      "    step: 250; loss: 3.496; l2dist: 1.777\n",
      "    step: 300; loss: 3.446; l2dist: 1.770\n",
      "    step: 350; loss: 3.427; l2dist: 1.763\n",
      "    step: 400; loss: 3.404; l2dist: 1.767\n",
      "    step: 450; loss: 3.391; l2dist: 1.762\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.627; l2dist: 0.000\n",
      "    step: 50; loss: 9.990; l2dist: 2.703\n",
      "    step: 100; loss: 5.027; l2dist: 2.091\n",
      "    step: 150; loss: 3.929; l2dist: 1.873\n",
      "    step: 200; loss: 3.645; l2dist: 1.811\n",
      "    step: 250; loss: 3.523; l2dist: 1.783\n",
      "    step: 300; loss: 3.437; l2dist: 1.772\n",
      "    step: 350; loss: 3.411; l2dist: 1.764\n",
      "    step: 400; loss: 3.410; l2dist: 1.768\n",
      "    step: 450; loss: 3.412; l2dist: 1.764\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 176.941; l2dist: 0.000\n",
      "    step: 50; loss: 20.939; l2dist: 4.318\n",
      "    step: 100; loss: 10.012; l2dist: 3.123\n",
      "    step: 150; loss: 6.630; l2dist: 2.530\n",
      "    step: 200; loss: 5.232; l2dist: 2.234\n",
      "    step: 250; loss: 4.508; l2dist: 2.078\n",
      "    step: 300; loss: 4.153; l2dist: 1.988\n",
      "    step: 350; loss: 3.963; l2dist: 1.941\n",
      "    step: 400; loss: 3.942; l2dist: 1.925\n",
      "    step: 450; loss: 3.894; l2dist: 1.919\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 164.554; l2dist: 0.000\n",
      "    step: 50; loss: 18.681; l2dist: 4.049\n",
      "    step: 100; loss: 8.398; l2dist: 2.816\n",
      "    step: 150; loss: 5.852; l2dist: 2.330\n",
      "    step: 200; loss: 4.810; l2dist: 2.116\n",
      "    step: 250; loss: 4.314; l2dist: 2.007\n",
      "    step: 300; loss: 4.085; l2dist: 1.960\n",
      "    step: 350; loss: 3.926; l2dist: 1.912\n",
      "    step: 400; loss: 3.831; l2dist: 1.892\n",
      "    step: 450; loss: 3.746; l2dist: 1.876\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 91.728; l2dist: 0.000\n",
      "    step: 50; loss: 14.675; l2dist: 3.545\n",
      "    step: 100; loss: 6.259; l2dist: 2.425\n",
      "    step: 150; loss: 4.649; l2dist: 2.088\n",
      "    step: 200; loss: 4.129; l2dist: 1.967\n",
      "    step: 250; loss: 3.858; l2dist: 1.907\n",
      "    step: 300; loss: 3.760; l2dist: 1.877\n",
      "    step: 350; loss: 3.674; l2dist: 1.863\n",
      "    step: 400; loss: 3.629; l2dist: 1.843\n",
      "    step: 450; loss: 3.606; l2dist: 1.839\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.913; l2dist: 0.000\n",
      "    step: 50; loss: 12.505; l2dist: 3.100\n",
      "    step: 100; loss: 5.510; l2dist: 2.249\n",
      "    step: 150; loss: 4.238; l2dist: 1.981\n",
      "    step: 200; loss: 3.864; l2dist: 1.885\n",
      "    step: 250; loss: 3.646; l2dist: 1.847\n",
      "    step: 300; loss: 3.594; l2dist: 1.832\n",
      "    step: 350; loss: 3.521; l2dist: 1.817\n",
      "    step: 400; loss: 3.502; l2dist: 1.811\n",
      "    step: 450; loss: 3.505; l2dist: 1.805\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.397; l2dist: 0.000\n",
      "    step: 50; loss: 11.584; l2dist: 2.935\n",
      "    step: 100; loss: 5.230; l2dist: 2.149\n",
      "    step: 150; loss: 4.073; l2dist: 1.927\n",
      "    step: 200; loss: 3.718; l2dist: 1.847\n",
      "    step: 250; loss: 3.584; l2dist: 1.812\n",
      "    step: 300; loss: 3.535; l2dist: 1.814\n",
      "    step: 350; loss: 3.498; l2dist: 1.795\n",
      "    step: 400; loss: 3.475; l2dist: 1.794\n",
      "    step: 450; loss: 3.459; l2dist: 1.789\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.881; l2dist: 0.000\n",
      "    step: 50; loss: 11.191; l2dist: 2.859\n",
      "    step: 100; loss: 5.155; l2dist: 2.121\n",
      "    step: 150; loss: 4.016; l2dist: 1.886\n",
      "    step: 200; loss: 3.688; l2dist: 1.825\n",
      "    step: 250; loss: 3.571; l2dist: 1.797\n",
      "    step: 300; loss: 3.524; l2dist: 1.798\n",
      "    step: 350; loss: 3.503; l2dist: 1.781\n",
      "    step: 400; loss: 3.470; l2dist: 1.786\n",
      "    step: 450; loss: 3.470; l2dist: 1.785\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.153; l2dist: 0.000\n",
      "    step: 50; loss: 11.082; l2dist: 2.846\n",
      "    step: 100; loss: 5.103; l2dist: 2.113\n",
      "    step: 150; loss: 3.964; l2dist: 1.894\n",
      "    step: 200; loss: 3.669; l2dist: 1.832\n",
      "    step: 250; loss: 3.561; l2dist: 1.801\n",
      "    step: 300; loss: 3.511; l2dist: 1.794\n",
      "    step: 350; loss: 3.483; l2dist: 1.795\n",
      "    step: 400; loss: 3.473; l2dist: 1.790\n",
      "    step: 450; loss: 3.450; l2dist: 1.785\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.705; l2dist: 0.000\n",
      "    step: 50; loss: 11.058; l2dist: 2.846\n",
      "    step: 100; loss: 5.108; l2dist: 2.122\n",
      "    step: 150; loss: 3.973; l2dist: 1.899\n",
      "    step: 200; loss: 3.666; l2dist: 1.837\n",
      "    step: 250; loss: 3.573; l2dist: 1.820\n",
      "    step: 300; loss: 3.513; l2dist: 1.800\n",
      "    step: 350; loss: 3.474; l2dist: 1.787\n",
      "    step: 400; loss: 3.469; l2dist: 1.789\n",
      "    step: 450; loss: 3.462; l2dist: 1.792\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.917; l2dist: 0.000\n",
      "    step: 50; loss: 11.011; l2dist: 2.837\n",
      "    step: 100; loss: 5.107; l2dist: 2.115\n",
      "    step: 150; loss: 3.956; l2dist: 1.903\n",
      "    step: 200; loss: 3.658; l2dist: 1.832\n",
      "    step: 250; loss: 3.562; l2dist: 1.813\n",
      "    step: 300; loss: 3.512; l2dist: 1.805\n",
      "    step: 350; loss: 3.469; l2dist: 1.790\n",
      "    step: 400; loss: 3.472; l2dist: 1.794\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.199; l2dist: 0.000\n",
      "    step: 50; loss: 11.060; l2dist: 2.848\n",
      "    step: 100; loss: 5.115; l2dist: 2.129\n",
      "    step: 150; loss: 3.969; l2dist: 1.903\n",
      "    step: 200; loss: 3.678; l2dist: 1.833\n",
      "    step: 250; loss: 3.573; l2dist: 1.810\n",
      "    step: 300; loss: 3.517; l2dist: 1.804\n",
      "    step: 350; loss: 3.466; l2dist: 1.797\n",
      "    step: 400; loss: 3.465; l2dist: 1.789\n",
      "    step: 450; loss: 3.511; l2dist: 1.792\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 172.979; l2dist: 0.000\n",
      "    step: 50; loss: 22.307; l2dist: 4.240\n",
      "    step: 100; loss: 9.540; l2dist: 3.050\n",
      "    step: 150; loss: 6.505; l2dist: 2.501\n",
      "    step: 200; loss: 5.166; l2dist: 2.221\n",
      "    step: 250; loss: 4.500; l2dist: 2.077\n",
      "    step: 300; loss: 4.251; l2dist: 2.008\n",
      "    step: 350; loss: 3.998; l2dist: 1.950\n",
      "    step: 400; loss: 3.877; l2dist: 1.916\n",
      "    step: 450; loss: 3.838; l2dist: 1.902\n",
      "binary step: 0; number of successful adv: 92/100\n",
      "    step: 0; loss: 220.639; l2dist: 0.000\n",
      "    step: 50; loss: 22.299; l2dist: 4.203\n",
      "    step: 100; loss: 10.446; l2dist: 3.145\n",
      "    step: 150; loss: 7.176; l2dist: 2.582\n",
      "    step: 200; loss: 5.770; l2dist: 2.295\n",
      "    step: 250; loss: 5.041; l2dist: 2.152\n",
      "    step: 300; loss: 4.611; l2dist: 2.062\n",
      "    step: 350; loss: 4.340; l2dist: 2.004\n",
      "    step: 400; loss: 4.192; l2dist: 1.966\n",
      "    step: 450; loss: 4.022; l2dist: 1.948\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.330; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 17.020; l2dist: 3.669\n",
      "    step: 100; loss: 7.459; l2dist: 2.638\n",
      "    step: 150; loss: 5.391; l2dist: 2.236\n",
      "    step: 200; loss: 4.597; l2dist: 2.065\n",
      "    step: 250; loss: 4.230; l2dist: 1.984\n",
      "    step: 300; loss: 3.969; l2dist: 1.926\n",
      "    step: 350; loss: 3.861; l2dist: 1.900\n",
      "    step: 400; loss: 3.769; l2dist: 1.883\n",
      "    step: 450; loss: 3.730; l2dist: 1.874\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.126; l2dist: 0.000\n",
      "    step: 50; loss: 14.096; l2dist: 3.281\n",
      "    step: 100; loss: 6.221; l2dist: 2.370\n",
      "    step: 150; loss: 4.664; l2dist: 2.066\n",
      "    step: 200; loss: 4.104; l2dist: 1.942\n",
      "    step: 250; loss: 3.878; l2dist: 1.893\n",
      "    step: 300; loss: 3.759; l2dist: 1.861\n",
      "    step: 350; loss: 3.676; l2dist: 1.842\n",
      "    step: 400; loss: 3.619; l2dist: 1.835\n",
      "    step: 450; loss: 3.594; l2dist: 1.832\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.689; l2dist: 0.000\n",
      "    step: 50; loss: 12.153; l2dist: 2.960\n",
      "    step: 100; loss: 5.553; l2dist: 2.190\n",
      "    step: 150; loss: 4.246; l2dist: 1.938\n",
      "    step: 200; loss: 3.894; l2dist: 1.863\n",
      "    step: 250; loss: 3.706; l2dist: 1.823\n",
      "    step: 300; loss: 3.584; l2dist: 1.803\n",
      "    step: 350; loss: 3.540; l2dist: 1.796\n",
      "    step: 400; loss: 3.520; l2dist: 1.780\n",
      "    step: 450; loss: 3.479; l2dist: 1.779\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.850; l2dist: 0.000\n",
      "    step: 50; loss: 11.661; l2dist: 2.873\n",
      "    step: 100; loss: 5.304; l2dist: 2.144\n",
      "    step: 150; loss: 4.095; l2dist: 1.898\n",
      "    step: 200; loss: 3.749; l2dist: 1.827\n",
      "    step: 250; loss: 3.609; l2dist: 1.800\n",
      "    step: 300; loss: 3.520; l2dist: 1.778\n",
      "    step: 350; loss: 3.479; l2dist: 1.765\n",
      "    step: 400; loss: 3.484; l2dist: 1.773\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.846; l2dist: 0.000\n",
      "    step: 50; loss: 11.507; l2dist: 2.837\n",
      "    step: 100; loss: 5.282; l2dist: 2.135\n",
      "    step: 150; loss: 4.062; l2dist: 1.905\n",
      "    step: 200; loss: 3.757; l2dist: 1.834\n",
      "    step: 250; loss: 3.631; l2dist: 1.812\n",
      "    step: 300; loss: 3.568; l2dist: 1.800\n",
      "    step: 350; loss: 3.499; l2dist: 1.783\n",
      "    step: 400; loss: 3.476; l2dist: 1.779\n",
      "    step: 450; loss: 3.464; l2dist: 1.773\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.690; l2dist: 0.000\n",
      "    step: 50; loss: 11.385; l2dist: 2.811\n",
      "    step: 100; loss: 5.227; l2dist: 2.133\n",
      "    step: 150; loss: 4.051; l2dist: 1.901\n",
      "    step: 200; loss: 3.744; l2dist: 1.840\n",
      "    step: 250; loss: 3.591; l2dist: 1.812\n",
      "    step: 300; loss: 3.525; l2dist: 1.797\n",
      "    step: 350; loss: 3.475; l2dist: 1.783\n",
      "    step: 400; loss: 3.481; l2dist: 1.784\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.567; l2dist: 0.000\n",
      "    step: 50; loss: 11.330; l2dist: 2.802\n",
      "    step: 100; loss: 5.202; l2dist: 2.127\n",
      "    step: 150; loss: 4.021; l2dist: 1.899\n",
      "    step: 200; loss: 3.720; l2dist: 1.831\n",
      "    step: 250; loss: 3.573; l2dist: 1.804\n",
      "    step: 300; loss: 3.512; l2dist: 1.787\n",
      "    step: 350; loss: 3.480; l2dist: 1.786\n",
      "    step: 400; loss: 3.449; l2dist: 1.780\n",
      "    step: 450; loss: 3.447; l2dist: 1.778\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.865; l2dist: 0.000\n",
      "    step: 50; loss: 11.383; l2dist: 2.811\n",
      "    step: 100; loss: 5.212; l2dist: 2.134\n",
      "    step: 150; loss: 4.042; l2dist: 1.903\n",
      "    step: 200; loss: 3.729; l2dist: 1.836\n",
      "    step: 250; loss: 3.615; l2dist: 1.817\n",
      "    step: 300; loss: 3.538; l2dist: 1.812\n",
      "    step: 350; loss: 3.502; l2dist: 1.790\n",
      "    step: 400; loss: 3.477; l2dist: 1.797\n",
      "    step: 450; loss: 3.452; l2dist: 1.790\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 142.156; l2dist: 0.000\n",
      "    step: 50; loss: 19.115; l2dist: 3.742\n",
      "    step: 100; loss: 7.416; l2dist: 2.630\n",
      "    step: 150; loss: 5.079; l2dist: 2.155\n",
      "    step: 200; loss: 4.188; l2dist: 1.954\n",
      "    step: 250; loss: 3.714; l2dist: 1.833\n",
      "    step: 300; loss: 3.494; l2dist: 1.775\n",
      "    step: 350; loss: 3.495; l2dist: 1.773\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 146.769; l2dist: 0.000\n",
      "    step: 50; loss: 17.684; l2dist: 3.572\n",
      "    step: 100; loss: 7.217; l2dist: 2.536\n",
      "    step: 150; loss: 5.000; l2dist: 2.092\n",
      "    step: 200; loss: 4.168; l2dist: 1.909\n",
      "    step: 250; loss: 3.737; l2dist: 1.816\n",
      "    step: 300; loss: 3.567; l2dist: 1.770\n",
      "    step: 350; loss: 3.465; l2dist: 1.742\n",
      "    step: 400; loss: 3.362; l2dist: 1.729\n",
      "    step: 450; loss: 3.296; l2dist: 1.710\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 85.880; l2dist: 0.000\n",
      "    step: 50; loss: 13.357; l2dist: 3.081\n",
      "    step: 100; loss: 5.519; l2dist: 2.201\n",
      "    step: 150; loss: 4.083; l2dist: 1.900\n",
      "    step: 200; loss: 3.538; l2dist: 1.776\n",
      "    step: 250; loss: 3.332; l2dist: 1.731\n",
      "    step: 300; loss: 3.248; l2dist: 1.699\n",
      "    step: 350; loss: 3.135; l2dist: 1.678\n",
      "    step: 400; loss: 3.142; l2dist: 1.671\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.230; l2dist: 0.000\n",
      "    step: 50; loss: 11.213; l2dist: 2.764\n",
      "    step: 100; loss: 4.941; l2dist: 2.074\n",
      "    step: 150; loss: 3.751; l2dist: 1.806\n",
      "    step: 200; loss: 3.385; l2dist: 1.733\n",
      "    step: 250; loss: 3.216; l2dist: 1.693\n",
      "    step: 300; loss: 3.110; l2dist: 1.670\n",
      "    step: 350; loss: 3.070; l2dist: 1.651\n",
      "    step: 400; loss: 3.052; l2dist: 1.651\n",
      "    step: 450; loss: 3.054; l2dist: 1.648\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.718; l2dist: 0.000\n",
      "    step: 50; loss: 10.277; l2dist: 2.625\n",
      "    step: 100; loss: 4.815; l2dist: 2.012\n",
      "    step: 150; loss: 3.650; l2dist: 1.780\n",
      "    step: 200; loss: 3.337; l2dist: 1.706\n",
      "    step: 250; loss: 3.199; l2dist: 1.669\n",
      "    step: 300; loss: 3.116; l2dist: 1.655\n",
      "    step: 350; loss: 3.093; l2dist: 1.642\n",
      "    step: 400; loss: 3.047; l2dist: 1.640\n",
      "    step: 450; loss: 3.047; l2dist: 1.634\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.995; l2dist: 0.000\n",
      "    step: 50; loss: 10.040; l2dist: 2.590\n",
      "    step: 100; loss: 4.734; l2dist: 1.967\n",
      "    step: 150; loss: 3.610; l2dist: 1.752\n",
      "    step: 200; loss: 3.298; l2dist: 1.679\n",
      "    step: 250; loss: 3.154; l2dist: 1.647\n",
      "    step: 300; loss: 3.062; l2dist: 1.629\n",
      "    step: 350; loss: 3.039; l2dist: 1.622\n",
      "    step: 400; loss: 3.021; l2dist: 1.613\n",
      "    step: 450; loss: 3.028; l2dist: 1.618\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.937; l2dist: 0.000\n",
      "    step: 50; loss: 9.850; l2dist: 2.556\n",
      "    step: 100; loss: 4.689; l2dist: 1.959\n",
      "    step: 150; loss: 3.568; l2dist: 1.744\n",
      "    step: 200; loss: 3.290; l2dist: 1.674\n",
      "    step: 250; loss: 3.125; l2dist: 1.643\n",
      "    step: 300; loss: 3.080; l2dist: 1.638\n",
      "    step: 350; loss: 3.020; l2dist: 1.618\n",
      "    step: 400; loss: 3.014; l2dist: 1.621\n",
      "    step: 450; loss: 3.025; l2dist: 1.621\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.370; l2dist: 0.000\n",
      "    step: 50; loss: 9.728; l2dist: 2.538\n",
      "    step: 100; loss: 4.674; l2dist: 1.959\n",
      "    step: 150; loss: 3.564; l2dist: 1.742\n",
      "    step: 200; loss: 3.295; l2dist: 1.683\n",
      "    step: 250; loss: 3.136; l2dist: 1.649\n",
      "    step: 300; loss: 3.052; l2dist: 1.627\n",
      "    step: 350; loss: 3.035; l2dist: 1.628\n",
      "    step: 400; loss: 3.025; l2dist: 1.630\n",
      "    step: 450; loss: 2.989; l2dist: 1.614\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.731; l2dist: 0.000\n",
      "    step: 50; loss: 9.683; l2dist: 2.531\n",
      "    step: 100; loss: 4.656; l2dist: 1.960\n",
      "    step: 150; loss: 3.565; l2dist: 1.740\n",
      "    step: 200; loss: 3.259; l2dist: 1.678\n",
      "    step: 250; loss: 3.149; l2dist: 1.659\n",
      "    step: 300; loss: 3.089; l2dist: 1.645\n",
      "    step: 350; loss: 3.041; l2dist: 1.625\n",
      "    step: 400; loss: 3.016; l2dist: 1.618\n",
      "    step: 450; loss: 2.988; l2dist: 1.619\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.893; l2dist: 0.000\n",
      "    step: 50; loss: 9.713; l2dist: 2.540\n",
      "    step: 100; loss: 4.658; l2dist: 1.963\n",
      "    step: 150; loss: 3.576; l2dist: 1.751\n",
      "    step: 200; loss: 3.255; l2dist: 1.681\n",
      "    step: 250; loss: 3.136; l2dist: 1.651\n",
      "    step: 300; loss: 3.072; l2dist: 1.630\n",
      "    step: 350; loss: 3.022; l2dist: 1.628\n",
      "    step: 400; loss: 3.036; l2dist: 1.625\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 158.645; l2dist: 0.000\n",
      "    step: 50; loss: 18.207; l2dist: 3.912\n",
      "    step: 100; loss: 7.780; l2dist: 2.730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 5.312; l2dist: 2.229\n",
      "    step: 200; loss: 4.387; l2dist: 2.013\n",
      "    step: 250; loss: 3.887; l2dist: 1.889\n",
      "    step: 300; loss: 3.704; l2dist: 1.842\n",
      "    step: 350; loss: 3.638; l2dist: 1.813\n",
      "    step: 400; loss: 3.576; l2dist: 1.800\n",
      "    step: 450; loss: 3.489; l2dist: 1.785\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 157.259; l2dist: 0.000\n",
      "    step: 50; loss: 16.064; l2dist: 3.670\n",
      "    step: 100; loss: 7.085; l2dist: 2.542\n",
      "    step: 150; loss: 5.025; l2dist: 2.123\n",
      "    step: 200; loss: 4.192; l2dist: 1.935\n",
      "    step: 250; loss: 3.810; l2dist: 1.851\n",
      "    step: 300; loss: 3.585; l2dist: 1.798\n",
      "    step: 350; loss: 3.500; l2dist: 1.782\n",
      "    step: 400; loss: 3.476; l2dist: 1.769\n",
      "    step: 450; loss: 3.359; l2dist: 1.739\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.131; l2dist: 0.000\n",
      "    step: 50; loss: 12.431; l2dist: 3.189\n",
      "    step: 100; loss: 5.529; l2dist: 2.229\n",
      "    step: 150; loss: 4.172; l2dist: 1.924\n",
      "    step: 200; loss: 3.698; l2dist: 1.811\n",
      "    step: 250; loss: 3.460; l2dist: 1.768\n",
      "    step: 300; loss: 3.372; l2dist: 1.744\n",
      "    step: 350; loss: 3.308; l2dist: 1.728\n",
      "    step: 400; loss: 3.280; l2dist: 1.722\n",
      "    step: 450; loss: 3.263; l2dist: 1.727\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.911; l2dist: 0.000\n",
      "    step: 50; loss: 10.552; l2dist: 2.803\n",
      "    step: 100; loss: 4.930; l2dist: 2.090\n",
      "    step: 150; loss: 3.792; l2dist: 1.840\n",
      "    step: 200; loss: 3.464; l2dist: 1.767\n",
      "    step: 250; loss: 3.306; l2dist: 1.729\n",
      "    step: 300; loss: 3.251; l2dist: 1.715\n",
      "    step: 350; loss: 3.214; l2dist: 1.700\n",
      "    step: 400; loss: 3.209; l2dist: 1.698\n",
      "    step: 450; loss: 3.189; l2dist: 1.691\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.842; l2dist: 0.000\n",
      "    step: 50; loss: 9.574; l2dist: 2.653\n",
      "    step: 100; loss: 4.750; l2dist: 2.030\n",
      "    step: 150; loss: 3.658; l2dist: 1.810\n",
      "    step: 200; loss: 3.399; l2dist: 1.736\n",
      "    step: 250; loss: 3.284; l2dist: 1.705\n",
      "    step: 300; loss: 3.229; l2dist: 1.701\n",
      "    step: 350; loss: 3.202; l2dist: 1.696\n",
      "    step: 400; loss: 3.233; l2dist: 1.698\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.605; l2dist: 0.000\n",
      "    step: 50; loss: 9.240; l2dist: 2.557\n",
      "    step: 100; loss: 4.652; l2dist: 1.967\n",
      "    step: 150; loss: 3.643; l2dist: 1.770\n",
      "    step: 200; loss: 3.377; l2dist: 1.720\n",
      "    step: 250; loss: 3.269; l2dist: 1.689\n",
      "    step: 300; loss: 3.252; l2dist: 1.687\n",
      "    step: 350; loss: 3.213; l2dist: 1.682\n",
      "    step: 400; loss: 3.187; l2dist: 1.681\n",
      "    step: 450; loss: 3.168; l2dist: 1.675\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.745; l2dist: 0.000\n",
      "    step: 50; loss: 9.170; l2dist: 2.544\n",
      "    step: 100; loss: 4.617; l2dist: 1.986\n",
      "    step: 150; loss: 3.607; l2dist: 1.785\n",
      "    step: 200; loss: 3.353; l2dist: 1.734\n",
      "    step: 250; loss: 3.272; l2dist: 1.710\n",
      "    step: 300; loss: 3.225; l2dist: 1.702\n",
      "    step: 350; loss: 3.217; l2dist: 1.696\n",
      "    step: 400; loss: 3.196; l2dist: 1.685\n",
      "    step: 450; loss: 3.195; l2dist: 1.686\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.031; l2dist: 0.000\n",
      "    step: 50; loss: 9.059; l2dist: 2.522\n",
      "    step: 100; loss: 4.608; l2dist: 1.976\n",
      "    step: 150; loss: 3.605; l2dist: 1.778\n",
      "    step: 200; loss: 3.369; l2dist: 1.724\n",
      "    step: 250; loss: 3.265; l2dist: 1.707\n",
      "    step: 300; loss: 3.259; l2dist: 1.697\n",
      "    step: 350; loss: 3.203; l2dist: 1.692\n",
      "    step: 400; loss: 3.182; l2dist: 1.694\n",
      "    step: 450; loss: 3.168; l2dist: 1.684\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.441; l2dist: 0.000\n",
      "    step: 50; loss: 9.014; l2dist: 2.513\n",
      "    step: 100; loss: 4.613; l2dist: 1.982\n",
      "    step: 150; loss: 3.608; l2dist: 1.792\n",
      "    step: 200; loss: 3.354; l2dist: 1.725\n",
      "    step: 250; loss: 3.264; l2dist: 1.705\n",
      "    step: 300; loss: 3.246; l2dist: 1.691\n",
      "    step: 350; loss: 3.207; l2dist: 1.693\n",
      "    step: 400; loss: 3.189; l2dist: 1.690\n",
      "    step: 450; loss: 3.182; l2dist: 1.690\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.753; l2dist: 0.000\n",
      "    step: 50; loss: 9.054; l2dist: 2.522\n",
      "    step: 100; loss: 4.621; l2dist: 1.987\n",
      "    step: 150; loss: 3.610; l2dist: 1.776\n",
      "    step: 200; loss: 3.369; l2dist: 1.731\n",
      "    step: 250; loss: 3.267; l2dist: 1.714\n",
      "    step: 300; loss: 3.228; l2dist: 1.697\n",
      "    step: 350; loss: 3.206; l2dist: 1.698\n",
      "    step: 400; loss: 3.193; l2dist: 1.697\n",
      "    step: 450; loss: 3.184; l2dist: 1.690\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 151.942; l2dist: 0.000\n",
      "    step: 50; loss: 15.919; l2dist: 3.843\n",
      "    step: 100; loss: 7.708; l2dist: 2.704\n",
      "    step: 150; loss: 5.184; l2dist: 2.199\n",
      "    step: 200; loss: 4.211; l2dist: 1.969\n",
      "    step: 250; loss: 3.772; l2dist: 1.856\n",
      "    step: 300; loss: 3.586; l2dist: 1.800\n",
      "    step: 350; loss: 3.494; l2dist: 1.779\n",
      "    step: 400; loss: 3.437; l2dist: 1.756\n",
      "    step: 450; loss: 3.430; l2dist: 1.758\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 190.806; l2dist: 0.000\n",
      "    step: 50; loss: 16.613; l2dist: 3.893\n",
      "    step: 100; loss: 8.145; l2dist: 2.743\n",
      "    step: 150; loss: 5.503; l2dist: 2.224\n",
      "    step: 200; loss: 4.425; l2dist: 1.990\n",
      "    step: 250; loss: 3.863; l2dist: 1.850\n",
      "    step: 300; loss: 3.578; l2dist: 1.795\n",
      "    step: 350; loss: 3.449; l2dist: 1.751\n",
      "    step: 400; loss: 3.369; l2dist: 1.739\n",
      "    step: 450; loss: 3.278; l2dist: 1.718\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 105.204; l2dist: 0.000\n",
      "    step: 50; loss: 12.519; l2dist: 3.337\n",
      "    step: 100; loss: 5.698; l2dist: 2.277\n",
      "    step: 150; loss: 4.165; l2dist: 1.930\n",
      "    step: 200; loss: 3.597; l2dist: 1.795\n",
      "    step: 250; loss: 3.392; l2dist: 1.746\n",
      "    step: 300; loss: 3.253; l2dist: 1.707\n",
      "    step: 350; loss: 3.152; l2dist: 1.690\n",
      "    step: 400; loss: 3.130; l2dist: 1.678\n",
      "    step: 450; loss: 3.103; l2dist: 1.677\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.008; l2dist: 0.000\n",
      "    step: 50; loss: 10.164; l2dist: 2.876\n",
      "    step: 100; loss: 4.918; l2dist: 2.073\n",
      "    step: 150; loss: 3.708; l2dist: 1.806\n",
      "    step: 200; loss: 3.336; l2dist: 1.716\n",
      "    step: 250; loss: 3.201; l2dist: 1.688\n",
      "    step: 300; loss: 3.162; l2dist: 1.678\n",
      "    step: 350; loss: 3.109; l2dist: 1.654\n",
      "    step: 400; loss: 3.066; l2dist: 1.646\n",
      "    step: 450; loss: 3.054; l2dist: 1.641\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.730; l2dist: 0.000\n",
      "    step: 50; loss: 8.959; l2dist: 2.629\n",
      "    step: 100; loss: 4.616; l2dist: 1.974\n",
      "    step: 150; loss: 3.567; l2dist: 1.752\n",
      "    step: 200; loss: 3.279; l2dist: 1.690\n",
      "    step: 250; loss: 3.132; l2dist: 1.654\n",
      "    step: 300; loss: 3.070; l2dist: 1.641\n",
      "    step: 350; loss: 3.036; l2dist: 1.635\n",
      "    step: 400; loss: 3.013; l2dist: 1.633\n",
      "    step: 450; loss: 3.016; l2dist: 1.633\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.392; l2dist: 0.000\n",
      "    step: 50; loss: 8.457; l2dist: 2.518\n",
      "    step: 100; loss: 4.445; l2dist: 1.912\n",
      "    step: 150; loss: 3.425; l2dist: 1.705\n",
      "    step: 200; loss: 3.192; l2dist: 1.643\n",
      "    step: 250; loss: 3.071; l2dist: 1.622\n",
      "    step: 300; loss: 3.010; l2dist: 1.609\n",
      "    step: 350; loss: 2.970; l2dist: 1.603\n",
      "    step: 400; loss: 2.973; l2dist: 1.602\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.345; l2dist: 0.000\n",
      "    step: 50; loss: 8.374; l2dist: 2.507\n",
      "    step: 100; loss: 4.421; l2dist: 1.907\n",
      "    step: 150; loss: 3.414; l2dist: 1.708\n",
      "    step: 200; loss: 3.164; l2dist: 1.656\n",
      "    step: 250; loss: 3.072; l2dist: 1.635\n",
      "    step: 300; loss: 3.030; l2dist: 1.626\n",
      "    step: 350; loss: 2.981; l2dist: 1.622\n",
      "    step: 400; loss: 2.958; l2dist: 1.613\n",
      "    step: 450; loss: 2.956; l2dist: 1.610\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.403; l2dist: 0.000\n",
      "    step: 50; loss: 8.332; l2dist: 2.508\n",
      "    step: 100; loss: 4.410; l2dist: 1.908\n",
      "    step: 150; loss: 3.416; l2dist: 1.705\n",
      "    step: 200; loss: 3.168; l2dist: 1.653\n",
      "    step: 250; loss: 3.081; l2dist: 1.633\n",
      "    step: 300; loss: 3.026; l2dist: 1.623\n",
      "    step: 350; loss: 2.988; l2dist: 1.618\n",
      "    step: 400; loss: 2.980; l2dist: 1.608\n",
      "    step: 450; loss: 2.966; l2dist: 1.612\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.429; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 8.300; l2dist: 2.501\n",
      "    step: 100; loss: 4.396; l2dist: 1.905\n",
      "    step: 150; loss: 3.393; l2dist: 1.711\n",
      "    step: 200; loss: 3.166; l2dist: 1.660\n",
      "    step: 250; loss: 3.075; l2dist: 1.633\n",
      "    step: 300; loss: 3.015; l2dist: 1.622\n",
      "    step: 350; loss: 2.989; l2dist: 1.620\n",
      "    step: 400; loss: 2.961; l2dist: 1.613\n",
      "    step: 450; loss: 2.954; l2dist: 1.615\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.762; l2dist: 0.000\n",
      "    step: 50; loss: 8.338; l2dist: 2.512\n",
      "    step: 100; loss: 4.410; l2dist: 1.917\n",
      "    step: 150; loss: 3.403; l2dist: 1.712\n",
      "    step: 200; loss: 3.168; l2dist: 1.658\n",
      "    step: 250; loss: 3.076; l2dist: 1.637\n",
      "    step: 300; loss: 3.011; l2dist: 1.626\n",
      "    step: 350; loss: 2.988; l2dist: 1.625\n",
      "    step: 400; loss: 2.975; l2dist: 1.615\n",
      "    step: 450; loss: 2.955; l2dist: 1.618\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 141.499; l2dist: 0.000\n",
      "    step: 50; loss: 17.050; l2dist: 3.785\n",
      "    step: 100; loss: 7.109; l2dist: 2.602\n",
      "    step: 150; loss: 4.721; l2dist: 2.087\n",
      "    step: 200; loss: 3.632; l2dist: 1.830\n",
      "    step: 250; loss: 3.190; l2dist: 1.689\n",
      "    step: 300; loss: 2.953; l2dist: 1.639\n",
      "    step: 350; loss: 2.755; l2dist: 1.575\n",
      "    step: 400; loss: 2.662; l2dist: 1.549\n",
      "    step: 450; loss: 2.626; l2dist: 1.547\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 140.995; l2dist: 0.000\n",
      "    step: 50; loss: 15.709; l2dist: 3.589\n",
      "    step: 100; loss: 6.702; l2dist: 2.456\n",
      "    step: 150; loss: 4.507; l2dist: 1.988\n",
      "    step: 200; loss: 3.595; l2dist: 1.773\n",
      "    step: 250; loss: 3.144; l2dist: 1.663\n",
      "    step: 300; loss: 2.890; l2dist: 1.600\n",
      "    step: 350; loss: 2.730; l2dist: 1.552\n",
      "    step: 400; loss: 2.652; l2dist: 1.533\n",
      "    step: 450; loss: 2.611; l2dist: 1.526\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 82.179; l2dist: 0.000\n",
      "    step: 50; loss: 12.394; l2dist: 3.151\n",
      "    step: 100; loss: 5.034; l2dist: 2.112\n",
      "    step: 150; loss: 3.518; l2dist: 1.759\n",
      "    step: 200; loss: 2.966; l2dist: 1.615\n",
      "    step: 250; loss: 2.708; l2dist: 1.553\n",
      "    step: 300; loss: 2.612; l2dist: 1.532\n",
      "    step: 350; loss: 2.497; l2dist: 1.498\n",
      "    step: 400; loss: 2.499; l2dist: 1.492\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.545; l2dist: 0.000\n",
      "    step: 50; loss: 10.373; l2dist: 2.777\n",
      "    step: 100; loss: 4.241; l2dist: 1.930\n",
      "    step: 150; loss: 3.043; l2dist: 1.648\n",
      "    step: 200; loss: 2.696; l2dist: 1.552\n",
      "    step: 250; loss: 2.542; l2dist: 1.504\n",
      "    step: 300; loss: 2.488; l2dist: 1.488\n",
      "    step: 350; loss: 2.443; l2dist: 1.477\n",
      "    step: 400; loss: 2.428; l2dist: 1.464\n",
      "    step: 450; loss: 2.392; l2dist: 1.464\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.704; l2dist: 0.000\n",
      "    step: 50; loss: 9.195; l2dist: 2.503\n",
      "    step: 100; loss: 3.906; l2dist: 1.807\n",
      "    step: 150; loss: 2.890; l2dist: 1.586\n",
      "    step: 200; loss: 2.596; l2dist: 1.513\n",
      "    step: 250; loss: 2.488; l2dist: 1.484\n",
      "    step: 300; loss: 2.441; l2dist: 1.463\n",
      "    step: 350; loss: 2.438; l2dist: 1.465\n",
      "    step: 400; loss: 2.390; l2dist: 1.452\n",
      "    step: 450; loss: 2.376; l2dist: 1.446\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.988; l2dist: 0.000\n",
      "    step: 50; loss: 8.751; l2dist: 2.402\n",
      "    step: 100; loss: 3.757; l2dist: 1.734\n",
      "    step: 150; loss: 2.805; l2dist: 1.526\n",
      "    step: 200; loss: 2.544; l2dist: 1.460\n",
      "    step: 250; loss: 2.455; l2dist: 1.446\n",
      "    step: 300; loss: 2.404; l2dist: 1.432\n",
      "    step: 350; loss: 2.377; l2dist: 1.424\n",
      "    step: 400; loss: 2.371; l2dist: 1.426\n",
      "    step: 450; loss: 2.349; l2dist: 1.418\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.900; l2dist: 0.000\n",
      "    step: 50; loss: 8.696; l2dist: 2.402\n",
      "    step: 100; loss: 3.784; l2dist: 1.764\n",
      "    step: 150; loss: 2.800; l2dist: 1.550\n",
      "    step: 200; loss: 2.569; l2dist: 1.488\n",
      "    step: 250; loss: 2.481; l2dist: 1.464\n",
      "    step: 300; loss: 2.414; l2dist: 1.451\n",
      "    step: 350; loss: 2.401; l2dist: 1.442\n",
      "    step: 400; loss: 2.378; l2dist: 1.440\n",
      "    step: 450; loss: 2.368; l2dist: 1.440\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.185; l2dist: 0.000\n",
      "    step: 50; loss: 8.682; l2dist: 2.410\n",
      "    step: 100; loss: 3.773; l2dist: 1.763\n",
      "    step: 150; loss: 2.793; l2dist: 1.545\n",
      "    step: 200; loss: 2.561; l2dist: 1.485\n",
      "    step: 250; loss: 2.465; l2dist: 1.464\n",
      "    step: 300; loss: 2.425; l2dist: 1.450\n",
      "    step: 350; loss: 2.411; l2dist: 1.443\n",
      "    step: 400; loss: 2.380; l2dist: 1.440\n",
      "    step: 450; loss: 2.373; l2dist: 1.437\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.083; l2dist: 0.000\n",
      "    step: 50; loss: 8.661; l2dist: 2.404\n",
      "    step: 100; loss: 3.765; l2dist: 1.762\n",
      "    step: 150; loss: 2.799; l2dist: 1.548\n",
      "    step: 200; loss: 2.548; l2dist: 1.485\n",
      "    step: 250; loss: 2.454; l2dist: 1.466\n",
      "    step: 300; loss: 2.414; l2dist: 1.448\n",
      "    step: 350; loss: 2.396; l2dist: 1.447\n",
      "    step: 400; loss: 2.377; l2dist: 1.442\n",
      "    step: 450; loss: 2.395; l2dist: 1.443\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.348; l2dist: 0.000\n",
      "    step: 50; loss: 8.712; l2dist: 2.417\n",
      "    step: 100; loss: 3.770; l2dist: 1.770\n",
      "    step: 150; loss: 2.802; l2dist: 1.556\n",
      "    step: 200; loss: 2.553; l2dist: 1.492\n",
      "    step: 250; loss: 2.482; l2dist: 1.478\n",
      "    step: 300; loss: 2.420; l2dist: 1.458\n",
      "    step: 350; loss: 2.408; l2dist: 1.451\n",
      "    step: 400; loss: 2.400; l2dist: 1.440\n",
      "    step: 450; loss: 2.388; l2dist: 1.449\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 133.354; l2dist: 0.000\n",
      "    step: 50; loss: 15.070; l2dist: 3.592\n",
      "    step: 100; loss: 6.263; l2dist: 2.424\n",
      "    step: 150; loss: 4.134; l2dist: 1.952\n",
      "    step: 200; loss: 3.319; l2dist: 1.744\n",
      "    step: 250; loss: 2.974; l2dist: 1.636\n",
      "    step: 300; loss: 2.852; l2dist: 1.584\n",
      "    step: 350; loss: 2.739; l2dist: 1.576\n",
      "    step: 400; loss: 2.622; l2dist: 1.536\n",
      "    step: 450; loss: 2.639; l2dist: 1.545\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 111.433; l2dist: 0.000\n",
      "    step: 50; loss: 12.676; l2dist: 3.268\n",
      "    step: 100; loss: 5.174; l2dist: 2.169\n",
      "    step: 150; loss: 3.596; l2dist: 1.798\n",
      "    step: 200; loss: 3.078; l2dist: 1.646\n",
      "    step: 250; loss: 2.801; l2dist: 1.585\n",
      "    step: 300; loss: 2.684; l2dist: 1.553\n",
      "    step: 350; loss: 2.688; l2dist: 1.549\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.991; l2dist: 0.000\n",
      "    step: 50; loss: 10.083; l2dist: 2.846\n",
      "    step: 100; loss: 4.128; l2dist: 1.932\n",
      "    step: 150; loss: 3.063; l2dist: 1.657\n",
      "    step: 200; loss: 2.730; l2dist: 1.562\n",
      "    step: 250; loss: 2.587; l2dist: 1.522\n",
      "    step: 300; loss: 2.510; l2dist: 1.501\n",
      "    step: 350; loss: 2.465; l2dist: 1.487\n",
      "    step: 400; loss: 2.448; l2dist: 1.489\n",
      "    step: 450; loss: 2.407; l2dist: 1.481\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.069; l2dist: 0.000\n",
      "    step: 50; loss: 8.973; l2dist: 2.591\n",
      "    step: 100; loss: 3.869; l2dist: 1.833\n",
      "    step: 150; loss: 2.903; l2dist: 1.602\n",
      "    step: 200; loss: 2.631; l2dist: 1.529\n",
      "    step: 250; loss: 2.504; l2dist: 1.491\n",
      "    step: 300; loss: 2.449; l2dist: 1.481\n",
      "    step: 350; loss: 2.409; l2dist: 1.465\n",
      "    step: 400; loss: 2.387; l2dist: 1.463\n",
      "    step: 450; loss: 2.398; l2dist: 1.456\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.541; l2dist: 0.000\n",
      "    step: 50; loss: 8.442; l2dist: 2.439\n",
      "    step: 100; loss: 3.729; l2dist: 1.760\n",
      "    step: 150; loss: 2.824; l2dist: 1.561\n",
      "    step: 200; loss: 2.565; l2dist: 1.500\n",
      "    step: 250; loss: 2.460; l2dist: 1.470\n",
      "    step: 300; loss: 2.428; l2dist: 1.462\n",
      "    step: 350; loss: 2.388; l2dist: 1.453\n",
      "    step: 400; loss: 2.376; l2dist: 1.445\n",
      "    step: 450; loss: 2.353; l2dist: 1.446\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.291; l2dist: 0.000\n",
      "    step: 50; loss: 8.101; l2dist: 2.366\n",
      "    step: 100; loss: 3.621; l2dist: 1.720\n",
      "    step: 150; loss: 2.759; l2dist: 1.526\n",
      "    step: 200; loss: 2.506; l2dist: 1.469\n",
      "    step: 250; loss: 2.416; l2dist: 1.449\n",
      "    step: 300; loss: 2.385; l2dist: 1.437\n",
      "    step: 350; loss: 2.353; l2dist: 1.430\n",
      "    step: 400; loss: 2.340; l2dist: 1.425\n",
      "    step: 450; loss: 2.351; l2dist: 1.423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.832; l2dist: 0.000\n",
      "    step: 50; loss: 8.052; l2dist: 2.358\n",
      "    step: 100; loss: 3.607; l2dist: 1.724\n",
      "    step: 150; loss: 2.729; l2dist: 1.522\n",
      "    step: 200; loss: 2.505; l2dist: 1.477\n",
      "    step: 250; loss: 2.408; l2dist: 1.450\n",
      "    step: 300; loss: 2.380; l2dist: 1.441\n",
      "    step: 350; loss: 2.354; l2dist: 1.433\n",
      "    step: 400; loss: 2.358; l2dist: 1.438\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.835; l2dist: 0.000\n",
      "    step: 50; loss: 7.995; l2dist: 2.345\n",
      "    step: 100; loss: 3.608; l2dist: 1.725\n",
      "    step: 150; loss: 2.722; l2dist: 1.522\n",
      "    step: 200; loss: 2.496; l2dist: 1.472\n",
      "    step: 250; loss: 2.416; l2dist: 1.456\n",
      "    step: 300; loss: 2.375; l2dist: 1.445\n",
      "    step: 350; loss: 2.357; l2dist: 1.440\n",
      "    step: 400; loss: 2.341; l2dist: 1.435\n",
      "    step: 450; loss: 2.363; l2dist: 1.434\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.400; l2dist: 0.000\n",
      "    step: 50; loss: 7.984; l2dist: 2.341\n",
      "    step: 100; loss: 3.598; l2dist: 1.720\n",
      "    step: 150; loss: 2.729; l2dist: 1.524\n",
      "    step: 200; loss: 2.501; l2dist: 1.469\n",
      "    step: 250; loss: 2.416; l2dist: 1.453\n",
      "    step: 300; loss: 2.385; l2dist: 1.442\n",
      "    step: 350; loss: 2.362; l2dist: 1.442\n",
      "    step: 400; loss: 2.348; l2dist: 1.433\n",
      "    step: 450; loss: 2.363; l2dist: 1.439\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.605; l2dist: 0.000\n",
      "    step: 50; loss: 8.000; l2dist: 2.350\n",
      "    step: 100; loss: 3.610; l2dist: 1.735\n",
      "    step: 150; loss: 2.725; l2dist: 1.522\n",
      "    step: 200; loss: 2.490; l2dist: 1.474\n",
      "    step: 250; loss: 2.416; l2dist: 1.456\n",
      "    step: 300; loss: 2.376; l2dist: 1.451\n",
      "    step: 350; loss: 2.358; l2dist: 1.433\n",
      "    step: 400; loss: 2.357; l2dist: 1.444\n",
      "    step: 450; loss: 2.338; l2dist: 1.431\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 139.335; l2dist: 0.000\n",
      "    step: 50; loss: 16.597; l2dist: 3.579\n",
      "    step: 100; loss: 6.699; l2dist: 2.500\n",
      "    step: 150; loss: 4.584; l2dist: 2.052\n",
      "    step: 200; loss: 3.725; l2dist: 1.835\n",
      "    step: 250; loss: 3.285; l2dist: 1.715\n",
      "    step: 300; loss: 3.083; l2dist: 1.660\n",
      "    step: 350; loss: 3.003; l2dist: 1.626\n",
      "    step: 400; loss: 2.944; l2dist: 1.612\n",
      "    step: 450; loss: 2.913; l2dist: 1.611\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 156.366; l2dist: 0.000\n",
      "    step: 50; loss: 15.629; l2dist: 3.508\n",
      "    step: 100; loss: 6.637; l2dist: 2.435\n",
      "    step: 150; loss: 4.520; l2dist: 1.991\n",
      "    step: 200; loss: 3.707; l2dist: 1.804\n",
      "    step: 250; loss: 3.297; l2dist: 1.694\n",
      "    step: 300; loss: 3.128; l2dist: 1.655\n",
      "    step: 350; loss: 3.020; l2dist: 1.628\n",
      "    step: 400; loss: 2.895; l2dist: 1.592\n",
      "    step: 450; loss: 2.840; l2dist: 1.581\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.427; l2dist: 0.000\n",
      "    step: 50; loss: 11.656; l2dist: 2.986\n",
      "    step: 100; loss: 4.768; l2dist: 2.048\n",
      "    step: 150; loss: 3.511; l2dist: 1.747\n",
      "    step: 200; loss: 3.096; l2dist: 1.646\n",
      "    step: 250; loss: 2.902; l2dist: 1.600\n",
      "    step: 300; loss: 2.810; l2dist: 1.576\n",
      "    step: 350; loss: 2.762; l2dist: 1.557\n",
      "    step: 400; loss: 2.757; l2dist: 1.558\n",
      "    step: 450; loss: 2.708; l2dist: 1.541\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.882; l2dist: 0.000\n",
      "    step: 50; loss: 9.699; l2dist: 2.631\n",
      "    step: 100; loss: 4.270; l2dist: 1.920\n",
      "    step: 150; loss: 3.227; l2dist: 1.675\n",
      "    step: 200; loss: 2.949; l2dist: 1.598\n",
      "    step: 250; loss: 2.838; l2dist: 1.569\n",
      "    step: 300; loss: 2.782; l2dist: 1.556\n",
      "    step: 350; loss: 2.760; l2dist: 1.557\n",
      "    step: 400; loss: 2.731; l2dist: 1.546\n",
      "    step: 450; loss: 2.731; l2dist: 1.542\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.705; l2dist: 0.000\n",
      "    step: 50; loss: 8.569; l2dist: 2.395\n",
      "    step: 100; loss: 3.980; l2dist: 1.816\n",
      "    step: 150; loss: 3.107; l2dist: 1.618\n",
      "    step: 200; loss: 2.873; l2dist: 1.566\n",
      "    step: 250; loss: 2.792; l2dist: 1.547\n",
      "    step: 300; loss: 2.770; l2dist: 1.545\n",
      "    step: 350; loss: 2.721; l2dist: 1.527\n",
      "    step: 400; loss: 2.728; l2dist: 1.527\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.712; l2dist: 0.000\n",
      "    step: 50; loss: 8.267; l2dist: 2.353\n",
      "    step: 100; loss: 3.892; l2dist: 1.792\n",
      "    step: 150; loss: 3.038; l2dist: 1.611\n",
      "    step: 200; loss: 2.822; l2dist: 1.556\n",
      "    step: 250; loss: 2.743; l2dist: 1.538\n",
      "    step: 300; loss: 2.712; l2dist: 1.525\n",
      "    step: 350; loss: 2.693; l2dist: 1.517\n",
      "    step: 400; loss: 2.662; l2dist: 1.518\n",
      "    step: 450; loss: 2.664; l2dist: 1.515\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.603; l2dist: 0.000\n",
      "    step: 50; loss: 8.082; l2dist: 2.313\n",
      "    step: 100; loss: 3.867; l2dist: 1.772\n",
      "    step: 150; loss: 3.021; l2dist: 1.592\n",
      "    step: 200; loss: 2.802; l2dist: 1.543\n",
      "    step: 250; loss: 2.728; l2dist: 1.523\n",
      "    step: 300; loss: 2.698; l2dist: 1.518\n",
      "    step: 350; loss: 2.659; l2dist: 1.508\n",
      "    step: 400; loss: 2.649; l2dist: 1.508\n",
      "    step: 450; loss: 2.631; l2dist: 1.505\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.971; l2dist: 0.000\n",
      "    step: 50; loss: 7.987; l2dist: 2.295\n",
      "    step: 100; loss: 3.839; l2dist: 1.755\n",
      "    step: 150; loss: 2.990; l2dist: 1.588\n",
      "    step: 200; loss: 2.794; l2dist: 1.542\n",
      "    step: 250; loss: 2.714; l2dist: 1.521\n",
      "    step: 300; loss: 2.672; l2dist: 1.514\n",
      "    step: 350; loss: 2.654; l2dist: 1.513\n",
      "    step: 400; loss: 2.637; l2dist: 1.507\n",
      "    step: 450; loss: 2.635; l2dist: 1.509\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.255; l2dist: 0.000\n",
      "    step: 50; loss: 7.961; l2dist: 2.288\n",
      "    step: 100; loss: 3.826; l2dist: 1.767\n",
      "    step: 150; loss: 2.980; l2dist: 1.585\n",
      "    step: 200; loss: 2.786; l2dist: 1.543\n",
      "    step: 250; loss: 2.699; l2dist: 1.522\n",
      "    step: 300; loss: 2.670; l2dist: 1.514\n",
      "    step: 350; loss: 2.648; l2dist: 1.511\n",
      "    step: 400; loss: 2.636; l2dist: 1.507\n",
      "    step: 450; loss: 2.628; l2dist: 1.504\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.423; l2dist: 0.000\n",
      "    step: 50; loss: 7.995; l2dist: 2.298\n",
      "    step: 100; loss: 3.832; l2dist: 1.768\n",
      "    step: 150; loss: 2.997; l2dist: 1.588\n",
      "    step: 200; loss: 2.783; l2dist: 1.541\n",
      "    step: 250; loss: 2.700; l2dist: 1.521\n",
      "    step: 300; loss: 2.655; l2dist: 1.517\n",
      "    step: 350; loss: 2.648; l2dist: 1.508\n",
      "    step: 400; loss: 2.653; l2dist: 1.509\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 124.922; l2dist: 0.000\n",
      "    step: 50; loss: 12.643; l2dist: 3.421\n",
      "    step: 100; loss: 5.836; l2dist: 2.320\n",
      "    step: 150; loss: 3.820; l2dist: 1.869\n",
      "    step: 200; loss: 3.085; l2dist: 1.668\n",
      "    step: 250; loss: 2.794; l2dist: 1.575\n",
      "    step: 300; loss: 2.665; l2dist: 1.533\n",
      "    step: 350; loss: 2.624; l2dist: 1.504\n",
      "    step: 400; loss: 2.570; l2dist: 1.511\n",
      "    step: 450; loss: 2.558; l2dist: 1.486\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 129.262; l2dist: 0.000\n",
      "    step: 50; loss: 12.347; l2dist: 3.257\n",
      "    step: 100; loss: 5.874; l2dist: 2.266\n",
      "    step: 150; loss: 3.892; l2dist: 1.825\n",
      "    step: 200; loss: 3.164; l2dist: 1.654\n",
      "    step: 250; loss: 2.865; l2dist: 1.565\n",
      "    step: 300; loss: 2.734; l2dist: 1.532\n",
      "    step: 350; loss: 2.624; l2dist: 1.507\n",
      "    step: 400; loss: 2.568; l2dist: 1.485\n",
      "    step: 450; loss: 2.459; l2dist: 1.464\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.776; l2dist: 0.000\n",
      "    step: 50; loss: 9.538; l2dist: 2.805\n",
      "    step: 100; loss: 4.312; l2dist: 1.941\n",
      "    step: 150; loss: 3.082; l2dist: 1.640\n",
      "    step: 200; loss: 2.713; l2dist: 1.534\n",
      "    step: 250; loss: 2.550; l2dist: 1.481\n",
      "    step: 300; loss: 2.455; l2dist: 1.461\n",
      "    step: 350; loss: 2.391; l2dist: 1.443\n",
      "    step: 400; loss: 2.457; l2dist: 1.458\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.478; l2dist: 0.000\n",
      "    step: 50; loss: 7.997; l2dist: 2.483\n",
      "    step: 100; loss: 3.750; l2dist: 1.805\n",
      "    step: 150; loss: 2.811; l2dist: 1.552\n",
      "    step: 200; loss: 2.519; l2dist: 1.482\n",
      "    step: 250; loss: 2.412; l2dist: 1.453\n",
      "    step: 300; loss: 2.349; l2dist: 1.431\n",
      "    step: 350; loss: 2.331; l2dist: 1.426\n",
      "    step: 400; loss: 2.309; l2dist: 1.410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 450; loss: 2.300; l2dist: 1.408\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.256; l2dist: 0.000\n",
      "    step: 50; loss: 7.121; l2dist: 2.238\n",
      "    step: 100; loss: 3.639; l2dist: 1.695\n",
      "    step: 150; loss: 2.747; l2dist: 1.514\n",
      "    step: 200; loss: 2.493; l2dist: 1.453\n",
      "    step: 250; loss: 2.400; l2dist: 1.418\n",
      "    step: 300; loss: 2.329; l2dist: 1.407\n",
      "    step: 350; loss: 2.311; l2dist: 1.400\n",
      "    step: 400; loss: 2.306; l2dist: 1.399\n",
      "    step: 450; loss: 2.290; l2dist: 1.396\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.434; l2dist: 0.000\n",
      "    step: 50; loss: 6.542; l2dist: 2.071\n",
      "    step: 100; loss: 3.407; l2dist: 1.559\n",
      "    step: 150; loss: 2.640; l2dist: 1.421\n",
      "    step: 200; loss: 2.401; l2dist: 1.374\n",
      "    step: 250; loss: 2.296; l2dist: 1.355\n",
      "    step: 300; loss: 2.255; l2dist: 1.338\n",
      "    step: 350; loss: 2.232; l2dist: 1.339\n",
      "    step: 400; loss: 2.220; l2dist: 1.330\n",
      "    step: 450; loss: 2.205; l2dist: 1.333\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.267; l2dist: 0.000\n",
      "    step: 50; loss: 6.530; l2dist: 2.061\n",
      "    step: 100; loss: 3.410; l2dist: 1.582\n",
      "    step: 150; loss: 2.625; l2dist: 1.435\n",
      "    step: 200; loss: 2.383; l2dist: 1.380\n",
      "    step: 250; loss: 2.291; l2dist: 1.362\n",
      "    step: 300; loss: 2.251; l2dist: 1.351\n",
      "    step: 350; loss: 2.236; l2dist: 1.347\n",
      "    step: 400; loss: 2.219; l2dist: 1.341\n",
      "    step: 450; loss: 2.212; l2dist: 1.345\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.202; l2dist: 0.000\n",
      "    step: 50; loss: 6.498; l2dist: 2.052\n",
      "    step: 100; loss: 3.400; l2dist: 1.578\n",
      "    step: 150; loss: 2.606; l2dist: 1.435\n",
      "    step: 200; loss: 2.377; l2dist: 1.390\n",
      "    step: 250; loss: 2.285; l2dist: 1.371\n",
      "    step: 300; loss: 2.255; l2dist: 1.358\n",
      "    step: 350; loss: 2.221; l2dist: 1.350\n",
      "    step: 400; loss: 2.211; l2dist: 1.347\n",
      "    step: 450; loss: 2.199; l2dist: 1.350\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.820; l2dist: 0.000\n",
      "    step: 50; loss: 6.490; l2dist: 2.048\n",
      "    step: 100; loss: 3.408; l2dist: 1.578\n",
      "    step: 150; loss: 2.604; l2dist: 1.431\n",
      "    step: 200; loss: 2.380; l2dist: 1.384\n",
      "    step: 250; loss: 2.292; l2dist: 1.363\n",
      "    step: 300; loss: 2.258; l2dist: 1.357\n",
      "    step: 350; loss: 2.248; l2dist: 1.355\n",
      "    step: 400; loss: 2.221; l2dist: 1.351\n",
      "    step: 450; loss: 2.215; l2dist: 1.346\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.052; l2dist: 0.000\n",
      "    step: 50; loss: 6.532; l2dist: 2.060\n",
      "    step: 100; loss: 3.402; l2dist: 1.592\n",
      "    step: 150; loss: 2.603; l2dist: 1.437\n",
      "    step: 200; loss: 2.375; l2dist: 1.389\n",
      "    step: 250; loss: 2.279; l2dist: 1.370\n",
      "    step: 300; loss: 2.251; l2dist: 1.358\n",
      "    step: 350; loss: 2.225; l2dist: 1.354\n",
      "    step: 400; loss: 2.216; l2dist: 1.353\n",
      "    step: 450; loss: 2.217; l2dist: 1.354\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 118.182; l2dist: 0.000\n",
      "    step: 50; loss: 12.210; l2dist: 3.224\n",
      "    step: 100; loss: 5.315; l2dist: 2.222\n",
      "    step: 150; loss: 3.584; l2dist: 1.803\n",
      "    step: 200; loss: 2.945; l2dist: 1.622\n",
      "    step: 250; loss: 2.656; l2dist: 1.531\n",
      "    step: 300; loss: 2.531; l2dist: 1.488\n",
      "    step: 350; loss: 2.436; l2dist: 1.467\n",
      "    step: 400; loss: 2.391; l2dist: 1.455\n",
      "    step: 450; loss: 2.329; l2dist: 1.438\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 99.285; l2dist: 0.000\n",
      "    step: 50; loss: 10.593; l2dist: 2.984\n",
      "    step: 100; loss: 4.693; l2dist: 2.035\n",
      "    step: 150; loss: 3.254; l2dist: 1.680\n",
      "    step: 200; loss: 2.709; l2dist: 1.532\n",
      "    step: 250; loss: 2.477; l2dist: 1.472\n",
      "    step: 300; loss: 2.379; l2dist: 1.443\n",
      "    step: 350; loss: 2.317; l2dist: 1.429\n",
      "    step: 400; loss: 2.286; l2dist: 1.410\n",
      "    step: 450; loss: 2.252; l2dist: 1.404\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.955; l2dist: 0.000\n",
      "    step: 50; loss: 8.207; l2dist: 2.535\n",
      "    step: 100; loss: 3.669; l2dist: 1.798\n",
      "    step: 150; loss: 2.689; l2dist: 1.536\n",
      "    step: 200; loss: 2.386; l2dist: 1.447\n",
      "    step: 250; loss: 2.285; l2dist: 1.417\n",
      "    step: 300; loss: 2.247; l2dist: 1.405\n",
      "    step: 350; loss: 2.204; l2dist: 1.389\n",
      "    step: 400; loss: 2.188; l2dist: 1.386\n",
      "    step: 450; loss: 2.199; l2dist: 1.390\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.438; l2dist: 0.000\n",
      "    step: 50; loss: 6.871; l2dist: 2.245\n",
      "    step: 100; loss: 3.283; l2dist: 1.694\n",
      "    step: 150; loss: 2.474; l2dist: 1.468\n",
      "    step: 200; loss: 2.276; l2dist: 1.407\n",
      "    step: 250; loss: 2.206; l2dist: 1.390\n",
      "    step: 300; loss: 2.150; l2dist: 1.369\n",
      "    step: 350; loss: 2.125; l2dist: 1.362\n",
      "    step: 400; loss: 2.118; l2dist: 1.362\n",
      "    step: 450; loss: 2.108; l2dist: 1.364\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.985; l2dist: 0.000\n",
      "    step: 50; loss: 6.200; l2dist: 2.036\n",
      "    step: 100; loss: 3.179; l2dist: 1.600\n",
      "    step: 150; loss: 2.424; l2dist: 1.425\n",
      "    step: 200; loss: 2.215; l2dist: 1.383\n",
      "    step: 250; loss: 2.141; l2dist: 1.355\n",
      "    step: 300; loss: 2.121; l2dist: 1.345\n",
      "    step: 350; loss: 2.089; l2dist: 1.347\n",
      "    step: 400; loss: 2.077; l2dist: 1.339\n",
      "    step: 450; loss: 2.066; l2dist: 1.341\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.545; l2dist: 0.000\n",
      "    step: 50; loss: 5.959; l2dist: 1.975\n",
      "    step: 100; loss: 3.103; l2dist: 1.557\n",
      "    step: 150; loss: 2.355; l2dist: 1.393\n",
      "    step: 200; loss: 2.179; l2dist: 1.349\n",
      "    step: 250; loss: 2.097; l2dist: 1.329\n",
      "    step: 300; loss: 2.070; l2dist: 1.325\n",
      "    step: 350; loss: 2.043; l2dist: 1.314\n",
      "    step: 400; loss: 2.035; l2dist: 1.314\n",
      "    step: 450; loss: 2.037; l2dist: 1.311\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.898; l2dist: 0.000\n",
      "    step: 50; loss: 5.808; l2dist: 1.944\n",
      "    step: 100; loss: 3.095; l2dist: 1.535\n",
      "    step: 150; loss: 2.360; l2dist: 1.377\n",
      "    step: 200; loss: 2.176; l2dist: 1.340\n",
      "    step: 250; loss: 2.104; l2dist: 1.319\n",
      "    step: 300; loss: 2.066; l2dist: 1.312\n",
      "    step: 350; loss: 2.072; l2dist: 1.309\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.289; l2dist: 0.000\n",
      "    step: 50; loss: 5.813; l2dist: 1.942\n",
      "    step: 100; loss: 3.106; l2dist: 1.541\n",
      "    step: 150; loss: 2.355; l2dist: 1.384\n",
      "    step: 200; loss: 2.179; l2dist: 1.337\n",
      "    step: 250; loss: 2.126; l2dist: 1.323\n",
      "    step: 300; loss: 2.082; l2dist: 1.316\n",
      "    step: 350; loss: 2.058; l2dist: 1.307\n",
      "    step: 400; loss: 2.050; l2dist: 1.308\n",
      "    step: 450; loss: 2.040; l2dist: 1.305\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.905; l2dist: 0.000\n",
      "    step: 50; loss: 5.799; l2dist: 1.940\n",
      "    step: 100; loss: 3.104; l2dist: 1.540\n",
      "    step: 150; loss: 2.351; l2dist: 1.382\n",
      "    step: 200; loss: 2.180; l2dist: 1.339\n",
      "    step: 250; loss: 2.115; l2dist: 1.322\n",
      "    step: 300; loss: 2.074; l2dist: 1.317\n",
      "    step: 350; loss: 2.064; l2dist: 1.315\n",
      "    step: 400; loss: 2.059; l2dist: 1.307\n",
      "    step: 450; loss: 2.039; l2dist: 1.305\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.175; l2dist: 0.000\n",
      "    step: 50; loss: 5.837; l2dist: 1.955\n",
      "    step: 100; loss: 3.115; l2dist: 1.550\n",
      "    step: 150; loss: 2.359; l2dist: 1.388\n",
      "    step: 200; loss: 2.186; l2dist: 1.352\n",
      "    step: 250; loss: 2.112; l2dist: 1.327\n",
      "    step: 300; loss: 2.085; l2dist: 1.320\n",
      "    step: 350; loss: 2.063; l2dist: 1.316\n",
      "    step: 400; loss: 2.056; l2dist: 1.310\n",
      "    step: 450; loss: 2.046; l2dist: 1.306\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 140.634; l2dist: 0.000\n",
      "    step: 50; loss: 14.261; l2dist: 3.533\n",
      "    step: 100; loss: 6.359; l2dist: 2.450\n",
      "    step: 150; loss: 4.212; l2dist: 1.967\n",
      "    step: 200; loss: 3.330; l2dist: 1.739\n",
      "    step: 250; loss: 2.907; l2dist: 1.611\n",
      "    step: 300; loss: 2.725; l2dist: 1.557\n",
      "    step: 350; loss: 2.571; l2dist: 1.512\n",
      "    step: 400; loss: 2.557; l2dist: 1.495\n",
      "    step: 450; loss: 2.502; l2dist: 1.480\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 107.385; l2dist: 0.000\n",
      "    step: 50; loss: 11.833; l2dist: 3.160\n",
      "    step: 100; loss: 5.085; l2dist: 2.153\n",
      "    step: 150; loss: 3.498; l2dist: 1.758\n",
      "    step: 200; loss: 2.904; l2dist: 1.593\n",
      "    step: 250; loss: 2.687; l2dist: 1.528\n",
      "    step: 300; loss: 2.543; l2dist: 1.482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 2.471; l2dist: 1.468\n",
      "    step: 400; loss: 2.425; l2dist: 1.454\n",
      "    step: 450; loss: 2.398; l2dist: 1.442\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.829; l2dist: 0.000\n",
      "    step: 50; loss: 9.308; l2dist: 2.750\n",
      "    step: 100; loss: 3.974; l2dist: 1.884\n",
      "    step: 150; loss: 2.908; l2dist: 1.601\n",
      "    step: 200; loss: 2.574; l2dist: 1.495\n",
      "    step: 250; loss: 2.429; l2dist: 1.453\n",
      "    step: 300; loss: 2.386; l2dist: 1.441\n",
      "    step: 350; loss: 2.350; l2dist: 1.433\n",
      "    step: 400; loss: 2.328; l2dist: 1.432\n",
      "    step: 450; loss: 2.341; l2dist: 1.433\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.396; l2dist: 0.000\n",
      "    step: 50; loss: 8.028; l2dist: 2.464\n",
      "    step: 100; loss: 3.541; l2dist: 1.771\n",
      "    step: 150; loss: 2.691; l2dist: 1.536\n",
      "    step: 200; loss: 2.446; l2dist: 1.462\n",
      "    step: 250; loss: 2.385; l2dist: 1.428\n",
      "    step: 300; loss: 2.302; l2dist: 1.418\n",
      "    step: 350; loss: 2.263; l2dist: 1.405\n",
      "    step: 400; loss: 2.254; l2dist: 1.403\n",
      "    step: 450; loss: 2.247; l2dist: 1.399\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.240; l2dist: 0.000\n",
      "    step: 50; loss: 7.067; l2dist: 2.257\n",
      "    step: 100; loss: 3.404; l2dist: 1.683\n",
      "    step: 150; loss: 2.590; l2dist: 1.488\n",
      "    step: 200; loss: 2.393; l2dist: 1.436\n",
      "    step: 250; loss: 2.293; l2dist: 1.406\n",
      "    step: 300; loss: 2.265; l2dist: 1.394\n",
      "    step: 350; loss: 2.243; l2dist: 1.389\n",
      "    step: 400; loss: 2.218; l2dist: 1.383\n",
      "    step: 450; loss: 2.225; l2dist: 1.388\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.332; l2dist: 0.000\n",
      "    step: 50; loss: 6.727; l2dist: 2.146\n",
      "    step: 100; loss: 3.285; l2dist: 1.619\n",
      "    step: 150; loss: 2.520; l2dist: 1.452\n",
      "    step: 200; loss: 2.343; l2dist: 1.398\n",
      "    step: 250; loss: 2.265; l2dist: 1.382\n",
      "    step: 300; loss: 2.222; l2dist: 1.371\n",
      "    step: 350; loss: 2.208; l2dist: 1.364\n",
      "    step: 400; loss: 2.201; l2dist: 1.364\n",
      "    step: 450; loss: 2.196; l2dist: 1.362\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.045; l2dist: 0.000\n",
      "    step: 50; loss: 6.644; l2dist: 2.110\n",
      "    step: 100; loss: 3.268; l2dist: 1.607\n",
      "    step: 150; loss: 2.524; l2dist: 1.437\n",
      "    step: 200; loss: 2.337; l2dist: 1.391\n",
      "    step: 250; loss: 2.257; l2dist: 1.375\n",
      "    step: 300; loss: 2.231; l2dist: 1.363\n",
      "    step: 350; loss: 2.212; l2dist: 1.355\n",
      "    step: 400; loss: 2.201; l2dist: 1.358\n",
      "    step: 450; loss: 2.197; l2dist: 1.356\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.000; l2dist: 0.000\n",
      "    step: 50; loss: 6.637; l2dist: 2.113\n",
      "    step: 100; loss: 3.292; l2dist: 1.626\n",
      "    step: 150; loss: 2.529; l2dist: 1.443\n",
      "    step: 200; loss: 2.355; l2dist: 1.399\n",
      "    step: 250; loss: 2.275; l2dist: 1.385\n",
      "    step: 300; loss: 2.241; l2dist: 1.370\n",
      "    step: 350; loss: 2.215; l2dist: 1.370\n",
      "    step: 400; loss: 2.210; l2dist: 1.365\n",
      "    step: 450; loss: 2.206; l2dist: 1.363\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.652; l2dist: 0.000\n",
      "    step: 50; loss: 6.628; l2dist: 2.112\n",
      "    step: 100; loss: 3.305; l2dist: 1.627\n",
      "    step: 150; loss: 2.536; l2dist: 1.454\n",
      "    step: 200; loss: 2.340; l2dist: 1.406\n",
      "    step: 250; loss: 2.280; l2dist: 1.390\n",
      "    step: 300; loss: 2.230; l2dist: 1.377\n",
      "    step: 350; loss: 2.218; l2dist: 1.372\n",
      "    step: 400; loss: 2.212; l2dist: 1.374\n",
      "    step: 450; loss: 2.217; l2dist: 1.368\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.904; l2dist: 0.000\n",
      "    step: 50; loss: 6.702; l2dist: 2.123\n",
      "    step: 100; loss: 3.299; l2dist: 1.629\n",
      "    step: 150; loss: 2.540; l2dist: 1.470\n",
      "    step: 200; loss: 2.348; l2dist: 1.418\n",
      "    step: 250; loss: 2.260; l2dist: 1.392\n",
      "    step: 300; loss: 2.238; l2dist: 1.384\n",
      "    step: 350; loss: 2.217; l2dist: 1.371\n",
      "    step: 400; loss: 2.228; l2dist: 1.373\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 136.020; l2dist: 0.000\n",
      "    step: 50; loss: 14.801; l2dist: 3.589\n",
      "    step: 100; loss: 6.566; l2dist: 2.508\n",
      "    step: 150; loss: 4.385; l2dist: 2.033\n",
      "    step: 200; loss: 3.501; l2dist: 1.812\n",
      "    step: 250; loss: 3.113; l2dist: 1.694\n",
      "    step: 300; loss: 2.966; l2dist: 1.659\n",
      "    step: 350; loss: 2.780; l2dist: 1.603\n",
      "    step: 400; loss: 2.809; l2dist: 1.607\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 147.192; l2dist: 0.000\n",
      "    step: 50; loss: 14.258; l2dist: 3.475\n",
      "    step: 100; loss: 6.421; l2dist: 2.426\n",
      "    step: 150; loss: 4.357; l2dist: 1.989\n",
      "    step: 200; loss: 3.615; l2dist: 1.803\n",
      "    step: 250; loss: 3.174; l2dist: 1.698\n",
      "    step: 300; loss: 2.991; l2dist: 1.641\n",
      "    step: 350; loss: 2.824; l2dist: 1.610\n",
      "    step: 400; loss: 2.802; l2dist: 1.602\n",
      "    step: 450; loss: 2.739; l2dist: 1.587\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 81.538; l2dist: 0.000\n",
      "    step: 50; loss: 10.871; l2dist: 2.977\n",
      "    step: 100; loss: 4.772; l2dist: 2.087\n",
      "    step: 150; loss: 3.458; l2dist: 1.772\n",
      "    step: 200; loss: 3.027; l2dist: 1.658\n",
      "    step: 250; loss: 2.794; l2dist: 1.598\n",
      "    step: 300; loss: 2.762; l2dist: 1.593\n",
      "    step: 350; loss: 2.643; l2dist: 1.557\n",
      "    step: 400; loss: 2.676; l2dist: 1.565\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.889; l2dist: 0.000\n",
      "    step: 50; loss: 9.023; l2dist: 2.662\n",
      "    step: 100; loss: 4.137; l2dist: 1.926\n",
      "    step: 150; loss: 3.111; l2dist: 1.671\n",
      "    step: 200; loss: 2.767; l2dist: 1.591\n",
      "    step: 250; loss: 2.648; l2dist: 1.556\n",
      "    step: 300; loss: 2.601; l2dist: 1.534\n",
      "    step: 350; loss: 2.548; l2dist: 1.524\n",
      "    step: 400; loss: 2.540; l2dist: 1.519\n",
      "    step: 450; loss: 2.546; l2dist: 1.531\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.371; l2dist: 0.000\n",
      "    step: 50; loss: 8.008; l2dist: 2.450\n",
      "    step: 100; loss: 3.841; l2dist: 1.832\n",
      "    step: 150; loss: 2.941; l2dist: 1.618\n",
      "    step: 200; loss: 2.686; l2dist: 1.545\n",
      "    step: 250; loss: 2.581; l2dist: 1.522\n",
      "    step: 300; loss: 2.538; l2dist: 1.508\n",
      "    step: 350; loss: 2.528; l2dist: 1.504\n",
      "    step: 400; loss: 2.506; l2dist: 1.501\n",
      "    step: 450; loss: 2.518; l2dist: 1.502\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.666; l2dist: 0.000\n",
      "    step: 50; loss: 7.621; l2dist: 2.362\n",
      "    step: 100; loss: 3.768; l2dist: 1.786\n",
      "    step: 150; loss: 2.880; l2dist: 1.585\n",
      "    step: 200; loss: 2.620; l2dist: 1.516\n",
      "    step: 250; loss: 2.541; l2dist: 1.492\n",
      "    step: 300; loss: 2.500; l2dist: 1.489\n",
      "    step: 350; loss: 2.483; l2dist: 1.479\n",
      "    step: 400; loss: 2.485; l2dist: 1.481\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.705; l2dist: 0.000\n",
      "    step: 50; loss: 7.536; l2dist: 2.329\n",
      "    step: 100; loss: 3.700; l2dist: 1.768\n",
      "    step: 150; loss: 2.829; l2dist: 1.573\n",
      "    step: 200; loss: 2.624; l2dist: 1.515\n",
      "    step: 250; loss: 2.542; l2dist: 1.499\n",
      "    step: 300; loss: 2.479; l2dist: 1.484\n",
      "    step: 350; loss: 2.458; l2dist: 1.478\n",
      "    step: 400; loss: 2.465; l2dist: 1.477\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.974; l2dist: 0.000\n",
      "    step: 50; loss: 7.509; l2dist: 2.326\n",
      "    step: 100; loss: 3.698; l2dist: 1.771\n",
      "    step: 150; loss: 2.850; l2dist: 1.571\n",
      "    step: 200; loss: 2.615; l2dist: 1.520\n",
      "    step: 250; loss: 2.548; l2dist: 1.492\n",
      "    step: 300; loss: 2.509; l2dist: 1.489\n",
      "    step: 350; loss: 2.499; l2dist: 1.492\n",
      "    step: 400; loss: 2.473; l2dist: 1.488\n",
      "    step: 450; loss: 2.459; l2dist: 1.476\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.192; l2dist: 0.000\n",
      "    step: 50; loss: 7.468; l2dist: 2.317\n",
      "    step: 100; loss: 3.681; l2dist: 1.769\n",
      "    step: 150; loss: 2.838; l2dist: 1.574\n",
      "    step: 200; loss: 2.618; l2dist: 1.516\n",
      "    step: 250; loss: 2.532; l2dist: 1.491\n",
      "    step: 300; loss: 2.485; l2dist: 1.488\n",
      "    step: 350; loss: 2.469; l2dist: 1.486\n",
      "    step: 400; loss: 2.446; l2dist: 1.478\n",
      "    step: 450; loss: 2.445; l2dist: 1.474\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.369; l2dist: 0.000\n",
      "    step: 50; loss: 7.500; l2dist: 2.328\n",
      "    step: 100; loss: 3.693; l2dist: 1.778\n",
      "    step: 150; loss: 2.833; l2dist: 1.581\n",
      "    step: 200; loss: 2.609; l2dist: 1.530\n",
      "    step: 250; loss: 2.524; l2dist: 1.498\n",
      "    step: 300; loss: 2.513; l2dist: 1.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 2.478; l2dist: 1.488\n",
      "    step: 400; loss: 2.457; l2dist: 1.485\n",
      "    step: 450; loss: 2.446; l2dist: 1.483\n",
      "binary step: 9; number of successful adv: 100/100\n"
     ]
    }
   ],
   "source": [
    "attack = CWL2Attack()\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            net, x[begin:end], y[begin:end], targeted=False,\n",
    "            binary_search_steps=10, max_iterations=500,\n",
    "            confidence=0, learning_rate=1e-1,\n",
    "            initial_const=1e1, abort_early=True)\n",
    "    return x_adv\n",
    "\n",
    "x_adv = attack_batch(x_test.cuda(), y_test[:10000].cuda(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = net(x_adv)\n",
    "(y_pred.argmax(1).cpu() == y_test[:10000]).numpy().sum() / y_pred.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dknn.classify(x_adv)\n",
    "(y_pred.argmax(1) == y_test[:10000].numpy()).sum() / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([139.,  46.,  23.,  19.,   3.,   3.,   4.,   2.,   2.,   3.]),\n",
       " array([0.0165    , 0.08763333, 0.15876667, 0.2299    , 0.30103333,\n",
       "        0.37216667, 0.4433    , 0.51443333, 0.58556667, 0.6567    ,\n",
       "        0.72783333]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cred = dknn.credibility(y_pred)\n",
    "suc_ind = y_pred.argmax(1) != y_test[:10000].numpy()\n",
    "plt.hist(cred[suc_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5e5ddbbac8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[0].cpu().detach().numpy().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3970, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_adv.cpu() - x_test[:10000]).view(10000, -1).norm(2, 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all random seeds\n",
    "exp_id = 0\n",
    "seed = 2019\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set up model directory\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'train_mnist_cav_exp%d.h5' % exp_id\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "net = ClassAuxVAE((1, 28, 28), num_classes=10, latent_dim=20)\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "net = net.module\n",
    "net.eval()\n",
    "\n",
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test) = load_mnist_all(\n",
    "    '/data', val_size=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "x_valid = x_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassAuxVAE(\n",
       "  (en_conv1): Conv2d(1, 64, kernel_size=(8, 8), stride=(2, 2), padding=(3, 3))\n",
       "  (relu1): ReLU(inplace)\n",
       "  (en_conv2): Conv2d(64, 128, kernel_size=(6, 6), stride=(2, 2), padding=(3, 3))\n",
       "  (relu2): ReLU(inplace)\n",
       "  (en_conv3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu3): ReLU(inplace)\n",
       "  (en_fc1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (relu4): ReLU(inplace)\n",
       "  (en_mu): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (en_logvar): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (de_fc1): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (de_fc2): Linear(in_features=128, out_features=1568, bias=True)\n",
       "  (ax_fc1): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (ax_fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers = ['en_mu']\n",
    "net = net.to(device)\n",
    "with torch.no_grad():\n",
    "    dknn = DKNN(net, x_train, y_train, x_valid, y_valid, layers, \n",
    "                k=10, num_classes=10)\n",
    "    y_pred = dknn.classify(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.964"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred.argmax(1) == y_test.numpy()).sum() / y_test.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 741., 1243.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        8016.]),\n",
       " array([0.01683333, 0.11515   , 0.21346667, 0.31178333, 0.4101    ,\n",
       "        0.50841667, 0.60673333, 0.70505   , 0.80336667, 0.90168333,\n",
       "        1.        ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cred = dknn.credibility(y_pred)\n",
    "plt.hist(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: -10.374; l2dist: 0.000\n",
      "    step: 50; loss: -16.516; l2dist: 2.206\n",
      "    step: 100; loss: -17.532; l2dist: 2.233\n",
      "    step: 150; loss: -21.379; l2dist: 2.383\n",
      "    step: 200; loss: -23.950; l2dist: 2.461\n",
      "    step: 250; loss: -25.525; l2dist: 2.522\n",
      "    step: 300; loss: -26.562; l2dist: 2.575\n",
      "    step: 350; loss: -27.237; l2dist: 2.610\n",
      "    step: 400; loss: -27.690; l2dist: 2.641\n",
      "    step: 450; loss: -27.980; l2dist: 2.662\n",
      "binary step: 0; number of successful adv: 732/1000\n",
      "    step: 0; loss: -31.082; l2dist: 0.000\n",
      "    step: 50; loss: -80.035; l2dist: 2.555\n",
      "    step: 100; loss: -102.080; l2dist: 2.891\n",
      "    step: 150; loss: -113.852; l2dist: 3.048\n",
      "    step: 200; loss: -119.545; l2dist: 3.074\n",
      "    step: 250; loss: -122.952; l2dist: 3.069\n",
      "    step: 300; loss: -125.118; l2dist: 3.057\n",
      "    step: 350; loss: -126.465; l2dist: 3.052\n",
      "    step: 400; loss: -127.398; l2dist: 3.048\n",
      "    step: 450; loss: -127.963; l2dist: 3.048\n",
      "binary step: 1; number of successful adv: 1000/1000\n",
      "    step: 0; loss: -18.453; l2dist: 0.000\n",
      "    step: 50; loss: -41.968; l2dist: 2.307\n",
      "    step: 100; loss: -50.826; l2dist: 2.505\n",
      "    step: 150; loss: -58.404; l2dist: 2.650\n",
      "    step: 200; loss: -62.506; l2dist: 2.697\n",
      "    step: 250; loss: -64.977; l2dist: 2.730\n",
      "    step: 300; loss: -66.444; l2dist: 2.758\n",
      "    step: 350; loss: -67.368; l2dist: 2.784\n",
      "    step: 400; loss: -68.046; l2dist: 2.805\n",
      "    step: 450; loss: -68.504; l2dist: 2.821\n",
      "binary step: 2; number of successful adv: 1000/1000\n",
      "    step: 0; loss: -12.291; l2dist: 0.000\n",
      "    step: 50; loss: -23.981; l2dist: 2.042\n",
      "    step: 100; loss: -27.347; l2dist: 2.139\n",
      "    step: 150; loss: -31.913; l2dist: 2.274\n",
      "    step: 200; loss: -34.881; l2dist: 2.359\n",
      "    step: 250; loss: -36.645; l2dist: 2.417\n",
      "    step: 300; loss: -37.730; l2dist: 2.470\n",
      "    step: 350; loss: -38.446; l2dist: 2.513\n",
      "    step: 400; loss: -38.975; l2dist: 2.552\n",
      "    step: 450; loss: -39.337; l2dist: 2.582\n",
      "binary step: 3; number of successful adv: 1000/1000\n",
      "    step: 0; loss: -9.416; l2dist: 0.000\n",
      "    step: 50; loss: -16.268; l2dist: 1.800\n",
      "    step: 100; loss: -17.725; l2dist: 1.852\n",
      "    step: 150; loss: -20.717; l2dist: 1.967\n",
      "    step: 200; loss: -22.756; l2dist: 2.050\n",
      "    step: 250; loss: -24.101; l2dist: 2.127\n",
      "    step: 300; loss: -24.973; l2dist: 2.191\n",
      "    step: 350; loss: -25.603; l2dist: 2.247\n",
      "    step: 400; loss: -26.060; l2dist: 2.291\n",
      "    step: 450; loss: -26.376; l2dist: 2.325\n",
      "binary step: 4; number of successful adv: 1000/1000\n"
     ]
    }
   ],
   "source": [
    "from lib.dknn_attack import DKNNAttack\n",
    "\n",
    "attack = DKNNAttack()\n",
    "x_adv = attack(dknn, x_test[:1000], y_test[:1000],\n",
    "               guide_layer='en_mu', binary_search_steps=5,\n",
    "               max_iterations=500, learning_rate=1e-1,\n",
    "               initial_const=1, abort_early=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dknn.classify(x_adv)\n",
    "(y_pred.argmax(1) == y_test[:1000].numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 128, 4, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000], device='cuda:0', grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.normalize(x.view(1000, -1), 2, 1).norm(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f452c17f7f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[3].cpu().detach().numpy().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9813, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_adv.cpu() - x_test[:1000]).view(1000, -1).norm(2, 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Soft DkNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_soft_dknn(dknn, x):\n",
    "    \n",
    "    train_reps = dknn.get_activations(dknn.x_train)[dknn.layers[0]]\n",
    "    dknn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_reps = dknn.get_activations(dknn.x_train)[dknn.layers[0]]\n",
    "    train_reps = F.normalize(train_reps.view(dknn.x_train.size(0), -1), 2, 1)\n",
    "    test_reps = dknn.get_activations(x_test)[dknn.layers[0]]\n",
    "    test_reps = F.normalize(test_reps.view(x_test.size(0), -1), 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_reps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a50f5f88d62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#         cos = (tr.unsqueeze(0) * train_reps).sum(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_reps' is not defined"
     ]
    }
   ],
   "source": [
    "temp = 2e-2\n",
    "k = 75\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = torch.empty((x_test.size(0), dknn.num_classes))\n",
    "    for i, tr in enumerate(test_reps):\n",
    "        cos = ((tr.unsqueeze(0) * train_reps).sum(1) / temp).exp()\n",
    "#         cos = (tr.unsqueeze(0) * train_reps).sum(1)\n",
    "        for label in range(dknn.num_classes):\n",
    "#             logits[i, label] = cos[dknn.y_train == label].topk(k)[0].mean()\n",
    "            logits[i, label] = cos[dknn.y_train == label].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LID & Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral norm of jacobian and LID of basic model\n",
    "\n",
    "layers = ['relu1', 'relu2', 'relu3', 'fc']\n",
    "dknn = DKNNL2(net, x_train.cuda(), y_train, x_valid.cuda(), y_valid, layers, \n",
    "              k=75, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_test.requires_grad_(True)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5763, 12.8764, 26.2434, 13.5448\n"
     ]
    }
   ],
   "source": [
    "norms = compute_spnorm(x, dknn, layers)\n",
    "print(', '.join('%.4f' % i for i in norms.mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5957, 11.1405, 8.5105, 4.0548\n"
     ]
    }
   ],
   "source": [
    "lid = np.zeros((x.size(0), len(layers)))\n",
    "reps = dknn.get_activations(x, requires_grad=False)\n",
    "train_reps = dknn.get_activations(x_train, requires_grad=False)\n",
    "\n",
    "for l, layer in enumerate(layers):\n",
    "    lid[:, l] = compute_lid(reps[layer], \n",
    "                         train_reps[layer], \n",
    "                         3000, \n",
    "                         exclude_self=False)\n",
    "print(', '.join('%.4f' % i for i in lid.mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (maybe) spectral norm is not good for comparing between layers\n",
    "# because of difference in dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral norm of jacobian and LID of VAE (exp4)\n",
    "\n",
    "layers = ['relu1', 'relu2', 'relu3', 'en_mu']\n",
    "dknn = DKNNL2(net, x_train.cuda(), y_train, x_valid.cuda(), y_valid, layers, \n",
    "              k=75, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_test.requires_grad_(True)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = compute_spnorm(x, dknn, layers)\n",
    "print(', '.join('%.4f' % i for i in norms.mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lid = np.zeros((x.size(0), len(layers)))\n",
    "reps = dknn.get_activations(x, requires_grad=False)\n",
    "train_reps = dknn.get_activations(x_train, requires_grad=False)\n",
    "\n",
    "for l, layer in enumerate(layers):\n",
    "    lid[:, l] = compute_lid(reps[layer], \n",
    "                         train_reps[layer], \n",
    "                         3000, \n",
    "                         exclude_self=False)\n",
    "print(', '.join('%.4f' % i for i in lid.mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.foolbox_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dknn_fb = DkNNFoolboxModel(dknn, (0, 1), 1, preprocessing=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foolbox.criteria import Misclassification\n",
    "from foolbox.distances import MeanSquaredDistance, Linfinity \n",
    "\n",
    "criterion = Misclassification()\n",
    "distance = MeanSquaredDistance\n",
    "# distance = Linfinity\n",
    "\n",
    "attack = foolbox.attacks.BoundaryAttack(\n",
    "    model=dknn_fb, criterion=criterion, distance=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neither starting_point nor initialization_attack given. Falling back to BlendedUniformNoiseAttack for initialization.\n",
      "Initial spherical_step = 1.00, source_step = 0.10\n",
      "Using 4 threads to create random numbers\n",
      "Step 0: 1.36508e-01, stepsizes = 1.0e+00/1.0e-01: \n",
      "  Boundary too non-linear, decreasing steps: 0.05 (100), 0.00 ( 5)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 6)\n",
      "Step 100: 8.79483e-02, stepsizes = 4.4e-01/4.4e-02:  (took 2.99109 seconds)\n",
      "Initializing generation and prediction time measurements. This can take a few seconds.\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 669.22764\n",
      "   1.0% for generation (6.46957)\n",
      "   15.3% for spherical prediction (102.47140)\n",
      "   70.9% for prediction (474.42754)\n",
      "   0.0% for hyperparameter update (0.01493)\n",
      "   12.8% for the rest (85.84422)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.39345837e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.57342741e-05 1.09945734e-05\n",
      " 3.46500397e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01518669 0.01552219\n",
      " 0.01948805]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.97 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "Using batch size   1, an average step would have taken 4.66979 = 0.06244 + 4.60734 seconds\n",
      "Using batch size   2, an average step would have taken 3.09637 = 0.01279 + 3.08358 seconds\n",
      "Using batch size   3, an average step would have taken 1.91815 = 0.00728 + 1.91087 seconds\n",
      "Using batch size   4, an average step would have taken 1.85581 = 0.00861 + 1.84721 seconds\n",
      "Using batch size   5, an average step would have taken 1.27467 = 0.00870 + 1.26597 seconds\n",
      "Using batch size   6, an average step would have taken 1.34586 = 0.00377 + 1.34209 seconds\n",
      "Using batch size   7, an average step would have taken 1.16225 = 0.00264 + 1.15961 seconds\n",
      "Using batch size   8, an average step would have taken 1.17434 = 0.00453 + 1.16980 seconds\n",
      "Using batch size   9, an average step would have taken 0.89344 = 0.00222 + 0.89122 seconds\n",
      "Using batch size  10, an average step would have taken 0.86904 = 0.00290 + 0.86614 seconds\n",
      "Using batch size  11, an average step would have taken 0.85464 = 0.00143 + 0.85321 seconds\n",
      "Using batch size  12, an average step would have taken 0.86713 = 0.00645 + 0.86069 seconds\n",
      "Using batch size  13, an average step would have taken 0.85672 = 0.00268 + 0.85404 seconds\n",
      "Using batch size  14, an average step would have taken 0.80715 = 0.00222 + 0.80493 seconds\n",
      "Using batch size  15, an average step would have taken 0.86389 = 0.00201 + 0.86188 seconds\n",
      "Using batch size  16, an average step would have taken 0.79313 = 0.00114 + 0.79200 seconds\n",
      "Using batch size  17, an average step would have taken 0.82766 = 0.00092 + 0.82674 seconds\n",
      "Using batch size  18, an average step would have taken 0.90182 = 0.00104 + 0.90078 seconds\n",
      "Using batch size  19, an average step would have taken 0.87731 = 0.00082 + 0.87649 seconds\n",
      "Using batch size  20, an average step would have taken 0.61433 = 0.00211 + 0.61222 seconds\n",
      "Using batch size  21, an average step would have taken 0.63512 = 0.00160 + 0.63352 seconds\n",
      "Using batch size  22, an average step would have taken 0.63183 = 0.00112 + 0.63071 seconds\n",
      "Using batch size  23, an average step would have taken 0.59036 = 0.00121 + 0.58915 seconds\n",
      "Using batch size  24, an average step would have taken 0.55836 = 0.00275 + 0.55562 seconds\n",
      "Using batch size  25, an average step would have taken 0.48807 = 0.00087 + 0.48720 seconds\n",
      "batch size was 1, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.48807\n",
      "improvement compared to old batch size (1): 9.6x\n",
      "improvement compared to worst batch size (1): 9.6x\n",
      "improvement compared to smallest batch size (1): 9.6x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 200\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 7)\n",
      "  Boundary too non-linear, decreasing steps: 0.09 (100), 0.00 (16)\n",
      "  Success rate too low, decreasing source step:  0.20 ( 75), 0.03 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.00 ( 2)\n",
      "Step 200: 5.77934e-02, stepsizes = 1.3e-01/8.8e-03:  (took 0.72004 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 715.59752\n",
      "   1.0% for generation (7.32684)\n",
      "   15.0% for spherical prediction (107.06094)\n",
      "   71.7% for prediction (512.98070)\n",
      "   0.0% for hyperparameter update (0.02662)\n",
      "   12.3% for the rest (88.20242)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.39345837e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.57342741e-05 1.09945734e-05\n",
      " 1.39235884e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01518669 0.01552219\n",
      " 0.01723488]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.91 0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.02 0.\n",
      " 0.01 0.   0.02 0.   0.01 0.   0.   0.   0.02 0.   0.   0.  ]\n",
      "Using batch size   1, an average step would have taken 4.61240 = 0.06168 + 4.55072 seconds\n",
      "Using batch size   2, an average step would have taken 3.05263 = 0.01252 + 3.04011 seconds\n",
      "Using batch size   3, an average step would have taken 1.89339 = 0.00709 + 1.88630 seconds\n",
      "Using batch size   4, an average step would have taken 1.82729 = 0.00839 + 1.81890 seconds\n",
      "Using batch size   5, an average step would have taken 1.26687 = 0.00865 + 1.25822 seconds\n",
      "Using batch size   6, an average step would have taken 1.32257 = 0.00361 + 1.31897 seconds\n",
      "Using batch size   7, an average step would have taken 1.15089 = 0.00260 + 1.14829 seconds\n",
      "Using batch size   8, an average step would have taken 1.15615 = 0.00437 + 1.15179 seconds\n",
      "Using batch size   9, an average step would have taken 0.88445 = 0.00220 + 0.88225 seconds\n",
      "Using batch size  10, an average step would have taken 0.86175 = 0.00283 + 0.85892 seconds\n",
      "Using batch size  11, an average step would have taken 0.84460 = 0.00140 + 0.84320 seconds\n",
      "Using batch size  12, an average step would have taken 0.85221 = 0.00627 + 0.84594 seconds\n",
      "Using batch size  13, an average step would have taken 0.85672 = 0.00268 + 0.85404 seconds\n",
      "Using batch size  14, an average step would have taken 0.80391 = 0.00221 + 0.80170 seconds\n",
      "Using batch size  15, an average step would have taken 0.86077 = 0.00200 + 0.85877 seconds\n",
      "Using batch size  16, an average step would have taken 0.78401 = 0.00111 + 0.78290 seconds\n",
      "Using batch size  17, an average step would have taken 0.81761 = 0.00090 + 0.81671 seconds\n",
      "Using batch size  18, an average step would have taken 0.88980 = 0.00102 + 0.88878 seconds\n",
      "Using batch size  19, an average step would have taken 0.86550 = 0.00081 + 0.86469 seconds\n",
      "Using batch size  20, an average step would have taken 0.60393 = 0.00204 + 0.60189 seconds\n",
      "Using batch size  21, an average step would have taken 0.62376 = 0.00156 + 0.62220 seconds\n",
      "Using batch size  22, an average step would have taken 0.61855 = 0.00108 + 0.61747 seconds\n",
      "Using batch size  23, an average step would have taken 0.57547 = 0.00116 + 0.57431 seconds\n",
      "Using batch size  24, an average step would have taken 0.54689 = 0.00259 + 0.54429 seconds\n",
      "Using batch size  25, an average step would have taken 0.43122 = 0.00035 + 0.43087 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.43122\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 10.7x\n",
      "improvement compared to smallest batch size (1): 10.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.12 (100), 0.14 (14)\n",
      "  Success rate too low, decreasing source step:  0.22 (100), 0.13 (30)\n",
      "Step 300: 3.50126e-02, stepsizes = 8.8e-02/3.9e-03:  (took 0.65270 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.05 (19)\n",
      "  Success rate too low, decreasing source step:  0.28 ( 50), 0.00 (30)\n",
      "Step 400: 2.67988e-02, stepsizes = 5.9e-02/1.7e-03: d. reduced by 0.35% (9.3189e-05) (took 0.67623 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 808.57654\n",
      "   1.1% for generation (8.91197)\n",
      "   14.2% for spherical prediction (115.18854)\n",
      "   72.9% for prediction (589.44378)\n",
      "   0.0% for hyperparameter update (0.04040)\n",
      "   11.7% for the rest (94.99185)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.39345837e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.57342741e-05 1.09945734e-05\n",
      " 1.30980064e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01518669 0.01552219\n",
      " 0.01707519]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.47  0.08  0.025 0.025 0.035 0.03  0.025 0.025 0.025 0.    0.015 0.025\n",
      " 0.03  0.03  0.03  0.015 0.005 0.02  0.015 0.01  0.015 0.    0.025 0.01\n",
      " 0.01  0.005]\n",
      "Using batch size   1, an average step would have taken 3.23595 = 0.04327 + 3.19268 seconds\n",
      "Using batch size   2, an average step would have taken 2.16417 = 0.00856 + 2.15561 seconds\n",
      "Using batch size   3, an average step would have taken 1.34992 = 0.00470 + 1.34522 seconds\n",
      "Using batch size   4, an average step would have taken 1.31941 = 0.00572 + 1.31369 seconds\n",
      "Using batch size   5, an average step would have taken 0.94300 = 0.00644 + 0.93656 seconds\n",
      "Using batch size   6, an average step would have taken 0.97216 = 0.00220 + 0.96997 seconds\n",
      "Using batch size   7, an average step would have taken 0.85558 = 0.00185 + 0.85373 seconds\n",
      "Using batch size   8, an average step would have taken 0.86513 = 0.00282 + 0.86231 seconds\n",
      "Using batch size   9, an average step would have taken 0.68970 = 0.00177 + 0.68793 seconds\n",
      "Using batch size  10, an average step would have taken 0.66944 = 0.00194 + 0.66750 seconds\n",
      "Using batch size  11, an average step would have taken 0.65580 = 0.00102 + 0.65478 seconds\n",
      "Using batch size  12, an average step would have taken 0.66228 = 0.00454 + 0.65774 seconds\n",
      "Using batch size  13, an average step would have taken 0.73969 = 0.00200 + 0.73769 seconds\n",
      "Using batch size  14, an average step would have taken 0.68755 = 0.00206 + 0.68549 seconds\n",
      "Using batch size  15, an average step would have taken 0.74397 = 0.00178 + 0.74219 seconds\n",
      "Using batch size  16, an average step would have taken 0.67458 = 0.00081 + 0.67378 seconds\n",
      "Using batch size  17, an average step would have taken 0.69024 = 0.00063 + 0.68961 seconds\n",
      "Using batch size  18, an average step would have taken 0.77407 = 0.00080 + 0.77327 seconds\n",
      "Using batch size  19, an average step would have taken 0.74888 = 0.00068 + 0.74820 seconds\n",
      "Using batch size  20, an average step would have taken 0.49727 = 0.00131 + 0.49596 seconds\n",
      "Using batch size  21, an average step would have taken 0.50730 = 0.00114 + 0.50616 seconds\n",
      "Using batch size  22, an average step would have taken 0.52672 = 0.00083 + 0.52589 seconds\n",
      "Using batch size  23, an average step would have taken 0.47001 = 0.00079 + 0.46922 seconds\n",
      "Using batch size  24, an average step would have taken 0.46367 = 0.00148 + 0.46219 seconds\n",
      "Using batch size  25, an average step would have taken 0.42721 = 0.00033 + 0.42688 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42721\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 7.6x\n",
      "improvement compared to smallest batch size (1): 7.6x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 800\n",
      "  Success rate too low, decreasing source step:  0.32 (100), 0.13 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.18 (17)\n",
      "Step 500: 2.19464e-02, stepsizes = 3.9e-02/7.7e-04: d. reduced by 0.15% (3.3869e-05) (took 0.66194 seconds)\n",
      "  Success rate too low, decreasing source step:  0.33 (100), 0.13 (30)\n",
      "  Success rate too high, increasing source step: 0.40 (100), 0.63 (30)\n",
      "Step 600: 1.96503e-02, stepsizes = 3.9e-02/7.7e-04:  (took 0.65492 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.14 (100), 0.00 ( 9)\n",
      "Step 700: 1.81176e-02, stepsizes = 2.6e-02/5.1e-04:  (took 0.80920 seconds)\n",
      "  Success rate too low, decreasing source step:  0.35 (100), 0.17 (30)\n",
      "Step 800: 1.69526e-02, stepsizes = 2.6e-02/3.4e-04: d. reduced by 0.07% (1.1620e-05) (took 0.58015 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 994.38740\n",
      "   1.2% for generation (12.14030)\n",
      "   13.2% for spherical prediction (131.45618)\n",
      "   74.5% for prediction (740.56397)\n",
      "   0.0% for hyperparameter update (0.13928)\n",
      "   11.1% for the rest (110.08767)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.39345837e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.57342741e-05 1.09945734e-05\n",
      " 1.29926135e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01518669 0.01552219\n",
      " 0.0169139 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.18   0.1325 0.0625 0.0825 0.0475 0.06   0.0425 0.0375 0.04   0.0325\n",
      " 0.015  0.035  0.03   0.015  0.0125 0.015  0.01   0.015  0.025  0.02\n",
      " 0.02   0.0225 0.015  0.01   0.0075 0.015 ]\n",
      "Using batch size   1, an average step would have taken 2.17085 = 0.02903 + 2.14182 seconds\n",
      "Using batch size   2, an average step would have taken 1.48030 = 0.00561 + 1.47469 seconds\n",
      "Using batch size   3, an average step would have taken 0.92185 = 0.00295 + 0.91890 seconds\n",
      "Using batch size   4, an average step would have taken 0.92711 = 0.00376 + 0.92335 seconds\n",
      "Using batch size   5, an average step would have taken 0.69001 = 0.00471 + 0.68530 seconds\n",
      "Using batch size   6, an average step would have taken 0.70012 = 0.00124 + 0.69888 seconds\n",
      "Using batch size   7, an average step would have taken 0.63272 = 0.00128 + 0.63144 seconds\n",
      "Using batch size   8, an average step would have taken 0.64900 = 0.00177 + 0.64723 seconds\n",
      "Using batch size   9, an average step would have taken 0.53173 = 0.00140 + 0.53033 seconds\n",
      "Using batch size  10, an average step would have taken 0.51589 = 0.00130 + 0.51458 seconds\n",
      "Using batch size  11, an average step would have taken 0.50359 = 0.00073 + 0.50286 seconds\n",
      "Using batch size  12, an average step would have taken 0.51320 = 0.00327 + 0.50993 seconds\n",
      "Using batch size  13, an average step would have taken 0.64933 = 0.00148 + 0.64785 seconds\n",
      "Using batch size  14, an average step would have taken 0.60836 = 0.00196 + 0.60640 seconds\n",
      "Using batch size  15, an average step would have taken 0.66766 = 0.00164 + 0.66603 seconds\n",
      "Using batch size  16, an average step would have taken 0.59859 = 0.00060 + 0.59799 seconds\n",
      "Using batch size  17, an average step would have taken 0.60812 = 0.00046 + 0.60766 seconds\n",
      "Using batch size  18, an average step would have taken 0.69742 = 0.00066 + 0.69676 seconds\n",
      "Using batch size  19, an average step would have taken 0.67064 = 0.00059 + 0.67005 seconds\n",
      "Using batch size  20, an average step would have taken 0.42704 = 0.00083 + 0.42620 seconds\n",
      "Using batch size  21, an average step would have taken 0.42421 = 0.00083 + 0.42338 seconds\n",
      "Using batch size  22, an average step would have taken 0.46421 = 0.00065 + 0.46356 seconds\n",
      "Using batch size  23, an average step would have taken 0.39991 = 0.00054 + 0.39937 seconds\n",
      "Using batch size  24, an average step would have taken 0.41010 = 0.00076 + 0.40934 seconds\n",
      "Using batch size  25, an average step would have taken 0.42317 = 0.00032 + 0.42285 seconds\n",
      "batch size was 25, optimal batch size would have been 23\n",
      "setting batch size to 23: expected step duration: 0.39991\n",
      "improvement compared to old batch size (25): 1.1x\n",
      "improvement compared to worst batch size (1): 5.4x\n",
      "improvement compared to smallest batch size (1): 5.4x\n",
      "improvement compared to largest batch size (25): 1.1x\n",
      "next batch size tuning in 200 steps, after step 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 900: 1.61476e-02, stepsizes = 2.6e-02/3.4e-04: d. reduced by 0.07% (1.1068e-05) (took 0.72218 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.30 (30)\n",
      "Step 1000: 1.56287e-02, stepsizes = 1.7e-02/2.3e-04: d. reduced by 0.05% (7.1404e-06) (took 0.80553 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1099.91178\n",
      "   1.3% for generation (13.83356)\n",
      "   12.8% for spherical prediction (140.92117)\n",
      "   75.2% for prediction (827.04217)\n",
      "   0.0% for hyperparameter update (0.19105)\n",
      "   10.7% for the rest (117.92383)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.08196449e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.48678152e-05 1.09945734e-05\n",
      " 1.29926135e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01516138 0.01552219\n",
      " 0.0169139 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.34  0.07  0.06  0.03  0.055 0.03  0.04  0.02  0.02  0.03  0.05  0.\n",
      " 0.055 0.02  0.015 0.025 0.02  0.01  0.01  0.01  0.02  0.015 0.015 0.01\n",
      " 0.01  0.02 ]\n",
      "Using batch size   1, an average step would have taken 2.83899 = 0.03796 + 2.80102 seconds\n",
      "Using batch size   2, an average step would have taken 1.89852 = 0.00694 + 1.89158 seconds\n",
      "Using batch size   3, an average step would have taken 1.18741 = 0.00402 + 1.18339 seconds\n",
      "Using batch size   4, an average step would have taken 1.16817 = 0.00495 + 1.16321 seconds\n",
      "Using batch size   5, an average step would have taken 0.84284 = 0.00575 + 0.83709 seconds\n",
      "Using batch size   6, an average step would have taken 0.86454 = 0.00181 + 0.86274 seconds\n",
      "Using batch size   7, an average step would have taken 0.77348 = 0.00163 + 0.77185 seconds\n",
      "Using batch size   8, an average step would have taken 0.78447 = 0.00241 + 0.78207 seconds\n",
      "Using batch size   9, an average step would have taken 0.63230 = 0.00164 + 0.63066 seconds\n",
      "Using batch size  10, an average step would have taken 0.60345 = 0.00168 + 0.60177 seconds\n",
      "Using batch size  11, an average step would have taken 0.59965 = 0.00091 + 0.59874 seconds\n",
      "Using batch size  12, an average step would have taken 0.59898 = 0.00400 + 0.59497 seconds\n",
      "Using batch size  13, an average step would have taken 0.70182 = 0.00178 + 0.70004 seconds\n",
      "Using batch size  14, an average step would have taken 0.65684 = 0.00202 + 0.65482 seconds\n",
      "Using batch size  15, an average step would have taken 0.71127 = 0.00172 + 0.70955 seconds\n",
      "Using batch size  16, an average step would have taken 0.63811 = 0.00071 + 0.63740 seconds\n",
      "Using batch size  17, an average step would have taken 0.65337 = 0.00056 + 0.65281 seconds\n",
      "Using batch size  18, an average step would have taken 0.74251 = 0.00075 + 0.74176 seconds\n",
      "Using batch size  19, an average step would have taken 0.71788 = 0.00065 + 0.71724 seconds\n",
      "Using batch size  20, an average step would have taken 0.46866 = 0.00112 + 0.46754 seconds\n",
      "Using batch size  21, an average step would have taken 0.47179 = 0.00101 + 0.47078 seconds\n",
      "Using batch size  22, an average step would have taken 0.50127 = 0.00076 + 0.50052 seconds\n",
      "Using batch size  23, an average step would have taken 0.44085 = 0.00064 + 0.44020 seconds\n",
      "Using batch size  24, an average step would have taken 0.44167 = 0.00118 + 0.44048 seconds\n",
      "Using batch size  25, an average step would have taken 0.42317 = 0.00032 + 0.42285 seconds\n",
      "batch size was 23, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42317\n",
      "improvement compared to old batch size (23): 1.0x\n",
      "improvement compared to worst batch size (1): 6.7x\n",
      "improvement compared to smallest batch size (1): 6.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 1100\n",
      "  Success rate too low, decreasing source step:  0.26 (100), 0.07 (30)\n",
      "  Boundary too linear, increasing steps:     0.58 (100), 0.73 (30)\n",
      "  Success rate too high, increasing source step: 0.58 (100), 0.73 (30)\n",
      "Step 1100: 1.52189e-02, stepsizes = 2.6e-02/3.4e-04: d. reduced by 0.07% (1.0432e-05) (took 0.57642 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1145.68716\n",
      "   1.3% for generation (14.68362)\n",
      "   12.6% for spherical prediction (144.85473)\n",
      "   75.4% for prediction (864.02238)\n",
      "   0.0% for hyperparameter update (0.20639)\n",
      "   10.6% for the rest (121.92004)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.08196449e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.48678152e-05 1.09945734e-05\n",
      " 1.30685647e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01516138 0.01552219\n",
      " 0.01685444]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.32 0.11 0.01 0.02 0.07 0.05 0.01 0.04 0.04 0.07 0.02 0.02 0.04 0.01\n",
      " 0.   0.01 0.05 0.   0.01 0.01 0.01 0.02 0.01 0.02 0.01 0.02]\n",
      "Using batch size   1, an average step would have taken 2.75481 = 0.03684 + 2.71797 seconds\n",
      "Using batch size   2, an average step would have taken 1.85624 = 0.00676 + 1.84948 seconds\n",
      "Using batch size   3, an average step would have taken 1.16035 = 0.00390 + 1.15645 seconds\n",
      "Using batch size   4, an average step would have taken 1.13593 = 0.00480 + 1.13114 seconds\n",
      "Using batch size   5, an average step would have taken 0.82724 = 0.00565 + 0.82159 seconds\n",
      "Using batch size   6, an average step would have taken 0.84743 = 0.00174 + 0.84569 seconds\n",
      "Using batch size   7, an average step would have taken 0.76321 = 0.00160 + 0.76161 seconds\n",
      "Using batch size   8, an average step would have taken 0.76221 = 0.00232 + 0.75989 seconds\n",
      "Using batch size   9, an average step would have taken 0.60653 = 0.00157 + 0.60496 seconds\n",
      "Using batch size  10, an average step would have taken 0.58995 = 0.00164 + 0.58831 seconds\n",
      "Using batch size  11, an average step would have taken 0.57965 = 0.00088 + 0.57877 seconds\n",
      "Using batch size  12, an average step would have taken 0.58138 = 0.00387 + 0.57751 seconds\n",
      "Using batch size  13, an average step would have taken 0.69150 = 0.00172 + 0.68978 seconds\n",
      "Using batch size  14, an average step would have taken 0.65199 = 0.00201 + 0.64998 seconds\n",
      "Using batch size  15, an average step would have taken 0.71127 = 0.00172 + 0.70955 seconds\n",
      "Using batch size  16, an average step would have taken 0.62899 = 0.00068 + 0.62831 seconds\n",
      "Using batch size  17, an average step would have taken 0.64667 = 0.00054 + 0.64612 seconds\n",
      "Using batch size  18, an average step would have taken 0.73650 = 0.00074 + 0.73576 seconds\n",
      "Using batch size  19, an average step would have taken 0.71198 = 0.00064 + 0.71134 seconds\n",
      "Using batch size  20, an average step would have taken 0.46606 = 0.00110 + 0.46496 seconds\n",
      "Using batch size  21, an average step would have taken 0.46753 = 0.00099 + 0.46654 seconds\n",
      "Using batch size  22, an average step would have taken 0.49906 = 0.00075 + 0.49831 seconds\n",
      "Using batch size  23, an average step would have taken 0.43588 = 0.00063 + 0.43526 seconds\n",
      "Using batch size  24, an average step would have taken 0.43784 = 0.00113 + 0.43671 seconds\n",
      "Using batch size  25, an average step would have taken 0.42169 = 0.00033 + 0.42136 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42169\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.5x\n",
      "improvement compared to smallest batch size (1): 6.5x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.03 (100), 0.33 ( 3)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.18 (22)\n",
      "  Success rate too low, decreasing source step:  0.56 ( 25), 0.13 (30)\n",
      "Step 1200: 1.50270e-02, stepsizes = 1.2e-02/1.0e-04: d. reduced by 0.02% (3.0507e-06) (took 0.83964 seconds)\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.17 (30)\n",
      "Step 1300: 1.47830e-02, stepsizes = 1.2e-02/6.8e-05: d. reduced by 0.01% (2.0007e-06) (took 0.70453 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1240.87653\n",
      "   1.3% for generation (16.44455)\n",
      "   12.4% for spherical prediction (153.30529)\n",
      "   75.9% for prediction (941.94015)\n",
      "   0.0% for hyperparameter update (0.29152)\n",
      "   10.4% for the rest (128.89502)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.08196449e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.48678152e-05 1.09945734e-05\n",
      " 1.32721420e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01516138 0.01552219\n",
      " 0.01694616]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.37  0.11  0.08  0.07  0.015 0.035 0.025 0.035 0.025 0.025 0.015 0.02\n",
      " 0.03  0.01  0.015 0.005 0.    0.02  0.015 0.01  0.01  0.005 0.025 0.005\n",
      " 0.01  0.015]\n",
      "Using batch size   1, an average step would have taken 2.73855 = 0.03662 + 2.70193 seconds\n",
      "Using batch size   2, an average step would have taken 1.84500 = 0.00681 + 1.83819 seconds\n",
      "Using batch size   3, an average step would have taken 1.14905 = 0.00396 + 1.14508 seconds\n",
      "Using batch size   4, an average step would have taken 1.14170 = 0.00490 + 1.13680 seconds\n",
      "Using batch size   5, an average step would have taken 0.82463 = 0.00563 + 0.81901 seconds\n",
      "Using batch size   6, an average step would have taken 0.85014 = 0.00185 + 0.84829 seconds\n",
      "Using batch size   7, an average step would have taken 0.75796 = 0.00162 + 0.75634 seconds\n",
      "Using batch size   8, an average step would have taken 0.77417 = 0.00244 + 0.77173 seconds\n",
      "Using batch size   9, an average step would have taken 0.61554 = 0.00159 + 0.61396 seconds\n",
      "Using batch size  10, an average step would have taken 0.59931 = 0.00169 + 0.59762 seconds\n",
      "Using batch size  11, an average step would have taken 0.58791 = 0.00090 + 0.58701 seconds\n",
      "Using batch size  12, an average step would have taken 0.59515 = 0.00402 + 0.59114 seconds\n",
      "Using batch size  13, an average step would have taken 0.69666 = 0.00175 + 0.69491 seconds\n",
      "Using batch size  14, an average step would have taken 0.65199 = 0.00201 + 0.64998 seconds\n",
      "Using batch size  15, an average step would have taken 0.71283 = 0.00172 + 0.71110 seconds\n",
      "Using batch size  16, an average step would have taken 0.64571 = 0.00073 + 0.64498 seconds\n",
      "Using batch size  17, an average step would have taken 0.65840 = 0.00057 + 0.65783 seconds\n",
      "Using batch size  18, an average step would have taken 0.74552 = 0.00075 + 0.74477 seconds\n",
      "Using batch size  19, an average step would have taken 0.72083 = 0.00065 + 0.72019 seconds\n",
      "Using batch size  20, an average step would have taken 0.47386 = 0.00115 + 0.47271 seconds\n",
      "Using batch size  21, an average step would have taken 0.48031 = 0.00104 + 0.47927 seconds\n",
      "Using batch size  22, an average step would have taken 0.50570 = 0.00077 + 0.50493 seconds\n",
      "Using batch size  23, an average step would have taken 0.44705 = 0.00066 + 0.44638 seconds\n",
      "Using batch size  24, an average step would have taken 0.44645 = 0.00125 + 0.44520 seconds\n",
      "Using batch size  25, an average step would have taken 0.42399 = 0.00033 + 0.42365 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42399\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.5x\n",
      "improvement compared to smallest batch size (1): 6.5x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 1700\n",
      "  Success rate too high, increasing source step: 0.45 (100), 0.73 (30)\n",
      "  Success rate too low, decreasing source step:  0.21 (100), 0.13 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.75 ( 4)\n",
      "Step 1400: 1.45872e-02, stepsizes = 7.7e-03/4.5e-05: d. reduced by 0.01% (1.9742e-06) (took 0.70996 seconds)\n",
      "  Success rate too high, increasing source step: 0.34 (100), 0.60 (30)\n",
      "  Success rate too low, decreasing source step:  0.21 (100), 0.17 (30)\n",
      "Step 1500: 1.44725e-02, stepsizes = 7.7e-03/4.5e-05: d. reduced by 0.01% (1.9585e-06) (took 0.68849 seconds)\n",
      "  Success rate too low, decreasing source step:  0.27 (100), 0.17 (30)\n",
      "  Boundary too linear, increasing steps:     0.55 (100), 0.60 (30)\n",
      "  Success rate too high, increasing source step: 0.55 (100), 0.60 (30)\n",
      "Step 1600: 1.43552e-02, stepsizes = 1.2e-02/6.8e-05: d. reduced by 0.01% (1.9429e-06) (took 0.75235 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.11 (100), 0.33 (30)\n",
      "Step 1700: 1.42372e-02, stepsizes = 7.7e-03/4.5e-05:  (took 0.76085 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1428.37038\n",
      "   1.4% for generation (19.74497)\n",
      "   11.9% for spherical prediction (170.20866)\n",
      "   76.6% for prediction (1094.83210)\n",
      "   0.0% for hyperparameter update (0.37186)\n",
      "   10.0% for the rest (143.21279)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.08196449e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.48678152e-05 1.09945734e-05\n",
      " 1.32520233e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01516138 0.01552219\n",
      " 0.01695808]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.26   0.1275 0.075  0.0475 0.0625 0.0525 0.0325 0.03   0.0375 0.03\n",
      " 0.0225 0.02   0.02   0.0275 0.0075 0.02   0.0175 0.015  0.015  0.01\n",
      " 0.0075 0.01   0.015  0.015  0.0075 0.015 ]\n",
      "Using batch size   1, an average step would have taken 2.37985 = 0.03182 + 2.34803 seconds\n",
      "Using batch size   2, an average step would have taken 1.61184 = 0.00583 + 1.60601 seconds\n",
      "Using batch size   3, an average step would have taken 1.01294 = 0.00336 + 1.00958 seconds\n",
      "Using batch size   4, an average step would have taken 1.00633 = 0.00420 + 1.00213 seconds\n",
      "Using batch size   5, an average step would have taken 0.73554 = 0.00502 + 0.73052 seconds\n",
      "Using batch size   6, an average step would have taken 0.75676 = 0.00149 + 0.75527 seconds\n",
      "Using batch size   7, an average step would have taken 0.68392 = 0.00142 + 0.68250 seconds\n",
      "Using batch size   8, an average step would have taken 0.69112 = 0.00203 + 0.68909 seconds\n",
      "Using batch size   9, an average step would have taken 0.55968 = 0.00146 + 0.55822 seconds\n",
      "Using batch size  10, an average step would have taken 0.54565 = 0.00145 + 0.54419 seconds\n",
      "Using batch size  11, an average step would have taken 0.53856 = 0.00080 + 0.53776 seconds\n",
      "Using batch size  12, an average step would have taken 0.54915 = 0.00359 + 0.54556 seconds\n",
      "Using batch size  13, an average step would have taken 0.66568 = 0.00157 + 0.66411 seconds\n",
      "Using batch size  14, an average step would have taken 0.62533 = 0.00198 + 0.62335 seconds\n",
      "Using batch size  15, an average step would have taken 0.68246 = 0.00166 + 0.68079 seconds\n",
      "Using batch size  16, an average step would have taken 0.61075 = 0.00063 + 0.61012 seconds\n",
      "Using batch size  17, an average step would have taken 0.62153 = 0.00049 + 0.62104 seconds\n",
      "Using batch size  18, an average step would have taken 0.71245 = 0.00069 + 0.71176 seconds\n",
      "Using batch size  19, an average step would have taken 0.68836 = 0.00061 + 0.68775 seconds\n",
      "Using batch size  20, an average step would have taken 0.44590 = 0.00096 + 0.44493 seconds\n",
      "Using batch size  21, an average step would have taken 0.44835 = 0.00092 + 0.44744 seconds\n",
      "Using batch size  22, an average step would have taken 0.48302 = 0.00071 + 0.48231 seconds\n",
      "Using batch size  23, an average step would have taken 0.41914 = 0.00057 + 0.41857 seconds\n",
      "Using batch size  24, an average step would have taken 0.42541 = 0.00097 + 0.42444 seconds\n",
      "Using batch size  25, an average step would have taken 0.42428 = 0.00033 + 0.42395 seconds\n",
      "batch size was 25, optimal batch size would have been 23\n",
      "setting batch size to 23: expected step duration: 0.41914\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 5.7x\n",
      "improvement compared to smallest batch size (1): 5.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 1900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.20 (30)\n",
      "  Success rate too low, decreasing source step:  0.20 ( 50), 0.13 (30)\n",
      "Step 1800: 1.41669e-02, stepsizes = 5.1e-03/2.0e-05: d. reduced by 0.00% (5.6826e-07) (took 0.60293 seconds)\n",
      "  Success rate too high, increasing source step: 0.40 (100), 0.57 (30)\n",
      "  Success rate too low, decreasing source step:  0.43 (100), 0.07 (30)\n",
      "Step 1900: 1.41043e-02, stepsizes = 5.1e-03/2.0e-05: d. reduced by 0.00% (5.6559e-07) (took 0.69565 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1523.01799\n",
      "   1.4% for generation (21.41045)\n",
      "   11.8% for spherical prediction (179.06828)\n",
      "   76.9% for prediction (1171.81344)\n",
      "   0.0% for hyperparameter update (0.42228)\n",
      "   9.9% for the rest (150.30354)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 5.31754805e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.47278694e-05 1.09945734e-05\n",
      " 1.32520233e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01538129 0.01552219\n",
      " 0.01695808]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.17  0.16  0.115 0.05  0.055 0.065 0.04  0.02  0.035 0.015 0.045 0.02\n",
      " 0.015 0.025 0.01  0.01  0.02  0.015 0.03  0.01  0.    0.015 0.02  0.01\n",
      " 0.02  0.01 ]\n",
      "Using batch size   1, an average step would have taken 2.03072 = 0.02715 + 2.00356 seconds\n",
      "Using batch size   2, an average step would have taken 1.38165 = 0.00623 + 1.37542 seconds\n",
      "Using batch size   3, an average step would have taken 0.87638 = 0.00279 + 0.87359 seconds\n",
      "Using batch size   4, an average step would have taken 0.88376 = 0.00357 + 0.88019 seconds\n",
      "Using batch size   5, an average step would have taken 0.65554 = 0.00447 + 0.65107 seconds\n",
      "Using batch size   6, an average step would have taken 0.66477 = 0.00116 + 0.66361 seconds\n",
      "Using batch size   7, an average step would have taken 0.61389 = 0.00125 + 0.61264 seconds\n",
      "Using batch size   8, an average step would have taken 0.62434 = 0.00168 + 0.62266 seconds\n",
      "Using batch size   9, an average step would have taken 0.51589 = 0.00137 + 0.51453 seconds\n",
      "Using batch size  10, an average step would have taken 0.49979 = 0.00127 + 0.49852 seconds\n",
      "Using batch size  11, an average step would have taken 0.49253 = 0.00071 + 0.49182 seconds\n",
      "Using batch size  12, an average step would have taken 0.50430 = 0.00319 + 0.50111 seconds\n",
      "Using batch size  13, an average step would have taken 0.63986 = 0.00142 + 0.63844 seconds\n",
      "Using batch size  14, an average step would have taken 0.60028 = 0.00194 + 0.59833 seconds\n",
      "Using batch size  15, an average step would have taken 0.66143 = 0.00162 + 0.65981 seconds\n",
      "Using batch size  16, an average step would have taken 0.58947 = 0.00057 + 0.58890 seconds\n",
      "Using batch size  17, an average step would have taken 0.59807 = 0.00044 + 0.59762 seconds\n",
      "Using batch size  18, an average step would have taken 0.68690 = 0.00064 + 0.68626 seconds\n",
      "Using batch size  19, an average step would have taken 0.66326 = 0.00058 + 0.66268 seconds\n",
      "Using batch size  20, an average step would have taken 0.42573 = 0.00082 + 0.42491 seconds\n",
      "Using batch size  21, an average step would have taken 0.42492 = 0.00083 + 0.42409 seconds\n",
      "Using batch size  22, an average step would have taken 0.46366 = 0.00065 + 0.46301 seconds\n",
      "Using batch size  23, an average step would have taken 0.40378 = 0.00055 + 0.40322 seconds\n",
      "Using batch size  24, an average step would have taken 0.40723 = 0.00072 + 0.40651 seconds\n",
      "Using batch size  25, an average step would have taken 0.42428 = 0.00033 + 0.42395 seconds\n",
      "batch size was 23, optimal batch size would have been 23\n",
      "setting batch size to 23: expected step duration: 0.40378\n",
      "improvement compared to old batch size (23): 1.0x\n",
      "improvement compared to worst batch size (1): 5.0x\n",
      "improvement compared to smallest batch size (1): 5.0x\n",
      "improvement compared to largest batch size (25): 1.1x\n",
      "next batch size tuning in 400 steps, after step 2300\n",
      "Step 2000: 1.40529e-02, stepsizes = 5.1e-03/2.0e-05:  (took 0.96835 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.43 (30)\n",
      "Step 2100: 1.40188e-02, stepsizes = 3.4e-03/1.3e-05: d. reduced by 0.00% (3.7490e-07) (took 0.58701 seconds)\n",
      "  Success rate too low, decreasing source step:  0.18 ( 96), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.25 (100), 0.53 (30)\n",
      "Step 2200: 1.39947e-02, stepsizes = 3.4e-03/1.3e-05: d. reduced by 0.00% (2.4947e-07) (took 0.80112 seconds)\n",
      "Step 2300: 1.39663e-02, stepsizes = 3.4e-03/1.3e-05:  (took 0.80567 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1726.21282\n",
      "   1.4% for generation (24.65294)\n",
      "   11.5% for spherical prediction (198.76615)\n",
      "   77.5% for prediction (1338.31684)\n",
      "   0.0% for hyperparameter update (0.49143)\n",
      "   9.5% for the rest (163.98547)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 6.15931780e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.44688775e-05 1.09945734e-05\n",
      " 1.32520233e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01606127 0.01552219\n",
      " 0.01695808]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.18   0.115  0.0675 0.055  0.05   0.0675 0.0575 0.0525 0.07   0.02\n",
      " 0.0275 0.0225 0.0275 0.02   0.0225 0.015  0.0125 0.01   0.02   0.01\n",
      " 0.0125 0.0125 0.0225 0.01   0.01   0.01  ]\n",
      "Using batch size   1, an average step would have taken 2.14359 = 0.02866 + 2.11492 seconds\n",
      "Using batch size   2, an average step would have taken 1.45592 = 0.00752 + 1.44840 seconds\n",
      "Using batch size   3, an average step would have taken 0.91536 = 0.00292 + 0.91244 seconds\n",
      "Using batch size   4, an average step would have taken 0.91337 = 0.00370 + 0.90967 seconds\n",
      "Using batch size   5, an average step would have taken 0.68091 = 0.00465 + 0.67626 seconds\n",
      "Using batch size   6, an average step would have taken 0.69252 = 0.00122 + 0.69130 seconds\n",
      "Using batch size   7, an average step would have taken 0.62512 = 0.00127 + 0.62385 seconds\n",
      "Using batch size   8, an average step would have taken 0.62710 = 0.00171 + 0.62538 seconds\n",
      "Using batch size   9, an average step would have taken 0.51965 = 0.00137 + 0.51828 seconds\n",
      "Using batch size  10, an average step would have taken 0.50524 = 0.00128 + 0.50396 seconds\n",
      "Using batch size  11, an average step would have taken 0.49738 = 0.00072 + 0.49666 seconds\n",
      "Using batch size  12, an average step would have taken 0.50708 = 0.00322 + 0.50385 seconds\n",
      "Using batch size  13, an average step would have taken 0.64244 = 0.00144 + 0.64101 seconds\n",
      "Using batch size  14, an average step would have taken 0.59866 = 0.00194 + 0.59672 seconds\n",
      "Using batch size  15, an average step would have taken 0.65832 = 0.00162 + 0.65670 seconds\n",
      "Using batch size  16, an average step would have taken 0.58871 = 0.00057 + 0.58814 seconds\n",
      "Using batch size  17, an average step would have taken 0.59890 = 0.00044 + 0.59846 seconds\n",
      "Using batch size  18, an average step would have taken 0.69066 = 0.00065 + 0.69001 seconds\n",
      "Using batch size  19, an average step would have taken 0.66695 = 0.00059 + 0.66636 seconds\n",
      "Using batch size  20, an average step would have taken 0.42573 = 0.00082 + 0.42491 seconds\n",
      "Using batch size  21, an average step would have taken 0.42563 = 0.00084 + 0.42479 seconds\n",
      "Using batch size  22, an average step would have taken 0.46366 = 0.00065 + 0.46301 seconds\n",
      "Using batch size  23, an average step would have taken 0.41944 = 0.00058 + 0.41886 seconds\n",
      "Using batch size  24, an average step would have taken 0.40914 = 0.00075 + 0.40839 seconds\n",
      "Using batch size  25, an average step would have taken 0.42428 = 0.00033 + 0.42395 seconds\n",
      "batch size was 23, optimal batch size would have been 24\n",
      "setting batch size to 24: expected step duration: 0.40914\n",
      "improvement compared to old batch size (23): 1.0x\n",
      "improvement compared to worst batch size (1): 5.2x\n",
      "improvement compared to smallest batch size (1): 5.2x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 2700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.23 (30)\n",
      "  Success rate too low, decreasing source step:  0.20 ( 74), 0.13 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.40 (15)\n",
      "Step 2400: 1.39531e-02, stepsizes = 1.5e-03/4.0e-06: d. reduced by 0.00% (1.1030e-07) (took 0.72685 seconds)\n",
      "  Success rate too high, increasing source step: 0.38 ( 72), 0.63 (30)\n",
      "  Success rate too low, decreasing source step:  0.33 (100), 0.17 (30)\n",
      "Step 2500: 1.39416e-02, stepsizes = 1.5e-03/4.0e-06: d. reduced by 0.00% (1.1042e-07) (took 0.82726 seconds)\n",
      "  Success rate too high, increasing source step: 0.41 (100), 0.53 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.24 (17)\n",
      "  Success rate too low, decreasing source step:  0.27 ( 49), 0.17 (30)\n",
      "Step 2600: 1.39340e-02, stepsizes = 1.0e-03/2.6e-06: d. reduced by 0.00% (7.3738e-08) (took 0.64675 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.52 (21)\n",
      "  Success rate too high, increasing source step: 0.71 ( 24), 0.57 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.32 (25)\n",
      "Step 2700: 1.39289e-02, stepsizes = 4.5e-04/1.8e-06: d. reduced by 0.00% (7.3528e-08) (took 0.64863 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1939.10042\n",
      "   1.4% for generation (27.99003)\n",
      "   11.2% for spherical prediction (217.72148)\n",
      "   78.1% for prediction (1514.15453)\n",
      "   0.0% for hyperparameter update (0.57562)\n",
      "   9.2% for the rest (178.65875)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.52754774e-03 6.15931780e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.44688775e-05 1.35511445e-05\n",
      " 1.32520233e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01606127 0.01572055\n",
      " 0.01695808]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.27   0.1525 0.07   0.055  0.04   0.0675 0.035  0.035  0.0175 0.0175\n",
      " 0.0175 0.005  0.025  0.02   0.0275 0.0225 0.01   0.02   0.015  0.0175\n",
      " 0.0175 0.0125 0.0025 0.005  0.0075 0.015 ]\n",
      "Using batch size   1, an average step would have taken 2.37373 = 0.03137 + 2.34237 seconds\n",
      "Using batch size   2, an average step would have taken 1.61449 = 0.00845 + 1.60604 seconds\n",
      "Using batch size   3, an average step would have taken 1.00931 = 0.00336 + 1.00595 seconds\n",
      "Using batch size   4, an average step would have taken 1.01107 = 0.00423 + 1.00684 seconds\n",
      "Using batch size   5, an average step would have taken 0.73163 = 0.00499 + 0.72664 seconds\n",
      "Using batch size   6, an average step would have taken 0.76014 = 0.00151 + 0.75863 seconds\n",
      "Using batch size   7, an average step would have taken 0.67436 = 0.00140 + 0.67296 seconds\n",
      "Using batch size   8, an average step would have taken 0.69470 = 0.00205 + 0.69265 seconds\n",
      "Using batch size   9, an average step would have taken 0.56346 = 0.00147 + 0.56198 seconds\n",
      "Using batch size  10, an average step would have taken 0.54616 = 0.00144 + 0.54472 seconds\n",
      "Using batch size  11, an average step would have taken 0.54664 = 0.00081 + 0.54583 seconds\n",
      "Using batch size  12, an average step would have taken 0.55794 = 0.00365 + 0.55429 seconds\n",
      "Using batch size  13, an average step would have taken 0.67515 = 0.00163 + 0.67352 seconds\n",
      "Using batch size  14, an average step would have taken 0.62775 = 0.00198 + 0.62577 seconds\n",
      "Using batch size  15, an average step would have taken 0.68402 = 0.00167 + 0.68235 seconds\n",
      "Using batch size  16, an average step would have taken 0.61455 = 0.00064 + 0.61391 seconds\n",
      "Using batch size  17, an average step would have taken 0.62404 = 0.00050 + 0.62355 seconds\n",
      "Using batch size  18, an average step would have taken 0.71471 = 0.00069 + 0.71401 seconds\n",
      "Using batch size  19, an average step would have taken 0.68836 = 0.00061 + 0.68775 seconds\n",
      "Using batch size  20, an average step would have taken 0.44329 = 0.00094 + 0.44235 seconds\n",
      "Using batch size  21, an average step would have taken 0.44480 = 0.00091 + 0.44390 seconds\n",
      "Using batch size  22, an average step would have taken 0.48302 = 0.00071 + 0.48231 seconds\n",
      "Using batch size  23, an average step would have taken 0.44243 = 0.00069 + 0.44174 seconds\n",
      "Using batch size  24, an average step would have taken 0.43213 = 0.00105 + 0.43109 seconds\n",
      "Using batch size  25, an average step would have taken 0.42428 = 0.00033 + 0.42395 seconds\n",
      "batch size was 24, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42428\n",
      "improvement compared to old batch size (24): 1.0x\n",
      "improvement compared to worst batch size (1): 5.6x\n",
      "improvement compared to smallest batch size (1): 5.6x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 3100\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.44 (100), 0.67 (30)\n",
      "Step 2800: 1.39258e-02, stepsizes = 4.5e-04/1.8e-06: d. reduced by 0.00% (4.8989e-08) (took 0.90690 seconds)\n",
      "  Success rate too low, decreasing source step:  0.23 (100), 0.10 (30)\n",
      "  Success rate too high, increasing source step: 0.37 (100), 0.57 (30)\n",
      "Step 2900: 1.39221e-02, stepsizes = 4.5e-04/1.8e-06: d. reduced by 0.00% (4.9017e-08) (took 1.41962 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.12 (100), 0.37 (30)\n",
      "  Success rate too low, decreasing source step:  0.38 ( 50), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.31 (100), 0.53 (30)\n",
      "Step 3000: 1.39198e-02, stepsizes = 3.0e-04/1.2e-06: d. reduced by 0.00% (2.1874e-08) (took 0.78424 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.12 (100), 0.20 (30)\n",
      "Step 3100: 1.39181e-02, stepsizes = 2.0e-04/7.8e-07: d. reduced by 0.00% (2.1734e-08) (took 0.72411 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 2121.51083\n",
      "   1.5% for generation (31.27089)\n",
      "   11.0% for spherical prediction (233.76282)\n",
      "   78.4% for prediction (1663.04005)\n",
      "   0.0% for hyperparameter update (0.70925)\n",
      "   9.1% for the rest (192.72783)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.52754774e-03 6.15931780e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.44688775e-05 1.35511445e-05\n",
      " 1.32234602e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01606127 0.01572055\n",
      " 0.01686603]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.2875 0.1    0.0725 0.06   0.05   0.05   0.035  0.0275 0.0225 0.0375\n",
      " 0.0175 0.0325 0.0275 0.0225 0.02   0.0175 0.03   0.015  0.0125 0.0075\n",
      " 0.005  0.0175 0.0075 0.0075 0.01   0.0075]\n",
      "Using batch size   1, an average step would have taken 2.48945 = 0.03290 + 2.45656 seconds\n",
      "Using batch size   2, an average step would have taken 1.68599 = 0.00882 + 1.67716 seconds\n",
      "Using batch size   3, an average step would have taken 1.04995 = 0.00350 + 1.04645 seconds\n",
      "Using batch size   4, an average step would have taken 1.04565 = 0.00437 + 1.04128 seconds\n",
      "Using batch size   5, an average step would have taken 0.76480 = 0.00522 + 0.75958 seconds\n",
      "Using batch size   6, an average step would have taken 0.77977 = 0.00155 + 0.77821 seconds\n",
      "Using batch size   7, an average step would have taken 0.70108 = 0.00146 + 0.69962 seconds\n",
      "Using batch size   8, an average step would have taken 0.71337 = 0.00211 + 0.71126 seconds\n",
      "Using batch size   9, an average step would have taken 0.57485 = 0.00150 + 0.57334 seconds\n",
      "Using batch size  10, an average step would have taken 0.56357 = 0.00151 + 0.56206 seconds\n",
      "Using batch size  11, an average step would have taken 0.55239 = 0.00083 + 0.55156 seconds\n",
      "Using batch size  12, an average step would have taken 0.56158 = 0.00368 + 0.55789 seconds\n",
      "Using batch size  13, an average step would have taken 0.67601 = 0.00163 + 0.67438 seconds\n",
      "Using batch size  14, an average step would have taken 0.63098 = 0.00199 + 0.62900 seconds\n",
      "Using batch size  15, an average step would have taken 0.68869 = 0.00168 + 0.68701 seconds\n",
      "Using batch size  16, an average step would have taken 0.61303 = 0.00064 + 0.61239 seconds\n",
      "Using batch size  17, an average step would have taken 0.62404 = 0.00050 + 0.62355 seconds\n",
      "Using batch size  18, an average step would have taken 0.71546 = 0.00070 + 0.71476 seconds\n",
      "Using batch size  19, an average step would have taken 0.69205 = 0.00062 + 0.69143 seconds\n",
      "Using batch size  20, an average step would have taken 0.44980 = 0.00099 + 0.44881 seconds\n",
      "Using batch size  21, an average step would have taken 0.45049 = 0.00093 + 0.44956 seconds\n",
      "Using batch size  22, an average step would have taken 0.48634 = 0.00071 + 0.48562 seconds\n",
      "Using batch size  23, an average step would have taken 0.44554 = 0.00071 + 0.44483 seconds\n",
      "Using batch size  24, an average step would have taken 0.43404 = 0.00107 + 0.43297 seconds\n",
      "Using batch size  25, an average step would have taken 0.42198 = 0.00033 + 0.42165 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42198\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 5.9x\n",
      "improvement compared to smallest batch size (1): 5.9x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 800 steps, after step 3900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Success rate too low, decreasing source step:  0.27 ( 75), 0.17 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.40 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.30 (30)\n",
      "Step 3200: 1.39172e-02, stepsizes = 8.9e-05/2.3e-07:  (took 0.52095 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.11 (100), 0.20 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.37 (30)\n",
      "Step 3300: 1.39169e-02, stepsizes = 4.0e-05/1.0e-07:  (took 0.54615 seconds)\n",
      "  Success rate too low, decreasing source step:  0.28 ( 50), 0.13 (30)\n",
      "Step 3320: 1.39168e-02, stepsizes = 4.0e-05/6.9e-08: \n",
      "Looks like attack has converged after 3321 steps for the first time. Resetting steps to be sure.\n",
      "  Boundary too non-linear, decreasing steps: 0.14 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 1)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 3)\n",
      "  Boundary too non-linear, decreasing steps: 0.07 (100), 0.00 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.00 (23)\n",
      "  Success rate too low, decreasing source step:  0.26 ( 50), 0.00 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.00 ( 2)\n",
      "  Success rate too low, decreasing source step:  0.29 (100), 0.00 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.00 (19)\n",
      "  Success rate too low, decreasing source step:  0.22 ( 50), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.28 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.32 (100), 0.00 (30)\n",
      "Step 3400: 1.39168e-02, stepsizes = 2.3e-05/2.0e-06:  (took 0.66678 seconds)\n",
      "  Success rate too low, decreasing source step:  0.32 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.25 (100), 0.00 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.00 (18)\n",
      "  Success rate too low, decreasing source step:  0.28 ( 50), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.31 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.31 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.29 (100), 0.00 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.00 (14)\n",
      "Step 3430: 1.39168e-02, stepsizes = 1.0e-05/7.8e-08: \n",
      "Looks like attack has converged after 3431 steps, 100 remaining\n",
      "Step 3431: 1.39168e-02, stepsizes = 1.0e-05/7.8e-08: \n",
      "Looks like attack has converged after 3432 steps, 99 remaining\n",
      "  Success rate too low, decreasing source step:  0.38 ( 50), 0.00 (30)\n",
      "Step 3432: 1.39168e-02, stepsizes = 1.0e-05/5.2e-08: \n",
      "Looks like attack has converged after 3433 steps, 98 remaining\n",
      "Step 3433: 1.39168e-02, stepsizes = 1.0e-05/5.2e-08: \n",
      "Looks like attack has converged after 3434 steps, 97 remaining\n",
      "Step 3434: 1.39168e-02, stepsizes = 1.0e-05/5.2e-08: \n",
      "Looks like attack has converged after 3435 steps, 96 remaining\n",
      "Step 3435: 1.39168e-02, stepsizes = 1.0e-05/5.2e-08: \n",
      "Looks like attack has converged after 3436 steps, 95 remaining\n",
      "  Success rate too low, decreasing source step:  0.32 (100), 0.00 (30)\n",
      "Step 3436: 1.39168e-02, stepsizes = 1.0e-05/3.5e-08: \n",
      "Looks like attack has converged after 3437 steps, 94 remaining\n",
      "Step 3437: 1.39168e-02, stepsizes = 1.0e-05/3.5e-08: \n",
      "Looks like attack has converged after 3438 steps, 93 remaining\n",
      "Step 3438: 1.39168e-02, stepsizes = 1.0e-05/3.5e-08: \n",
      "Looks like attack has converged after 3439 steps, 92 remaining\n",
      "Step 3439: 1.39168e-02, stepsizes = 1.0e-05/3.5e-08: \n",
      "Looks like attack has converged after 3440 steps, 91 remaining\n",
      "  Success rate too low, decreasing source step:  0.31 (100), 0.00 (30)\n",
      "Step 3440: 1.39168e-02, stepsizes = 1.0e-05/2.3e-08: \n",
      "Looks like attack has converged after 3441 steps, 90 remaining\n",
      "Step 3441: 1.39168e-02, stepsizes = 1.0e-05/2.3e-08: \n",
      "Looks like attack has converged after 3442 steps, 89 remaining\n",
      "Step 3442: 1.39168e-02, stepsizes = 1.0e-05/2.3e-08: \n",
      "Looks like attack has converged after 3443 steps, 88 remaining\n",
      "Step 3443: 1.39168e-02, stepsizes = 1.0e-05/2.3e-08: \n",
      "Looks like attack has converged after 3444 steps, 87 remaining\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.07 (30)\n",
      "Step 3444: 1.39168e-02, stepsizes = 1.0e-05/1.5e-08: \n",
      "Looks like attack has converged after 3445 steps, 86 remaining\n",
      "Step 3445: 1.39168e-02, stepsizes = 1.0e-05/1.5e-08: \n",
      "Looks like attack has converged after 3446 steps, 85 remaining\n",
      "Step 3446: 1.39168e-02, stepsizes = 1.0e-05/1.5e-08: \n",
      "Looks like attack has converged after 3447 steps, 84 remaining\n",
      "Step 3447: 1.39168e-02, stepsizes = 1.0e-05/1.5e-08: \n",
      "Looks like attack has converged after 3448 steps, 83 remaining\n",
      "Step 3448: 1.39168e-02, stepsizes = 1.0e-05/1.5e-08: \n",
      "Looks like attack has converged after 3449 steps, 82 remaining\n",
      "  Success rate too low, decreasing source step:  0.25 (100), 0.00 (30)\n",
      "Step 3449: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3450 steps, 81 remaining\n",
      "Step 3450: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3451 steps, 80 remaining\n",
      "Step 3451: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3452 steps, 79 remaining\n",
      "Step 3452: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3453 steps, 78 remaining\n",
      "Step 3453: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3454 steps, 77 remaining\n",
      "Step 3454: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3455 steps, 76 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.03 (30)\n",
      "  Success rate too low, decreasing source step:  0.18 (100), 0.03 (30)\n",
      "Step 3455: 1.39168e-02, stepsizes = 6.8e-06/4.6e-09: \n",
      "Looks like attack has converged after 3456 steps, 75 remaining\n",
      "Step 3456: 1.39168e-02, stepsizes = 6.8e-06/4.6e-09: \n",
      "Looks like attack has converged after 3457 steps, 74 remaining\n",
      "Step 3457: 1.39168e-02, stepsizes = 6.8e-06/4.6e-09: \n",
      "Looks like attack has converged after 3458 steps, 73 remaining\n",
      "  Success rate too low, decreasing source step:  0.48 ( 75), 0.03 (30)\n",
      "Step 3458: 1.39168e-02, stepsizes = 6.8e-06/3.1e-09: \n",
      "Looks like attack has converged after 3459 steps, 72 remaining\n",
      "Step 3459: 1.39168e-02, stepsizes = 6.8e-06/3.1e-09: \n",
      "Looks like attack has converged after 3460 steps, 71 remaining\n",
      "  Boundary too linear, increasing steps:     0.55 (100), 0.31 (29)\n",
      "Step 3460: 1.39168e-02, stepsizes = 1.0e-05/4.6e-09: \n",
      "Looks like attack has converged after 3461 steps, 70 remaining\n",
      "Step 3461: 1.39168e-02, stepsizes = 1.0e-05/4.6e-09: \n",
      "Looks like attack has converged after 3462 steps, 69 remaining\n",
      "Step 3462: 1.39168e-02, stepsizes = 1.0e-05/4.6e-09: \n",
      "Looks like attack has converged after 3463 steps, 68 remaining\n",
      "Step 3463: 1.39168e-02, stepsizes = 1.0e-05/4.6e-09: \n",
      "Looks like attack has converged after 3464 steps, 67 remaining\n",
      "  Success rate too low, decreasing source step:  0.45 (100), 0.17 (30)\n",
      "Step 3464: 1.39168e-02, stepsizes = 1.0e-05/3.1e-09: \n",
      "Looks like attack has converged after 3465 steps, 66 remaining\n",
      "Step 3465: 1.39168e-02, stepsizes = 1.0e-05/3.1e-09: \n",
      "Looks like attack has converged after 3466 steps, 65 remaining\n",
      "Step 3466: 1.39168e-02, stepsizes = 1.0e-05/3.1e-09: \n",
      "Looks like attack has converged after 3467 steps, 64 remaining\n",
      "Step 3467: 1.39168e-02, stepsizes = 1.0e-05/3.1e-09: \n",
      "Looks like attack has converged after 3468 steps, 63 remaining\n",
      "  Success rate too low, decreasing source step:  0.35 (100), 0.07 (30)\n",
      "Step 3468: 1.39168e-02, stepsizes = 1.0e-05/2.0e-09: \n",
      "Looks like attack has converged after 3469 steps, 62 remaining\n",
      "Step 3469: 1.39168e-02, stepsizes = 1.0e-05/2.0e-09: \n",
      "Looks like attack has converged after 3470 steps, 61 remaining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3470: 1.39168e-02, stepsizes = 1.0e-05/2.0e-09: \n",
      "Looks like attack has converged after 3471 steps, 60 remaining\n",
      "  Boundary too linear, increasing steps:     0.59 (100), 0.23 (30)\n",
      "Step 3471: 1.39168e-02, stepsizes = 1.5e-05/3.1e-09: \n",
      "Looks like attack has converged after 3472 steps, 59 remaining\n",
      "Step 3472: 1.39168e-02, stepsizes = 1.5e-05/3.1e-09: \n",
      "Looks like attack has converged after 3473 steps, 58 remaining\n",
      "  Success rate too high, increasing source step: 0.68 ( 50), 0.53 (30)\n",
      "Step 3473: 1.39168e-02, stepsizes = 1.5e-05/4.6e-09: \n",
      "Looks like attack has converged after 3474 steps, 57 remaining\n",
      "Step 3474: 1.39168e-02, stepsizes = 1.5e-05/4.6e-09: \n",
      "Looks like attack has converged after 3475 steps, 56 remaining\n",
      "  Boundary too linear, increasing steps:     0.67 (100), 0.77 (30)\n",
      "  Success rate too high, increasing source step: 0.67 (100), 0.77 (30)\n",
      "Step 3475: 1.39168e-02, stepsizes = 2.3e-05/1.0e-08: \n",
      "Looks like attack has converged after 3476 steps, 55 remaining\n",
      "Step 3476: 1.39168e-02, stepsizes = 2.3e-05/1.0e-08: \n",
      "Looks like attack has converged after 3477 steps, 54 remaining\n",
      "  Success rate too high, increasing source step: 0.76 ( 50), 0.80 (30)\n",
      "Step 3477: 1.39168e-02, stepsizes = 2.3e-05/1.5e-08: \n",
      "Looks like attack has converged after 3478 steps, 53 remaining\n",
      "Step 3478: 1.39168e-02, stepsizes = 2.3e-05/1.5e-08: \n",
      "Looks like attack has converged after 3479 steps, 52 remaining\n",
      "  Boundary too linear, increasing steps:     0.65 (100), 0.67 (27)\n",
      "Step 3479: 1.39168e-02, stepsizes = 3.4e-05/2.3e-08: \n",
      "Looks like attack has converged after 3480 steps, 51 remaining\n",
      "  Success rate too high, increasing source step: 0.28 ( 25), 0.63 (30)\n",
      "Step 3480: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3481 steps, 50 remaining\n",
      "Step 3481: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3482 steps, 49 remaining\n",
      "Step 3482: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3483 steps, 48 remaining\n",
      "Step 3483: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3484 steps, 47 remaining\n",
      "Step 3484: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3485 steps, 46 remaining\n",
      "Step 3485: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3486 steps, 45 remaining\n",
      "Step 3486: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3487 steps, 44 remaining\n",
      "Step 3487: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3488 steps, 43 remaining\n",
      "Step 3488: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3489 steps, 42 remaining\n",
      "Step 3489: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3490 steps, 41 remaining\n",
      "  Success rate too high, increasing source step: 0.37 (100), 0.53 (30)\n",
      "Step 3490: 1.39168e-02, stepsizes = 3.4e-05/5.2e-08: \n",
      "Looks like attack has converged after 3491 steps, 40 remaining\n",
      "Step 3491: 1.39168e-02, stepsizes = 3.4e-05/5.2e-08: \n",
      "Looks like attack has converged after 3492 steps, 39 remaining\n",
      "Step 3492: 1.39168e-02, stepsizes = 3.4e-05/5.2e-08: \n",
      "Looks like attack has converged after 3493 steps, 38 remaining\n",
      "  Boundary too linear, increasing steps:     0.61 (100), 0.53 (30)\n",
      "  Success rate too high, increasing source step: 0.61 (100), 0.53 (30)\n",
      "Step 3493: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3494 steps, 37 remaining\n",
      "Step 3494: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3495 steps, 36 remaining\n",
      "Step 3495: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3496 steps, 35 remaining\n",
      "Step 3496: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3497 steps, 34 remaining\n",
      "Step 3497: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3498 steps, 33 remaining\n",
      "Step 3498: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3499 steps, 32 remaining\n",
      "Step 3499: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3500 steps, 31 remaining\n",
      "Step 3500: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: d. reduced by 0.00% (3.2660e-09) (took 0.58886 seconds)\n",
      "Step 3500: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3501 steps, 30 remaining\n",
      "Step 3501: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3502 steps, 29 remaining\n",
      "Step 3502: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3503 steps, 28 remaining\n",
      "Step 3503: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3504 steps, 27 remaining\n",
      "  Success rate too low, decreasing source step:  0.31 (100), 0.13 (30)\n",
      "Step 3504: 1.39168e-02, stepsizes = 5.1e-05/7.8e-08: \n",
      "Looks like attack has converged after 3505 steps, 26 remaining\n",
      "Step 3505: 1.39168e-02, stepsizes = 5.1e-05/7.8e-08: \n",
      "Looks like attack has converged after 3506 steps, 25 remaining\n",
      "Step 3506: 1.39168e-02, stepsizes = 5.1e-05/7.8e-08: \n",
      "Looks like attack has converged after 3507 steps, 24 remaining\n",
      "Step 3507: 1.39168e-02, stepsizes = 5.1e-05/7.8e-08: \n",
      "Looks like attack has converged after 3508 steps, 23 remaining\n",
      "Step 3508: 1.39168e-02, stepsizes = 5.1e-05/7.8e-08: \n",
      "Looks like attack has converged after 3509 steps, 22 remaining\n",
      "  Success rate too high, increasing source step: 0.37 (100), 0.53 (30)\n",
      "Step 3509: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3510 steps, 21 remaining\n",
      "Step 3510: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3511 steps, 20 remaining\n",
      "Step 3511: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3512 steps, 19 remaining\n",
      "Step 3512: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3513 steps, 18 remaining\n",
      "Step 3513: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3514 steps, 17 remaining\n",
      "Step 3514: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3515 steps, 16 remaining\n",
      "Step 3515: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3516 steps, 15 remaining\n",
      "Step 3516: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3517 steps, 14 remaining\n",
      "  Success rate too high, increasing source step: 0.41 (100), 0.53 (30)\n",
      "Step 3517: 1.39167e-02, stepsizes = 5.1e-05/1.8e-07: \n",
      "Looks like attack has converged after 3518 steps, 13 remaining\n",
      "Step 3518: 1.39167e-02, stepsizes = 5.1e-05/1.8e-07: \n",
      "Looks like attack has converged after 3519 steps, 12 remaining\n",
      "Step 3519: 1.39167e-02, stepsizes = 5.1e-05/1.8e-07: \n",
      "Looks like attack has converged after 3520 steps, 11 remaining\n",
      "  Boundary too linear, increasing steps:     0.51 (100), 0.30 (30)\n",
      "Step 3520: 1.39167e-02, stepsizes = 7.7e-05/2.6e-07: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/foolbox/attacks/boundary_attack.py:357: UserWarning: Attack has not converged!\n",
      "  warnings.warn('Attack has not converged!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.07 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "Step 3600: 1.39167e-02, stepsizes = 3.4e-05/1.2e-07:  (took 0.44585 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.27 (30)\n",
      "Step 3640: 1.39167e-02, stepsizes = 2.3e-05/7.8e-08: \n",
      "Looks like attack has converged after 3641 steps for the first time. Resetting steps to be sure.\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.20 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.20 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.20 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.20 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.20 (30)\n",
      "  Success rate too low, decreasing source step:  0.04 ( 25), 0.17 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), -1.00 ( 0)\n",
      "Step 3700: 1.39167e-02, stepsizes = 2.3e-05/1.5e-05:  (took 0.43982 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 2)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.00 ( 2)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 4)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 5)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 6)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 7)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.00 ( 7)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 9)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.00 ( 9)\n",
      "  Boundary too non-linear, decreasing steps: 0.05 (100), 0.00 (14)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 (15)\n",
      "  Boundary too non-linear, decreasing steps: 0.08 (100), 0.00 (23)\n",
      "  Success rate too low, decreasing source step:  0.12 ( 75), 0.00 (30)\n",
      "Step 3751: 1.39167e-02, stepsizes = 1.8e-07/7.8e-08: \n",
      "Looks like attack has converged after 3752 steps, 100 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.10 (100), 0.00 ( 1)\n",
      "Step 3752: 1.39167e-02, stepsizes = 1.2e-07/5.2e-08: \n",
      "Looks like attack has converged after 3753 steps, 99 remaining\n",
      "Step 3753: 1.39167e-02, stepsizes = 1.2e-07/5.2e-08: \n",
      "Looks like attack has converged after 3754 steps, 98 remaining\n",
      "Step 3754: 1.39167e-02, stepsizes = 1.2e-07/5.2e-08: \n",
      "Looks like attack has converged after 3755 steps, 97 remaining\n",
      "Step 3755: 1.39167e-02, stepsizes = 1.2e-07/5.2e-08: \n",
      "Looks like attack has converged after 3756 steps, 96 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.06 (100), 0.00 ( 7)\n",
      "Step 3756: 1.39167e-02, stepsizes = 7.8e-08/3.5e-08: \n",
      "Looks like attack has converged after 3757 steps, 95 remaining\n",
      "Step 3757: 1.39167e-02, stepsizes = 7.8e-08/3.5e-08: \n",
      "Looks like attack has converged after 3758 steps, 94 remaining\n",
      "Step 3758: 1.39167e-02, stepsizes = 7.8e-08/3.5e-08: \n",
      "Looks like attack has converged after 3759 steps, 93 remaining\n",
      "Step 3759: 1.39167e-02, stepsizes = 7.8e-08/3.5e-08: \n",
      "Looks like attack has converged after 3760 steps, 92 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.00 (20)\n",
      "Step 3760: 1.39167e-02, stepsizes = 5.2e-08/2.3e-08: \n",
      "Looks like attack has converged after 3761 steps, 91 remaining\n",
      "Step 3761: 1.39167e-02, stepsizes = 5.2e-08/2.3e-08: \n",
      "Looks like attack has converged after 3762 steps, 90 remaining\n",
      "Step 3762: 1.39167e-02, stepsizes = 5.2e-08/2.3e-08: \n",
      "Looks like attack has converged after 3763 steps, 89 remaining\n",
      "Step 3763: 1.39167e-02, stepsizes = 5.2e-08/2.3e-08: \n",
      "Looks like attack has converged after 3764 steps, 88 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.13 (100), 0.00 (30)\n",
      "Step 3764: 1.39167e-02, stepsizes = 3.5e-08/1.0e-08: \n",
      "Looks like attack has converged after 3765 steps, 87 remaining\n",
      "Step 3765: 1.39167e-02, stepsizes = 3.5e-08/1.0e-08: \n",
      "Looks like attack has converged after 3766 steps, 86 remaining\n",
      "Step 3766: 1.39167e-02, stepsizes = 3.5e-08/1.0e-08: \n",
      "Looks like attack has converged after 3767 steps, 85 remaining\n",
      "Step 3767: 1.39167e-02, stepsizes = 3.5e-08/1.0e-08: \n",
      "Looks like attack has converged after 3768 steps, 84 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.09 (100), 0.00 ( 9)\n",
      "Step 3768: 1.39167e-02, stepsizes = 2.3e-08/6.9e-09: \n",
      "Looks like attack has converged after 3769 steps, 83 remaining\n",
      "Step 3769: 1.39167e-02, stepsizes = 2.3e-08/6.9e-09: \n",
      "Looks like attack has converged after 3770 steps, 82 remaining\n",
      "Step 3770: 1.39167e-02, stepsizes = 2.3e-08/6.9e-09: \n",
      "Looks like attack has converged after 3771 steps, 81 remaining\n",
      "Step 3771: 1.39167e-02, stepsizes = 2.3e-08/6.9e-09: \n",
      "Looks like attack has converged after 3772 steps, 80 remaining\n",
      "  Success rate too low, decreasing source step:  0.21 (100), 0.00 (30)\n",
      "Step 3772: 1.39167e-02, stepsizes = 2.3e-08/4.6e-09: \n",
      "Looks like attack has converged after 3773 steps, 79 remaining\n",
      "Step 3773: 1.39167e-02, stepsizes = 2.3e-08/4.6e-09: \n",
      "Looks like attack has converged after 3774 steps, 78 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.00 ( 8)\n",
      "Step 3774: 1.39167e-02, stepsizes = 1.5e-08/3.1e-09: \n",
      "Looks like attack has converged after 3775 steps, 77 remaining\n",
      "Step 3775: 1.39167e-02, stepsizes = 1.5e-08/3.1e-09: \n",
      "Looks like attack has converged after 3776 steps, 76 remaining\n",
      "Step 3776: 1.39167e-02, stepsizes = 1.5e-08/3.1e-09: \n",
      "Looks like attack has converged after 3777 steps, 75 remaining\n",
      "Step 3777: 1.39167e-02, stepsizes = 1.5e-08/3.1e-09: \n",
      "Looks like attack has converged after 3778 steps, 74 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.00 (26)\n",
      "Step 3778: 1.39167e-02, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 3779 steps, 73 remaining\n",
      "  Success rate too low, decreasing source step:  0.20 ( 25), 0.00 (30)\n",
      "Step 3779: 1.39167e-02, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 3780 steps, 72 remaining\n",
      "Step 3780: 1.39167e-02, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 3781 steps, 71 remaining\n",
      "Step 3781: 1.39167e-02, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 3782 steps, 70 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.00 (13)\n",
      "Step 3782: 1.39167e-02, stepsizes = 6.9e-09/9.0e-10: \n",
      "Looks like attack has converged after 3783 steps, 69 remaining\n",
      "Step 3783: 1.39167e-02, stepsizes = 6.9e-09/9.0e-10: \n",
      "Looks like attack has converged after 3784 steps, 68 remaining\n",
      "Step 3784: 1.39167e-02, stepsizes = 6.9e-09/9.0e-10: \n",
      "Looks like attack has converged after 3785 steps, 67 remaining\n",
      "  Success rate too low, decreasing source step:  0.27 ( 75), 0.00 (30)\n",
      "Step 3785: 1.39167e-02, stepsizes = 6.9e-09/6.0e-10: \n",
      "Looks like attack has converged after 3786 steps, 66 remaining\n",
      "Step 3786: 1.39167e-02, stepsizes = 6.9e-09/6.0e-10: \n",
      "Looks like attack has converged after 3787 steps, 65 remaining\n",
      "Step 3787: 1.39167e-02, stepsizes = 6.9e-09/6.0e-10: \n",
      "Looks like attack has converged after 3788 steps, 64 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.00 ( 9)\n",
      "Step 3788: 1.39167e-02, stepsizes = 4.6e-09/4.0e-10: \n",
      "Looks like attack has converged after 3789 steps, 63 remaining\n",
      "Step 3789: 1.39167e-02, stepsizes = 4.6e-09/4.0e-10: \n",
      "Looks like attack has converged after 3790 steps, 62 remaining\n",
      "Step 3790: 1.39167e-02, stepsizes = 4.6e-09/4.0e-10: \n",
      "Looks like attack has converged after 3791 steps, 61 remaining\n",
      "Step 3791: 1.39167e-02, stepsizes = 4.6e-09/4.0e-10: \n",
      "Looks like attack has converged after 3792 steps, 60 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.14 (100), 0.00 (23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3792: 1.39167e-02, stepsizes = 3.1e-09/2.7e-10: \n",
      "Looks like attack has converged after 3793 steps, 59 remaining\n",
      "Step 3793: 1.39167e-02, stepsizes = 3.1e-09/2.7e-10: \n",
      "Looks like attack has converged after 3794 steps, 58 remaining\n",
      "  Success rate too low, decreasing source step:  0.16 ( 50), 0.00 (30)\n",
      "Step 3794: 1.39167e-02, stepsizes = 3.1e-09/1.8e-10: \n",
      "Looks like attack has converged after 3795 steps, 57 remaining\n",
      "Step 3795: 1.39167e-02, stepsizes = 3.1e-09/1.8e-10: \n",
      "Looks like attack has converged after 3796 steps, 56 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.16 (100), 0.00 ( 8)\n",
      "Step 3796: 1.39167e-02, stepsizes = 2.0e-09/1.2e-10: \n",
      "Looks like attack has converged after 3797 steps, 55 remaining\n",
      "Step 3797: 1.39167e-02, stepsizes = 2.0e-09/1.2e-10: \n",
      "Looks like attack has converged after 3798 steps, 54 remaining\n",
      "Step 3798: 1.39167e-02, stepsizes = 2.0e-09/1.2e-10: \n",
      "Looks like attack has converged after 3799 steps, 53 remaining\n",
      "Step 3799: 1.39167e-02, stepsizes = 2.0e-09/1.2e-10: \n",
      "Looks like attack has converged after 3800 steps, 52 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.00 (23)\n",
      "Step 3800: 1.39167e-02, stepsizes = 1.4e-09/7.9e-11:  (took 0.70639 seconds)\n",
      "Step 3800: 1.39167e-02, stepsizes = 1.4e-09/7.9e-11: \n",
      "Looks like attack has converged after 3801 steps, 51 remaining\n",
      "Step 3801: 1.39167e-02, stepsizes = 1.4e-09/7.9e-11: \n",
      "Looks like attack has converged after 3802 steps, 50 remaining\n",
      "  Success rate too low, decreasing source step:  0.20 ( 50), 0.00 (30)\n",
      "Step 3802: 1.39167e-02, stepsizes = 1.4e-09/5.3e-11: \n",
      "Looks like attack has converged after 3803 steps, 49 remaining\n",
      "Step 3803: 1.39167e-02, stepsizes = 1.4e-09/5.3e-11: \n",
      "Looks like attack has converged after 3804 steps, 48 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.00 ( 5)\n",
      "Step 3804: 1.39167e-02, stepsizes = 9.0e-10/3.5e-11: \n",
      "Looks like attack has converged after 3805 steps, 47 remaining\n",
      "Step 3805: 1.39167e-02, stepsizes = 9.0e-10/3.5e-11: \n",
      "Looks like attack has converged after 3806 steps, 46 remaining\n",
      "Step 3806: 1.39167e-02, stepsizes = 9.0e-10/3.5e-11: \n",
      "Looks like attack has converged after 3807 steps, 45 remaining\n",
      "Step 3807: 1.39167e-02, stepsizes = 9.0e-10/3.5e-11: \n",
      "Looks like attack has converged after 3808 steps, 44 remaining\n",
      "Step 3808: 1.39167e-02, stepsizes = 9.0e-10/3.5e-11: \n",
      "Looks like attack has converged after 3809 steps, 43 remaining\n",
      "  Success rate too low, decreasing source step:  0.20 (100), 0.00 (30)\n",
      "Step 3809: 1.39167e-02, stepsizes = 9.0e-10/2.4e-11: \n",
      "Looks like attack has converged after 3810 steps, 42 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.16 (100), 0.00 ( 4)\n",
      "Step 3810: 1.39167e-02, stepsizes = 6.0e-10/1.6e-11: \n",
      "Looks like attack has converged after 3811 steps, 41 remaining\n",
      "Step 3811: 1.39167e-02, stepsizes = 6.0e-10/1.6e-11: \n",
      "Looks like attack has converged after 3812 steps, 40 remaining\n",
      "Step 3812: 1.39167e-02, stepsizes = 6.0e-10/1.6e-11: \n",
      "Looks like attack has converged after 3813 steps, 39 remaining\n",
      "Step 3813: 1.39167e-02, stepsizes = 6.0e-10/1.6e-11: \n",
      "Looks like attack has converged after 3814 steps, 38 remaining\n",
      "Step 3814: 1.39167e-02, stepsizes = 6.0e-10/1.6e-11: \n",
      "Looks like attack has converged after 3815 steps, 37 remaining\n",
      "  Success rate too low, decreasing source step:  0.21 (100), 0.00 (30)\n",
      "Step 3815: 1.39167e-02, stepsizes = 6.0e-10/1.0e-11: \n",
      "Looks like attack has converged after 3816 steps, 36 remaining\n",
      "Step 3816: 1.39167e-02, stepsizes = 6.0e-10/1.0e-11: \n",
      "Looks like attack has converged after 3817 steps, 35 remaining\n",
      "Step 3817: 1.39167e-02, stepsizes = 6.0e-10/1.0e-11: \n",
      "Looks like attack has converged after 3818 steps, 34 remaining\n",
      "Step 3818: 1.39167e-02, stepsizes = 6.0e-10/1.0e-11: \n",
      "Looks like attack has converged after 3819 steps, 33 remaining\n",
      "Step 3819: 1.39167e-02, stepsizes = 6.0e-10/1.0e-11: \n",
      "Looks like attack has converged after 3820 steps, 32 remaining\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.00 (30)\n",
      "Step 3820: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3821 steps, 31 remaining\n",
      "Step 3821: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3822 steps, 30 remaining\n",
      "Step 3822: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3823 steps, 29 remaining\n",
      "Step 3823: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3824 steps, 28 remaining\n",
      "Step 3824: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3825 steps, 27 remaining\n",
      "Step 3825: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3826 steps, 26 remaining\n",
      "  Success rate too low, decreasing source step:  0.26 (100), 0.00 (30)\n",
      "Step 3826: 1.39167e-02, stepsizes = 6.0e-10/4.6e-12: \n",
      "Looks like attack has converged after 3827 steps, 25 remaining\n",
      "Step 3827: 1.39167e-02, stepsizes = 6.0e-10/4.6e-12: \n",
      "Looks like attack has converged after 3828 steps, 24 remaining\n",
      "Step 3828: 1.39167e-02, stepsizes = 6.0e-10/4.6e-12: \n",
      "Looks like attack has converged after 3829 steps, 23 remaining\n",
      "Step 3829: 1.39167e-02, stepsizes = 6.0e-10/4.6e-12: \n",
      "Looks like attack has converged after 3830 steps, 22 remaining\n",
      "Step 3830: 1.39167e-02, stepsizes = 6.0e-10/4.6e-12: \n",
      "Looks like attack has converged after 3831 steps, 21 remaining\n",
      "  Success rate too low, decreasing source step:  0.29 (100), 0.00 (30)\n",
      "Step 3831: 1.39167e-02, stepsizes = 6.0e-10/3.1e-12: \n",
      "Looks like attack has converged after 3832 steps, 20 remaining\n",
      "Step 3832: 1.39167e-02, stepsizes = 6.0e-10/3.1e-12: \n",
      "Looks like attack has converged after 3833 steps, 19 remaining\n",
      "Step 3833: 1.39167e-02, stepsizes = 6.0e-10/3.1e-12: \n",
      "Looks like attack has converged after 3834 steps, 18 remaining\n",
      "Step 3834: 1.39167e-02, stepsizes = 6.0e-10/3.1e-12: \n",
      "Looks like attack has converged after 3835 steps, 17 remaining\n",
      "Step 3835: 1.39167e-02, stepsizes = 6.0e-10/3.1e-12: \n",
      "Looks like attack has converged after 3836 steps, 16 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.00 (28)\n",
      "Step 3836: 1.39167e-02, stepsizes = 4.0e-10/2.1e-12: \n",
      "Looks like attack has converged after 3837 steps, 15 remaining\n",
      "  Success rate too low, decreasing source step:  0.24 ( 25), 0.00 (30)\n",
      "Step 3837: 1.39167e-02, stepsizes = 4.0e-10/1.4e-12: \n",
      "Looks like attack has converged after 3838 steps, 14 remaining\n",
      "Step 3838: 1.39167e-02, stepsizes = 4.0e-10/1.4e-12: \n",
      "Looks like attack has converged after 3839 steps, 13 remaining\n",
      "Step 3839: 1.39167e-02, stepsizes = 4.0e-10/1.4e-12: \n",
      "Looks like attack has converged after 3840 steps, 12 remaining\n",
      "Step 3840: 1.39167e-02, stepsizes = 4.0e-10/1.4e-12: \n",
      "Looks like attack has converged after 3841 steps, 11 remaining\n",
      "Step 3841: 1.39167e-02, stepsizes = 4.0e-10/1.4e-12: \n",
      "Looks like attack has converged after 3842 steps, 10 remaining\n",
      "  Success rate too low, decreasing source step:  0.28 (100), 0.00 (30)\n",
      "Step 3842: 1.39167e-02, stepsizes = 4.0e-10/9.2e-13: \n",
      "Looks like attack has converged after 3843 steps, 9 remaining\n",
      "Step 3843: 1.39167e-02, stepsizes = 4.0e-10/9.2e-13: \n",
      "Looks like attack has converged after 3844 steps, 8 remaining\n",
      "Step 3844: 1.39167e-02, stepsizes = 4.0e-10/9.2e-13: \n",
      "Looks like attack has converged after 3845 steps, 7 remaining\n",
      "Step 3845: 1.39167e-02, stepsizes = 4.0e-10/9.2e-13: \n",
      "Looks like attack has converged after 3846 steps, 6 remaining\n",
      "  Success rate too low, decreasing source step:  0.35 (100), 0.00 (30)\n",
      "Step 3846: 1.39167e-02, stepsizes = 4.0e-10/6.1e-13: \n",
      "Looks like attack has converged after 3847 steps, 5 remaining\n",
      "Step 3847: 1.39167e-02, stepsizes = 4.0e-10/6.1e-13: \n",
      "Looks like attack has converged after 3848 steps, 4 remaining\n",
      "Step 3848: 1.39167e-02, stepsizes = 4.0e-10/6.1e-13: \n",
      "Looks like attack has converged after 3849 steps, 3 remaining\n",
      "Step 3849: 1.39167e-02, stepsizes = 4.0e-10/6.1e-13: \n",
      "Looks like attack has converged after 3850 steps, 2 remaining\n",
      "Step 3850: 1.39167e-02, stepsizes = 4.0e-10/6.1e-13: \n",
      "Looks like attack has converged after 3851 steps, 1 remaining\n",
      "Time since beginning: 2527.47226\n",
      "   1.5% for generation (37.22626)\n",
      "   16.5% for spherical prediction (417.51385)\n",
      "   70.7% for prediction (1786.45050)\n",
      "   0.1% for hyperparameter update (1.26638)\n",
      "   11.3% for the rest (285.01528)\n"
     ]
    }
   ],
   "source": [
    "attack_params = {\n",
    "    'iterations': 10000,\n",
    "    'max_directions': 25,\n",
    "    'starting_point': None,\n",
    "    'initialization_attack': None,\n",
    "    'log_every_n_steps': 100,\n",
    "    'spherical_step': 1.0,\n",
    "    'source_step': 0.1,\n",
    "    'step_adaptation': 1.5,\n",
    "    'batch_size': 1,\n",
    "    'tune_batch_size': True, \n",
    "    'threaded_rnd': True, \n",
    "    'threaded_gen': True, \n",
    "    'alternative_generator': False\n",
    "}\n",
    "\n",
    "num = 1\n",
    "x_adv = np.zeros_like(x_test[:num].numpy())\n",
    "for i in range(num):\n",
    "    x_adv[i] = attack(x_test[i].numpy(), label=y_test[i].numpy(), \n",
    "                      unpack=True, verbose=True, **attack_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = dknn.classify(torch.tensor(x_adv))\n",
    "print((y_pred.argmax(1) == y_test[:num].numpy()).sum() / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3031292\n"
     ]
    }
   ],
   "source": [
    "dist = np.sqrt(np.sum((x_adv - x_test[:num].numpy())**2, (1, 2, 3)))\n",
    "print(dist.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f35985edcc0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[0].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neither starting_point nor initialization_attack given. Falling back to BlendedUniformNoiseAttack for initialization.\n",
      "Initial spherical_step = 0.50, source_step = 0.05\n",
      "Using 4 threads to create random numbers\n",
      "Step 0: 1.18628e-01, stepsizes = 5.0e-01/5.0e-02: \n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.33 ( 3)\n",
      "  Boundary too non-linear, decreasing steps: 0.06 (100), 0.22 ( 9)\n",
      "Step 100: 5.23035e-02, stepsizes = 2.2e-01/2.2e-02:  (took 6.82330 seconds)\n",
      "Initializing generation and prediction time measurements. This can take a few seconds.\n",
      "During initialization, a better adversarial has been found. Continuing from there.\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 769.55649\n",
      "   0.8% for generation (6.29083)\n",
      "   16.2% for spherical prediction (125.01746)\n",
      "   69.0% for prediction (531.29734)\n",
      "   0.0% for hyperparameter update (0.00170)\n",
      "   13.9% for the rest (106.94915)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 1.38908625e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 3.34980807e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 4.15630341e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01644211 0.0179603  0.01686172 0.02211609\n",
      " 0.02132043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.91 0.   0.   0.   0.   0.03 0.01 0.   0.   0.   0.   0.   0.   0.01\n",
      " 0.01 0.   0.01 0.01 0.   0.   0.   0.   0.01 0.   0.   0.  ]\n",
      "Using batch size   1, an average step would have taken 5.16179 = 0.06095 + 5.10084 seconds\n",
      "Using batch size   2, an average step would have taken 3.83510 = 0.00604 + 3.82905 seconds\n",
      "Using batch size   3, an average step would have taken 2.54300 = 0.01353 + 2.52947 seconds\n",
      "Using batch size   4, an average step would have taken 2.14147 = 0.00554 + 2.13593 seconds\n",
      "Using batch size   5, an average step would have taken 1.44762 = 0.00206 + 1.44556 seconds\n",
      "Using batch size   6, an average step would have taken 1.54937 = 0.00476 + 1.54461 seconds\n",
      "Using batch size   7, an average step would have taken 1.41372 = 0.00422 + 1.40950 seconds\n",
      "Using batch size   8, an average step would have taken 1.23135 = 0.00370 + 1.22764 seconds\n",
      "Using batch size   9, an average step would have taken 0.98253 = 0.00213 + 0.98040 seconds\n",
      "Using batch size  10, an average step would have taken 0.93446 = 0.00110 + 0.93336 seconds\n",
      "Using batch size  11, an average step would have taken 0.91873 = 0.00333 + 0.91540 seconds\n",
      "Using batch size  12, an average step would have taken 0.91625 = 0.00402 + 0.91223 seconds\n",
      "Using batch size  13, an average step would have taken 0.85709 = 0.00113 + 0.85596 seconds\n",
      "Using batch size  14, an average step would have taken 0.87574 = 0.00138 + 0.87436 seconds\n",
      "Using batch size  15, an average step would have taken 0.93166 = 0.00097 + 0.93069 seconds\n",
      "Using batch size  16, an average step would have taken 0.90052 = 0.00107 + 0.89944 seconds\n",
      "Using batch size  17, an average step would have taken 0.87820 = 0.00082 + 0.87738 seconds\n",
      "Using batch size  18, an average step would have taken 0.94862 = 0.00145 + 0.94717 seconds\n",
      "Using batch size  19, an average step would have taken 0.97661 = 0.00125 + 0.97536 seconds\n",
      "Using batch size  20, an average step would have taken 0.67108 = 0.00116 + 0.66992 seconds\n",
      "Using batch size  21, an average step would have taken 0.65594 = 0.00121 + 0.65473 seconds\n",
      "Using batch size  22, an average step would have taken 0.67406 = 0.00162 + 0.67244 seconds\n",
      "Using batch size  23, an average step would have taken 0.67710 = 0.00073 + 0.67637 seconds\n",
      "Using batch size  24, an average step would have taken 0.72881 = 0.00283 + 0.72598 seconds\n",
      "Using batch size  25, an average step would have taken 0.53405 = 0.00104 + 0.53301 seconds\n",
      "batch size was 1, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.53405\n",
      "improvement compared to old batch size (1): 9.7x\n",
      "improvement compared to worst batch size (1): 9.7x\n",
      "improvement compared to smallest batch size (1): 9.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 200\n",
      "  Boundary too non-linear, decreasing steps: 0.07 (100), 0.12 (16)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.07 (29)\n",
      "  Success rate too low, decreasing source step:  0.12 ( 25), 0.03 (30)\n",
      "Step 200: 3.35918e-02, stepsizes = 9.9e-02/6.6e-03:  (took 0.80635 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 819.75231\n",
      "   0.9% for generation (7.11420)\n",
      "   15.8% for spherical prediction (129.40454)\n",
      "   69.9% for prediction (573.16435)\n",
      "   0.0% for hyperparameter update (0.01590)\n",
      "   13.4% for the rest (110.05331)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 1.38908625e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 3.34980807e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 1.34550189e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01644211 0.0179603  0.01686172 0.02211609\n",
      " 0.018723  ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.87 0.01 0.01 0.   0.   0.   0.01 0.01 0.   0.   0.   0.01 0.   0.\n",
      " 0.   0.   0.01 0.   0.   0.02 0.   0.01 0.02 0.   0.   0.02]\n",
      "Using batch size   1, an average step would have taken 5.14659 = 0.06077 + 5.08582 seconds\n",
      "Using batch size   2, an average step would have taken 3.82441 = 0.00599 + 3.81842 seconds\n",
      "Using batch size   3, an average step would have taken 2.53866 = 0.01348 + 2.52518 seconds\n",
      "Using batch size   4, an average step would have taken 2.13039 = 0.00548 + 2.12491 seconds\n",
      "Using batch size   5, an average step would have taken 1.45366 = 0.00207 + 1.45159 seconds\n",
      "Using batch size   6, an average step would have taken 1.55207 = 0.00472 + 1.54735 seconds\n",
      "Using batch size   7, an average step would have taken 1.41416 = 0.00422 + 1.40993 seconds\n",
      "Using batch size   8, an average step would have taken 1.23058 = 0.00366 + 1.22692 seconds\n",
      "Using batch size   9, an average step would have taken 0.99014 = 0.00215 + 0.98799 seconds\n",
      "Using batch size  10, an average step would have taken 0.93446 = 0.00110 + 0.93336 seconds\n",
      "Using batch size  11, an average step would have taken 0.90934 = 0.00329 + 0.90605 seconds\n",
      "Using batch size  12, an average step would have taken 0.90824 = 0.00396 + 0.90428 seconds\n",
      "Using batch size  13, an average step would have taken 0.85709 = 0.00113 + 0.85596 seconds\n",
      "Using batch size  14, an average step would have taken 0.87901 = 0.00139 + 0.87762 seconds\n",
      "Using batch size  15, an average step would have taken 0.93501 = 0.00098 + 0.93403 seconds\n",
      "Using batch size  16, an average step would have taken 0.90374 = 0.00108 + 0.90266 seconds\n",
      "Using batch size  17, an average step would have taken 0.88536 = 0.00083 + 0.88453 seconds\n",
      "Using batch size  18, an average step would have taken 0.95623 = 0.00147 + 0.95476 seconds\n",
      "Using batch size  19, an average step would have taken 0.97661 = 0.00125 + 0.97536 seconds\n",
      "Using batch size  20, an average step would have taken 0.67108 = 0.00116 + 0.66992 seconds\n",
      "Using batch size  21, an average step would have taken 0.65257 = 0.00121 + 0.65136 seconds\n",
      "Using batch size  22, an average step would have taken 0.66793 = 0.00159 + 0.66634 seconds\n",
      "Using batch size  23, an average step would have taken 0.67075 = 0.00072 + 0.67003 seconds\n",
      "Using batch size  24, an average step would have taken 0.72447 = 0.00278 + 0.72169 seconds\n",
      "Using batch size  25, an average step would have taken 0.46841 = 0.00034 + 0.46808 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.46841\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 11.0x\n",
      "improvement compared to smallest batch size (1): 11.0x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.03 (30)\n",
      "  Success rate too low, decreasing source step:  0.19 (100), 0.03 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.16 (100), 0.18 (22)\n",
      "Step 300: 2.30754e-02, stepsizes = 4.4e-02/2.0e-03:  (took 1.00404 seconds)\n",
      "  Success rate too low, decreasing source step:  0.30 ( 50), 0.13 (30)\n",
      "Step 400: 1.86159e-02, stepsizes = 4.4e-02/1.3e-03: d. reduced by 0.26% (4.8519e-05) (took 0.63047 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 920.39514\n",
      "   0.9% for generation (8.68309)\n",
      "   15.0% for spherical prediction (138.45500)\n",
      "   71.3% for prediction (656.50427)\n",
      "   0.0% for hyperparameter update (0.03242)\n",
      "   12.7% for the rest (116.72037)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 1.38908625e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 3.34980807e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 1.28543917e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01644211 0.0179603  0.01686172 0.02211609\n",
      " 0.01858963]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.395 0.12  0.035 0.065 0.05  0.035 0.02  0.03  0.025 0.025 0.035 0.01\n",
      " 0.02  0.02  0.015 0.01  0.01  0.035 0.015 0.01  0.005 0.005 0.005 0.\n",
      " 0.005 0.   ]\n",
      "Using batch size   1, an average step would have taken 3.11704 = 0.03681 + 3.08024 seconds\n",
      "Using batch size   2, an average step would have taken 2.36001 = 0.00333 + 2.35668 seconds\n",
      "Using batch size   3, an average step would have taken 1.58457 = 0.00817 + 1.57641 seconds\n",
      "Using batch size   4, an average step would have taken 1.35419 = 0.00310 + 1.35108 seconds\n",
      "Using batch size   5, an average step would have taken 0.95198 = 0.00136 + 0.95063 seconds\n",
      "Using batch size   6, an average step would have taken 1.01161 = 0.00268 + 1.00894 seconds\n",
      "Using batch size   7, an average step would have taken 0.95095 = 0.00296 + 0.94799 seconds\n",
      "Using batch size   8, an average step would have taken 0.83517 = 0.00201 + 0.83316 seconds\n",
      "Using batch size   9, an average step would have taken 0.67637 = 0.00132 + 0.67505 seconds\n",
      "Using batch size  10, an average step would have taken 0.64636 = 0.00074 + 0.64563 seconds\n",
      "Using batch size  11, an average step would have taken 0.62867 = 0.00216 + 0.62651 seconds\n",
      "Using batch size  12, an average step would have taken 0.64678 = 0.00233 + 0.64445 seconds\n",
      "Using batch size  13, an average step would have taken 0.69574 = 0.00075 + 0.69499 seconds\n",
      "Using batch size  14, an average step would have taken 0.73042 = 0.00093 + 0.72949 seconds\n",
      "Using batch size  15, an average step would have taken 0.77928 = 0.00081 + 0.77847 seconds\n",
      "Using batch size  16, an average step would have taken 0.75372 = 0.00085 + 0.75286 seconds\n",
      "Using batch size  17, an average step would have taken 0.70650 = 0.00059 + 0.70590 seconds\n",
      "Using batch size  18, an average step would have taken 0.76021 = 0.00081 + 0.75940 seconds\n",
      "Using batch size  19, an average step would have taken 0.79883 = 0.00093 + 0.79790 seconds\n",
      "Using batch size  20, an average step would have taken 0.51695 = 0.00094 + 0.51601 seconds\n",
      "Using batch size  21, an average step would have taken 0.48243 = 0.00093 + 0.48151 seconds\n",
      "Using batch size  22, an average step would have taken 0.51790 = 0.00087 + 0.51702 seconds\n",
      "Using batch size  23, an average step would have taken 0.51522 = 0.00057 + 0.51466 seconds\n",
      "Using batch size  24, an average step would have taken 0.61702 = 0.00151 + 0.61551 seconds\n",
      "Using batch size  25, an average step would have taken 0.46506 = 0.00032 + 0.46474 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.46506\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.7x\n",
      "improvement compared to smallest batch size (1): 6.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 800\n",
      "Step 500: 1.51164e-02, stepsizes = 4.4e-02/1.3e-03: d. reduced by 0.26% (3.9398e-05) (took 0.92866 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.37 (30)\n",
      "  Boundary too linear, increasing steps:     0.51 (100), 0.27 (30)\n",
      "Step 600: 1.33413e-02, stepsizes = 4.4e-02/1.3e-03:  (took 0.61772 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.08 (100), 0.23 (30)\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.17 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), -1.00 ( 0)\n",
      "Step 700: 1.24061e-02, stepsizes = 2.0e-02/3.9e-04:  (took 0.51961 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.16 (100), 0.30 (30)\n",
      "Step 800: 1.16133e-02, stepsizes = 1.3e-02/2.6e-04: d. reduced by 0.05% (5.9694e-06) (took 0.84386 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1123.15431\n",
      "   1.1% for generation (11.92368)\n",
      "   14.0% for spherical prediction (157.32430)\n",
      "   73.2% for prediction (822.08008)\n",
      "   0.0% for hyperparameter update (0.11799)\n",
      "   11.7% for the rest (131.70826)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 1.38908625e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 3.34980807e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 1.29160137e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01644211 0.0179603  0.01686172 0.02211609\n",
      " 0.01848043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.29   0.1    0.0775 0.055  0.065  0.0425 0.04   0.0225 0.025  0.04\n",
      " 0.0225 0.02   0.01   0.0225 0.01   0.0225 0.02   0.0175 0.0175 0.02\n",
      " 0.0125 0.02   0.01   0.005  0.01   0.0025]\n",
      "Using batch size   1, an average step would have taken 2.83377 = 0.03346 + 2.80031 seconds\n",
      "Using batch size   2, an average step would have taken 2.15049 = 0.00288 + 2.14761 seconds\n",
      "Using batch size   3, an average step would have taken 1.44521 = 0.00735 + 1.43786 seconds\n",
      "Using batch size   4, an average step would have taken 1.24181 = 0.00269 + 1.23912 seconds\n",
      "Using batch size   5, an average step would have taken 0.88625 = 0.00126 + 0.88499 seconds\n",
      "Using batch size   6, an average step would have taken 0.93304 = 0.00231 + 0.93072 seconds\n",
      "Using batch size   7, an average step would have taken 0.88531 = 0.00279 + 0.88252 seconds\n",
      "Using batch size   8, an average step would have taken 0.77089 = 0.00169 + 0.76920 seconds\n",
      "Using batch size   9, an average step would have taken 0.63527 = 0.00122 + 0.63405 seconds\n",
      "Using batch size  10, an average step would have taken 0.60771 = 0.00069 + 0.60702 seconds\n",
      "Using batch size  11, an average step would have taken 0.58075 = 0.00197 + 0.57879 seconds\n",
      "Using batch size  12, an average step would have taken 0.60620 = 0.00202 + 0.60418 seconds\n",
      "Using batch size  13, an average step would have taken 0.67649 = 0.00070 + 0.67579 seconds\n",
      "Using batch size  14, an average step would have taken 0.71490 = 0.00088 + 0.71403 seconds\n",
      "Using batch size  15, an average step would have taken 0.75918 = 0.00079 + 0.75839 seconds\n",
      "Using batch size  16, an average step would have taken 0.73113 = 0.00082 + 0.73031 seconds\n",
      "Using batch size  17, an average step would have taken 0.68772 = 0.00057 + 0.68715 seconds\n",
      "Using batch size  18, an average step would have taken 0.73928 = 0.00074 + 0.73854 seconds\n",
      "Using batch size  19, an average step would have taken 0.77595 = 0.00089 + 0.77506 seconds\n",
      "Using batch size  20, an average step would have taken 0.49503 = 0.00091 + 0.49413 seconds\n",
      "Using batch size  21, an average step would have taken 0.45296 = 0.00088 + 0.45208 seconds\n",
      "Using batch size  22, an average step would have taken 0.48957 = 0.00074 + 0.48883 seconds\n",
      "Using batch size  23, an average step would have taken 0.48427 = 0.00053 + 0.48374 seconds\n",
      "Using batch size  24, an average step would have taken 0.59477 = 0.00125 + 0.59353 seconds\n",
      "Using batch size  25, an average step would have taken 0.46233 = 0.00032 + 0.46201 seconds\n",
      "batch size was 25, optimal batch size would have been 21\n",
      "setting batch size to 21: expected step duration: 0.45296\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.3x\n",
      "improvement compared to smallest batch size (1): 6.3x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too linear, increasing steps:     0.51 (100), 0.37 (30)\n",
      "  Success rate too low, decreasing source step:  0.21 ( 71), 0.17 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.44 ( 9)\n",
      "Step 900: 1.10514e-02, stepsizes = 1.3e-02/1.7e-04: d. reduced by 0.05% (5.6806e-06) (took 0.48744 seconds)\n",
      "  Success rate too high, increasing source step: 0.38 (100), 0.63 (30)\n",
      "  Success rate too low, decreasing source step:  0.37 (100), 0.10 (30)\n",
      "Step 1000: 1.06682e-02, stepsizes = 1.3e-02/1.7e-04:  (took 1.29117 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1220.04727\n",
      "   1.1% for generation (13.49967)\n",
      "   13.6% for spherical prediction (166.38187)\n",
      "   73.8% for prediction (900.47613)\n",
      "   0.0% for hyperparameter update (0.15246)\n",
      "   11.4% for the rest (139.53713)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 3.13494574e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.60472022e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 1.29160137e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.0179194  0.0179603  0.01686172 0.02211609\n",
      " 0.01848043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.13  0.165 0.1   0.105 0.075 0.065 0.03  0.045 0.04  0.04  0.015 0.03\n",
      " 0.015 0.015 0.015 0.005 0.02  0.04  0.005 0.    0.01  0.    0.015 0.005\n",
      " 0.005 0.01 ]\n",
      "Using batch size   1, an average step would have taken 1.94815 = 0.02300 + 1.92515 seconds\n",
      "Using batch size   2, an average step would have taken 1.51430 = 0.00187 + 1.51243 seconds\n",
      "Using batch size   3, an average step would have taken 1.03472 = 0.00515 + 1.02956 seconds\n",
      "Using batch size   4, an average step would have taken 0.90478 = 0.00361 + 0.90117 seconds\n",
      "Using batch size   5, an average step would have taken 0.66941 = 0.00095 + 0.66846 seconds\n",
      "Using batch size   6, an average step would have taken 0.70279 = 0.00157 + 0.70122 seconds\n",
      "Using batch size   7, an average step would have taken 0.68752 = 0.00233 + 0.68519 seconds\n",
      "Using batch size   8, an average step would have taken 0.60096 = 0.00112 + 0.59984 seconds\n",
      "Using batch size   9, an average step would have taken 0.49733 = 0.00087 + 0.49646 seconds\n",
      "Using batch size  10, an average step would have taken 0.49194 = 0.00055 + 0.49140 seconds\n",
      "Using batch size  11, an average step would have taken 0.46721 = 0.00153 + 0.46568 seconds\n",
      "Using batch size  12, an average step would have taken 0.49792 = 0.00145 + 0.49647 seconds\n",
      "Using batch size  13, an average step would have taken 0.60407 = 0.00053 + 0.60354 seconds\n",
      "Using batch size  14, an average step would have taken 0.64877 = 0.00067 + 0.64810 seconds\n",
      "Using batch size  15, an average step would have taken 0.69722 = 0.00072 + 0.69650 seconds\n",
      "Using batch size  16, an average step would have taken 0.67144 = 0.00073 + 0.67072 seconds\n",
      "Using batch size  17, an average step would have taken 0.61349 = 0.00047 + 0.61302 seconds\n",
      "Using batch size  18, an average step would have taken 0.66506 = 0.00049 + 0.66457 seconds\n",
      "Using batch size  19, an average step would have taken 0.71434 = 0.00078 + 0.71356 seconds\n",
      "Using batch size  20, an average step would have taken 0.44290 = 0.00083 + 0.44207 seconds\n",
      "Using batch size  21, an average step would have taken 0.43235 = 0.00054 + 0.43181 seconds\n",
      "Using batch size  22, an average step would have taken 0.44135 = 0.00051 + 0.44084 seconds\n",
      "Using batch size  23, an average step would have taken 0.43428 = 0.00048 + 0.43380 seconds\n",
      "Using batch size  24, an average step would have taken 0.56167 = 0.00085 + 0.56082 seconds\n",
      "Using batch size  25, an average step would have taken 0.46233 = 0.00032 + 0.46201 seconds\n",
      "batch size was 21, optimal batch size would have been 21\n",
      "setting batch size to 21: expected step duration: 0.43235\n",
      "improvement compared to old batch size (21): 1.0x\n",
      "improvement compared to worst batch size (1): 4.5x\n",
      "improvement compared to smallest batch size (1): 4.5x\n",
      "improvement compared to largest batch size (25): 1.1x\n",
      "next batch size tuning in 400 steps, after step 1400\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.29 (21)\n",
      "  Success rate too high, increasing source step: 0.43 ( 42), 0.53 (30)\n",
      "Step 1100: 1.03822e-02, stepsizes = 8.7e-03/1.7e-04:  (took 1.42516 seconds)\n",
      "  Success rate too low, decreasing source step:  0.37 (100), 0.10 (30)\n",
      "Step 1200: 1.01304e-02, stepsizes = 8.7e-03/1.1e-04: d. reduced by 0.02% (2.3139e-06) (took 0.66313 seconds)\n",
      "  Success rate too low, decreasing source step:  0.33 (100), 0.13 (30)\n",
      "Step 1300: 9.91978e-03, stepsizes = 8.7e-03/7.6e-05: d. reduced by 0.02% (2.2657e-06) (took 0.63895 seconds)\n",
      "  Boundary too linear, increasing steps:     0.55 (100), 0.63 (30)\n",
      "  Success rate too high, increasing source step: 0.55 (100), 0.63 (30)\n",
      "Step 1400: 9.72352e-03, stepsizes = 1.3e-02/1.7e-04: d. reduced by 0.03% (3.3317e-06) (took 0.59100 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1423.38077\n",
      "   1.2% for generation (16.64117)\n",
      "   13.0% for spherical prediction (184.81929)\n",
      "   74.9% for prediction (1066.12624)\n",
      "   0.0% for hyperparameter update (0.22944)\n",
      "   10.9% for the rest (155.56463)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 2.53410491e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59968701e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 1.29160137e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01798002 0.0179603  0.01686172 0.02211609\n",
      " 0.01848043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.1725 0.145  0.085  0.0775 0.07   0.045  0.05   0.0375 0.045  0.0225\n",
      " 0.025  0.01   0.0275 0.0275 0.015  0.02   0.015  0.0175 0.02   0.005\n",
      " 0.0175 0.0025 0.0175 0.0125 0.0075 0.01  ]\n",
      "Using batch size   1, an average step would have taken 2.28894 = 0.02703 + 2.26191 seconds\n",
      "Using batch size   2, an average step would have taken 1.75127 = 0.00221 + 1.74906 seconds\n",
      "Using batch size   3, an average step would have taken 1.19245 = 0.00597 + 1.18647 seconds\n",
      "Using batch size   4, an average step would have taken 1.02893 = 0.00344 + 1.02549 seconds\n",
      "Using batch size   5, an average step would have taken 0.75403 = 0.00107 + 0.75296 seconds\n",
      "Using batch size   6, an average step would have taken 0.79034 = 0.00182 + 0.78853 seconds\n",
      "Using batch size   7, an average step would have taken 0.76217 = 0.00253 + 0.75964 seconds\n",
      "Using batch size   8, an average step would have taken 0.65758 = 0.00129 + 0.65629 seconds\n",
      "Using batch size   9, an average step would have taken 0.55220 = 0.00100 + 0.55120 seconds\n",
      "Using batch size  10, an average step would have taken 0.53528 = 0.00060 + 0.53468 seconds\n",
      "Using batch size  11, an average step would have taken 0.51513 = 0.00171 + 0.51342 seconds\n",
      "Using batch size  12, an average step would have taken 0.53832 = 0.00164 + 0.53668 seconds\n",
      "Using batch size  13, an average step would have taken 0.63066 = 0.00060 + 0.63006 seconds\n",
      "Using batch size  14, an average step would have taken 0.67245 = 0.00075 + 0.67170 seconds\n",
      "Using batch size  15, an average step would have taken 0.71648 = 0.00074 + 0.71574 seconds\n",
      "Using batch size  16, an average step would have taken 0.69161 = 0.00076 + 0.69085 seconds\n",
      "Using batch size  17, an average step would have taken 0.64390 = 0.00051 + 0.64338 seconds\n",
      "Using batch size  18, an average step would have taken 0.69170 = 0.00058 + 0.69112 seconds\n",
      "Using batch size  19, an average step would have taken 0.73723 = 0.00082 + 0.73640 seconds\n",
      "Using batch size  20, an average step would have taken 0.46028 = 0.00086 + 0.45942 seconds\n",
      "Using batch size  21, an average step would have taken 0.45214 = 0.00056 + 0.45158 seconds\n",
      "Using batch size  22, an average step would have taken 0.45742 = 0.00059 + 0.45684 seconds\n",
      "Using batch size  23, an average step would have taken 0.44856 = 0.00050 + 0.44807 seconds\n",
      "Using batch size  24, an average step would have taken 0.57090 = 0.00096 + 0.56993 seconds\n",
      "Using batch size  25, an average step would have taken 0.46233 = 0.00032 + 0.46201 seconds\n",
      "batch size was 21, optimal batch size would have been 23\n",
      "setting batch size to 23: expected step duration: 0.44856\n",
      "improvement compared to old batch size (21): 1.0x\n",
      "improvement compared to worst batch size (1): 5.1x\n",
      "improvement compared to smallest batch size (1): 5.1x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Success rate too low, decreasing source step:  0.31 (100), 0.10 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.30 (30)\n",
      "Step 1500: 9.54275e-03, stepsizes = 8.7e-03/7.6e-05: d. reduced by 0.02% (1.4531e-06) (took 0.69538 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.47 (30)\n",
      "  Success rate too high, increasing source step: 0.51 ( 69), 0.57 (30)\n",
      "Step 1600: 9.44063e-03, stepsizes = 5.8e-03/7.6e-05: d. reduced by 0.02% (1.4374e-06) (took 0.65566 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1524.51953\n",
      "   1.2% for generation (18.25969)\n",
      "   12.7% for spherical prediction (193.50963)\n",
      "   75.4% for prediction (1149.34780)\n",
      "   0.0% for hyperparameter update (0.27283)\n",
      "   10.7% for the rest (163.12957)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 2.53410491e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59968701e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.29160137e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01798002 0.0179603  0.01690723 0.02211609\n",
      " 0.01848043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.165 0.14  0.1   0.065 0.115 0.055 0.03  0.045 0.03  0.03  0.035 0.01\n",
      " 0.045 0.02  0.02  0.005 0.025 0.01  0.01  0.005 0.005 0.005 0.005 0.01\n",
      " 0.01  0.005]\n",
      "Using batch size   1, an average step would have taken 2.12072 = 0.02504 + 2.09568 seconds\n",
      "Using batch size   2, an average step would have taken 1.63130 = 0.00938 + 1.62192 seconds\n",
      "Using batch size   3, an average step would have taken 1.12084 = 0.00561 + 1.11523 seconds\n",
      "Using batch size   4, an average step would have taken 0.95790 = 0.00320 + 0.95470 seconds\n",
      "Using batch size   5, an average step would have taken 0.70568 = 0.00101 + 0.70467 seconds\n",
      "Using batch size   6, an average step would have taken 0.74275 = 0.00170 + 0.74104 seconds\n",
      "Using batch size   7, an average step would have taken 0.71663 = 0.00238 + 0.71424 seconds\n",
      "Using batch size   8, an average step would have taken 0.62714 = 0.00122 + 0.62592 seconds\n",
      "Using batch size   9, an average step would have taken 0.52839 = 0.00094 + 0.52745 seconds\n",
      "Using batch size  10, an average step would have taken 0.51424 = 0.00057 + 0.51367 seconds\n",
      "Using batch size  11, an average step would have taken 0.49742 = 0.00165 + 0.49577 seconds\n",
      "Using batch size  12, an average step would have taken 0.51360 = 0.00155 + 0.51205 seconds\n",
      "Using batch size  13, an average step would have taken 0.61140 = 0.00055 + 0.61085 seconds\n",
      "Using batch size  14, an average step would have taken 0.65367 = 0.00069 + 0.65298 seconds\n",
      "Using batch size  15, an average step would have taken 0.70225 = 0.00073 + 0.70152 seconds\n",
      "Using batch size  16, an average step would have taken 0.67467 = 0.00073 + 0.67394 seconds\n",
      "Using batch size  17, an average step would have taken 0.62780 = 0.00049 + 0.62731 seconds\n",
      "Using batch size  18, an average step would have taken 0.67838 = 0.00054 + 0.67784 seconds\n",
      "Using batch size  19, an average step would have taken 0.72490 = 0.00080 + 0.72410 seconds\n",
      "Using batch size  20, an average step would have taken 0.45348 = 0.00085 + 0.45263 seconds\n",
      "Using batch size  21, an average step would have taken 0.44370 = 0.00053 + 0.44317 seconds\n",
      "Using batch size  22, an average step would have taken 0.45359 = 0.00057 + 0.45303 seconds\n",
      "Using batch size  23, an average step would have taken 0.44659 = 0.00065 + 0.44594 seconds\n",
      "Using batch size  24, an average step would have taken 0.56818 = 0.00093 + 0.56725 seconds\n",
      "Using batch size  25, an average step would have taken 0.46233 = 0.00032 + 0.46201 seconds\n",
      "batch size was 23, optimal batch size would have been 21\n",
      "setting batch size to 21: expected step duration: 0.44370\n",
      "improvement compared to old batch size (23): 1.0x\n",
      "improvement compared to worst batch size (1): 4.8x\n",
      "improvement compared to smallest batch size (1): 4.8x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 1700\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.40 (25)\n",
      "  Success rate too low, decreasing source step:  0.34 (100), 0.13 (30)\n",
      "Step 1700: 9.36429e-03, stepsizes = 3.9e-03/3.4e-05: d. reduced by 0.01% (6.3367e-07) (took 0.82177 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1582.74611\n",
      "   1.2% for generation (19.02943)\n",
      "   12.5% for spherical prediction (198.58924)\n",
      "   75.7% for prediction (1197.59842)\n",
      "   0.0% for hyperparameter update (0.30173)\n",
      "   10.6% for the rest (167.22729)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 2.12542666e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.60338336e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.29160137e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802147 0.0179603  0.01690723 0.02211609\n",
      " 0.01848043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.32 0.07 0.07 0.03 0.06 0.03 0.02 0.03 0.04 0.02 0.02 0.06 0.05 0.01\n",
      " 0.01 0.01 0.02 0.02 0.01 0.   0.03 0.   0.02 0.   0.02 0.03]\n",
      "Using batch size   1, an average step would have taken 3.14960 = 0.03719 + 3.11241 seconds\n",
      "Using batch size   2, an average step would have taken 2.37829 = 0.01382 + 2.36447 seconds\n",
      "Using batch size   3, an average step would have taken 1.59471 = 0.00815 + 1.58656 seconds\n",
      "Using batch size   4, an average step would have taken 1.34384 = 0.00409 + 1.33975 seconds\n",
      "Using batch size   5, an average step would have taken 0.97012 = 0.00138 + 0.96873 seconds\n",
      "Using batch size   6, an average step would have taken 1.00536 = 0.00257 + 1.00280 seconds\n",
      "Using batch size   7, an average step would have taken 0.95743 = 0.00310 + 0.95433 seconds\n",
      "Using batch size   8, an average step would have taken 0.82719 = 0.00189 + 0.82530 seconds\n",
      "Using batch size   9, an average step would have taken 0.68576 = 0.00133 + 0.68443 seconds\n",
      "Using batch size  10, an average step would have taken 0.65707 = 0.00075 + 0.65632 seconds\n",
      "Using batch size  11, an average step would have taken 0.61949 = 0.00212 + 0.61737 seconds\n",
      "Using batch size  12, an average step would have taken 0.62601 = 0.00219 + 0.62383 seconds\n",
      "Using batch size  13, an average step would have taken 0.68841 = 0.00073 + 0.68768 seconds\n",
      "Using batch size  14, an average step would have taken 0.72552 = 0.00091 + 0.72461 seconds\n",
      "Using batch size  15, an average step would have taken 0.77425 = 0.00080 + 0.77345 seconds\n",
      "Using batch size  16, an average step would have taken 0.74565 = 0.00084 + 0.74481 seconds\n",
      "Using batch size  17, an average step would have taken 0.70292 = 0.00059 + 0.70233 seconds\n",
      "Using batch size  18, an average step would have taken 0.75831 = 0.00081 + 0.75750 seconds\n",
      "Using batch size  19, an average step would have taken 0.80059 = 0.00094 + 0.79966 seconds\n",
      "Using batch size  20, an average step would have taken 0.51090 = 0.00093 + 0.50997 seconds\n",
      "Using batch size  21, an average step would have taken 0.51030 = 0.00067 + 0.50963 seconds\n",
      "Using batch size  22, an average step would have taken 0.50871 = 0.00083 + 0.50788 seconds\n",
      "Using batch size  23, an average step would have taken 0.50718 = 0.00099 + 0.50619 seconds\n",
      "Using batch size  24, an average step would have taken 0.60725 = 0.00139 + 0.60586 seconds\n",
      "Using batch size  25, an average step would have taken 0.46233 = 0.00032 + 0.46201 seconds\n",
      "batch size was 21, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.46233\n",
      "improvement compared to old batch size (21): 1.1x\n",
      "improvement compared to worst batch size (1): 6.8x\n",
      "improvement compared to smallest batch size (1): 6.8x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 1800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Success rate too high, increasing source step: 0.47 (100), 0.63 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.16 (100), 0.17 (30)\n",
      "  Success rate too low, decreasing source step:  0.16 (100), 0.17 (30)\n",
      "Step 1800: 9.30334e-03, stepsizes = 2.6e-03/2.3e-05: d. reduced by 0.01% (9.4427e-07) (took 0.79132 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1632.05951\n",
      "   1.2% for generation (19.98013)\n",
      "   12.4% for spherical prediction (202.64156)\n",
      "   75.8% for prediction (1237.83260)\n",
      "   0.0% for hyperparameter update (0.32096)\n",
      "   10.5% for the rest (171.28426)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 2.12542666e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.60338336e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.32025525e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802147 0.0179603  0.01690723 0.02211609\n",
      " 0.01840602]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.26 0.14 0.1  0.04 0.07 0.05 0.03 0.03 0.04 0.02 0.02 0.05 0.   0.01\n",
      " 0.02 0.04 0.   0.04 0.01 0.01 0.   0.01 0.   0.   0.01 0.  ]\n",
      "Using batch size   1, an average step would have taken 2.52663 = 0.02983 + 2.49679 seconds\n",
      "Using batch size   2, an average step would have taken 1.94102 = 0.01124 + 1.92977 seconds\n",
      "Using batch size   3, an average step would have taken 1.30572 = 0.00663 + 1.29909 seconds\n",
      "Using batch size   4, an average step would have taken 1.12873 = 0.00337 + 1.12536 seconds\n",
      "Using batch size   5, an average step would have taken 0.80994 = 0.00115 + 0.80879 seconds\n",
      "Using batch size   6, an average step would have taken 0.85557 = 0.00210 + 0.85347 seconds\n",
      "Using batch size   7, an average step would have taken 0.82181 = 0.00268 + 0.81913 seconds\n",
      "Using batch size   8, an average step would have taken 0.71465 = 0.00154 + 0.71311 seconds\n",
      "Using batch size   9, an average step would have taken 0.58788 = 0.00109 + 0.58679 seconds\n",
      "Using batch size  10, an average step would have taken 0.57359 = 0.00065 + 0.57294 seconds\n",
      "Using batch size  11, an average step would have taken 0.54314 = 0.00183 + 0.54131 seconds\n",
      "Using batch size  12, an average step would have taken 0.57347 = 0.00188 + 0.57160 seconds\n",
      "Using batch size  13, an average step would have taken 0.65541 = 0.00065 + 0.65475 seconds\n",
      "Using batch size  14, an average step would have taken 0.69286 = 0.00081 + 0.69205 seconds\n",
      "Using batch size  15, an average step would have taken 0.73071 = 0.00076 + 0.72996 seconds\n",
      "Using batch size  16, an average step would have taken 0.71016 = 0.00079 + 0.70937 seconds\n",
      "Using batch size  17, an average step would have taken 0.65642 = 0.00053 + 0.65589 seconds\n",
      "Using batch size  18, an average step would have taken 0.70883 = 0.00064 + 0.70819 seconds\n",
      "Using batch size  19, an average step would have taken 0.75131 = 0.00085 + 0.75046 seconds\n",
      "Using batch size  20, an average step would have taken 0.47766 = 0.00088 + 0.47678 seconds\n",
      "Using batch size  21, an average step would have taken 0.46983 = 0.00057 + 0.46927 seconds\n",
      "Using batch size  22, an average step would have taken 0.47809 = 0.00068 + 0.47741 seconds\n",
      "Using batch size  23, an average step would have taken 0.47529 = 0.00081 + 0.47448 seconds\n",
      "Using batch size  24, an average step would have taken 0.58772 = 0.00116 + 0.58656 seconds\n",
      "Using batch size  25, an average step would have taken 0.46048 = 0.00033 + 0.46015 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.46048\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 5.5x\n",
      "improvement compared to smallest batch size (1): 5.5x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 2000\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.47 (30)\n",
      "Step 1900: 9.26314e-03, stepsizes = 1.7e-03/1.5e-05: d. reduced by 0.00% (4.1786e-07) (took 0.92739 seconds)\n",
      "  Success rate too high, increasing source step: 0.40 (100), 0.57 (30)\n",
      "Step 2000: 9.24088e-03, stepsizes = 1.7e-03/2.3e-05: d. reduced by 0.00% (4.1676e-07) (took 0.68977 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1733.85853\n",
      "   1.2% for generation (21.61129)\n",
      "   12.2% for spherical prediction (211.65356)\n",
      "   76.2% for prediction (1321.38706)\n",
      "   0.0% for hyperparameter update (0.37750)\n",
      "   10.3% for the rest (178.82912)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 2.12542666e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.60338336e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.31719256e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802147 0.0179603  0.01690723 0.02211609\n",
      " 0.0184382 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.14  0.15  0.095 0.1   0.045 0.05  0.04  0.04  0.045 0.045 0.05  0.02\n",
      " 0.02  0.015 0.035 0.01  0.02  0.    0.01  0.02  0.    0.015 0.01  0.005\n",
      " 0.    0.02 ]\n",
      "Using batch size   1, an average step would have taken 2.10118 = 0.02481 + 2.07637 seconds\n",
      "Using batch size   2, an average step would have taken 1.62754 = 0.00935 + 1.61819 seconds\n",
      "Using batch size   3, an average step would have taken 1.10336 = 0.00551 + 1.09785 seconds\n",
      "Using batch size   4, an average step would have taken 0.96371 = 0.00275 + 0.96096 seconds\n",
      "Using batch size   5, an average step would have taken 0.70416 = 0.00100 + 0.70316 seconds\n",
      "Using batch size   6, an average step would have taken 0.74938 = 0.00169 + 0.74768 seconds\n",
      "Using batch size   7, an average step would have taken 0.71746 = 0.00236 + 0.71510 seconds\n",
      "Using batch size   8, an average step would have taken 0.62676 = 0.00120 + 0.62557 seconds\n",
      "Using batch size   9, an average step would have taken 0.52839 = 0.00094 + 0.52745 seconds\n",
      "Using batch size  10, an average step would have taken 0.50620 = 0.00056 + 0.50563 seconds\n",
      "Using batch size  11, an average step would have taken 0.48160 = 0.00159 + 0.48002 seconds\n",
      "Using batch size  12, an average step would have taken 0.51143 = 0.00153 + 0.50990 seconds\n",
      "Using batch size  13, an average step would have taken 0.61324 = 0.00055 + 0.61268 seconds\n",
      "Using batch size  14, an average step would have taken 0.65040 = 0.00068 + 0.64973 seconds\n",
      "Using batch size  15, an average step would have taken 0.69722 = 0.00072 + 0.69650 seconds\n",
      "Using batch size  16, an average step would have taken 0.67144 = 0.00073 + 0.67072 seconds\n",
      "Using batch size  17, an average step would have taken 0.62780 = 0.00049 + 0.62731 seconds\n",
      "Using batch size  18, an average step would have taken 0.67838 = 0.00054 + 0.67784 seconds\n",
      "Using batch size  19, an average step would have taken 0.71962 = 0.00079 + 0.71883 seconds\n",
      "Using batch size  20, an average step would have taken 0.45046 = 0.00084 + 0.44961 seconds\n",
      "Using batch size  21, an average step would have taken 0.43780 = 0.00049 + 0.43731 seconds\n",
      "Using batch size  22, an average step would have taken 0.44594 = 0.00053 + 0.44541 seconds\n",
      "Using batch size  23, an average step would have taken 0.44021 = 0.00061 + 0.43960 seconds\n",
      "Using batch size  24, an average step would have taken 0.56601 = 0.00091 + 0.56511 seconds\n",
      "Using batch size  25, an average step would have taken 0.46128 = 0.00033 + 0.46096 seconds\n",
      "batch size was 25, optimal batch size would have been 21\n",
      "setting batch size to 21: expected step duration: 0.43780\n",
      "improvement compared to old batch size (25): 1.1x\n",
      "improvement compared to worst batch size (1): 4.8x\n",
      "improvement compared to smallest batch size (1): 4.8x\n",
      "improvement compared to largest batch size (25): 1.1x\n",
      "next batch size tuning in 100 steps, after step 2100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2100: 9.21258e-03, stepsizes = 1.7e-03/2.3e-05: d. reduced by 0.00% (4.1564e-07) (took 0.73708 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1792.42641\n",
      "   1.2% for generation (22.34715)\n",
      "   12.1% for spherical prediction (216.67831)\n",
      "   76.4% for prediction (1369.85085)\n",
      "   0.0% for hyperparameter update (0.39626)\n",
      "   10.2% for the rest (183.15384)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.31719256e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01690723 0.02211609\n",
      " 0.0184382 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.32 0.03 0.05 0.04 0.02 0.04 0.07 0.01 0.06 0.06 0.02 0.02 0.02 0.03\n",
      " 0.03 0.01 0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.03 0.02 0.01]\n",
      "Using batch size   1, an average step would have taken 3.27984 = 0.03873 + 3.24111 seconds\n",
      "Using batch size   2, an average step would have taken 2.47917 = 0.01436 + 2.46482 seconds\n",
      "Using batch size   3, an average step would have taken 1.63936 = 0.00833 + 1.63103 seconds\n",
      "Using batch size   4, an average step would have taken 1.40991 = 0.00382 + 1.40609 seconds\n",
      "Using batch size   5, an average step would have taken 1.00034 = 0.00143 + 0.99891 seconds\n",
      "Using batch size   6, an average step would have taken 1.03623 = 0.00258 + 1.03365 seconds\n",
      "Using batch size   7, an average step would have taken 0.98403 = 0.00315 + 0.98088 seconds\n",
      "Using batch size   8, an average step would have taken 0.84073 = 0.00187 + 0.83887 seconds\n",
      "Using batch size   9, an average step would have taken 0.68633 = 0.00134 + 0.68500 seconds\n",
      "Using batch size  10, an average step would have taken 0.65674 = 0.00075 + 0.65599 seconds\n",
      "Using batch size  11, an average step would have taken 0.63234 = 0.00217 + 0.63018 seconds\n",
      "Using batch size  12, an average step would have taken 0.64367 = 0.00219 + 0.64149 seconds\n",
      "Using batch size  13, an average step would have taken 0.70308 = 0.00077 + 0.70231 seconds\n",
      "Using batch size  14, an average step would have taken 0.73205 = 0.00093 + 0.73112 seconds\n",
      "Using batch size  15, an average step would have taken 0.78095 = 0.00081 + 0.78014 seconds\n",
      "Using batch size  16, an average step would have taken 0.75210 = 0.00085 + 0.75125 seconds\n",
      "Using batch size  17, an average step would have taken 0.71007 = 0.00060 + 0.70948 seconds\n",
      "Using batch size  18, an average step would have taken 0.76212 = 0.00082 + 0.76130 seconds\n",
      "Using batch size  19, an average step would have taken 0.79707 = 0.00093 + 0.79614 seconds\n",
      "Using batch size  20, an average step would have taken 0.51392 = 0.00093 + 0.51299 seconds\n",
      "Using batch size  21, an average step would have taken 0.51028 = 0.00063 + 0.50965 seconds\n",
      "Using batch size  22, an average step would have taken 0.51177 = 0.00085 + 0.51093 seconds\n",
      "Using batch size  23, an average step would have taken 0.50080 = 0.00095 + 0.49985 seconds\n",
      "Using batch size  24, an average step would have taken 0.60291 = 0.00134 + 0.60157 seconds\n",
      "Using batch size  25, an average step would have taken 0.46128 = 0.00033 + 0.46096 seconds\n",
      "batch size was 21, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.46128\n",
      "improvement compared to old batch size (21): 1.1x\n",
      "improvement compared to worst batch size (1): 7.1x\n",
      "improvement compared to smallest batch size (1): 7.1x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 2200\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.17 (30)\n",
      "  Success rate too low, decreasing source step:  0.17 (100), 0.17 (30)\n",
      "Step 2200: 9.19754e-03, stepsizes = 1.1e-03/1.0e-05: d. reduced by 0.00% (1.8428e-07) (took 0.68767 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1838.05904\n",
      "   1.3% for generation (23.23772)\n",
      "   12.0% for spherical prediction (220.51828)\n",
      "   76.6% for prediction (1407.13701)\n",
      "   0.0% for hyperparameter update (0.41888)\n",
      "   10.2% for the rest (186.74715)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.32697557e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01690723 0.02211609\n",
      " 0.0182692 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.31 0.09 0.05 0.06 0.02 0.02 0.02 0.06 0.01 0.06 0.03 0.03 0.   0.03\n",
      " 0.   0.02 0.02 0.   0.03 0.05 0.   0.01 0.04 0.01 0.01 0.02]\n",
      "Using batch size   1, an average step would have taken 3.17999 = 0.03755 + 3.14244 seconds\n",
      "Using batch size   2, an average step would have taken 2.42496 = 0.01405 + 2.41091 seconds\n",
      "Using batch size   3, an average step would have taken 1.61486 = 0.00821 + 1.60665 seconds\n",
      "Using batch size   4, an average step would have taken 1.37957 = 0.00375 + 1.37582 seconds\n",
      "Using batch size   5, an average step would have taken 0.97918 = 0.00139 + 0.97779 seconds\n",
      "Using batch size   6, an average step would have taken 1.03623 = 0.00258 + 1.03365 seconds\n",
      "Using batch size   7, an average step would have taken 0.96881 = 0.00310 + 0.96571 seconds\n",
      "Using batch size   8, an average step would have taken 0.84073 = 0.00187 + 0.83887 seconds\n",
      "Using batch size   9, an average step would have taken 0.69072 = 0.00136 + 0.68936 seconds\n",
      "Using batch size  10, an average step would have taken 0.65004 = 0.00074 + 0.64930 seconds\n",
      "Using batch size  11, an average step would have taken 0.61336 = 0.00209 + 0.61127 seconds\n",
      "Using batch size  12, an average step would have taken 0.64001 = 0.00218 + 0.63783 seconds\n",
      "Using batch size  13, an average step would have taken 0.69941 = 0.00076 + 0.69865 seconds\n",
      "Using batch size  14, an average step would have taken 0.73858 = 0.00095 + 0.73763 seconds\n",
      "Using batch size  15, an average step would have taken 0.78430 = 0.00081 + 0.78349 seconds\n",
      "Using batch size  16, an average step would have taken 0.75533 = 0.00086 + 0.75447 seconds\n",
      "Using batch size  17, an average step would have taken 0.72081 = 0.00061 + 0.72019 seconds\n",
      "Using batch size  18, an average step would have taken 0.76973 = 0.00085 + 0.76888 seconds\n",
      "Using batch size  19, an average step would have taken 0.79355 = 0.00092 + 0.79263 seconds\n",
      "Using batch size  20, an average step would have taken 0.51392 = 0.00093 + 0.51299 seconds\n",
      "Using batch size  21, an average step would have taken 0.51028 = 0.00063 + 0.50965 seconds\n",
      "Using batch size  22, an average step would have taken 0.50259 = 0.00080 + 0.50178 seconds\n",
      "Using batch size  23, an average step would have taken 0.49761 = 0.00093 + 0.49668 seconds\n",
      "Using batch size  24, an average step would have taken 0.60291 = 0.00134 + 0.60157 seconds\n",
      "Using batch size  25, an average step would have taken 0.45706 = 0.00033 + 0.45673 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.45706\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 7.0x\n",
      "improvement compared to smallest batch size (1): 7.0x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Success rate too high, increasing source step: 0.48 (100), 0.80 (30)\n",
      "Step 2300: 9.18427e-03, stepsizes = 1.1e-03/1.5e-05:  (took 0.73214 seconds)\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.10 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.30 (30)\n",
      "Step 2400: 9.16937e-03, stepsizes = 7.6e-04/6.7e-06: d. reduced by 0.00% (1.8384e-07) (took 0.86292 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1938.11571\n",
      "   1.3% for generation (24.89770)\n",
      "   11.9% for spherical prediction (229.76535)\n",
      "   76.8% for prediction (1488.70167)\n",
      "   0.0% for hyperparameter update (0.45575)\n",
      "   10.0% for the rest (194.29523)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.32713063e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01690723 0.02211609\n",
      " 0.01824716]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.285 0.12  0.05  0.04  0.04  0.035 0.035 0.035 0.04  0.03  0.04  0.045\n",
      " 0.015 0.025 0.015 0.015 0.01  0.03  0.01  0.    0.015 0.015 0.015 0.005\n",
      " 0.02  0.015]\n",
      "Using batch size   1, an average step would have taken 2.91843 = 0.03446 + 2.88397 seconds\n",
      "Using batch size   2, an average step would have taken 2.22394 = 0.01288 + 2.21105 seconds\n",
      "Using batch size   3, an average step would have taken 1.48894 = 0.00757 + 1.48137 seconds\n",
      "Using batch size   4, an average step would have taken 1.27698 = 0.00346 + 1.27352 seconds\n",
      "Using batch size   5, an average step would have taken 0.91421 = 0.00130 + 0.91290 seconds\n",
      "Using batch size   6, an average step would have taken 0.95403 = 0.00237 + 0.95166 seconds\n",
      "Using batch size   7, an average step would have taken 0.90628 = 0.00291 + 0.90337 seconds\n",
      "Using batch size   8, an average step would have taken 0.78593 = 0.00173 + 0.78420 seconds\n",
      "Using batch size   9, an average step would have taken 0.64898 = 0.00124 + 0.64774 seconds\n",
      "Using batch size  10, an average step would have taken 0.62137 = 0.00071 + 0.62066 seconds\n",
      "Using batch size  11, an average step would have taken 0.58611 = 0.00199 + 0.58412 seconds\n",
      "Using batch size  12, an average step would have taken 0.60599 = 0.00204 + 0.60396 seconds\n",
      "Using batch size  13, an average step would have taken 0.67374 = 0.00070 + 0.67304 seconds\n",
      "Using batch size  14, an average step would have taken 0.71082 = 0.00087 + 0.70996 seconds\n",
      "Using batch size  15, an average step would have taken 0.75751 = 0.00078 + 0.75672 seconds\n",
      "Using batch size  16, an average step would have taken 0.73275 = 0.00082 + 0.73192 seconds\n",
      "Using batch size  17, an average step would have taken 0.68503 = 0.00057 + 0.68447 seconds\n",
      "Using batch size  18, an average step would have taken 0.73928 = 0.00074 + 0.73854 seconds\n",
      "Using batch size  19, an average step would have taken 0.78299 = 0.00091 + 0.78209 seconds\n",
      "Using batch size  20, an average step would have taken 0.50032 = 0.00091 + 0.49941 seconds\n",
      "Using batch size  21, an average step would have taken 0.49342 = 0.00059 + 0.49283 seconds\n",
      "Using batch size  22, an average step would have taken 0.49493 = 0.00076 + 0.49417 seconds\n",
      "Using batch size  23, an average step would have taken 0.49123 = 0.00090 + 0.49034 seconds\n",
      "Using batch size  24, an average step would have taken 0.59640 = 0.00126 + 0.59514 seconds\n",
      "Using batch size  25, an average step would have taken 0.45651 = 0.00033 + 0.45618 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.45651\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.4x\n",
      "improvement compared to smallest batch size (1): 6.4x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 2800\n",
      "  Success rate too high, increasing source step: 0.37 (100), 0.60 (30)\n",
      "Step 2500: 9.15883e-03, stepsizes = 7.6e-04/1.0e-05: d. reduced by 0.00% (1.8368e-07) (took 0.79139 seconds)\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.42 (100), 0.60 (30)\n",
      "Step 2600: 9.15143e-03, stepsizes = 7.6e-04/1.0e-05: d. reduced by 0.00% (1.8353e-07) (took 0.82041 seconds)\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.07 (30)\n",
      "Step 2700: 9.14348e-03, stepsizes = 7.6e-04/6.7e-06:  (took 0.74502 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.23 (30)\n",
      "Step 2800: 9.13632e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.1425e-08) (took 0.96539 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 2138.30275\n",
      "   1.3% for generation (28.18408)\n",
      "   11.6% for spherical prediction (247.64650)\n",
      "   77.3% for prediction (1652.56177)\n",
      "   0.0% for hyperparameter update (0.53936)\n",
      "   9.8% for the rest (209.37104)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.32417279e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01690723 0.02211609\n",
      " 0.01823766]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.345  0.11   0.06   0.06   0.0425 0.0375 0.0275 0.0125 0.025  0.02\n",
      " 0.0325 0.03   0.015  0.0275 0.0175 0.01   0.015  0.0225 0.0075 0.0175\n",
      " 0.0125 0.0075 0.0225 0.0125 0.     0.01  ]\n",
      "Using batch size   1, an average step would have taken 3.06983 = 0.03625 + 3.03358 seconds\n",
      "Using batch size   2, an average step would have taken 2.33393 = 0.01357 + 2.32036 seconds\n",
      "Using batch size   3, an average step would have taken 1.56364 = 0.00801 + 1.55564 seconds\n",
      "Using batch size   4, an average step would have taken 1.33611 = 0.00370 + 1.33241 seconds\n",
      "Using batch size   5, an average step would have taken 0.94669 = 0.00135 + 0.94535 seconds\n",
      "Using batch size   6, an average step would have taken 0.99853 = 0.00257 + 0.99596 seconds\n",
      "Using batch size   7, an average step would have taken 0.94312 = 0.00302 + 0.94010 seconds\n",
      "Using batch size   8, an average step would have taken 0.82201 = 0.00190 + 0.82012 seconds\n",
      "Using batch size   9, an average step would have taken 0.68054 = 0.00133 + 0.67922 seconds\n",
      "Using batch size  10, an average step would have taken 0.64677 = 0.00074 + 0.64604 seconds\n",
      "Using batch size  11, an average step would have taken 0.61627 = 0.00211 + 0.61416 seconds\n",
      "Using batch size  12, an average step would have taken 0.63718 = 0.00222 + 0.63496 seconds\n",
      "Using batch size  13, an average step would have taken 0.69208 = 0.00074 + 0.69134 seconds\n",
      "Using batch size  14, an average step would have taken 0.72633 = 0.00091 + 0.72542 seconds\n",
      "Using batch size  15, an average step would have taken 0.77509 = 0.00080 + 0.77429 seconds\n",
      "Using batch size  16, an average step would have taken 0.74807 = 0.00084 + 0.74723 seconds\n",
      "Using batch size  17, an average step would have taken 0.70471 = 0.00059 + 0.70412 seconds\n",
      "Using batch size  18, an average step would have taken 0.76117 = 0.00082 + 0.76035 seconds\n",
      "Using batch size  19, an average step would have taken 0.79707 = 0.00093 + 0.79614 seconds\n",
      "Using batch size  20, an average step would have taken 0.51317 = 0.00093 + 0.51223 seconds\n",
      "Using batch size  21, an average step would have taken 0.51028 = 0.00063 + 0.50965 seconds\n",
      "Using batch size  22, an average step would have taken 0.50794 = 0.00083 + 0.50712 seconds\n",
      "Using batch size  23, an average step would have taken 0.50239 = 0.00096 + 0.50143 seconds\n",
      "Using batch size  24, an average step would have taken 0.60834 = 0.00141 + 0.60693 seconds\n",
      "Using batch size  25, an average step would have taken 0.45627 = 0.00033 + 0.45594 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.45627\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.7x\n",
      "improvement compared to smallest batch size (1): 6.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 800 steps, after step 3600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too linear, increasing steps:     0.63 (100), 0.50 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.40 (30)\n",
      "Step 2900: 9.12883e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.1391e-08) (took 0.58794 seconds)\n",
      "Step 3000: 9.12257e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.1249e-08) (took 0.76017 seconds)\n",
      "Step 3100: 9.11550e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.1233e-08) (took 0.87935 seconds)\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.13 (30)\n",
      "  Success rate too high, increasing source step: 0.36 (100), 0.60 (30)\n",
      "  Success rate too low, decreasing source step:  0.38 (100), 0.17 (30)\n",
      "Step 3200: 9.11036e-03, stepsizes = 5.1e-04/3.0e-06:  (took 0.89250 seconds)\n",
      "  Success rate too high, increasing source step: 0.39 (100), 0.73 (30)\n",
      "Step 3300: 9.10530e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.1109e-08) (took 0.71822 seconds)\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.13 (30)\n",
      "  Success rate too high, increasing source step: 0.41 (100), 0.60 (30)\n",
      "Step 3400: 9.09989e-03, stepsizes = 5.1e-04/4.5e-06:  (took 0.78015 seconds)\n",
      "  Success rate too low, decreasing source step:  0.29 (100), 0.13 (30)\n",
      "  Success rate too high, increasing source step: 0.42 (100), 0.70 (30)\n",
      "  Success rate too low, decreasing source step:  0.33 (100), 0.17 (30)\n",
      "Step 3500: 9.09341e-03, stepsizes = 5.1e-04/3.0e-06: d. reduced by 0.00% (8.0966e-08) (took 0.73891 seconds)\n",
      "  Success rate too high, increasing source step: 0.39 (100), 0.70 (30)\n",
      "Step 3600: 9.08765e-03, stepsizes = 5.1e-04/4.5e-06:  (took 0.64684 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 2540.68506\n",
      "   1.4% for generation (34.72876)\n",
      "   11.2% for spherical prediction (284.36337)\n",
      "   78.0% for prediction (1981.35456)\n",
      "   0.0% for hyperparameter update (0.73212)\n",
      "   9.4% for the rest (239.50624)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.31929928e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01690723 0.02211609\n",
      " 0.0182468 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.19375 0.15    0.095   0.0725  0.05875 0.04625 0.05125 0.02875 0.035\n",
      " 0.03875 0.0275  0.03125 0.01375 0.02375 0.00875 0.02    0.01    0.01625\n",
      " 0.0175  0.00375 0.0075  0.01375 0.01375 0.00875 0.00625 0.0075 ]\n",
      "Using batch size   1, an average step would have taken 2.30576 = 0.02723 + 2.27854 seconds\n",
      "Using batch size   2, an average step would have taken 1.77759 = 0.01025 + 1.76735 seconds\n",
      "Using batch size   3, an average step would have taken 1.19728 = 0.00602 + 1.19126 seconds\n",
      "Using batch size   4, an average step would have taken 1.04486 = 0.00274 + 1.04212 seconds\n",
      "Using batch size   5, an average step would have taken 0.76045 = 0.00108 + 0.75937 seconds\n",
      "Using batch size   6, an average step would have taken 0.79441 = 0.00186 + 0.79255 seconds\n",
      "Using batch size   7, an average step would have taken 0.76881 = 0.00249 + 0.76632 seconds\n",
      "Using batch size   8, an average step would have taken 0.67015 = 0.00135 + 0.66880 seconds\n",
      "Using batch size   9, an average step would have taken 0.55641 = 0.00102 + 0.55539 seconds\n",
      "Using batch size  10, an average step would have taken 0.54128 = 0.00061 + 0.54067 seconds\n",
      "Using batch size  11, an average step would have taken 0.51199 = 0.00171 + 0.51028 seconds\n",
      "Using batch size  12, an average step would have taken 0.53918 = 0.00168 + 0.53750 seconds\n",
      "Using batch size  13, an average step would have taken 0.62882 = 0.00059 + 0.62823 seconds\n",
      "Using batch size  14, an average step would have taken 0.67286 = 0.00075 + 0.67211 seconds\n",
      "Using batch size  15, an average step would have taken 0.71690 = 0.00074 + 0.71616 seconds\n",
      "Using batch size  16, an average step would have taken 0.69363 = 0.00076 + 0.69286 seconds\n",
      "Using batch size  17, an average step would have taken 0.64658 = 0.00051 + 0.64606 seconds\n",
      "Using batch size  18, an average step would have taken 0.69551 = 0.00060 + 0.69491 seconds\n",
      "Using batch size  19, an average step would have taken 0.74119 = 0.00083 + 0.74036 seconds\n",
      "Using batch size  20, an average step would have taken 0.46670 = 0.00087 + 0.46584 seconds\n",
      "Using batch size  21, an average step would have taken 0.45634 = 0.00051 + 0.45584 seconds\n",
      "Using batch size  22, an average step would have taken 0.46163 = 0.00061 + 0.46103 seconds\n",
      "Using batch size  23, an average step would have taken 0.45536 = 0.00070 + 0.45466 seconds\n",
      "Using batch size  24, an average step would have taken 0.57497 = 0.00101 + 0.57395 seconds\n",
      "Using batch size  25, an average step would have taken 0.45650 = 0.00033 + 0.45617 seconds\n",
      "batch size was 25, optimal batch size would have been 23\n",
      "setting batch size to 23: expected step duration: 0.45536\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 5.1x\n",
      "improvement compared to smallest batch size (1): 5.1x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 4000\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.20 (30)\n",
      "  Success rate too low, decreasing source step:  0.18 ( 50), 0.17 (30)\n",
      "  Boundary too linear, increasing steps:     0.52 (100), 0.57 (30)\n",
      "  Success rate too high, increasing source step: 0.52 (100), 0.57 (30)\n",
      "Step 3700: 9.08375e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (3.5927e-08) (took 1.02761 seconds)\n",
      "Step 3800: 9.07841e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.0971e-08) (took 0.73447 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.20 (30)\n",
      "Step 3900: 9.07480e-03, stepsizes = 2.3e-04/2.0e-06:  (took 0.94020 seconds)\n",
      "  Success rate too low, decreasing source step:  0.04 ( 25), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.35 (100), 0.53 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.25 ( 8)\n",
      "Step 4000: 9.07253e-03, stepsizes = 1.5e-04/1.3e-06: d. reduced by 0.00% (2.3886e-08) (took 1.12495 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 2768.93237\n",
      "   1.4% for generation (37.99286)\n",
      "   11.1% for spherical prediction (306.22283)\n",
      "   78.4% for prediction (2170.45993)\n",
      "   0.0% for hyperparameter update (0.80250)\n",
      "   9.2% for the rest (253.45425)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 6.63739184e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40466803e-05 2.06563208e-05\n",
      " 1.31929928e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01728265 0.02211609\n",
      " 0.0182468 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.2875 0.1225 0.085  0.0875 0.05   0.04   0.03   0.0325 0.0275 0.01\n",
      " 0.0175 0.0125 0.0175 0.0175 0.02   0.0175 0.015  0.0125 0.02   0.0225\n",
      " 0.01   0.0025 0.0125 0.0075 0.0075 0.015 ]\n",
      "Using batch size   1, an average step would have taken 2.74532 = 0.03242 + 2.71290 seconds\n",
      "Using batch size   2, an average step would have taken 2.09239 = 0.00922 + 2.08317 seconds\n",
      "Using batch size   3, an average step would have taken 1.40680 = 0.00718 + 1.39963 seconds\n",
      "Using batch size   4, an average step would have taken 1.20421 = 0.00330 + 1.20091 seconds\n",
      "Using batch size   5, an average step would have taken 0.86358 = 0.00123 + 0.86235 seconds\n",
      "Using batch size   6, an average step would have taken 0.91672 = 0.00230 + 0.91442 seconds\n",
      "Using batch size   7, an average step would have taken 0.86580 = 0.00278 + 0.86302 seconds\n",
      "Using batch size   8, an average step would have taken 0.75338 = 0.00169 + 0.75170 seconds\n",
      "Using batch size   9, an average step would have taken 0.62772 = 0.00120 + 0.62652 seconds\n",
      "Using batch size  10, an average step would have taken 0.60201 = 0.00068 + 0.60133 seconds\n",
      "Using batch size  11, an average step would have taken 0.58218 = 0.00198 + 0.58021 seconds\n",
      "Using batch size  12, an average step would have taken 0.60378 = 0.00204 + 0.60175 seconds\n",
      "Using batch size  13, an average step would have taken 0.67374 = 0.00070 + 0.67304 seconds\n",
      "Using batch size  14, an average step would have taken 0.70919 = 0.00086 + 0.70833 seconds\n",
      "Using batch size  15, an average step would have taken 0.75499 = 0.00078 + 0.75421 seconds\n",
      "Using batch size  16, an average step would have taken 0.72871 = 0.00082 + 0.72790 seconds\n",
      "Using batch size  17, an average step would have taken 0.68682 = 0.00057 + 0.68625 seconds\n",
      "Using batch size  18, an average step would have taken 0.73738 = 0.00074 + 0.73664 seconds\n",
      "Using batch size  19, an average step would have taken 0.77331 = 0.00089 + 0.77242 seconds\n",
      "Using batch size  20, an average step would have taken 0.49352 = 0.00090 + 0.49262 seconds\n",
      "Using batch size  21, an average step would have taken 0.49005 = 0.00058 + 0.48947 seconds\n",
      "Using batch size  22, an average step would have taken 0.49263 = 0.00075 + 0.49188 seconds\n",
      "Using batch size  23, an average step would have taken 0.49653 = 0.00073 + 0.49580 seconds\n",
      "Using batch size  24, an average step would have taken 0.59694 = 0.00127 + 0.59567 seconds\n",
      "Using batch size  25, an average step would have taken 0.45650 = 0.00033 + 0.45617 seconds\n",
      "batch size was 23, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.45650\n",
      "improvement compared to old batch size (23): 1.1x\n",
      "improvement compared to worst batch size (1): 6.0x\n",
      "improvement compared to smallest batch size (1): 6.0x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 4200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.17 (30)\n",
      "  Success rate too low, decreasing source step:  0.19 (100), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.51 ( 75), 0.53 (30)\n",
      "Step 4100: 9.07145e-03, stepsizes = 1.0e-04/8.8e-07:  (took 0.76283 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.15 (13)\n",
      "  Boundary too non-linear, decreasing steps: 0.04 (100), 0.12 (17)\n",
      "  Boundary too non-linear, decreasing steps: 0.12 (100), 0.10 (29)\n",
      "  Success rate too low, decreasing source step:  0.12 ( 25), 0.07 (30)\n",
      "Step 4200: 9.07117e-03, stepsizes = 3.0e-05/1.7e-07:  (took 0.90357 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 2875.01473\n",
      "   1.4% for generation (39.73211)\n",
      "   11.0% for spherical prediction (316.08568)\n",
      "   78.6% for prediction (2259.24545)\n",
      "   0.0% for hyperparameter update (0.81603)\n",
      "   9.0% for the rest (259.13545)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 6.63739184e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40466803e-05 2.06563208e-05\n",
      " 1.32463813e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01728265 0.02211609\n",
      " 0.01835649]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.535 0.085 0.02  0.045 0.035 0.015 0.025 0.005 0.035 0.02  0.01  0.025\n",
      " 0.015 0.005 0.015 0.015 0.01  0.01  0.015 0.02  0.005 0.005 0.01  0.01\n",
      " 0.005 0.005]\n",
      "Using batch size   1, an average step would have taken 3.78343 = 0.04467 + 3.73875 seconds\n",
      "Using batch size   2, an average step would have taken 2.84765 = 0.01277 + 2.83488 seconds\n",
      "Using batch size   3, an average step would have taken 1.89163 = 0.00986 + 1.88177 seconds\n",
      "Using batch size   4, an average step would have taken 1.60213 = 0.00468 + 1.59745 seconds\n",
      "Using batch size   5, an average step would have taken 1.11971 = 0.00160 + 1.11812 seconds\n",
      "Using batch size   6, an average step would have taken 1.18566 = 0.00330 + 1.18236 seconds\n",
      "Using batch size   7, an average step would have taken 1.10963 = 0.00351 + 1.10612 seconds\n",
      "Using batch size   8, an average step would have taken 0.95965 = 0.00250 + 0.95715 seconds\n",
      "Using batch size   9, an average step would have taken 0.77979 = 0.00159 + 0.77819 seconds\n",
      "Using batch size  10, an average step would have taken 0.74328 = 0.00086 + 0.74242 seconds\n",
      "Using batch size  11, an average step would have taken 0.71859 = 0.00252 + 0.71606 seconds\n",
      "Using batch size  12, an average step would have taken 0.72776 = 0.00282 + 0.72494 seconds\n",
      "Using batch size  13, an average step would have taken 0.75075 = 0.00088 + 0.74987 seconds\n",
      "Using batch size  14, an average step would have taken 0.77940 = 0.00108 + 0.77832 seconds\n",
      "Using batch size  15, an average step would have taken 0.82784 = 0.00086 + 0.82698 seconds\n",
      "Using batch size  16, an average step would have taken 0.80050 = 0.00092 + 0.79958 seconds\n",
      "Using batch size  17, an average step would have taken 0.76731 = 0.00067 + 0.76663 seconds\n",
      "Using batch size  18, an average step would have taken 0.82492 = 0.00103 + 0.82389 seconds\n",
      "Using batch size  19, an average step would have taken 0.85516 = 0.00104 + 0.85412 seconds\n",
      "Using batch size  20, an average step would have taken 0.56530 = 0.00101 + 0.56429 seconds\n",
      "Using batch size  21, an average step would have taken 0.56927 = 0.00076 + 0.56851 seconds\n",
      "Using batch size  22, an average step would have taken 0.56536 = 0.00110 + 0.56425 seconds\n",
      "Using batch size  23, an average step would have taken 0.57136 = 0.00105 + 0.57032 seconds\n",
      "Using batch size  24, an average step would have taken 0.64850 = 0.00188 + 0.64662 seconds\n",
      "Using batch size  25, an average step would have taken 0.45924 = 0.00033 + 0.45891 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.45924\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 8.2x\n",
      "improvement compared to smallest batch size (1): 8.2x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 4600\n",
      "  Success rate too high, increasing source step: 0.51 ( 75), 0.60 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.25 ( 4)\n",
      "  Boundary too non-linear, decreasing steps: 0.03 (100), 0.14 ( 7)\n",
      "Step 4300: 9.07102e-03, stepsizes = 1.3e-05/1.2e-07:  (took 0.61410 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.03 (100), 0.10 (10)\n",
      "Step 4330: 9.07102e-03, stepsizes = 8.8e-06/7.7e-08: \n",
      "Looks like attack has converged after 4331 steps for the first time. Resetting steps to be sure.\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.09 (11)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.08 (12)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.08 (12)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.07 (14)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.06 (16)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.06 (17)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.06 (18)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.05 (19)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.05 (21)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.05 (21)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.05 (21)\n",
      "Step 4400: 9.07102e-03, stepsizes = 1.0e-05/1.0e-05:  (took 0.46576 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.05 (21)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.05 (22)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.04 (23)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.04 (25)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.04 (27)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.04 (28)\n",
      "  Success rate too low, decreasing source step:  0.04 ( 50), 0.03 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.05 (100), 0.00 ( 3)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 5)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 7)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.00 ( 7)\n",
      "  Boundary too non-linear, decreasing steps: 0.03 (100), 0.00 (10)\n",
      "Step 4442: 9.07102e-03, stepsizes = 1.2e-07/7.8e-08: \n",
      "Looks like attack has converged after 4443 steps, 100 remaining\n",
      "Step 4443: 9.07102e-03, stepsizes = 1.2e-07/7.8e-08: \n",
      "Looks like attack has converged after 4444 steps, 99 remaining\n",
      "Step 4444: 9.07102e-03, stepsizes = 1.2e-07/7.8e-08: \n",
      "Looks like attack has converged after 4445 steps, 98 remaining\n",
      "Step 4445: 9.07102e-03, stepsizes = 1.2e-07/7.8e-08: \n",
      "Looks like attack has converged after 4446 steps, 97 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.04 (100), 0.00 (14)\n",
      "Step 4446: 9.07102e-03, stepsizes = 7.8e-08/5.2e-08: \n",
      "Looks like attack has converged after 4447 steps, 96 remaining\n",
      "Step 4447: 9.07102e-03, stepsizes = 7.8e-08/5.2e-08: \n",
      "Looks like attack has converged after 4448 steps, 95 remaining\n",
      "Step 4448: 9.07102e-03, stepsizes = 7.8e-08/5.2e-08: \n",
      "Looks like attack has converged after 4449 steps, 94 remaining\n",
      "Step 4449: 9.07102e-03, stepsizes = 7.8e-08/5.2e-08: \n",
      "Looks like attack has converged after 4450 steps, 93 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.04 (100), 0.00 (18)\n",
      "Step 4450: 9.07102e-03, stepsizes = 5.2e-08/3.5e-08: \n",
      "Looks like attack has converged after 4451 steps, 92 remaining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4451: 9.07102e-03, stepsizes = 5.2e-08/3.5e-08: \n",
      "Looks like attack has converged after 4452 steps, 91 remaining\n",
      "Step 4452: 9.07102e-03, stepsizes = 5.2e-08/3.5e-08: \n",
      "Looks like attack has converged after 4453 steps, 90 remaining\n",
      "Step 4453: 9.07102e-03, stepsizes = 5.2e-08/3.5e-08: \n",
      "Looks like attack has converged after 4454 steps, 89 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.10 (100), 0.00 (28)\n",
      "Step 4454: 9.07102e-03, stepsizes = 3.5e-08/2.3e-08: \n",
      "Looks like attack has converged after 4455 steps, 88 remaining\n",
      "  Success rate too low, decreasing source step:  0.08 ( 25), 0.00 (30)\n",
      "Step 4455: 9.07102e-03, stepsizes = 3.5e-08/1.5e-08: \n",
      "Looks like attack has converged after 4456 steps, 87 remaining\n",
      "Step 4456: 9.07102e-03, stepsizes = 3.5e-08/1.5e-08: \n",
      "Looks like attack has converged after 4457 steps, 86 remaining\n",
      "Step 4457: 9.07102e-03, stepsizes = 3.5e-08/1.5e-08: \n",
      "Looks like attack has converged after 4458 steps, 85 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.09 (100), 0.00 ( 7)\n",
      "Step 4458: 9.07102e-03, stepsizes = 2.3e-08/1.0e-08: \n",
      "Looks like attack has converged after 4459 steps, 84 remaining\n",
      "Step 4459: 9.07102e-03, stepsizes = 2.3e-08/1.0e-08: \n",
      "Looks like attack has converged after 4460 steps, 83 remaining\n",
      "Step 4460: 9.07102e-03, stepsizes = 2.3e-08/1.0e-08: \n",
      "Looks like attack has converged after 4461 steps, 82 remaining\n",
      "Step 4461: 9.07102e-03, stepsizes = 2.3e-08/1.0e-08: \n",
      "Looks like attack has converged after 4462 steps, 81 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.06 (100), 0.00 (13)\n",
      "Step 4462: 9.07102e-03, stepsizes = 1.5e-08/6.9e-09: \n",
      "Looks like attack has converged after 4463 steps, 80 remaining\n",
      "Step 4463: 9.07102e-03, stepsizes = 1.5e-08/6.9e-09: \n",
      "Looks like attack has converged after 4464 steps, 79 remaining\n",
      "Step 4464: 9.07102e-03, stepsizes = 1.5e-08/6.9e-09: \n",
      "Looks like attack has converged after 4465 steps, 78 remaining\n",
      "Step 4465: 9.07102e-03, stepsizes = 1.5e-08/6.9e-09: \n",
      "Looks like attack has converged after 4466 steps, 77 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.18 (100), 0.00 (30)\n",
      "Step 4466: 9.07102e-03, stepsizes = 1.0e-08/3.1e-09: \n",
      "Looks like attack has converged after 4467 steps, 76 remaining\n",
      "Step 4467: 9.07102e-03, stepsizes = 1.0e-08/3.1e-09: \n",
      "Looks like attack has converged after 4468 steps, 75 remaining\n",
      "Step 4468: 9.07102e-03, stepsizes = 1.0e-08/3.1e-09: \n",
      "Looks like attack has converged after 4469 steps, 74 remaining\n",
      "Step 4469: 9.07102e-03, stepsizes = 1.0e-08/3.1e-09: \n",
      "Looks like attack has converged after 4470 steps, 73 remaining\n",
      "Step 4470: 9.07102e-03, stepsizes = 1.0e-08/3.1e-09: \n",
      "Looks like attack has converged after 4471 steps, 72 remaining\n",
      "  Success rate too low, decreasing source step:  0.28 (100), 0.00 (30)\n",
      "Step 4471: 9.07102e-03, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 4472 steps, 71 remaining\n",
      "Step 4472: 9.07102e-03, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 4473 steps, 70 remaining\n",
      "Step 4473: 9.07102e-03, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 4474 steps, 69 remaining\n",
      "Step 4474: 9.07102e-03, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 4475 steps, 68 remaining\n",
      "Step 4475: 9.07102e-03, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 4476 steps, 67 remaining\n",
      "  Success rate too low, decreasing source step:  0.31 (100), 0.00 (30)\n",
      "Step 4476: 9.07102e-03, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 4477 steps, 66 remaining\n",
      "Step 4477: 9.07102e-03, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 4478 steps, 65 remaining\n",
      "Step 4478: 9.07102e-03, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 4479 steps, 64 remaining\n",
      "Step 4479: 9.07102e-03, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 4480 steps, 63 remaining\n",
      "Step 4480: 9.07102e-03, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 4481 steps, 62 remaining\n",
      "  Success rate too low, decreasing source step:  0.25 (100), 0.00 (30)\n",
      "Step 4481: 9.07102e-03, stepsizes = 1.0e-08/9.0e-10: \n",
      "Looks like attack has converged after 4482 steps, 61 remaining\n",
      "Step 4482: 9.07102e-03, stepsizes = 1.0e-08/9.0e-10: \n",
      "Looks like attack has converged after 4483 steps, 60 remaining\n",
      "Step 4483: 9.07102e-03, stepsizes = 1.0e-08/9.0e-10: \n",
      "Looks like attack has converged after 4484 steps, 59 remaining\n",
      "Step 4484: 9.07102e-03, stepsizes = 1.0e-08/9.0e-10: \n",
      "Looks like attack has converged after 4485 steps, 58 remaining\n",
      "Step 4485: 9.07102e-03, stepsizes = 1.0e-08/9.0e-10: \n",
      "Looks like attack has converged after 4486 steps, 57 remaining\n",
      "  Success rate too low, decreasing source step:  0.26 (100), 0.00 (30)\n",
      "Step 4486: 9.07102e-03, stepsizes = 1.0e-08/6.0e-10: \n",
      "Looks like attack has converged after 4487 steps, 56 remaining\n",
      "Step 4487: 9.07102e-03, stepsizes = 1.0e-08/6.0e-10: \n",
      "Looks like attack has converged after 4488 steps, 55 remaining\n",
      "Step 4488: 9.07102e-03, stepsizes = 1.0e-08/6.0e-10: \n",
      "Looks like attack has converged after 4489 steps, 54 remaining\n",
      "Step 4489: 9.07102e-03, stepsizes = 1.0e-08/6.0e-10: \n",
      "Looks like attack has converged after 4490 steps, 53 remaining\n",
      "Step 4490: 9.07102e-03, stepsizes = 1.0e-08/6.0e-10: \n",
      "Looks like attack has converged after 4491 steps, 52 remaining\n",
      "  Success rate too low, decreasing source step:  0.22 (100), 0.00 (30)\n",
      "Step 4491: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4492 steps, 51 remaining\n",
      "Step 4492: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4493 steps, 50 remaining\n",
      "Step 4493: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4494 steps, 49 remaining\n",
      "Step 4494: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4495 steps, 48 remaining\n",
      "Step 4495: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4496 steps, 47 remaining\n",
      "Step 4496: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4497 steps, 46 remaining\n",
      "  Success rate too low, decreasing source step:  0.22 (100), 0.00 (30)\n",
      "Step 4497: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4498 steps, 45 remaining\n",
      "Step 4498: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4499 steps, 44 remaining\n",
      "Step 4499: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4500 steps, 43 remaining\n",
      "Step 4500: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10:  (took 0.84592 seconds)\n",
      "Step 4500: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4501 steps, 42 remaining\n",
      "Step 4501: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4502 steps, 41 remaining\n",
      "Step 4502: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4503 steps, 40 remaining\n",
      "  Success rate too low, decreasing source step:  0.23 (100), 0.00 (30)\n",
      "Step 4503: 9.07102e-03, stepsizes = 1.0e-08/1.8e-10: \n",
      "Looks like attack has converged after 4504 steps, 39 remaining\n",
      "Step 4504: 9.07102e-03, stepsizes = 1.0e-08/1.8e-10: \n",
      "Looks like attack has converged after 4505 steps, 38 remaining\n",
      "Step 4505: 9.07102e-03, stepsizes = 1.0e-08/1.8e-10: \n",
      "Looks like attack has converged after 4506 steps, 37 remaining\n",
      "Step 4506: 9.07102e-03, stepsizes = 1.0e-08/1.8e-10: \n",
      "Looks like attack has converged after 4507 steps, 36 remaining\n",
      "Step 4507: 9.07102e-03, stepsizes = 1.0e-08/1.8e-10: \n",
      "Looks like attack has converged after 4508 steps, 35 remaining\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.00 (30)\n",
      "Step 4508: 9.07102e-03, stepsizes = 1.0e-08/1.2e-10: \n",
      "Looks like attack has converged after 4509 steps, 34 remaining\n",
      "Step 4509: 9.07102e-03, stepsizes = 1.0e-08/1.2e-10: \n",
      "Looks like attack has converged after 4510 steps, 33 remaining\n",
      "Step 4510: 9.07102e-03, stepsizes = 1.0e-08/1.2e-10: \n",
      "Looks like attack has converged after 4511 steps, 32 remaining\n",
      "Step 4511: 9.07102e-03, stepsizes = 1.0e-08/1.2e-10: \n",
      "Looks like attack has converged after 4512 steps, 31 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.00 (15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4512: 9.07102e-03, stepsizes = 6.9e-09/7.9e-11: \n",
      "Looks like attack has converged after 4513 steps, 30 remaining\n",
      "Step 4513: 9.07102e-03, stepsizes = 6.9e-09/7.9e-11: \n",
      "Looks like attack has converged after 4514 steps, 29 remaining\n",
      "  Success rate too low, decreasing source step:  0.32 ( 50), 0.00 (30)\n",
      "Step 4514: 9.07102e-03, stepsizes = 6.9e-09/5.3e-11: \n",
      "Looks like attack has converged after 4515 steps, 28 remaining\n",
      "Step 4515: 9.07102e-03, stepsizes = 6.9e-09/5.3e-11: \n",
      "Looks like attack has converged after 4516 steps, 27 remaining\n",
      "Step 4516: 9.07102e-03, stepsizes = 6.9e-09/5.3e-11: \n",
      "Looks like attack has converged after 4517 steps, 26 remaining\n",
      "  Success rate too low, decreasing source step:  0.39 (100), 0.00 (30)\n",
      "Step 4517: 9.07102e-03, stepsizes = 6.9e-09/3.5e-11: \n",
      "Looks like attack has converged after 4518 steps, 25 remaining\n",
      "Step 4518: 9.07102e-03, stepsizes = 6.9e-09/3.5e-11: \n",
      "Looks like attack has converged after 4519 steps, 24 remaining\n",
      "Step 4519: 9.07102e-03, stepsizes = 6.9e-09/3.5e-11: \n",
      "Looks like attack has converged after 4520 steps, 23 remaining\n",
      "Step 4520: 9.07102e-03, stepsizes = 6.9e-09/3.5e-11: \n",
      "Looks like attack has converged after 4521 steps, 22 remaining\n",
      "  Success rate too low, decreasing source step:  0.39 (100), 0.00 (30)\n",
      "Step 4521: 9.07102e-03, stepsizes = 6.9e-09/2.4e-11: \n",
      "Looks like attack has converged after 4522 steps, 21 remaining\n",
      "Step 4522: 9.07102e-03, stepsizes = 6.9e-09/2.4e-11: \n",
      "Looks like attack has converged after 4523 steps, 20 remaining\n",
      "Step 4523: 9.07102e-03, stepsizes = 6.9e-09/2.4e-11: \n",
      "Looks like attack has converged after 4524 steps, 19 remaining\n",
      "Step 4524: 9.07102e-03, stepsizes = 6.9e-09/2.4e-11: \n",
      "Looks like attack has converged after 4525 steps, 18 remaining\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.00 (30)\n",
      "Step 4525: 9.07102e-03, stepsizes = 6.9e-09/1.6e-11: \n",
      "Looks like attack has converged after 4526 steps, 17 remaining\n",
      "Step 4526: 9.07102e-03, stepsizes = 6.9e-09/1.6e-11: \n",
      "Looks like attack has converged after 4527 steps, 16 remaining\n",
      "Step 4527: 9.07102e-03, stepsizes = 6.9e-09/1.6e-11: \n",
      "Looks like attack has converged after 4528 steps, 15 remaining\n",
      "Step 4528: 9.07102e-03, stepsizes = 6.9e-09/1.6e-11: \n",
      "Looks like attack has converged after 4529 steps, 14 remaining\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.00 (30)\n",
      "Step 4529: 9.07102e-03, stepsizes = 6.9e-09/1.0e-11: \n",
      "Looks like attack has converged after 4530 steps, 13 remaining\n",
      "Step 4530: 9.07102e-03, stepsizes = 6.9e-09/1.0e-11: \n",
      "Looks like attack has converged after 4531 steps, 12 remaining\n",
      "Step 4531: 9.07102e-03, stepsizes = 6.9e-09/1.0e-11: \n",
      "Looks like attack has converged after 4532 steps, 11 remaining\n",
      "Step 4532: 9.07102e-03, stepsizes = 6.9e-09/1.0e-11: \n",
      "Looks like attack has converged after 4533 steps, 10 remaining\n",
      "Step 4533: 9.07102e-03, stepsizes = 6.9e-09/1.0e-11: \n",
      "Looks like attack has converged after 4534 steps, 9 remaining\n",
      "  Success rate too low, decreasing source step:  0.33 (100), 0.00 (30)\n",
      "Step 4534: 9.07102e-03, stepsizes = 6.9e-09/7.0e-12: \n",
      "Looks like attack has converged after 4535 steps, 8 remaining\n",
      "Step 4535: 9.07102e-03, stepsizes = 6.9e-09/7.0e-12: \n",
      "Looks like attack has converged after 4536 steps, 7 remaining\n",
      "Step 4536: 9.07102e-03, stepsizes = 6.9e-09/7.0e-12: \n",
      "Looks like attack has converged after 4537 steps, 6 remaining\n",
      "Step 4537: 9.07102e-03, stepsizes = 6.9e-09/7.0e-12: \n",
      "Looks like attack has converged after 4538 steps, 5 remaining\n",
      "  Success rate too low, decreasing source step:  0.34 (100), 0.00 (30)\n",
      "Step 4538: 9.07102e-03, stepsizes = 6.9e-09/4.6e-12: \n",
      "Looks like attack has converged after 4539 steps, 4 remaining\n",
      "Step 4539: 9.07102e-03, stepsizes = 6.9e-09/4.6e-12: \n",
      "Looks like attack has converged after 4540 steps, 3 remaining\n",
      "Step 4540: 9.07102e-03, stepsizes = 6.9e-09/4.6e-12: \n",
      "Looks like attack has converged after 4541 steps, 2 remaining\n",
      "  Success rate too low, decreasing source step:  0.36 (100), 0.00 (30)\n",
      "Step 4541: 9.07102e-03, stepsizes = 6.9e-09/3.1e-12: \n",
      "Looks like attack has converged after 4542 steps, 1 remaining\n",
      "Time since beginning: 3089.00910\n",
      "   1.4% for generation (42.57724)\n",
      "   13.9% for spherical prediction (430.48540)\n",
      "   75.1% for prediction (2321.06994)\n",
      "   0.0% for hyperparameter update (1.03542)\n",
      "   9.5% for the rest (293.84109)\n"
     ]
    }
   ],
   "source": [
    "attack_params = {\n",
    "    'iterations': 10000,\n",
    "    'max_directions': 25,\n",
    "    'starting_point': None,\n",
    "    'initialization_attack': None,\n",
    "    'log_every_n_steps': 100,\n",
    "    'spherical_step': 0.5,\n",
    "    'source_step': 0.05,\n",
    "    'step_adaptation': 1.5,\n",
    "    'batch_size': 1,\n",
    "    'tune_batch_size': True, \n",
    "    'threaded_rnd': True, \n",
    "    'threaded_gen': True, \n",
    "    'alternative_generator': False\n",
    "}\n",
    "\n",
    "num = 1\n",
    "x_adv = np.zeros_like(x_test[:num].numpy())\n",
    "for i in range(num):\n",
    "    x_adv[i] = attack(x_test[i].numpy(), label=y_test[i].numpy(), \n",
    "                      unpack=True, verbose=True, **attack_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = dknn.classify(torch.tensor(x_adv))\n",
    "print((y_pred.argmax(1) == y_test[:num].numpy()).sum() / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6667733\n"
     ]
    }
   ],
   "source": [
    "dist = np.sqrt(np.sum((x_adv - x_test[:num].numpy())**2, (1, 2, 3)))\n",
    "print(dist.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f352498c588>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[0].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foolbox.criteria import Misclassification\n",
    "from foolbox.distances import MeanSquaredDistance, Linfinity \n",
    "\n",
    "criterion = Misclassification()\n",
    "distance = MeanSquaredDistance\n",
    "# distance = Linfinity\n",
    "\n",
    "attack = foolbox.attacks.SinglePixelAttack(\n",
    "    model=dknn_fb, criterion=criterion, distance=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10\n",
    "x_adv = np.zeros_like(x_test[:num].numpy())\n",
    "for i in range(num):\n",
    "    x_adv[i] = attack(x_test[i].numpy(), label=y_test[i].numpy(), \n",
    "                      unpack=True, max_pixels=784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "y_pred = dknn.classify(torch.tensor(x_adv))\n",
    "print((y_pred.argmax(1) == y_test[:num].numpy()).sum() / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "dist = np.sqrt(np.sum((x_adv - x_test[:num].numpy())**2, (1, 2, 3)))\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-pixel attack never succeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foolbox.criteria import Misclassification\n",
    "from foolbox.distances import MeanSquaredDistance, Linfinity \n",
    "\n",
    "criterion = Misclassification()\n",
    "distance = MeanSquaredDistance\n",
    "# distance = Linfinity\n",
    "\n",
    "attack = foolbox.attacks.LocalSearchAttack(\n",
    "    model=dknn_fb, criterion=criterion, distance=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10\n",
    "x_adv = np.zeros_like(x_test[:num].numpy())\n",
    "for i in range(num):\n",
    "    x_adv[i] = attack(x_test[i].numpy(), label=y_test[i].numpy(), \n",
    "                      unpack=True, r=1.5, p=10.0, d=5, t=5, R=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "y_pred = dknn.classify(torch.tensor(x_adv))\n",
    "print((y_pred.argmax(1) == y_test[:num].numpy()).sum() / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "dist = np.sqrt(np.sum((x_adv - x_test[:num].numpy())**2, (1, 2, 3)))\n",
    "print(dist.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f19e78a1278>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD0RJREFUeJzt3X+IXeWdx/HP1yQVtAU1WW0w2Z1s0GFFMF2GuOq6ZllSbKgkRRqaP0KU0ukfLWyhfyghUv+wIMv2h38shanGjtDYFFo1oC6VsOqGLMUYQtXNxsYym2YTMja/C4aY5Lt/zEmZxrnPc3Ofe+45k+/7BTL3nueee76eO5/ce+c5z/OYuwtAPFc1XQCAZhB+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBzR3kwcystZcT3nrrrcn2999/v5Z9I6v7vOWev+TYbX7N3d26eZyVXN5rZvdLekrSHElPu/uTmce3Nvyvv/56sn3FihW17BtZ3ect9/wlx27za95t+Hv+2G9mcyT9m6QvSLpN0jozu63X5wMwWCXf+ZdL2u/uv3P3s5J+Jml1f8oCULeS8N8s6ffT7h+stv0ZMxs1s11mtqvgWAD6rOQPfjN9r/jEd3p3H5M0JrX7Oz8QTck7/0FJi6fdXyTpUFk5AAalJPxvSbrFzJaY2ackfUXStv6UBaBupV19qyT9UFNdfZvd/buZx9f2sb+kW6cbqa6biYmJ5L5DQ0NFx25ztxJmVvKalf4+ddvVV3SRj7u/IumVkucA0Awu7wWCIvxAUIQfCIrwA0ERfiAowg8ENdDx/G1W0ldedz9+nc/d5msESs9Lnf9vdZ7X3O9T6tijo6NdH4d3fiAowg8ERfiBoAg/EBThB4Ii/EBQRUN6L/tgV+hMPqXdPk12x9XdnVbnDLo5qWO3+TUZ1JBe3vmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICj6+btU99TgTWlzf3dO3VOmN6X0nNPPDyCJ8ANBEX4gKMIPBEX4gaAIPxAU4QeCKpq628wmJJ2WdF7SOXcf6UdRnZSMzy7V5mmg2zxu/YEHHujYdvLkyeS+S5cuLWpPaXJJdyl9jcKgpu7ux7z9/+juf+jD8wAYID72A0GVht8l/crM3jaz7j9vAGhc6cf+e9z9kJndKOk1M/sfd39z+gOqfxT4hwFomaJ3fnc/VP2clPSCpOUzPGbM3Ufq/mMggMvTc/jN7Foz+8zF25I+L+ndfhUGoF4lH/tvkvSCmV18ni3u/u99qQpA7QY6nn94eNjHxsY6ts/m+enr1OZ5/R988MFk+549ezq23X333cl9Dxw4kGy/5pprku0fffRRx7am+/nrvGaF8fwAkgg/EBThB4Ii/EBQhB8IivADQfVjVN+sUOc0zrkppEunmC4Z4rlmzZrkvidOnEi25wwPDyfbFy1a1PNzb9myJdl+5syZZHvqvJSe8ysB7/xAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFSYJbrbPCy2yeHCOVdffXWyPdfXfvTo0Y5t8+fPT+67atWqZPurr76abC8Ztlv3a5K69iN3XQhLdAMoQviBoAg/EBThB4Ii/EBQhB8IivADQYXp588pHXNfp1x/daq2ZcuWJfdNTa0tSXfccUeyvWQ+gHPnziXb582b1/NzS2XLYLdZbonuffv20c8PoDPCDwRF+IGgCD8QFOEHgiL8QFCEHwgqO2+/mW2W9EVJk+5+e7XtBklbJQ1JmpC01t2P11dmXumY+Vw/f51LKueUPP/cuemXODce/5FHHkm2p8brS+n5AObMmZPct1Sbr81owxwO3bzz/0TS/Zdse1TSdne/RdL26j6AWSQbfnd/U9KxSzavljRe3R6XlF4WBkDr9Pqd/yZ3PyxJ1c8b+1cSgEGofa0+MxuVNFr3cQBcnl7f+Y+Y2UJJqn5Odnqgu4+5+4i7j/R4LAA16DX82yRtqG5vkPRSf8oBMCjZ8JvZ85L+S9KwmR00s69KelLSSjP7raSV1X0As0irxvNfd911yf1ffPHFjm25ftPSftc6+/lLxut3056yc+fOZPuSJUuK2k+ePNmx7c4770zum5troE6zoZ++E+btB5BE+IGgCD8QFOEHgiL8QFCEHwiqVV19UZVOG17SDXn8eHokdq77NTekN7V/brhxk+rsGs7tX3psuvoAJBF+ICjCDwRF+IGgCD8QFOEHgiL8QFBXTD//bB6CWadcP/2pU6eS7ZOTHSdpkiTNnz8/2f7EE090bLv33nuT+5b2pZeo+/elrtpZohtAFuEHgiL8QFCEHwiK8ANBEX4gKMIPBNXeAdV9lhszn2sv6ffN9bWfOHGi5+fOPX9u+uv33nuv6Ni52p9++umObevXr0/uW9oXnnrNSpZkzz13qUFdk8I7PxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ElR3Pb2abJX1R0qS7315te1zS1yR9WD1so7u/kj1Y4Xj+OpfJblLuOoCc1NLlmzZtSu77xhtvJNtz8/rv27cv2f7xxx93bMu9ZqXrGaQ0OS9/3fo5b/9PJN0/w/YfuPuy6r9s8AG0Szb87v6mpGMDqAXAAJV85/+mmf3GzDab2fV9qwjAQPQa/h9JWippmaTDkr7X6YFmNmpmu8xsV4/HAlCDnsLv7kfc/by7X5D0Y0nLE48dc/cRdx/ptUgA/ddT+M1s4bS7X5L0bn/KATAo2SG9Zva8pBWSFpjZQUnfkbTCzJZJckkTkr5eY40AapANv7uvm2HzMzXUktVk32mqXzfX35xrLx3Pn/Lss88m23P9+Ll5+XPzAYyOjibbU3L9/CXzJDR9XUid8/Z3iyv8gKAIPxAU4QeCIvxAUIQfCIrwA0FdMUt0z2Z1TlH98ssvJ/dds2ZNsj23RPeCBQuS7du3b+/YVtrd1uQS3XUONy7VzyG9AK5AhB8IivADQRF+ICjCDwRF+IGgCD8Q1ED7+YeHh31sbKxje9PDLJtS2me8aNGijm2nT59O7psbTnz+/Plk+9y56VHhqWG3dS5N3o/nL1EytXfp7wP9/ACSCD8QFOEHgiL8QFCEHwiK8ANBEX4gqFk1nr+tS3Q3vVzz2rVrO7Zt3bo1ue99992XbF+5cmWy/bHHHku279y5s2PbqlWrkvvW2U/f9GtWJ/r5ASQRfiAowg8ERfiBoAg/EBThB4Ii/EBQ2SW6zWyxpOckfVbSBUlj7v6Umd0gaaukIUkTkta6e3q950Jt7XvN1VV3n3JqGe2jR48m9922bVuy/dixY8n2XD//2bNnO7bl+vHb3Bff5tq61c07/zlJ33b3v5H0d5K+YWa3SXpU0nZ3v0XS9uo+gFkiG353P+zuu6vbpyXtlXSzpNWSxquHjUtKL/0CoFUu6zu/mQ1J+pykX0u6yd0PS1P/QEi6sd/FAahP9jv/RWb2aUm/kPQtdz9l1tXlwzKzUUmjvZUHoC5dvfOb2TxNBf+n7v7LavMRM1tYtS+UNOOKju4+5u4j7j7Sj4IB9Ec2/Db1Fv+MpL3u/v1pTdskbahub5D0Uv/LA1CXbj723yNpvaR3zGxPtW2jpCcl/dzMvirpgKQv11Pi7Ff3UtTHj/few7p///5k+1133ZVsz02fXaK0C7Wp554tsuF39x2SOn3B/6f+lgNgULjCDwiK8ANBEX4gKMIPBEX4gaAIPxDUrJq6u8RsHoI5b968ZPuZM2c6tl11Vfrf9x07diTbc0t0l5y3K3WJ7aYxdTeAJMIPBEX4gaAIPxAU4QeCIvxAUIQfCKrrabxmuzr7ZevuE75w4UKyfXx8vGPbww8/nNz3gw8+SLY/9NBDyfYSbe7Hj4B3fiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IKkw/f53qHtudG1O/adOmjm25+Rp2797dU03dSo3Zr7ufP9WX3+bx+IPCOz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBJWdt9/MFkt6TtJnJV2QNObuT5nZ45K+JunD6qEb3f2VzHM1Nm9/qTrXgi89dsnzl45rb3N/+cTERMe2oaGhgdUxaN3O29/NRT7nJH3b3Xeb2WckvW1mr1VtP3D3f+21SADNyYbf3Q9LOlzdPm1meyXdXHdhAOp1Wd/5zWxI0uck/bra9E0z+42ZbTaz6zvsM2pmu8xsV1GlAPqq6/Cb2acl/ULSt9z9lKQfSVoqaZmmPhl8b6b93H3M3UfcfaQP9QLok67Cb2bzNBX8n7r7LyXJ3Y+4+3l3vyDpx5KW11cmgH7Lht/MTNIzkva6+/enbV847WFfkvRu/8sDUJdu/tp/j6T1kt4xsz3Vto2S1pnZMkkuaULS12upsCWa7NJqc3caZq9u/tq/Q9JM/YbJPn0A7cYVfkBQhB8IivADQRF+ICjCDwRF+IGgskN6+3qwWTykF4PX5HDjupddr1O3Q3p55weCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAbdz/+hpP+dtmmBpD8MrIDL09ba2lqXRG296mdtf+Xuf9HNAwca/k8c3GxXW+f2a2ttba1LorZeNVUbH/uBoAg/EFTT4R9r+Pgpba2trXVJ1NarRmpr9Ds/gOY0/c4PoCGNhN/M7jezfWa238webaKGTsxswszeMbM9TS8xVi2DNmlm707bdoOZvWZmv61+zrhMWkO1PW5m/1eduz1mtqqh2hab2X+Y2V4ze8/M/rna3ui5S9TVyHkb+Md+M5sj6X1JKyUdlPSWpHXu/t8DLaQDM5uQNOLujfcJm9k/SPqjpOfc/fZq279IOubuT1b/cF7v7o+0pLbHJf2x6ZWbqwVlFk5fWVrSGkkPqcFzl6hrrRo4b0288y+XtN/df+fuZyX9TNLqBupoPXd/U9KxSzavljRe3R7X1C/PwHWorRXc/bC7765un5Z0cWXpRs9doq5GNBH+myX9ftr9g2rXkt8u6Vdm9raZjTZdzAxuqpZNv7h8+o0N13Op7MrNg3TJytKtOXe9rHjdb02Ef6YphtrU5XCPu/+tpC9I+kb18Rbd6Wrl5kGZYWXpVuh1xet+ayL8ByUtnnZ/kaRDDdQxI3c/VP2clPSC2rf68JGLi6RWPycbrudP2rRy80wrS6sF565NK143Ef63JN1iZkvM7FOSviJpWwN1fIKZXVv9IUZmdq2kz6t9qw9vk7Shur1B0ksN1vJn2rJyc6eVpdXwuWvbiteNXORTdWX8UNIcSZvd/bsDL2IGZvbXmnq3l6YWMd3SZG1m9rykFZoa9XVE0nckvSjp55L+UtIBSV9294H/4a1DbSs09dH1Tys3X/yOPeDa/l7Sf0p6R9KFavNGTX2/buzcJepapwbOG1f4AUFxhR8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaD+H38PlETAyLb8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[5].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local-search attack only works 1/10 and noise is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers = ['relu1', 'relu2', 'relu3', 'fc']\n",
    "dknn = DKNNL2(net, x_train, y_train, x_valid, y_valid, layers, \n",
    "              k=75, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9607, 0.8579, 0.5358, 0.5033\n"
     ]
    }
   ],
   "source": [
    "# Verify Lipschitzness\n",
    "\n",
    "x = x_test.requires_grad_(True)[:1000]\n",
    "\n",
    "norms = compute_spnorm(x, dknn, layers)\n",
    "print(', '.join('%.4f' % i for i in norms.mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9607, 0.8579, 0.5358, 0.5033\n"
     ]
    }
   ],
   "source": [
    "print(', '.join('%.4f' % i for i in norms.mean(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0235, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3.297e+03, 2.081e+03, 1.830e+03, 1.407e+03, 8.290e+02, 3.950e+02,\n",
       "        1.240e+02, 3.300e+01, 3.000e+00, 1.000e+00]),\n",
       " array([5.9045851e-07, 1.1226014e-02, 2.2451438e-02, 3.3676863e-02,\n",
       "        4.4902287e-02, 5.6127708e-02, 6.7353129e-02, 7.8578554e-02,\n",
       "        8.9803979e-02, 1.0102940e-01, 1.1225483e-01], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gap = y_pred.max(1)[0] - y_pred.sort()[0][:, -2]\n",
    "print(gap.mean().data)\n",
    "plt.hist(gap.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_guide_samples(dknn, x, label, k=1, layer='relu1'):\n",
    "    \"\"\"\n",
    "    find k nearest neighbors of the same class (not equal to y_Q) but\n",
    "    closest to Q\n",
    "    \"\"\"\n",
    "    num_classes = dknn.num_classes\n",
    "    nn = torch.zeros((k, ) + x.size()).permute(1, 0, 2, 3, 4)\n",
    "    D, I = dknn.get_neighbors(\n",
    "        x, k=dknn.x_train.size(0), layers=[layer])[0]\n",
    "\n",
    "    for i, (d, ind) in enumerate(zip(D, I)):\n",
    "        mean_dist = np.zeros((num_classes, ))\n",
    "        for j in range(num_classes):\n",
    "            mean_dist[j] = np.mean(\n",
    "                d[np.where(dknn.y_train[ind] == j)[0]][:k])\n",
    "        # TODO: this may depend on the index used\n",
    "        mean_dist[label[i]] += 1e20\n",
    "        nearest_label = mean_dist.argmin()\n",
    "        # mean_dist[label[i]] -= INFTY\n",
    "        # nearest_label = mean_dist.argmax()\n",
    "        nn_ind = np.where(dknn.y_train[ind] == nearest_label)[0][:k]\n",
    "        nn[i] = dknn.x_train[ind[nn_ind]]\n",
    "\n",
    "    return nn\n",
    "\n",
    "\n",
    "def find_nn_diff_class(x_train, y_train, x, label, num_classes=10):\n",
    "    \"\"\"\n",
    "    find k nearest neighbors of the same class (not equal to y_Q) but\n",
    "    closest to Q\n",
    "    \"\"\"\n",
    "    nn = torch.zeros_like(x)\n",
    "\n",
    "    for i in range(x.size(0)):\n",
    "        d = ((x[i].unsqueeze(0) - x_train)**2).view(x_train.size(0), -1).sum(1)\n",
    "        sorted_ind = d.argsort()\n",
    "        for j in range(x_train.size(0)):\n",
    "            if label[i] != y_train[sorted_ind[j]]:\n",
    "                nn[i] = x_train[sorted_ind[j]]\n",
    "                break\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['relu3']\n",
    "dknn = DKNNL2(net, x_train, y_train, x_valid, y_valid, layers, \n",
    "              k=1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8868\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = dknn.classify(x_test)\n",
    "    print((y_pred.argmax(1) == y_test.numpy()).sum() / y_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rep_train = dknn.get_activations(x_train, requires_grad=False, device='cpu')[layers[0]]\n",
    "rep_test = dknn.get_activations(x_test, requires_grad=False, device='cpu')[layers[0]]\n",
    "rep_valid = dknn.get_activations(x_valid, requires_grad=False, device='cpu')[layers[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNL2(rep_train, y_train, rep_valid, y_valid, k=1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_nn = knn.find_nn_diff_class(rep_test, y_test)\n",
    "rep_adv = knn.get_min_dist(rep_test, y_test, rep_nn, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = ((rep_test - rep_nn)**2).view(rep_test.size(0), -1).sum(1).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4930)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 195., 1660., 3101., 2666., 1381.,  600.,  265.,  100.,   23.,\n",
       "           9.]),\n",
       " array([0.15380737, 0.26018655, 0.36656573, 0.47294492, 0.57932407,\n",
       "        0.6857033 , 0.7920824 , 0.89846164, 1.0048409 , 1.11122   ,\n",
       "        1.2175992 ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dist.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "perts = [1, 2, 3]\n",
    "a = []\n",
    "for pert in perts:\n",
    "    a.append((dist.cpu().detach().numpy() > math.sqrt(2) * pert).mean())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.knn import KNNL2\n",
    "\n",
    "knn = KNNL2(x_train, y_train, x_valid, y_valid, k=1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9683\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.classify(x_test)\n",
    "print((y_pred.argmax(1) == y_test.numpy()).sum() / y_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nn = knn.find_nn_diff_class(x_test, y_test)\n",
    "x_adv = knn.get_min_dist(x_test, y_test, x_nn, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = ((x_adv - x_test)**2).view(x_test.size(0), -1).sum(1).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8145)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 679.,  736., 1334., 2089., 2505., 1551.,  776.,  288.,   38.,\n",
       "           4.]),\n",
       " array([2.7532987e-03, 4.6384671e-01, 9.2494017e-01, 1.3860335e+00,\n",
       "        1.8471270e+00, 2.3082204e+00, 2.7693138e+00, 3.2304072e+00,\n",
       "        3.6915007e+00, 4.1525941e+00, 4.6136875e+00], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dist.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert = 3\n",
    "(dist.cpu().detach().numpy() > math.sqrt(2) * pert).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc7362b66a0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[0].numpy().reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare dist boundary attack vs. gradient-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_ba = np.array([2.7173007,  4.0937552,  3.4191384,  1.6031944,  3.594387,   2.5332072,\n",
    " 3.1598704,  2.518583,   3.6933036,  3.0579975,  2.9061797,  4.599596,\n",
    " 1.9611715,  2.870469,   4.006445 ,  2.6593897,  3.179534 ,  2.2678053,\n",
    " 2.0259533,  2.5902615,  1.120602 ,  4.2945986,  2.2937922,  1.3928429,\n",
    " 2.6499465,  3.1759217,  3.6729615,  3.3439362,  3.4493277,  0.19866979,\n",
    " 3.5768917,  3.328962 ,  3.0830424,  3.2222579,  1.7716709,  3.6560466,\n",
    " 3.0997128,  3.3147688,  2.922413 ,  1.3362972, 3.202453  , 1.2746702,\n",
    " 2.8451028,  3.0420713,  1.318453 ,  2.1046436, 2.1327326,  3.6705463,\n",
    " 4.567136 ,  2.427258 ,  3.970506 ,  2.5043063, 2.5586333,  3.8086731,\n",
    " 3.4100363,  3.1971416,  3.7394974,  1.1956439, 1.8544152,  0.78272134,\n",
    " 3.3026252,  0.26096398, 2.7703447,  1.7209489, 2.395035 ,  3.507264,\n",
    " 4.095256 ,  3.8400688 , 2.6986618,  0.11038034, 3.152288,   3.9797633,\n",
    " 2.1022258,  0.53569657, 1.976561 ,  4.650692 ,  1.0298939,  2.0315764,\n",
    " 4.4992127,  1.8037227 , 4.5831075,  3.06325  ,  4.4558234,  4.922709,\n",
    " 3.6997101,  3.4707859 , 2.8384383,  3.68947  ,  2.4836898,  3.036961,\n",
    " 2.8549154,  2.9565084 , 3.5523722,  2.0092041,  2.9175615])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ba fail 3, 10, 11, 20, 92\n",
    "# grad fail 17,  36 --> shifted to 14, 32 for ba\n",
    "fail_ind = [3, 10, 11, 17, 20, 36, 92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3b7ae85b38>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.delete(dist_ba, [14, 32]), np.delete(dist[:100], fail_ind))\n",
    "plt.plot(np.arange(9), np.arange(9), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dist_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 17,  36, 252, 521, 626, 676, 756]),)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_pred.argmax(1) == y_test[:1000].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.7173007 , 4.0937552 , 3.4191384 , 3.594387  , 2.5332072 ,\n",
       "       3.1598704 , 2.518583  , 3.6933036 , 3.0579975 , 1.9611715 ,\n",
       "       2.870469  , 4.006445  , 2.6593897 , 3.179534  , 2.0259533 ,\n",
       "       2.5902615 , 4.2945986 , 2.2937922 , 1.3928429 , 2.6499465 ,\n",
       "       3.1759217 , 3.6729615 , 3.3439362 , 3.4493277 , 0.19866979,\n",
       "       3.5768917 , 3.328962  , 3.0830424 , 3.2222579 , 1.7716709 ,\n",
       "       3.6560466 , 3.3147688 , 2.922413  , 1.3362972 , 3.202453  ,\n",
       "       1.2746702 , 2.8451028 , 3.0420713 , 1.318453  , 2.1046436 ,\n",
       "       2.1327326 , 3.6705463 , 4.567136  , 2.427258  , 3.970506  ,\n",
       "       2.5043063 , 2.5586333 , 3.8086731 , 3.4100363 , 3.1971416 ,\n",
       "       3.7394974 , 1.1956439 , 1.8544152 , 0.78272134, 3.3026252 ,\n",
       "       0.26096398, 2.7703447 , 1.7209489 , 2.395035  , 3.507264  ,\n",
       "       4.095256  , 3.8400688 , 2.6986618 , 0.11038034, 3.152288  ,\n",
       "       3.9797633 , 2.1022258 , 0.53569657, 1.976561  , 4.650692  ,\n",
       "       1.0298939 , 2.0315764 , 4.4992127 , 1.8037227 , 4.5831075 ,\n",
       "       3.06325   , 4.4558234 , 4.922709  , 3.6997101 , 3.4707859 ,\n",
       "       2.8384383 , 3.68947   , 2.4836898 , 3.036961  , 2.8549154 ,\n",
       "       2.9565084 , 2.0092041 , 2.9175615 ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.delete(dist_ba, fail_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_ba.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'relu1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dknn = DKNNL2(net, x_train, y_train, x_valid, y_valid, [layer], \n",
    "              k=1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_train = dknn.get_activations(\n",
    "    x_train, requires_grad=False, device='cpu')[layer]\n",
    "rep_test = dknn.get_activations(\n",
    "    x_test, requires_grad=False, device='cpu')[layer]\n",
    "rep_valid = dknn.get_activations(\n",
    "    x_valid, requires_grad=False, device='cpu')[layer]\n",
    "rep_train = rep_train.numpy()\n",
    "rep_test = rep_test.numpy()\n",
    "rep_valid = rep_valid.numpy()\n",
    "\n",
    "# rep_train = dknn.get_activations(\n",
    "#     x_train, requires_grad=False, device='cuda')[layer]\n",
    "# rep_test = dknn.get_activations(\n",
    "#     x_test, requires_grad=False, device='cuda')[layer]\n",
    "# rep_valid = dknn.get_activations(\n",
    "#     x_valid, requires_grad=False, device='cuda')[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep_test = dknn.get_activations(\n",
    "#     x_test, requires_grad=False, device='cpu')[layer]\n",
    "# rep_test = rep_test.numpy()\n",
    "\n",
    "rep_test = rep_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.32913279533386\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "knn = KNNL2NP(rep_train, y_train, rep_valid, y_valid,\n",
    "              k=1, num_classes=10)\n",
    "\n",
    "# knn = KNNL2(rep_train, y_train,\n",
    "#             rep_valid, y_valid,\n",
    "#             k=1, num_classes=10)\n",
    "\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 to 8 reduce time from 0.15 to 0.047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04758858680725098\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "D, I = knn.get_neighbors(rep_test, k=1)\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27398037910461426\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(100):\n",
    "    D, I = knn.get_neighbors(rep_test[i][np.newaxis], k=1)\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "II == I[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.46697807312012\n"
     ]
    }
   ],
   "source": [
    "rep_test = rep_test[:100]\n",
    "\n",
    "start_time = time.time()\n",
    "DD = np.zeros(len(rep_test))\n",
    "II = np.zeros(len(rep_test))\n",
    "\n",
    "for i in range(len(rep_test)):\n",
    "    dist = np.linalg.norm((rep_test[i] - rep_train).reshape(len(rep_train), -1), axis=1)\n",
    "    ind = dist.argmin()\n",
    "    II[i] = ind\n",
    "    DD[i] = dist[ind]\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003238677978515625\n"
     ]
    }
   ],
   "source": [
    "rep_test = rep_test[:100]\n",
    "\n",
    "start_time = time.time()\n",
    "DD = np.zeros(len(rep_test))\n",
    "# II = np.zeros(len(rep_test))\n",
    "\n",
    "for i in range(len(rep_test)):\n",
    "    dist = np.linalg.norm((rep_test[i] - rep_train[I[i, 0]]).reshape(-1))\n",
    "#     ind = dist.argmin()\n",
    "#     II[i] = ind\n",
    "#     DD[i] = dist[ind]\n",
    "    DD[i] = dist\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_nn = knn.find_nn_diff_class(rep_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is doing rep_nn - rep_test line search\n",
    "rep_adv = knn.get_min_dist(rep_test, y_test, rep_nn, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "len 2262\n",
      "1\n",
      "len 28413\n",
      "4\n",
      "len 34315\n",
      "5\n",
      "len 835\n",
      "6\n",
      "len 47315\n",
      "7\n",
      "len 47927\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "rep_adv = knn.opt_attack(rep_test[:10], y_test[:10], pert_bound=2, iterations=10)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6109.93701338768\n"
     ]
    }
   ],
   "source": [
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cur_ind = knn.get_neighbors(rep_test, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_cur = knn.x_train[cur_ind[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cur_nn = rep_nn - rep_cur\n",
    "ind = np.where(np.sum(dir_cur_nn, (1, 2, 3)) != 0)[0][:10]\n",
    "dir_cur_nn = dir_cur_nn[ind]\n",
    "\n",
    "rep_test = rep_test[ind]\n",
    "rep_cur = rep_cur[ind]\n",
    "rep_adv = rep_adv[ind]\n",
    "rep_nn = rep_nn[ind]\n",
    "\n",
    "dir_cur_test = rep_test - rep_cur\n",
    "\n",
    "proj_len = np.sum(dir_cur_nn * dir_cur_test, (1, 2, 3)) / \\\n",
    "    np.linalg.norm(dir_cur_nn.reshape(len(ind), -1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_proj = rep_cur + proj_len.reshape(len(ind), 1, 1, 1) * dir_cur_nn / \\\n",
    "    np.linalg.norm(dir_cur_test.reshape(len(ind), -1), axis=1).reshape(len(ind), 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = rep_proj - (rep_cur + rep_nn) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2 = rep_test - rep_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.5359656 , -1.654562  , -0.19769609, -0.79179096, -0.7190167 ,\n",
       "       -0.4098643 , -0.14255747, -0.13414311,  0.25864434, -0.84196204],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(dir1.reshape(len(ind), -1), axis=1) - \\\n",
    "np.linalg.norm(dir2.reshape(len(ind), -1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 2.06411960e-06,  1.53112429e-04, -2.81240529e-04, ...,\n",
       "           1.15505864e-04, -6.04362867e-05,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.51331060e-05, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -2.08362962e-05, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -8.75978367e-06, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 2.83887130e-06, -1.89587954e-05, -2.45307863e-04, ...,\n",
       "           1.28549407e-04, -6.93261391e-05,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -5.35257277e-06, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -2.72718262e-05, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.34761558e-05, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  1.57582792e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.65316174e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  6.09169729e-05, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -6.26338704e-04, ...,\n",
       "           5.65410417e-04,  9.25785862e-04,  1.13635429e-03],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -3.38897313e-04, ...,\n",
       "           6.17242418e-04,  8.27293436e-04,  2.86677910e-04],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  5.91836797e-05, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  3.00009880e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  2.36361477e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -6.32539508e-04, ...,\n",
       "           6.89571432e-04,  1.13540713e-03,  1.16045645e-03],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -2.42248454e-04, ...,\n",
       "           6.55793410e-04,  8.27830692e-04,  3.04347341e-04],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.01602745e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -3.65264314e-05, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           5.98419341e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           1.57663948e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           8.03332659e-04,  1.25868872e-04,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           1.16264785e-03,  9.49381865e-05,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           5.09158883e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           6.85417734e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           1.65930041e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           9.02339758e-04,  1.37320501e-04,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           1.08455343e-03,  9.08834627e-05,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           5.12467173e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -1.00402732e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -4.39544347e-05, ...,\n",
       "          -9.97707713e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           2.02425090e-05,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           1.05190920e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -4.43056168e-04,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -1.11290254e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.55459293e-05, ...,\n",
       "          -1.03010028e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -5.72722456e-05,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           1.34187561e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -3.93489026e-04,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_cur_nn / np.linalg.norm(dir_cur_test.reshape(100, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(dir_cur_test.reshape(100, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.5429404, 4.7540145, 1.6396883, 4.4017715, 2.8920696, 1.9550104,\n",
       "       2.8386908, 4.2976856, 4.268932 , 3.5984843, 4.3964534, 4.6141334,\n",
       "       2.4303572, 4.1099367, 1.872004 , 3.386416 , 2.7185218, 3.2608824,\n",
       "       5.1369367, 2.243653 , 3.154792 , 2.6387503, 4.6719103, 2.657795 ,\n",
       "       0.       , 5.1182566, 3.0427396, 3.2279894, 4.6553946, 2.2387195,\n",
       "       3.561123 , 1.7437388, 3.848828 , 4.913284 , 3.094768 , 5.7521324,\n",
       "       3.1924255, 1.5042968, 3.4970992, 2.2462595, 1.8532374, 2.4730437,\n",
       "       3.5004077, 2.5560434, 3.962498 , 2.9110358, 3.421017 , 2.674062 ,\n",
       "       4.053219 , 3.528131 , 2.8836854, 4.485406 , 3.8754244, 2.6241255,\n",
       "       4.699321 , 4.3814144, 3.4391522, 2.1022289, 3.7259045, 3.1375005,\n",
       "       5.136118 , 5.722063 , 2.5642853, 3.8793497, 5.0472045, 0.       ,\n",
       "       3.3695402, 2.815485 , 3.6099753, 4.140637 , 3.8820674, 5.7168493,\n",
       "       5.114574 , 3.826572 , 2.1349726, 3.8785858, 2.9137254, 3.9947731,\n",
       "       2.4986846, 3.7165396, 3.0753493, 3.7309399, 4.890325 , 2.8476346,\n",
       "       5.236655 , 3.9972205, 2.8157895, 4.177303 , 3.829434 , 2.4987748,\n",
       "       3.7148201, 2.8747463, 1.9170463, 3.819681 , 3.0732522, 5.4399114,\n",
       "       2.2358828, 3.3622727, 3.8125243, 2.7437925], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(dir_cur_nn.reshape(100, -1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -6.70605898e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 1.57952309e-05,  1.17313862e-03, -2.02691555e-03, ...,\n",
       "          -1.16413832e-03,  1.93715096e-05,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  4.82797623e-06, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  6.67572021e-06, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  2.80141830e-06, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -7.52395391e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 2.17556953e-05, -1.45316124e-04, -2.16513872e-03, ...,\n",
       "          -1.05631351e-03,  2.22325325e-05,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.72853470e-06, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  8.76188278e-06, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  4.29153442e-06, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  5.90324402e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.31672621e-03, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  6.53922558e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  3.18723917e-03, ...,\n",
       "           1.02207661e-02,  5.46133518e-03,  4.25690413e-03],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  7.93933868e-04, ...,\n",
       "           5.31351566e-03,  3.12101841e-03,  1.07389688e-03],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  2.21669674e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.42580271e-03, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  2.39712000e-03, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  2.70026922e-03, ...,\n",
       "           1.02733374e-02,  6.16925955e-03,  4.34720516e-03],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  5.67495823e-04, ...,\n",
       "           5.53268194e-03,  3.12459469e-03,  1.14011765e-03],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -3.86476517e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -3.21018696e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -4.43339348e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -3.22031975e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.87873840e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.82092190e-04, ...,\n",
       "           1.39474273e-02, -1.48677826e-03,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           6.84434175e-03, -1.42574310e-03,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           3.32492590e-03, -3.31878662e-04,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -6.66975975e-05, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.43587589e-04, ...,\n",
       "           1.35625005e-02, -1.12807751e-03,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           6.83343410e-03, -1.10709667e-03,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           2.65741348e-03, -3.90946865e-04,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           1.98924541e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.68442726e-04, ...,\n",
       "           9.81688499e-04, -7.10123777e-03, -3.09562683e-03],\n",
       "         [ 0.00000000e+00, -1.08480453e-05,  4.12464142e-05, ...,\n",
       "           2.62439251e-04, -7.16853142e-03, -4.43637371e-03],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           1.83582306e-04,  5.30481339e-06,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           2.58517265e-03,  3.16321850e-04,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           1.53917074e-03,  5.19752502e-05,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           2.28190422e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -5.95450401e-05, ...,\n",
       "           1.26731396e-03, -7.15732574e-03, -3.41600180e-03],\n",
       "         [ 0.00000000e+00, -3.81469727e-06,  3.73780727e-04, ...,\n",
       "           1.30009651e-03, -7.70378113e-03, -4.92918491e-03],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          -5.04553318e-04, -9.23871994e-06,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           2.53933668e-03,  2.49147415e-04,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           1.67125463e-03,  5.55515289e-05,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           8.36658478e-03,  2.80129910e-03, -2.14099884e-04],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -3.67224216e-04, ...,\n",
       "          -2.80058384e-03, -6.86585903e-04,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -6.13331795e-05, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -4.82857227e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           8.61191750e-03,  2.77686119e-03, -2.23934650e-04],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -2.79426575e-04, ...,\n",
       "          -2.71773338e-03, -7.34567642e-04,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -2.27332115e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -5.13374805e-04, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir1 - dir2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNL2NP(x_train.numpy(), y_train, x_valid.numpy(), y_valid, k=1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.classify(x_test.numpy())\n",
    "ind = np.where(y_pred.argmax(1) == y_test.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dd410f8e9829>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_nn_diff_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_margin_bound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_nn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/research/entangle-rep/lib/knn.py\u001b[0m in \u001b[0;36mfind_nn_diff_class\u001b[0;34m(self, x, label)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# least one sample of a different class is found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound_diff_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/research/entangle-rep/lib/knn.py\u001b[0m in \u001b[0;36mget_neighbors\u001b[0;34m(self, x, k)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# uncomment when using GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# res.syncDefaultStreamCurrentDevice()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/faiss/__init__.py\u001b[0m in \u001b[0;36mreplacement_search\u001b[0;34m(self, x, k)\u001b[0m\n\u001b[1;32m    129\u001b[0m         self.search_c(n, swig_ptr(x),\n\u001b[1;32m    130\u001b[0m                       \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                       swig_ptr(labels))\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/faiss/swigfaiss_gpu.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, n, x, k, distances, labels)\u001b[0m\n\u001b[1;32m   3771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3772\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3773\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexHNSW_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3775\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_nn = knn.find_nn_diff_class(x_test.numpy()[ind], y_test[ind])\n",
    "gap, ind = knn.get_margin_bound(x_test.numpy()[ind], y_test[ind], x_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ''\n",
    "perts = [0.5, 1, 1.5, 2]\n",
    "for pert in perts[:-1]:\n",
    "    output += '%.4f, ' % (gap[ind] > 2 * pert).mean()\n",
    "output += '%.4f' % (gap[ind] > 2 * perts[-1]).mean()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.classify(x_test.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9659"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred.argmax(1) == y_test.numpy()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8135974"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap[ind].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpt = pickle.load(open('mnist_l2nnn_advtrain.v2.pickle', 'rb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1, 8)\n",
      "(8,)\n",
      "(3, 3, 16, 64)\n",
      "(64,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c81164bf2a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for cpt in checkpt:\n",
    "    print(cpt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = checkpt[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000925"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w.reshape(9*16, 64), ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4931903"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w.reshape(9*16, 64), ord=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.6863008"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(net.fc1.weight.data.cpu().numpy(), ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.2259, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infty_norm_ub(net.fc1.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7494014"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(net.fc1.weight.data.cpu().numpy() / infty_norm_ub(net.fc1.weight.data).cpu().numpy(), ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82836175\n",
      "0.49575722\n",
      "0.6324819\n"
     ]
    }
   ],
   "source": [
    "l2 = np.linalg.norm(net.conv1.weight.data.cpu().numpy().reshape(64, 64), ord=2)\n",
    "print(l2)\n",
    "l2 = np.linalg.norm(net.conv2.weight.data.cpu().numpy().reshape(64 * 6 ** 2, 128), ord=2)\n",
    "print(l2)\n",
    "l2 = np.linalg.norm(net.conv3.weight.data.cpu().numpy().reshape(128 * 5 ** 2, 128), ord=2)\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8353, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "linf = infty_norm_ub(net.conv1.weight.data.view(64, 64))\n",
    "print(linf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rep_train = dknn.get_activations(x_train, requires_grad=False)['fc2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.sum((rep_train[0] - rep_train)**2, 1).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   15,    34,    71, ..., 53997, 53998, 53999]),)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((y_train[torch.argsort(dist)] != 8).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30498, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(dist)[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[30498]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6199, device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[30498]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6199, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((rep_train[0] - rep_train[30498])**2).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = dknn.get_activations(x_train, requires_grad=False)['fc2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.sum((rep[1] - rep)**2, 1).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = dist.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 14382, 11989,  ...,  7952,  7526, 50530], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind[y_train[ind] == y_train[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25245,  8917, 29620,  ..., 13666, 46653, 50487], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind[y_train[ind] != y_train[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0409, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[14382]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8286, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[25245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.631e+03, 1.561e+03, 1.100e+02, 1.700e+01, 5.000e+00, 1.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]),\n",
       " array([0.        , 0.51982224, 1.0396445 , 1.5594666 , 2.079289  ,\n",
       "        2.599111  , 3.1189332 , 3.6387556 , 4.158578  , 4.6784    ,\n",
       "        5.198222  ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEgZJREFUeJzt3X+MXeWd3/H3Zw35oSQtpkyQa1s12vV2l0Rag6YGCalKSRYMidas1EggNbEiVG8loybqqi3kHzZJkVipG9pIWSTv4gbaNNTaJIpF3GW9hChCWsDjrONgnJQpoWFqC8/WhARFpYJ++8d9rF7M2HNnPDPXzPN+SUf3nO95zj3PI8vzmfOcc++kqpAk9edXxt0BSdJ4GACS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkTl007g6cy2WXXVabNm0adzck6W3l0KFDf1NVE/O1u6ADYNOmTUxNTY27G5L0tpLkf4zSzikgSeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnq1AX9SeDztenOb4/lvC/c+9GxnFeSFsIrAEnqlAEgSZ0yACSpU/MGQJJ3JXk6yQ+SHE3yuVb/SpKfJDncli2tniRfSjKd5EiSq4fea0eS59qyY/mGJUmazyg3gV8Drq+qV5NcDDyR5L+2ff+yqv7sjPY3AZvbcg1wP3BNkkuBu4FJoIBDSfZV1ctLMRBJ0sLMewVQA6+2zYvbUuc4ZDvwUDvuSeCSJOuAG4EDVXWq/dA/AGw7v+5LkhZrpHsASdYkOQycZPBD/Km26542zXNfkne22nrgxaHDZ1rtbPUzz7UzyVSSqdnZ2QUOR5I0qpECoKreqKotwAZga5IPAncBvwH8A+BS4F+35pnrLc5RP/Ncu6tqsqomJybm/YtmkqRFWtBTQFX1M+C7wLaqOtGmeV4D/gOwtTWbATYOHbYBOH6OuiRpDEZ5CmgiySVt/d3AR4AftXl9kgS4BXimHbIP+GR7Guha4JWqOgE8CtyQZG2StcANrSZJGoNRngJaBzyYZA2DwNhbVY8k+U6SCQZTO4eBf9ba7wduBqaBXwKfAqiqU0m+ABxs7T5fVaeWbiiSpIWYNwCq6ghw1Rz168/SvoBdZ9m3B9izwD5KkpaBnwSWpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOjVvACR5V5Knk/wgydEkn2v1K5I8leS5JP8lyTta/Z1te7rt3zT0Xne1+o+T3Lhcg5IkzW+UK4DXgOur6reALcC2JNcCfwjcV1WbgZeB21v724GXq+rXgPtaO5JcCdwKfADYBvxxkjVLORhJ0ujmDYAaeLVtXtyWAq4H/qzVHwRuaevb2zZt/4eTpNUfrqrXquonwDSwdUlGIUlasJHuASRZk+QwcBI4APx34GdV9XprMgOsb+vrgRcB2v5XgL8zXJ/jGEnSChspAKrqjaraAmxg8Fv7b87VrL3mLPvOVn+TJDuTTCWZmp2dHaV7kqRFWNBTQFX1M+C7wLXAJUkuars2AMfb+gywEaDt/9vAqeH6HMcMn2N3VU1W1eTExMRCuidJWoBRngKaSHJJW3838BHgGPA48I9bsx3At9r6vrZN2/+dqqpWv7U9JXQFsBl4eqkGIklamIvmb8I64MH2xM6vAHur6pEkzwIPJ/k3wF8DD7T2DwD/Mck0g9/8bwWoqqNJ9gLPAq8Du6rqjaUdjiRpVPMGQFUdAa6ao/48czzFU1X/G/j4Wd7rHuCehXdTkrTU/CSwJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6NW8AJNmY5PEkx5IcTfLpVv+DJP8zyeG23Dx0zF1JppP8OMmNQ/VtrTad5M7lGZIkaRQXjdDmdeD3q+r7Sd4HHEpyoO27r6r+7XDjJFcCtwIfAP4u8JdJfr3t/jLw28AMcDDJvqp6dikGIklamHkDoKpOACfa+i+SHAPWn+OQ7cDDVfUa8JMk08DWtm+6qp4HSPJwa2sASNIYLOgeQJJNwFXAU610R5IjSfYkWdtq64EXhw6babWz1c88x84kU0mmZmdnF9I9SdICjBwASd4LfB34TFX9HLgf+FVgC4MrhD863XSOw+sc9TcXqnZX1WRVTU5MTIzaPUnSAo1yD4AkFzP44f/VqvoGQFW9NLT/T4BH2uYMsHHo8A3A8bZ+trokaYWN8hRQgAeAY1X1xaH6uqFmvws809b3AbcmeWeSK4DNwNPAQWBzkiuSvIPBjeJ9SzMMSdJCjXIFcB3wCeCHSQ632meB25JsYTCN8wLwewBVdTTJXgY3d18HdlXVGwBJ7gAeBdYAe6rq6BKORZK0AKM8BfQEc8/f7z/HMfcA98xR33+u4yRJK8dPAktSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdmjcAkmxM8niSY0mOJvl0q1+a5ECS59rr2lZPki8lmU5yJMnVQ++1o7V/LsmO5RuWJGk+o1wBvA78flX9JnAtsCvJlcCdwGNVtRl4rG0D3ARsbstO4H4YBAZwN3ANsBW4+3RoSJJW3rwBUFUnqur7bf0XwDFgPbAdeLA1exC4pa1vBx6qgSeBS5KsA24EDlTVqap6GTgAbFvS0UiSRragewBJNgFXAU8Bl1fVCRiEBPD+1mw98OLQYTOtdra6JGkMRg6AJO8Fvg58pqp+fq6mc9TqHPUzz7MzyVSSqdnZ2VG7J0laoJECIMnFDH74f7WqvtHKL7WpHdrryVafATYOHb4BOH6O+ptU1e6qmqyqyYmJiYWMRZK0AKM8BRTgAeBYVX1xaNc+4PSTPDuAbw3VP9meBroWeKVNET0K3JBkbbv5e0OrSZLG4KIR2lwHfAL4YZLDrfZZ4F5gb5LbgZ8CH2/79gM3A9PAL4FPAVTVqSRfAA62dp+vqlNLMgpJ0oLNGwBV9QRzz98DfHiO9gXsOst77QH2LKSDkqTl4SeBJalTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUqXn/KHySPcDHgJNV9cFW+wPgnwKzrdlnq2p/23cXcDvwBvDPq+rRVt8G/HtgDfCnVXXv0g7lwrHpzm+P7dwv3PvRsZ1b0tvLKFcAXwG2zVG/r6q2tOX0D/8rgVuBD7Rj/jjJmiRrgC8DNwFXAre1tpKkMZn3CqCqvpdk04jvtx14uKpeA36SZBrY2vZNV9XzAEkebm2fXXCPJUlL4nzuAdyR5EiSPUnWttp64MWhNjOtdrb6WyTZmWQqydTs7OxcTSRJS2CxAXA/8KvAFuAE8Eetnjna1jnqby1W7a6qyaqanJiYWGT3JEnzmXcKaC5V9dLp9SR/AjzSNmeAjUNNNwDH2/rZ6pKkMVjUFUCSdUObvws809b3AbcmeWeSK4DNwNPAQWBzkiuSvIPBjeJ9i++2JOl8jfIY6NeADwGXJZkB7gY+lGQLg2mcF4DfA6iqo0n2Mri5+zqwq6reaO9zB/Aog8dA91TV0SUfjSRpZKM8BXTbHOUHztH+HuCeOer7gf0L6p0kadn4SWBJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpU/MGQJI9SU4meWaodmmSA0mea69rWz1JvpRkOsmRJFcPHbOjtX8uyY7lGY4kaVSjXAF8Bdh2Ru1O4LGq2gw81rYBbgI2t2UncD8MAgO4G7gG2ArcfTo0JEnjMW8AVNX3gFNnlLcDD7b1B4FbhuoP1cCTwCVJ1gE3Ageq6lRVvQwc4K2hIklaQYu9B3B5VZ0AaK/vb/X1wItD7WZa7Wx1SdKYLPVN4MxRq3PU3/oGyc4kU0mmZmdnl7RzkqT/b7EB8FKb2qG9nmz1GWDjULsNwPFz1N+iqnZX1WRVTU5MTCyye5Kk+Sw2APYBp5/k2QF8a6j+yfY00LXAK22K6FHghiRr283fG1pNkjQmF83XIMnXgA8BlyWZYfA0z73A3iS3Az8FPt6a7wduBqaBXwKfAqiqU0m+ABxs7T5fVWfeWJYkraB5A6CqbjvLrg/P0baAXWd5nz3AngX1TpK0bPwksCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOnVeAZDkhSQ/THI4yVSrXZrkQJLn2uvaVk+SLyWZTnIkydVLMQBJ0uIsxRXAP6qqLVU12bbvBB6rqs3AY20b4CZgc1t2AvcvwbklSYu0HFNA24EH2/qDwC1D9Ydq4EngkiTrluH8kqQRnG8AFPAXSQ4l2dlql1fVCYD2+v5WXw+8OHTsTKu9SZKdSaaSTM3Ozp5n9yRJZ3PReR5/XVUdT/J+4ECSH52jbeao1VsKVbuB3QCTk5Nv2S9JWhrndQVQVcfb60ngm8BW4KXTUzvt9WRrPgNsHDp8A3D8fM4vSVq8RQdAkvcked/pdeAG4BlgH7CjNdsBfKut7wM+2Z4GuhZ45fRUkSRp5Z3PFNDlwDeTnH6f/1xVf57kILA3ye3AT4GPt/b7gZuBaeCXwKfO49ySpPO06ACoqueB35qj/r+AD89RL2DXYs8nSVpafhJYkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOnU+fxJSF6BNd357LOd94d6PjuW8khbPKwBJ6tSKB0CSbUl+nGQ6yZ0rfX5J0sCKBkCSNcCXgZuAK4Hbkly5kn2QJA2s9BXAVmC6qp6vqv8DPAxsX+E+SJJY+ZvA64EXh7ZngGtWuA9aBt58lt5+VjoAMket3tQg2QnsbJuvJvnxeZzvMuBvzuP4t4texglnjDV/OMaeLK9u/01XsZUc598bpdFKB8AMsHFoewNwfLhBVe0Gdi/FyZJMVdXkUrzXhayXcUI/Y+1lnNDPWC/Eca70PYCDwOYkVyR5B3ArsG+F+yBJYoWvAKrq9SR3AI8Ca4A9VXV0JfsgSRpY8U8CV9V+YP8KnW5JppLeBnoZJ/Qz1l7GCf2M9YIbZ6pq/laSpFXHr4KQpE6tygDo5esmkuxJcjLJM+Puy3JKsjHJ40mOJTma5NPj7tNySfKuJE8n+UEb6+fG3afllGRNkr9O8si4+7KckryQ5IdJDieZGnd/Tlt1U0Dt6yb+G/DbDB47PQjcVlXPjrVjyyDJPwReBR6qqg+Ouz/LJck6YF1VfT/J+4BDwC2r9N80wHuq6tUkFwNPAJ+uqifH3LVlkeRfAJPA36qqj427P8slyQvAZFVdUJ93WI1XAN183URVfQ84Ne5+LLeqOlFV32/rvwCOMfhU+apTA6+2zYvbsrp+S2uSbAA+CvzpuPvSq9UYAHN93cSq/GHRoySbgKuAp8bbk+XTpkUOAyeBA1W1Wsf674B/BfzfcXdkBRTwF0kOtW87uCCsxgCY9+sm9PaU5L3A14HPVNXPx92f5VJVb1TVFgaflN+aZNVN7yX5GHCyqg6Nuy8r5LqquprBNyHvatO3Y7caA2Der5vQ20+bD/868NWq+sa4+7MSqupnwHeBbWPuynK4DvidNjf+MHB9kv803i4tn6o63l5PAt9kMFU9dqsxAPy6iVWm3Rh9ADhWVV8cd3+WU5KJJJe09XcDHwF+NN5eLb2ququqNlTVJgb/R79TVf9kzN1aFkne0x5eIMl7gBuAC+LJvVUXAFX1OnD66yaOAXtX69dNJPka8FfA308yk+T2cfdpmVwHfILBb4mH23LzuDu1TNYBjyc5wuCXmQNVtaofkezA5cATSX4APA18u6r+fMx9AlbhY6CSpNGsuisASdJoDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjr1/wBkRA0EROgmpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dist.cpu()[ind[y_train[ind] == y_train[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.2000e+01, 7.6000e+01, 1.6200e+03, 7.4390e+03, 1.0539e+04,\n",
       "        6.7360e+03, 1.2780e+03, 6.8290e+03, 1.2892e+04, 1.2530e+03]),\n",
       " array([1.8285676, 2.268264 , 2.7079606, 3.147657 , 3.5873535, 4.02705  ,\n",
       "        4.4667463, 4.9064426, 5.346139 , 5.7858357, 6.225532 ],\n",
       "       dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dist.cpu()[ind[y_train[ind] != y_train[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = net(x_test[:10].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f33d7aebe48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAE5NJREFUeJzt3W+MlPW1B/DvYd2Vfyu6LP+UXUFFkWDu9maDGJuLN0L5EwQbgykvrmgaaGKN1vTFNbywvrkJubltry9uqtsLKSbFtgmImJDeEpVYEkNYDBEUavlnu7CwLKCAILsL577YoVlxn3OGeWaeZ+B8PwnZ3Tnzm/nNM3OYeeb8/oiqgojiGZJ3B4goH0x+oqCY/ERBMfmJgmLyEwXF5CcKislPFBSTnygoJj9RUDdleWdDhgzRmpqaLO+SKJRLly7h8uXLUsx1UyW/iMwD8CqAGgD/q6qrrOvX1NSgsbExzV1el7wh1CJFPVdEru7u7qKvW/LHfhGpAfA/AOYDmAZgqYhMK/X2iChbac75ZwDYr6oHVbUHwO8ALC5Pt4io0tIk/x0A/j7g747CZd8gIitEpF1E2i9fvpzi7oionNIk/2Anqt86uVXVNlVtVdXWIUNYXCCqFmmysQNA04C/JwI4mq47RJSVNMm/A8AUEZksInUAfgBgU3m6RUSVVnKpT1X7ROQ5AP+H/lLfGlX9pIh2ibEbteSV9ruONGMjvFOtS5culXzbgP+cWffvlUDTxq2+ef2OsMJVqjq/qm4GsLlMfSGiDPEbOKKgmPxEQTH5iYJi8hMFxeQnCorJTxRUpvP5gRuzlu/V0r3H3NfXZ8YvXrxoxq1xAMOGDUt1394YBe+xW+17e3vNtjfdZL88a2trS75vb3yDN7Yiz3EAacYvDMR3fqKgmPxEQTH5iYJi8hMFxeQnCorJTxRU5qW+auWVSNKUjdLyyk5WScwr5XmP2yuneazjVldXZ7ZN85x4ca9U5z2nXhky7XTkUttey+3ynZ8oKCY/UVBMfqKgmPxEQTH5iYJi8hMFxeQnCqqq6vyVXE45zymYXq3ce9xff/11yfHbb7/dbLt8+XIzvnTpUjN+4MABM/7yyy8nxt577z2z7dixY824N07AmgrtPSfedONKL4meBb7zEwXF5CcKislPFBSTnygoJj9RUEx+oqCY/ERBparzi8hhAGcBXALQp6qtaW4vTS3eq7v29PSkam/N3047PsGrCXt1fmvO/n333We2nTt3rhlvaGgw4x0dHWZ85syZibFdu3aZbT1eLd46Lt4YAW8NhbTrJFjxrMaklGOQz7+qancZboeIMsSP/URBpU1+BfAnEdkpIivK0SEiykbaj/0Pq+pRERkLYIuI7FPVDwZeofCfwgrAP68mouykykZVPVr42QXgLQAzBrlOm6q2qmork5+oepScjSIyQkTqr/wO4HsA9pSrY0RUWWk+9o8D8FahZHETgHWq+sey9IqIKq7k5FfVgwD+6VraiEjF6pveGu7e/O00W017NV+vju9toz106FAzfvfddyfGFi9ebLYdPny4Gf/yyy9TtbeOq/d8nzt3zow3NjaWfN/eGAEv7r2e0qz77+0JYN02t+gmIheTnygoJj9RUEx+oqCY/ERBMfmJgsp06W5VNcs7aaZBelN2026ZbE3x9Epxaaf8LliwwIw/9thjibEHH3zQbOvxphN7U1+nTp2aGHvqqafMtu3t7Wb88OHDZtxautt7TrxyW5rScDHts5B/D4goF0x+oqCY/ERBMfmJgmLyEwXF5CcKislPFJRkuXV1bW2tetMwLWn6ei1THQeTZgrm6dOnzfj06dPN+KpVq8z4tGnTEmMnT54023pTdm+99VYz7o1xsKYre7XwTz75xIyvWbPGjG/YsCEx5vV7zJgxZvz8+fNm/MKFC2bcqvN7YyesPOju7kZPT09RL3a+8xMFxeQnCorJTxQUk58oKCY/UVBMfqKgmPxEQWU6n99butuTZhloT5q1BI4dO2a2vfnmm834okWLzHhLS4sZt+bcjxo1ymw7btw4M+6NA/Aeu7XEtXdcHnroITN+5swZM75jx47EmLdOgfe4vaW508zXr+Rtf+N2ynIrRHTdYfITBcXkJwqKyU8UFJOfKCgmP1FQTH6ioNw6v4isAbAQQJeqTi9c1gDg9wAmATgM4ElVtSetl4FV3/Tq9Gm3ZLbmf3vzr+fOnWvGvXX59+3bZ8br6+sTY83NzWbbrq4uM/7OO++Y8S1btphx63mx1iEAgGeffdaMe3sSzJs3LzG2ceNGs21nZ6cZ9+b7p5mT77HWQbiW2y3mnf83AK4+ii8BeFdVpwB4t/A3EV1H3ORX1Q8AnLrq4sUA1hZ+Xwvg8TL3i4gqrNRz/nGq2gkAhZ9jy9clIspCxcf2i8gKACsA/zyIiLJT6jv/cRGZAACFn4nfGqlqm6q2qmprNWxOSET9Ss3GTQCWFX5fBuDt8nSHiLLiJr+IvAngQwD3iUiHiPwQwCoAc0TkrwDmFP4mouuIe86vqksTQo9e652paqr6ZiVPG7zvI/r6+hJjU6ZMMdu+8MILZnz06NFm3Ft735qzv337drPt1q1bzfj69evNuDef3xo/ce7cObPtrFmzzLg3fmLJkiWJsY6ODrPtiRMnzLg3rsR7rVpz9r223n4HxeJJOFFQTH6ioJj8REEx+YmCYvITBcXkJwoq06W7PV4JwyqPeG17enrM+PDhw824tUy0t0W3t821N524oaHBjG/bti0xtnr1arPtZ599Zsa9MqO3/La1BPaBAwfMtps3bzbjd911lxmfMWNGYuyZZ54x2x49etSMHzp0yIx7r0erVFiuUp6H7/xEQTH5iYJi8hMFxeQnCorJTxQUk58oKCY/UVBVVedPs323N1XYu22vXm1N+T1//rzZ1lse25vS6y2f/dprryXGjhw5Yrb96quvzLg3/sE7bhMnTkyMedtk79y504x7y4Y//fTTiTFrDAAA3HnnnWbcq/NbU8CB6ljSju/8REEx+YmCYvITBcXkJwqKyU8UFJOfKCgmP1FQ11Wd36qdessd19bWltSnK+rq6hJjTU1NZltraW3A79vrr79uxq0tvK06O2CvkQD4z8nZs2dLvn2vzt/S0mLGJ0yYYMaPHz+eGPMelzduJG3cWsPBeq0Vc9vF4js/UVBMfqKgmPxEQTH5iYJi8hMFxeQnCorJTxSUW+cXkTUAFgLoUtXphcteAbAcwJV9jFeqqr3IehlYtVmv9umtre/VnK3519OnTzfbevVob6vqzz//3IwPHTo0MWatmw/4+xnU19eb8REjRphxq9Y+btw4s+39999vxr2t0W+77bbEWGdnp9n24sWLZtzjrXNg1fnLVcf3FPPO/xsA8wa5/Jeq2lL4V/HEJ6LycpNfVT8AcCqDvhBRhtKc8z8nIh+LyBoRSf58RURVqdTk/xWAuwG0AOgE8POkK4rIChFpF5H2rPYgIyJfScmvqsdV9ZKqXgbwawCJqyGqapuqtqpqqzf5hoiyU1I2isjAr6+/D2BPebpDRFkpptT3JoBHADSKSAeAnwF4RERaACiAwwB+VME+ElEFuMmvqksHudje9L0KefPWv/jiCzNuzcl/4IEHzLZWvRnw6/jeGAWrpuyNX2hsbDTjVj26mNufOnVqYsw7brNnzzbj3loF1hiFPXvsD6snT54049Ww7n5aPAknCorJTxQUk58oKCY/UVBMfqKgmPxEQVXV0t1peEOHvaWavdKNtWXz+PHjzbZeOczbJtubNmuVpbyppd59nzplz+lqbm424wsWLEiMPfHEE2bbyZMnm/Fjx46Z8U8//TQxtnq1Xa3evXu3GR82bJgZT7O0d9plxYvFd36ioJj8REEx+YmCYvITBcXkJwqKyU8UFJOfKKjM6/xWPd5b6ceqb3p1fq+O79XSrdqr12+vbust3e0tI221HzNmjNnWO2733nuvGV+yZIkZnzNnTmLMm+rc1dVlxr2trLdu3ZoY27hxo9nWOy6VHFdSTUt3E9ENiMlPFBSTnygoJj9RUEx+oqCY/ERBMfmJgsq8zm/VxNPMY/baerV4r5Z+6NChxJi1DTXgz6n3asaPPvpoybe/cOFCs+2kSZPM+PDhw824t/S3tUX4kSNHzLbWGgqA/5xu2bIlMeaN6+jr6zPj3nN64cIFM846PxHlhslPFBSTnygoJj9RUEx+oqCY/ERBMfmJgnLr/CLSBOANAOMBXAbQpqqvikgDgN8DmATgMIAnVfV05brq9tOMe3VbbxvsEydOJMY+/vhjs+2iRYvMuDdn/vnnnzfj1mO/5557zLbe+IbTp+2n1Gvf09OTGLvlllvMtt422W1tbWb8ww8/LPm+vfUf0s7nt7aM9+47y3X7+wD8VFXvBzATwI9FZBqAlwC8q6pTALxb+JuIrhNu8qtqp6p+VPj9LIC9AO4AsBjA2sLV1gJ4vFKdJKLyu6ZzfhGZBOA7ALYDGKeqnUD/fxAAxpa7c0RUOUWP7ReRkQDWA/iJqp7xzmkGtFsBYAXgj8UmouwUlY0iUov+xP+tqm4oXHxcRCYU4hMADLraoqq2qWqrqrYy+Ymqh5uN0v8WvxrAXlX9xYDQJgDLCr8vA/B2+btHRJUiXtlARL4L4M8AdqO/1AcAK9F/3v8HAM0A/gZgiaqa+znX1taqNQW0iL6YcYtX6hs1apQZt7aqnjZtmtn2xRdfNOOzZs0y4yNHjjTj3d3diTFvi23vmI4da3+V4x03ayr0tm3bzLabNm0y49u3bzfjvb29iTFvSm7aT6mVLPVZZcbu7m709vYWlSjuOb+qbgOQdGP2RHMiqlo8CScKislPFBSTnygoJj9RUEx+oqCY/ERBuXX+cvLq/B6rvunVVb0pmN5SztZSzN59z5w504zPmDHDjM+ePduMNzU1Jca8enV9fb0Z96b07tu3z4y///77ibF169aZba1p1AAwevRoM+5NN7Z4eVHJMSlpXEudn+/8REEx+YmCYvITBcXkJwqKyU8UFJOfKCgmP1FQ11WdPw1vPn+a+dvest/eMfbmb8+fP9+MW1t4Nzc3m233799vxq3lrwF7vj4AHDx4sOS2DQ0NZtw7btaceY/3nHrjRrzXm9f3UrHOT0QuJj9RUEx+oqCY/ERBMfmJgmLyEwXF5CcK6oap83uPw6v5eu3r6uoSY15N2JtXnnZLZqumXFtba7b1+ua19+atW8fGWiMBAIYNG2bGveNibQ/ujevwHlfa9SPSzPe32nZ3d6Onp4d1fiJKxuQnCorJTxQUk58oKCY/UVBMfqKgmPxEQblbdItIE4A3AIwHcBlAm6q+KiKvAFgO4Mri6itVdXOlOuqp9H7qVk057dxtbwxCmpqxNT6hmNv2jqvX3nps3vgI77h6rL6lfb3kuW6/dd/XMm7HTX4AfQB+qqofiUg9gJ0isqUQ+6Wq/lfR90ZEVcNNflXtBNBZ+P2siOwFcEelO0ZElXVNn31EZBKA7wDYXrjoORH5WETWiMhtCW1WiEi7iLR7HxGJKDtFJ7+IjASwHsBPVPUMgF8BuBtAC/o/Gfx8sHaq2qaqraramvY8i4jKp6hsFJFa9Cf+b1V1AwCo6nFVvaSqlwH8GoC92yQRVRU3+aX/a8vVAPaq6i8GXD5hwNW+D2BP+btHRJVSzLf9DwP4NwC7RWRX4bKVAJaKSAsABXAYwI8q0sMipS29pJ2iafH6lnbarFVOs6a1AkBvb68ZHzp0qBlP07e0y2N7ce/2LWlKv9eLYr7t3wZgsCORW02fiNLjN3BEQTH5iYJi8hMFxeQnCorJTxQUk58oqNILoTeYNOMEvCm5Xs3Yq7WnmRLs3bc3xiDtEtSV2ooaSD8t13Ij1PE9fOcnCorJTxQUk58oKCY/UVBMfqKgmPxEQTH5iYLKdItuETkB4PMBFzUC6M6sA9emWvtWrf0C2LdSlbNvd6rqmGKumGnyf+vORdpVtTW3DhiqtW/V2i+AfStVXn3jx36ioJj8REHlnfxtOd+/pVr7Vq39Ati3UuXSt1zP+YkoP3m/8xNRTnJJfhGZJyJ/EZH9IvJSHn1IIiKHRWS3iOwSkfac+7JGRLpEZM+AyxpEZIuI/LXwc9Bt0nLq2ysicqRw7HaJyIKc+tYkIu+LyF4R+UREXihcnuuxM/qVy3HL/GO/iNQA+AzAHAAdAHYAWKqqn2bakQQichhAq6rmXhMWkX8BcA7AG6o6vXDZfwI4paqrCv9x3qaq/14lfXsFwLm8d24ubCgzYeDO0gAeB/A0cjx2Rr+eRA7HLY93/hkA9qvqQVXtAfA7AItz6EfVU9UPAJy66uLFANYWfl+L/hdP5hL6VhVUtVNVPyr8fhbAlZ2lcz12Rr9ykUfy3wHg7wP+7kB1bfmtAP4kIjtFZEXenRnEuMK26Ve2Tx+bc3+u5u7cnKWrdpaummNXyo7X5ZZH8g+27lM1lRweVtV/BjAfwI8LH2+pOEXt3JyVQXaWrgql7nhdbnkkfweApgF/TwRwNId+DEpVjxZ+dgF4C9W3+/DxK5ukFn525dyff6imnZsH21kaVXDsqmnH6zySfweAKSIyWUTqAPwAwKYc+vEtIjKi8EUMRGQEgO+h+nYf3gRgWeH3ZQDezrEv31AtOzcn7SyNnI9dte14ncsgn0Ip478B1ABYo6r/kXknBiEid6H/3R7oX9l4XZ59E5E3ATyC/llfxwH8DMBGAH8A0AzgbwCWqGrmX7wl9O0R9H90/cfOzVfOsTPu23cB/BnAbgBXlh9eif7z69yOndGvpcjhuHGEH1FQHOFHFBSTnygoJj9RUEx+oqCY/ERBMfmJgmLyEwXF5CcK6v8BjDbhJYEYn0wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out[1].cpu().numpy()[i, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f33d7a48978>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADbVJREFUeJzt3W2IXPUVx/HfSWzfpH2hZE3jU9I2EitCTVljoRKtxZKUStIX0YhIiqUbJRoLfVFJwEaKINqmLRgSthi6BbUK0bqE0KaINBWCuJFaNVtblTVNs2yMEWsI0picvti7siY7/zuZuU+b8/2AzMOZuXO8+tt7Z/733r+5uwDEM6PuBgDUg/ADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwjqnCo/zMw4nBAombtbO6/rastvZkvN7A0ze9PM7u1mWQCqZZ0e229mMyX9U9INkg5IeknSLe6+L/EetvxAyarY8i+W9Ka7v+3u/5P0e0nLu1gegAp1E/4LJf170uMD2XOfYmZ9ZjZkZkNdfBaAgnXzg99Uuxan7da7e7+kfondfqBJutnyH5B08aTHF0k62F07AKrSTfhfknSpmX3RzD4raZWkwWLaAlC2jnf73f1jM7tL0p8kzZS0zd1fL6wzAKXqeKivow/jOz9QukoO8gEwfRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFSlU3SjerNmzUrWH3744WR9zZo1yfrevXuT9ZUrV7asvfPOO8n3olxs+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqK5m6TWzEUkfSjoh6WN37815PbP0VmzBggXJ+vDwcFfLnzEjvf1Yt25dy9rmzZu7+mxMrd1Zeos4yOeb7n64gOUAqBC7/UBQ3YbfJe0ys71m1ldEQwCq0e1u/zfc/aCZnS/pz2b2D3ffPfkF2R8F/jAADdPVlt/dD2a3hyQ9I2nxFK/pd/fevB8DAVSr4/Cb2Swz+/zEfUnflvRaUY0BKFc3u/1zJD1jZhPLedzd/1hIVwBK13H43f1tSV8tsBd0qKenp2VtYGCgwk4wnTDUBwRF+IGgCD8QFOEHgiL8QFCEHwiKS3dPA6nTYiVpxYoVLWuLF5920GWllixZ0rKWdzrwK6+8kqzv3r07WUcaW34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCKqrS3ef8Ydx6e6OnDhxIlk/efJkRZ2cLm+svpve8qbwvvnmm5P1vOnDz1btXrqbLT8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMU4fwPs3LkzWV+2bFmyXuc4/3vvvZesHz16tGVt3rx5RbfzKTNnzix1+U3FOD+AJMIPBEX4gaAIPxAU4QeCIvxAUIQfCCr3uv1mtk3SdyUdcvcrsufOk/SkpPmSRiTd5O7vl9fm9Hbttdcm6wsXLkzW88bxyxzn37p1a7K+a9euZP2DDz5oWbv++uuT792wYUOynufOO+9sWduyZUtXyz4btLPl/62kpac8d6+k59z9UknPZY8BTCO54Xf33ZKOnPL0ckkD2f0BSa2njAHQSJ1+55/j7qOSlN2eX1xLAKpQ+lx9ZtYnqa/szwFwZjrd8o+Z2VxJym4PtXqhu/e7e6+793b4WQBK0Gn4ByWtzu6vlvRsMe0AqEpu+M3sCUl7JC00swNm9gNJD0q6wcz+JemG7DGAaYTz+Qswf/78ZH3Pnj3J+uzZs5P1bq6Nn3ft++3btyfr999/f7J+7NixZD0l73z+vPXW09OTrH/00Ucta/fdd1/yvY888kiyfvz48WS9TpzPDyCJ8ANBEX4gKMIPBEX4gaAIPxAUQ30FWLBgQbI+PDzc1fLzhvqef/75lrVVq1Yl33v48OGOeqrC3Xffnaxv2rQpWU+tt7zToC+77LJk/a233krW68RQH4Akwg8ERfiBoAg/EBThB4Ii/EBQhB8IqvTLeKF7Q0NDyfrtt9/estbkcfw8g4ODyfqtt96arF911VVFtnPWYcsPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzl+BvPPx81x99dUFdTK9mKVPS89br92s940bNybrt912W8fLbgq2/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVO44v5ltk/RdSYfc/YrsuY2Sfijp3exl6919Z1lNNt0dd9yRrOddIx5Tu/HGG5P1RYsWJeup9Z733yRvnP9s0M6W/7eSlk7x/C/d/crsn7DBB6ar3PC7+25JRyroBUCFuvnOf5eZ/d3MtpnZuYV1BKASnYZ/i6QvS7pS0qikX7R6oZn1mdmQmaUvRAegUh2F393H3P2Eu5+U9BtJixOv7Xf3Xnfv7bRJAMXrKPxmNnfSw+9Jeq2YdgBUpZ2hvickXSdptpkdkPRTSdeZ2ZWSXNKIpDUl9gigBLnhd/dbpnj60RJ6mbbyxqMj6+npaVm7/PLLk+9dv3590e184t13303Wjx8/XtpnNwVH+AFBEX4gKMIPBEX4gaAIPxAU4QeC4tLdKNWGDRta1tauXVvqZ4+MjLSsrV69Ovne/fv3F9xN87DlB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOdHV3buTF+4eeHChRV1crp9+/a1rL3wwgsVdtJMbPmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjG+QtgZsn6jBnd/Y1dtmxZx+/t7+9P1i+44IKOly3l/7vVOT05l1RPY8sPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0HljvOb2cWSfifpC5JOSup391+b2XmSnpQ0X9KIpJvc/f3yWm2uLVu2JOsPPfRQV8vfsWNHst7NWHrZ4/BlLn/r1q2lLTuCdrb8H0v6sbt/RdLXJa01s8sl3SvpOXe/VNJz2WMA00Ru+N191N1fzu5/KGlY0oWSlksayF42IGlFWU0CKN4Zfec3s/mSFkl6UdIcdx+Vxv9ASDq/6OYAlKftY/vN7HOStkv6kbv/N+949knv65PU11l7AMrS1pbfzD6j8eA/5u5PZ0+PmdncrD5X0qGp3uvu/e7e6+69RTQMoBi54bfxTfyjkobdfdOk0qCkialOV0t6tvj2AJTF3D39ArNrJP1V0qsaH+qTpPUa/97/lKRLJO2XtNLdj+QsK/1h09S8efOS9T179iTrPT09yXqTT5vN621sbKxlbXh4OPnevr70t8XR0dFk/dixY8n62crd2/pOnvud391fkNRqYd86k6YANAdH+AFBEX4gKMIPBEX4gaAIPxAU4QeCyh3nL/TDztJx/jxLlixJ1lesSJ8Tdc899yTrTR7nX7duXcva5s2bi24Han+cny0/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFOP80sHTp0mQ9dd573jTVg4ODyXreFN95l3Pbt29fy9r+/fuT70VnGOcHkET4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzg+cZRjnB5BE+IGgCD8QFOEHgiL8QFCEHwiK8ANB5YbfzC42s+fNbNjMXjeze7LnN5rZf8zsb9k/3ym/XQBFyT3Ix8zmSprr7i+b2ecl7ZW0QtJNko66+8/b/jAO8gFK1+5BPue0saBRSaPZ/Q/NbFjShd21B6BuZ/Sd38zmS1ok6cXsqbvM7O9mts3Mzm3xnj4zGzKzoa46BVCoto/tN7PPSfqLpAfc/WkzmyPpsCSX9DONfzW4PWcZ7PYDJWt3t7+t8JvZZyTtkPQnd980RX2+pB3ufkXOcgg/ULLCTuyx8cuzPippeHLwsx8CJ3xP0mtn2iSA+rTza/81kv4q6VVJE3NBr5d0i6QrNb7bPyJpTfbjYGpZbPmBkhW6218Uwg+Uj/P5ASQRfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgsq9gGfBDkt6Z9Lj2dlzTdTU3pral0RvnSqyt3ntvrDS8/lP+3CzIXfvra2BhKb21tS+JHrrVF29sdsPBEX4gaDqDn9/zZ+f0tTemtqXRG+dqqW3Wr/zA6hP3Vt+ADWpJfxmttTM3jCzN83s3jp6aMXMRszs1Wzm4VqnGMumQTtkZq9Neu48M/uzmf0ru51ymrSaemvEzM2JmaVrXXdNm/G68t1+M5sp6Z+SbpB0QNJLkm5x932VNtKCmY1I6nX32seEzWyJpKOSfjcxG5KZPSTpiLs/mP3hPNfdf9KQ3jbqDGduLqm3VjNLf181rrsiZ7wuQh1b/sWS3nT3t939f5J+L2l5DX00nrvvlnTklKeXSxrI7g9o/H+eyrXorRHcfdTdX87ufyhpYmbpWtddoq9a1BH+CyX9e9LjA2rWlN8uaZeZ7TWzvrqbmcKciZmRstvza+7nVLkzN1fplJmlG7PuOpnxumh1hH+q2USaNOTwDXf/mqRlktZmu7dozxZJX9b4NG6jkn5RZzPZzNLbJf3I3f9bZy+TTdFXLeutjvAfkHTxpMcXSTpYQx9TcveD2e0hSc9o/GtKk4xNTJKa3R6quZ9PuPuYu59w95OSfqMa1102s/R2SY+5+9PZ07Wvu6n6qmu91RH+lyRdamZfNLPPSlolabCGPk5jZrOyH2JkZrMkfVvNm314UNLq7P5qSc/W2MunNGXm5lYzS6vmdde0Ga9rOcgnG8r4laSZkra5+wOVNzEFM/uSxrf20vgZj4/X2ZuZPSHpOo2f9TUm6aeS/iDpKUmXSNovaaW7V/7DW4vertMZztxcUm+tZpZ+UTWuuyJnvC6kH47wA2LiCD8gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0H9HwAENgeMtPBpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[i].numpy()[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.utils.save_image(x_test[:100], 'x_clean.png', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adv = pickle.load(open('x_adv_ae_mnist_exp0.h5.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dknn attack 2.9775 (0.0373)\n"
     ]
    }
   ],
   "source": [
    "num = 10000\n",
    "y_pred = dknn.classify(x_adv)\n",
    "acc = (y_pred.argmax(1) == y_test[ind][:num].numpy()).sum() / len(y_pred)\n",
    "\n",
    "dist = np.mean(np.sqrt(np.sum((x_adv.cpu().detach().numpy() -\n",
    "                               x_test.numpy()[ind][:num])**2, (1, 2, 3))))\n",
    "\n",
    "print('dknn attack %.4f (%.4f)' % (dist, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
