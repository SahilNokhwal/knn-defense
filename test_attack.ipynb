{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import foolbox\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from lib.dataset_utils import *\n",
    "from lib.mnist_model import *\n",
    "from lib.adv_model import *\n",
    "from lib.dknn_attack import DKNNAttack\n",
    "from lib.dknn_attack_l2 import DKNNL2Attack\n",
    "from lib.dknn_attack_linf import DKNNLinfAttack\n",
    "from lib.cwl2_attack import CWL2Attack\n",
    "from lib.dknn import DKNNL2\n",
    "from lib.utils import *\n",
    "from lib.lip_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Subsamples dataset and reduces to two classes (1 and 7)\n",
    "# Code adapted from https://github.com/yangarbiter/adversarial-nonparametrics\n",
    "\n",
    "num_samples = 2200\n",
    "num_val = 300 # can pick anything > 0, does not really get used here\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X, y), (_, _) = mnist.load_data()\n",
    "np.random.seed(2019)\n",
    "idx1 = np.random.choice(\n",
    "    np.where(y == 1)[0], num_samples // 2, replace=False)\n",
    "idx2 = np.random.choice(\n",
    "    np.where(y == 7)[0], num_samples // 2, replace=False)\n",
    "y[idx1] = 0\n",
    "y[idx2] = 1\n",
    "X = np.vstack((X[idx1], X[idx2])).astype(np.float32) / 255.\n",
    "y = np.concatenate((y[idx1], y[idx2]))\n",
    "\n",
    "idxs = np.arange(num_samples)\n",
    "np.random.shuffle(idxs)\n",
    "x_train_sub = torch.tensor(X[idxs[:-200]])\n",
    "x_test_sub = torch.tensor(X[idxs[-200:]])\n",
    "y_train_sub = torch.tensor(y[idxs[:-200]])\n",
    "y_test_sub = torch.tensor(y[idxs[-200:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_knn = KNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['identity']\n",
    "knn = DKNNL2(net_knn, x_train_sub, y_train_sub, \n",
    "             x_test_sub, y_test_sub, layers, \n",
    "             k=1, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = knn.classify(x_test_sub)\n",
    "    ind = np.where(y_pred.argmax(1) == y_test_sub.numpy())[0]\n",
    "    print((y_pred.argmax(1) == y_test_sub.numpy()).sum() / y_test_sub.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack for L2 DkNN\n",
    "\n",
    "attack = DKNNL2Attack()\n",
    "# attack = DKNNLinfAttack()\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            knn, x[begin:end], y[begin:end],\n",
    "            guide_layer=layers[0], m=75, binary_search_steps=15,\n",
    "            max_iterations=1000, learning_rate=1e-2, guide_mode=2,\n",
    "            initial_const=1e-2, abort_early=True, random_start=False)\n",
    "    return x_adv\n",
    "\n",
    "num = 200\n",
    "x_adv = attack_batch(x_test_sub[:num].cuda(), y_test_sub[:num], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Implement gradient-based attack on DkNN with L-2 constraint'''\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "INFTY = 1e20\n",
    "\n",
    "\n",
    "class DKNNExpAttack(object):\n",
    "    \"\"\"\n",
    "    Implement gradient-based attack on Deep k-Nearest Neigbhor that uses\n",
    "    L-2 distance as a metric\n",
    "    \"\"\"\n",
    "    def __init__(self, dknn):\n",
    "        self.dknn = dknn\n",
    "        self.device = dknn.device\n",
    "        self.layers = dknn.layers\n",
    "        self.guide_reps = {}\n",
    "        self.thres = None\n",
    "        self.coeff = None\n",
    "\n",
    "    def __call__(self, x_orig, label, guide_layer='relu1', m=100,\n",
    "                 binary_search_steps=5, max_iterations=500,\n",
    "                 learning_rate=1e-2, initial_const=1, max_linf=None, \n",
    "                 random_start=False, thres_steps=100, check_adv_steps=100,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dknn : DKNN object\n",
    "            DkNN (defined in lin/dknn.py) that we want to attack\n",
    "        x_orig : torch.tensor\n",
    "            tensor of the original samples to attack. Does not need to require\n",
    "            gradients, shape is (num_samples, ) + input_shape\n",
    "        label : torch.tensor\n",
    "            tensor of the label corresponding to x_orig\n",
    "        guide_layer : str. optional\n",
    "            layer name in which we want to find guide samples. Default is\n",
    "            'relu1'\n",
    "        m : int, optional\n",
    "            number of guide samples. Default is 100\n",
    "        binary_search_step : int, optional\n",
    "            number of steps for binary search on the norm penalty constant.\n",
    "            Default is 5\n",
    "        max_iterations : int, optional\n",
    "            number of optimization steps (per one binary search). Default is\n",
    "            500\n",
    "        learning_rate : float , optional\n",
    "            step size or learning rate for the optimizer. Default is 1e-2\n",
    "        initial_const : float, optional\n",
    "            a number the norm penalty constant should be initialized to.\n",
    "            Default is 1\n",
    "        abort_early : bool, optional\n",
    "            whether or not to abort the optimization early (before reaching\n",
    "            max_iterations) if the objective does not improve from the past\n",
    "            (max_iterations // 10) steps. Default is True\n",
    "        max_linf : float, optional\n",
    "            use to bound the L-inf norm of the attacks (addition to L-2 norm\n",
    "            penalty). Set to None to not use this option. Default is None\n",
    "        random_start : bool, optional\n",
    "            whether or not to initialize the perturbation with small isotropic\n",
    "            Gaussian noise. Default is False\n",
    "        guide_mode : int, optional\n",
    "            Choose the guide_mode to use between 1 and 2. Default is 1\n",
    "            - guide_mode == 1: find m nearest neighbors to input that all have\n",
    "            the same class but not equal its original label.\n",
    "            - guide_mode == 2: find the nearest neighbor that has a different\n",
    "            class from the input and find its m - 1 neighbors\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_adv : torch.tensor\n",
    "            adversarial examples found. If adversarial examples for some inputs\n",
    "            are not found, return those inputs.\n",
    "        \"\"\"\n",
    "\n",
    "        min_, max_ = x_orig.min(), x_orig.max()\n",
    "        if max_linf is not None:\n",
    "            min_ = torch.max(x_orig - max_linf, min_)\n",
    "            max_ = torch.min(x_orig + max_linf, max_)\n",
    "        batch_size = x_orig.size(0)\n",
    "        x_adv = x_orig.clone()\n",
    "        label = label.cpu().numpy()\n",
    "        input_shape = x_orig.detach().cpu().numpy().shape\n",
    "        # initialize coeff for guide samples\n",
    "        self.coeff = torch.zeros((x_orig.size(0), m))\n",
    "        self.coeff[:, :m // 2] += 1\n",
    "        self.coeff[:, m // 2:] -= 1\n",
    "\n",
    "        def to_attack_space(x):\n",
    "            # map from [min_, max_] to [-1, +1]\n",
    "            a = (min_ + max_) / 2\n",
    "            b = (max_ - min_) / 2\n",
    "            x = (x - a) / b\n",
    "\n",
    "            # from [-1, +1] to approx. (-1, +1)\n",
    "            x = x * 0.999999\n",
    "\n",
    "            # from (-1, +1) to (-inf, +inf)\n",
    "            return self.atanh(x)\n",
    "\n",
    "        def to_model_space(x):\n",
    "            \"\"\"Transforms an input from the attack space\n",
    "            to the model space. This transformation and\n",
    "            the returned gradient are elementwise.\"\"\"\n",
    "\n",
    "            # from (-inf, +inf) to (-1, +1)\n",
    "            x = torch.tanh(x)\n",
    "\n",
    "            # map from (-1, +1) to (min_, max_)\n",
    "            a = (min_ + max_) / 2\n",
    "            b = (max_ - min_) / 2\n",
    "            x = x * b + a\n",
    "\n",
    "            return x\n",
    "\n",
    "        # variables representing inputs in attack space will be prefixed with z\n",
    "        z_orig = to_attack_space(x_orig)\n",
    "        x_recon = to_model_space(z_orig)\n",
    "\n",
    "        # declare tensors that keep track of constants and binary search\n",
    "        const = torch.zeros((batch_size, ), device=self.device)\n",
    "        const += initial_const\n",
    "        lower_bound = torch.zeros_like(const)\n",
    "        upper_bound = torch.zeros_like(const) + INFTY\n",
    "        best_l2dist = torch.zeros_like(const) + INFTY\n",
    "\n",
    "        for binary_search_step in range(binary_search_steps):\n",
    "            if (binary_search_step == binary_search_steps - 1 and\n",
    "                    binary_search_steps >= 10):\n",
    "                    # in the last binary search step, use the upper_bound instead\n",
    "                    # to ensure that unsuccessful attacks use the largest\n",
    "                    # possible constant\n",
    "                const = upper_bound\n",
    "\n",
    "            # initialize perturbation in transformed space\n",
    "            if not random_start:\n",
    "                z_delta = torch.zeros_like(z_orig, requires_grad=True)\n",
    "            else:\n",
    "                rand = np.random.randn(*input_shape) * 1e-2\n",
    "                z_delta = torch.tensor(\n",
    "                    rand, dtype=torch.float32, requires_grad=True, \n",
    "                    device=self.device)\n",
    "            loss_at_previous_check = torch.zeros(1, device=self.device) + INFTY\n",
    "\n",
    "            # create a new optimizer\n",
    "            optimizer = optim.RMSprop([z_delta], lr=learning_rate)\n",
    "#             optimizer = torch.optim.LBFGS([z_delta], lr=1, max_iter=20, max_eval=None, \n",
    "#                                           tolerance_grad=1e-07, tolerance_change=1e-09, \n",
    "#                                           history_size=100, line_search_fn=None)\n",
    "            \n",
    "            # add learning rate scheduler\n",
    "            lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=150, \n",
    "                threshold=0.01, threshold_mode='rel')\n",
    "\n",
    "            for iteration in range(max_iterations):\n",
    "                optimizer.zero_grad()\n",
    "                x = to_model_space(z_orig + z_delta)\n",
    "                \n",
    "                # adaptively choose threshold and guide samples every \n",
    "                # <thres_steps> iterations\n",
    "                with torch.no_grad():\n",
    "                    if iteration % thres_steps == 0:\n",
    "#                         order = (self.dknn.k + 1) // 2 - 1\n",
    "#                         thres = self.dknn.get_neighbors(x)[0][0][:, order]\n",
    "                        thres = self.dknn.get_neighbors(x)[0][0][:, -1]\n",
    "                        self.thres = torch.tensor(thres).to(self.device).view(\n",
    "                            batch_size, 1)\n",
    "                        self.find_guide_samples(\n",
    "                            x, label, m=m, layer=guide_layer)\n",
    "\n",
    "                reps = self.dknn.get_activations(x, requires_grad=True)\n",
    "                loss, l2dist = self.loss_function(\n",
    "                    x, reps, const, x_recon)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#                 lr_scheduler.step(loss)\n",
    "\n",
    "                if (verbose and iteration % \\\n",
    "                    (np.ceil(max_iterations / 10)) == 0):\n",
    "                    print('    step: %d; loss: %.3f; l2dist: %.3f' %\n",
    "                          (iteration, loss.cpu().detach().numpy(),\n",
    "                           l2dist.mean().cpu().detach().numpy()))\n",
    "                \n",
    "                # every <check_adv_steps>, save adversarial samples \n",
    "                # with minimal perturbation\n",
    "                if iteration % check_adv_steps == 0:\n",
    "                    is_adv = self.check_adv(x, label)\n",
    "                    for i in range(batch_size):\n",
    "                        if is_adv[i] and best_l2dist[i] > l2dist[i]:\n",
    "                            x_adv[i] = x[i]\n",
    "                            best_l2dist[i] = l2dist[i]\n",
    "\n",
    "            # check how many attacks have succeeded\n",
    "            with torch.no_grad():\n",
    "                is_adv = self.check_adv(x, label)\n",
    "                if verbose:\n",
    "                    print(is_adv.sum())\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # set new upper and lower bounds\n",
    "                if is_adv[i]:\n",
    "                    upper_bound[i] = const[i]\n",
    "                else:\n",
    "                    lower_bound[i] = const[i]\n",
    "                # set new const\n",
    "                if upper_bound[i] == INFTY:\n",
    "                    # exponential search if adv has not been found\n",
    "                    const[i] *= 10\n",
    "                else:\n",
    "                    # binary search if adv has been found\n",
    "                    const[i] = (lower_bound[i] + upper_bound[i]) / 2\n",
    "                # only keep adv with smallest l2dist\n",
    "                if is_adv[i] and best_l2dist[i] > l2dist[i]:\n",
    "                    x_adv[i] = x[i]\n",
    "                    best_l2dist[i] = l2dist[i]\n",
    "\n",
    "            # check the current attack success rate (combined with previous\n",
    "            # binary search steps)\n",
    "            if verbose:\n",
    "                with torch.no_grad():\n",
    "                    is_adv = self.check_adv(x_adv, label)\n",
    "                    print('binary step: %d; number of successful adv: %d/%d' %\n",
    "                          (binary_search_step, is_adv.sum(), batch_size))\n",
    "\n",
    "        return x_adv\n",
    "\n",
    "    def check_adv(self, x, label):\n",
    "        \"\"\"Check if label of <x> predicted by <dknn> matches with <label>\"\"\"\n",
    "        y_pred = self.dknn.classify(x).argmax(1)\n",
    "        return torch.tensor((y_pred != label).astype(np.float32)).to(self.device)\n",
    "\n",
    "    def loss_function(self, x, reps, const, x_recon):\n",
    "        \"\"\"Returns the loss averaged over the batch (first dimension of x) and\n",
    "        L-2 norm squared of the perturbation\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        adv_loss = torch.zeros((batch_size, len(layers)), device=self.device)\n",
    "        # find squared L-2 distance between original samples and their\n",
    "        # adversarial examples at each layer\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            rep = reps[layer].view(batch_size, 1, -1)\n",
    "            dist = ((rep - self.guide_reps[layer])**2).sum(2)\n",
    "#             fx = self.sigmoid((self.thres - dist).clamp(-80 / self.a, 80 / self.a), a=self.a)\n",
    "#             fx = -dist\n",
    "            fx = self.thres - dist\n",
    "#             Fx = (coeff.to(device) * fx).sum(1)\n",
    "            Fx = torch.max(torch.tensor(0., device=self.device), \n",
    "                           self.coeff.to(self.device) * fx).sum(1)\n",
    "#             adv_loss[:, l] = torch.max(torch.tensor(-1., device=device), Fx)\n",
    "            adv_loss[:, l] = Fx\n",
    "        # find L-2 norm squared of perturbation\n",
    "        l2dist = torch.norm((x - x_recon).view(batch_size, -1), dim=1)**2\n",
    "        # total_loss is sum of squared perturbation norm and squared distance\n",
    "        # of representations, multiplied by constant\n",
    "        total_loss = l2dist + const * adv_loss.mean(1)\n",
    "\n",
    "        return total_loss.mean(), l2dist.sqrt()\n",
    "\n",
    "    def find_guide_samples(self, x, label, m=100, layer='relu1'):\n",
    "        \"\"\"Find k nearest neighbors to <x> that all have the same class but not\n",
    "        equal to <label>\n",
    "        \"\"\"\n",
    "        num_classes = self.dknn.num_classes\n",
    "        x_train = self.dknn.x_train\n",
    "        y_train = self.dknn.y_train\n",
    "        batch_size = x.size(0)\n",
    "        nn = torch.zeros((m, ) + x.size()).transpose(0, 1)\n",
    "        D, I = self.dknn.get_neighbors(\n",
    "            x, k=x_train.size(0), layers=[layer])[0]\n",
    "\n",
    "        for i, (d, ind) in enumerate(zip(D, I)):\n",
    "            mean_dist = np.zeros((num_classes, ))\n",
    "            for j in range(num_classes):\n",
    "                mean_dist[j] = np.mean(\n",
    "                    d[np.where(y_train[ind] == j)[0]][:m // 2])\n",
    "            mean_dist[label[i]] += INFTY\n",
    "            nearest_label = mean_dist.argmin()\n",
    "            nn_ind = np.where(y_train[ind] == nearest_label)[0][:m // 2]\n",
    "            nn[i, m // 2:] = x_train[ind[nn_ind]]\n",
    "            nn_ind = np.where(y_train[ind] == label[i])[0][:m // 2]\n",
    "            nn[i, :m // 2] = x_train[ind[nn_ind]]\n",
    "            \n",
    "        # initialize self.guide_reps if empty\n",
    "        if not self.guide_reps:\n",
    "            guide_rep = self.dknn.get_activations(\n",
    "                nn[0], requires_grad=False)\n",
    "            for layer in self.layers:\n",
    "                # set a zero tensor before filling it\n",
    "                size = (batch_size, ) + guide_rep[layer].view(m, -1).size()\n",
    "                self.guide_reps[layer] = torch.zeros(size, device=self.device)\n",
    "        \n",
    "        # fill self.guide_reps\n",
    "        for i in range(batch_size):\n",
    "            guide_rep = self.dknn.get_activations(\n",
    "                nn[i], requires_grad=False)\n",
    "            self.guide_reps[layer][i] = guide_rep[layer].view(\n",
    "                m, -1).detach()\n",
    "\n",
    "    @staticmethod\n",
    "    def atanh(x):\n",
    "        return 0.5 * torch.log((1 + x) / (1 - x))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x, a=1):\n",
    "        return 1 / (1 + torch.exp(-a * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Implement gradient-based attack on DkNN with L-2 constraint'''\n",
    "\n",
    "# import logging\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "\n",
    "# INFTY = 1e20\n",
    "\n",
    "\n",
    "# class DKNNExpAttack(object):\n",
    "#     \"\"\"\n",
    "#     Implement gradient-based attack on Deep k-Nearest Neigbhor that uses\n",
    "#     L-2 distance as a metric\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __call__(self, dknn, x_orig, label, guide_layer='relu1', m=100,\n",
    "#                  binary_search_steps=5, max_iterations=500,\n",
    "#                  learning_rate=1e-2, initial_const=1, abort_early=True,\n",
    "#                  max_linf=None, random_start=False, guide_mode=1, thres=0, a=1):\n",
    "#         \"\"\"\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         dknn : DKNN object\n",
    "#             DkNN (defined in lin/dknn.py) that we want to attack\n",
    "#         x_orig : torch.tensor\n",
    "#             tensor of the original samples to attack. Does not need to require\n",
    "#             gradients, shape is (num_samples, ) + input_shape\n",
    "#         label : torch.tensor\n",
    "#             tensor of the label corresponding to x_orig\n",
    "#         guide_layer : str. optional\n",
    "#             layer name in which we want to find guide samples. Default is\n",
    "#             'relu1'\n",
    "#         m : int, optional\n",
    "#             number of guide samples. Default is 100\n",
    "#         binary_search_step : int, optional\n",
    "#             number of steps for binary search on the norm penalty constant.\n",
    "#             Default is 5\n",
    "#         max_iterations : int, optional\n",
    "#             number of optimization steps (per one binary search). Default is\n",
    "#             500\n",
    "#         learning_rate : float , optional\n",
    "#             step size or learning rate for the optimizer. Default is 1e-2\n",
    "#         initial_const : float, optional\n",
    "#             a number the norm penalty constant should be initialized to.\n",
    "#             Default is 1\n",
    "#         abort_early : bool, optional\n",
    "#             whether or not to abort the optimization early (before reaching\n",
    "#             max_iterations) if the objective does not improve from the past\n",
    "#             (max_iterations // 10) steps. Default is True\n",
    "#         max_linf : float, optional\n",
    "#             use to bound the L-inf norm of the attacks (addition to L-2 norm\n",
    "#             penalty). Set to None to not use this option. Default is None\n",
    "#         random_start : bool, optional\n",
    "#             whether or not to initialize the perturbation with small isotropic\n",
    "#             Gaussian noise. Default is False\n",
    "#         guide_mode : int, optional\n",
    "#             Choose the guide_mode to use between 1 and 2. Default is 1\n",
    "#             - guide_mode == 1: find m nearest neighbors to input that all have\n",
    "#             the same class but not equal its original label.\n",
    "#             - guide_mode == 2: find the nearest neighbor that has a different\n",
    "#             class from the input and find its m - 1 neighbors\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         x_adv : torch.tensor\n",
    "#             adversarial examples found. If adversarial examples for some inputs\n",
    "#             are not found, return those inputs.\n",
    "#         \"\"\"\n",
    "\n",
    "#         min_, max_ = x_orig.min(), x_orig.max()\n",
    "#         if max_linf is not None:\n",
    "#             min_ = torch.max(x_orig - max_linf, min_)\n",
    "#             max_ = torch.min(x_orig + max_linf, max_)\n",
    "#         batch_size = x_orig.size(0)\n",
    "#         x_adv = x_orig.clone()\n",
    "#         label = label.cpu().numpy()\n",
    "#         input_shape = x_orig.detach().cpu().numpy().shape\n",
    "#         device = dknn.device\n",
    "#         self.a = a\n",
    "#         self.thres = thres.to(device).view(batch_size, 1)\n",
    "\n",
    "#         def to_attack_space(x):\n",
    "#             # map from [min_, max_] to [-1, +1]\n",
    "#             a = (min_ + max_) / 2\n",
    "#             b = (max_ - min_) / 2\n",
    "#             x = (x - a) / b\n",
    "\n",
    "#             # from [-1, +1] to approx. (-1, +1)\n",
    "#             x = x * 0.999999\n",
    "\n",
    "#             # from (-1, +1) to (-inf, +inf)\n",
    "#             return self.atanh(x)\n",
    "\n",
    "#         def to_model_space(x):\n",
    "#             \"\"\"Transforms an input from the attack space\n",
    "#             to the model space. This transformation and\n",
    "#             the returned gradient are elementwise.\"\"\"\n",
    "\n",
    "#             # from (-inf, +inf) to (-1, +1)\n",
    "#             x = torch.tanh(x)\n",
    "\n",
    "#             # map from (-1, +1) to (min_, max_)\n",
    "#             a = (min_ + max_) / 2\n",
    "#             b = (max_ - min_) / 2\n",
    "#             x = x * b + a\n",
    "\n",
    "#             return x\n",
    "\n",
    "#         # variables representing inputs in attack space will be prefixed with z\n",
    "#         z_orig = to_attack_space(x_orig)\n",
    "#         x_recon = to_model_space(z_orig)\n",
    "\n",
    "#         # declare tensors that keep track of constants and binary search\n",
    "#         const = torch.zeros((batch_size, ), device=device)\n",
    "#         const += initial_const\n",
    "#         lower_bound = torch.zeros_like(const)\n",
    "#         upper_bound = torch.zeros_like(const) + INFTY\n",
    "#         best_l2dist = torch.zeros_like(const) + INFTY\n",
    "\n",
    "#         with torch.no_grad():\n",
    "\n",
    "#             # choose guide samples and get their representations\n",
    "#             x_guide, coeff = self.find_guide_samples_v2(\n",
    "#                 dknn, x_orig, label, k=m, layer=guide_layer)\n",
    "#             guide_reps = {}\n",
    "#             for i in range(batch_size):\n",
    "#                 guide_rep = dknn.get_activations(\n",
    "#                     x_guide[i], requires_grad=False)\n",
    "#                 for layer in dknn.layers:\n",
    "#                     if i == 0:\n",
    "#                         # set a zero tensor before filling it\n",
    "#                         size = (batch_size, ) + \\\n",
    "#                             guide_rep[layer].view(m, -1).size()\n",
    "#                         guide_reps[layer] = torch.zeros(size, device=device)\n",
    "#                     guide_reps[layer][i] = guide_rep[layer].view(\n",
    "#                         m, -1).detach()\n",
    "\n",
    "#         for binary_search_step in range(binary_search_steps):\n",
    "#             if (binary_search_step == binary_search_steps - 1 and\n",
    "#                     binary_search_steps >= 10):\n",
    "#                     # in the last binary search step, use the upper_bound instead\n",
    "#                     # to ensure that unsuccessful attacks use the largest\n",
    "#                     # possible constant\n",
    "#                 const = upper_bound\n",
    "\n",
    "#             if not random_start:\n",
    "#                 z_delta = torch.zeros_like(z_orig, requires_grad=True)\n",
    "#             else:\n",
    "#                 rand = np.random.randn(*input_shape) * 1e-2\n",
    "#                 z_delta = torch.tensor(\n",
    "#                     rand, dtype=torch.float32, requires_grad=True, device=device)\n",
    "#             loss_at_previous_check = torch.zeros(1, device=device) + INFTY\n",
    "\n",
    "#             # create a new optimizer\n",
    "# #             optimizer = optim.Adam([z_delta], lr=learning_rate)\n",
    "#             # optimizer = optim.SGD([z_delta], lr=learning_rate)\n",
    "#             optimizer = optim.RMSprop([z_delta], lr=learning_rate)\n",
    "            \n",
    "#             # add learning rate scheduler\n",
    "#             lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#                 optimizer, mode='min', factor=0.5, patience=150, \n",
    "#                 threshold=0.01, threshold_mode='rel')\n",
    "\n",
    "#             for iteration in range(max_iterations):\n",
    "#                 optimizer.zero_grad()\n",
    "#                 x = to_model_space(z_orig + z_delta)\n",
    "#                 # adjust threshold\n",
    "# #                 with torch.no_grad():\n",
    "# #                     if iteration % 100 == 0:\n",
    "# #                         thres = dknn.get_neighbors(x)[0][0][:, -1]\n",
    "# #                         self.thres = torch.tensor(thres).to(device).view(batch_size, 1)\n",
    "# #                         x_guide, coeff = self.find_guide_samples_v2(\n",
    "# #                             dknn, x, label, k=m, layer=guide_layer)\n",
    "# #                         guide_reps = {}\n",
    "# #                         for i in range(batch_size):\n",
    "# #                             guide_rep = dknn.get_activations(\n",
    "# #                                 x_guide[i], requires_grad=False)\n",
    "# #                             for layer in dknn.layers:\n",
    "# #                                 if i == 0:\n",
    "# #                                     # set a zero tensor before filling it\n",
    "# #                                     size = (batch_size, ) + \\\n",
    "# #                                         guide_rep[layer].view(m, -1).size()\n",
    "# #                                     guide_reps[layer] = torch.zeros(size, device=device)\n",
    "# #                                 guide_reps[layer][i] = guide_rep[layer].view(\n",
    "# #                                     m, -1).detach()\n",
    "                    \n",
    "#                 reps = dknn.get_activations(x, requires_grad=True)\n",
    "#                 loss, l2dist = self.loss_function(\n",
    "#                     x, reps, guide_reps, coeff, dknn.layers, const, x_recon, device)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "# #                 lr_scheduler.step(loss)\n",
    "\n",
    "# #                 if iteration == 0 or iteration == 900:\n",
    "# #                     import pdb\n",
    "# #                     pdb.set_trace()\n",
    "# #                 import pdb; pdb.set_trace()\n",
    "\n",
    "#                 if iteration % (np.ceil(max_iterations / 10)) == 0:\n",
    "#                     print('    step: %d; loss: %.3f; l2dist: %.3f' %\n",
    "#                           (iteration, loss.cpu().detach().numpy(),\n",
    "#                            l2dist.mean().cpu().detach().numpy()))\n",
    "#                 # DEBUG:\n",
    "#                 # for i in range(5):\n",
    "#                 #     print(z_delta.grad[i].view(-1).norm().item())\n",
    "\n",
    "#                 if abort_early and iteration % 10 == 0:\n",
    "#                     # # after each tenth of the iterations, check progress\n",
    "#                     # if torch.gt(loss, .9999 * loss_at_previous_check):\n",
    "#                     #     break  # stop Adam if there has not been progress\n",
    "#                     # loss_at_previous_check = loss\n",
    "#                     is_adv = self.check_adv(dknn, x, label)\n",
    "# #                     print(is_adv.sum())\n",
    "#                     for i in range(batch_size):\n",
    "#                         if is_adv[i] and best_l2dist[i] > l2dist[i]:\n",
    "#                             x_adv[i] = x[i]\n",
    "#                             best_l2dist[i] = l2dist[i]\n",
    "\n",
    "#             # check how many attacks have succeeded\n",
    "#             with torch.no_grad():\n",
    "#                 is_adv = self.check_adv(dknn, x, label)\n",
    "#                 print(is_adv.sum())\n",
    "\n",
    "#             # if binary_search_step == 14:\n",
    "#             #     import pdb\n",
    "#             #     pdb.set_trace()\n",
    "\n",
    "#             for i in range(batch_size):\n",
    "#                 # set new upper and lower bounds\n",
    "#                 if is_adv[i]:\n",
    "#                     upper_bound[i] = const[i]\n",
    "#                 else:\n",
    "#                     lower_bound[i] = const[i]\n",
    "#                 # set new const\n",
    "#                 if upper_bound[i] == INFTY:\n",
    "#                     # exponential search if adv has not been found\n",
    "#                     const[i] *= 10\n",
    "#                 else:\n",
    "#                     # binary search if adv has been found\n",
    "#                     const[i] = (lower_bound[i] + upper_bound[i]) / 2\n",
    "#                 # only keep adv with smallest l2dist\n",
    "#                 if is_adv[i] and best_l2dist[i] > l2dist[i]:\n",
    "#                     x_adv[i] = x[i]\n",
    "#                     best_l2dist[i] = l2dist[i]\n",
    "\n",
    "#             # check the current attack success rate (combined with previous\n",
    "#             # binary search steps)\n",
    "#             with torch.no_grad():\n",
    "#                 is_adv = self.check_adv(dknn, x_adv, label)\n",
    "#             print('binary step: %d; number of successful adv: %d/%d' %\n",
    "#                   (binary_search_step, is_adv.sum(), batch_size))\n",
    "\n",
    "#         return x_adv\n",
    "\n",
    "#     @classmethod\n",
    "#     def check_adv(cls, dknn, x, label):\n",
    "#         \"\"\"Check if label of <x> predicted by <dknn> matches with <label>\"\"\"\n",
    "#         y_pred = dknn.classify(x).argmax(1)\n",
    "#         return torch.tensor((y_pred != label).astype(np.float32)).to(dknn.device)\n",
    "\n",
    "#     def loss_function(self, x, reps, guide_reps, coeff, layers, const, x_recon, device):\n",
    "#         \"\"\"Returns the loss averaged over the batch (first dimension of x) and\n",
    "#         L-2 norm squared of the perturbation\n",
    "#         \"\"\"\n",
    "\n",
    "#         batch_size = x.size(0)\n",
    "#         adv_loss = torch.zeros((batch_size, len(layers)), device=device)\n",
    "#         # find squared L-2 distance between original samples and their\n",
    "#         # adversarial examples at each layer\n",
    "#         for l, layer in enumerate(layers):\n",
    "#             rep = reps[layer].view(batch_size, 1, -1)\n",
    "#             dist = ((rep - guide_reps[layer])**2).sum(2)\n",
    "# #             import pdb; pdb.set_trace()\n",
    "# #             fx = self.sigmoid((self.thres - dist).clamp(-80 / self.a, 80 / self.a), a=self.a)\n",
    "# #             fx = -dist\n",
    "#             fx = self.thres - dist\n",
    "# #             Fx = (coeff.to(device) * fx).sum(1)\n",
    "#             Fx = torch.max(torch.tensor(0., device=device), coeff.to(device) * fx).sum(1)\n",
    "# #             adv_loss[:, l] = torch.max(torch.tensor(-1., device=device), Fx)\n",
    "#             adv_loss[:, l] = Fx\n",
    "#         # find L-2 norm squared of perturbation\n",
    "#         l2dist = torch.norm((x - x_recon).view(batch_size, -1), dim=1)**2\n",
    "#         # total_loss is sum of squared perturbation norm and squared distance\n",
    "#         # of representations, multiplied by constant\n",
    "#         total_loss = l2dist + const * adv_loss.mean(1)\n",
    "\n",
    "#         return total_loss.mean(), l2dist.sqrt()\n",
    "\n",
    "#     @staticmethod\n",
    "#     def find_guide_samples(dknn, x, label, k=100, layer='relu1'):\n",
    "#         \"\"\"Find k nearest neighbors to <x> that all have the same class but not\n",
    "#         equal to <label>\n",
    "#         \"\"\"\n",
    "#         num_classes = dknn.num_classes\n",
    "#         nn = torch.zeros((k, ) + x.size()).transpose(0, 1)\n",
    "#         coeff = torch.zeros((x.size(0), k))\n",
    "#         # coeff[:, :k // 2] -= 1\n",
    "#         # coeff[:, k // 2:] += 1\n",
    "#         coeff[:, :k // 2] += 1\n",
    "#         coeff[:, k // 2:] -= 1\n",
    "#         # coeff += 1\n",
    "#         D, I = dknn.get_neighbors(\n",
    "#             x, k=dknn.x_train.size(0), layers=[layer])[0]\n",
    "\n",
    "#         for i, (d, ind) in enumerate(zip(D, I)):\n",
    "#             mean_dist = np.zeros((num_classes, ))\n",
    "#             for j in range(num_classes):\n",
    "#                 mean_dist[j] = np.mean(\n",
    "#                     d[np.where(dknn.y_train[ind] == j)[0]][:k // 2])\n",
    "#             mean_dist[label[i]] += INFTY\n",
    "#             nearest_label = mean_dist.argmin()\n",
    "#             nn_ind = np.where(dknn.y_train[ind] == nearest_label)[0][:k // 2]\n",
    "#             nn[i, k // 2:] = dknn.x_train[ind[nn_ind]]\n",
    "#             nn_ind = np.where(dknn.y_train[ind] == label[i])[0][:k // 2]\n",
    "#             nn[i, :k // 2] = dknn.x_train[ind[nn_ind]]\n",
    "#             # nn_ind = np.where(dknn.y_train[ind] == nearest_label)[0][:k]\n",
    "#             # nn[i] = dknn.x_train[ind[nn_ind]]\n",
    "\n",
    "#         return nn, coeff\n",
    "    \n",
    "#     def find_guide_samples_v2(cls, dknn, x, label, k=100, layer='relu1'):\n",
    "#         \"\"\"Find the nearest neighbor to <x> that has a different label from\n",
    "#         <label>. Then find other <k> - 1 training samples that are closest to\n",
    "#         the neighbor and has the same class\n",
    "#         \"\"\"\n",
    "#         x_nn = torch.zeros((k, ) + x.size()).transpose(0, 1)\n",
    "#         # find nearest sample with same class\n",
    "#         _, I = dknn.get_neighbors(x, k=1, layers=[layer])[0]\n",
    "#         x_nn[:, :k // 2] = cls.find_nn_same_class(dknn, I[0], k=k // 2, layer=layer)\n",
    "        \n",
    "#         # find nearest sample with different class\n",
    "#         nn = dknn.find_nn_diff_class(x, label)\n",
    "#         # now find k neighbors that has the same class as x_nn\n",
    "#         x_nn[:, k // 2:] = cls.find_nn_same_class(dknn, nn, k=k // 2, layer=layer)\n",
    "        \n",
    "#         coeff = torch.zeros((x.size(0), k))\n",
    "#         # coeff[:, :k // 2] -= 1\n",
    "#         # coeff[:, k // 2:] += 1\n",
    "#         coeff[:, :k // 2] += 1\n",
    "#         coeff[:, k // 2:] -= 1\n",
    "        \n",
    "#         return x_nn, coeff\n",
    "\n",
    "#     @staticmethod\n",
    "#     def find_nn_same_class(dknn, ind_x, k=100, layer='relu1'):\n",
    "#         \"\"\"Find <k> training samples with the same class as and closest to the\n",
    "#         training sample with index <ind_x> in representation space at <layer>\n",
    "#         \"\"\"\n",
    "\n",
    "#         batch_size = ind_x.shape[0]\n",
    "#         label = dknn.y_train[ind_x]\n",
    "#         x_nn = torch.zeros((batch_size, k) + dknn.x_train[0].size())\n",
    "#         _, I = dknn.get_neighbors(\n",
    "#             dknn.x_train[ind_x], k=dknn.x_train.size(0), layers=[layer])[0]\n",
    "\n",
    "#         for i, ind in enumerate(I):\n",
    "#             nn_ind = np.where(dknn.y_train[ind] == label[i])[0][:k]\n",
    "#             x_nn[i] = dknn.x_train[ind[nn_ind]]\n",
    "\n",
    "#         return x_nn\n",
    "\n",
    "#     @staticmethod\n",
    "#     def atanh(x):\n",
    "#         return 0.5 * torch.log((1 + x) / (1 - x))\n",
    "\n",
    "#     @staticmethod\n",
    "#     def sigmoid(x, a=1):\n",
    "#         return 1 / (1 + torch.exp(-a * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 3.188; l2dist: 0.000\n",
      "    step: 100; loss: 2.927; l2dist: 0.472\n",
      "    step: 200; loss: 2.926; l2dist: 0.472\n",
      "    step: 300; loss: 2.926; l2dist: 0.473\n",
      "    step: 400; loss: 2.927; l2dist: 0.473\n",
      "    step: 500; loss: 2.927; l2dist: 0.473\n",
      "    step: 600; loss: 2.927; l2dist: 0.473\n",
      "    step: 700; loss: 2.927; l2dist: 0.473\n",
      "    step: 800; loss: 2.927; l2dist: 0.473\n",
      "    step: 900; loss: 2.927; l2dist: 0.472\n",
      "tensor(2., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 3/200\n",
      "    step: 0; loss: 31.876; l2dist: 0.000\n",
      "    step: 100; loss: 19.202; l2dist: 1.654\n",
      "    step: 200; loss: 18.137; l2dist: 1.851\n",
      "    step: 300; loss: 17.832; l2dist: 1.921\n",
      "    step: 400; loss: 16.257; l2dist: 1.982\n",
      "    step: 500; loss: 13.356; l2dist: 2.139\n",
      "    step: 600; loss: 12.258; l2dist: 2.227\n",
      "    step: 700; loss: 11.792; l2dist: 2.278\n",
      "    step: 800; loss: 11.596; l2dist: 2.303\n",
      "    step: 900; loss: 11.509; l2dist: 2.320\n",
      "tensor(34., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 41/200\n",
      "    step: 0; loss: 293.428; l2dist: 0.000\n",
      "    step: 100; loss: 148.893; l2dist: 1.835\n",
      "    step: 200; loss: 118.031; l2dist: 2.074\n",
      "    step: 300; loss: 67.753; l2dist: 2.369\n",
      "    step: 400; loss: 57.985; l2dist: 2.477\n",
      "    step: 500; loss: 54.651; l2dist: 2.516\n",
      "    step: 600; loss: 52.990; l2dist: 2.545\n",
      "    step: 700; loss: 52.082; l2dist: 2.565\n",
      "    step: 800; loss: 51.584; l2dist: 2.575\n",
      "    step: 900; loss: 51.285; l2dist: 2.579\n",
      "tensor(39., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 58/200\n",
      "    step: 0; loss: 2640.208; l2dist: 0.000\n",
      "    step: 100; loss: 1354.168; l2dist: 1.847\n",
      "    step: 200; loss: 832.355; l2dist: 2.176\n",
      "    step: 300; loss: 556.402; l2dist: 2.431\n",
      "    step: 400; loss: 495.160; l2dist: 2.508\n",
      "    step: 500; loss: 468.974; l2dist: 2.543\n",
      "    step: 600; loss: 455.691; l2dist: 2.564\n",
      "    step: 700; loss: 448.221; l2dist: 2.576\n",
      "    step: 800; loss: 443.764; l2dist: 2.586\n",
      "    step: 900; loss: 441.078; l2dist: 2.593\n",
      "tensor(40., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 60/200\n",
      "    step: 0; loss: 26064.938; l2dist: 0.000\n",
      "    step: 100; loss: 13353.946; l2dist: 1.851\n",
      "    step: 200; loss: 7914.708; l2dist: 2.202\n",
      "    step: 300; loss: 5447.151; l2dist: 2.432\n",
      "    step: 400; loss: 4867.014; l2dist: 2.513\n",
      "    step: 500; loss: 4611.903; l2dist: 2.553\n",
      "    step: 600; loss: 4481.533; l2dist: 2.574\n",
      "    step: 700; loss: 4408.999; l2dist: 2.589\n",
      "    step: 800; loss: 4365.605; l2dist: 2.598\n",
      "    step: 900; loss: 4339.403; l2dist: 2.604\n",
      "tensor(44., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 60/200\n",
      "    step: 0; loss: 260459.453; l2dist: 0.000\n",
      "    step: 100; loss: 133391.609; l2dist: 1.847\n",
      "    step: 200; loss: 78750.922; l2dist: 2.199\n",
      "    step: 300; loss: 54438.180; l2dist: 2.421\n",
      "    step: 400; loss: 48589.367; l2dist: 2.505\n",
      "    step: 500; loss: 46055.117; l2dist: 2.550\n",
      "    step: 600; loss: 44769.223; l2dist: 2.575\n",
      "    step: 700; loss: 44048.266; l2dist: 2.590\n",
      "    step: 800; loss: 43612.633; l2dist: 2.598\n",
      "    step: 900; loss: 43348.551; l2dist: 2.602\n",
      "tensor(42., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 60/200\n",
      "    step: 0; loss: 2604393.000; l2dist: 0.000\n",
      "    step: 100; loss: 1334014.375; l2dist: 1.842\n",
      "    step: 200; loss: 787041.562; l2dist: 2.195\n",
      "    step: 300; loss: 543382.750; l2dist: 2.416\n",
      "    step: 400; loss: 485524.062; l2dist: 2.498\n",
      "    step: 500; loss: 460149.875; l2dist: 2.544\n",
      "    step: 600; loss: 447318.219; l2dist: 2.571\n",
      "    step: 700; loss: 440177.844; l2dist: 2.588\n",
      "    step: 800; loss: 435886.000; l2dist: 2.595\n",
      "    step: 900; loss: 433287.000; l2dist: 2.599\n",
      "tensor(34., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 60/200\n",
      "    step: 0; loss: 26043720.000; l2dist: 0.000\n",
      "    step: 100; loss: 13337154.000; l2dist: 1.842\n",
      "    step: 200; loss: 7866900.500; l2dist: 2.194\n",
      "    step: 300; loss: 5432089.000; l2dist: 2.414\n",
      "    step: 400; loss: 4854787.500; l2dist: 2.495\n",
      "    step: 500; loss: 4603869.500; l2dist: 2.543\n",
      "    step: 600; loss: 4474594.000; l2dist: 2.571\n",
      "    step: 700; loss: 4403006.000; l2dist: 2.588\n",
      "    step: 800; loss: 4359972.500; l2dist: 2.596\n",
      "    step: 900; loss: 4333047.000; l2dist: 2.600\n",
      "tensor(40., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 60/200\n",
      "    step: 0; loss: 260436992.000; l2dist: 0.000\n",
      "    step: 100; loss: 133352504.000; l2dist: 1.841\n",
      "    step: 200; loss: 78687704.000; l2dist: 2.193\n",
      "    step: 300; loss: 54377344.000; l2dist: 2.413\n",
      "    step: 400; loss: 48573368.000; l2dist: 2.493\n",
      "    step: 500; loss: 46039368.000; l2dist: 2.542\n",
      "    step: 600; loss: 44752660.000; l2dist: 2.571\n",
      "    step: 700; loss: 44031696.000; l2dist: 2.587\n",
      "    step: 800; loss: 43602704.000; l2dist: 2.596\n",
      "    step: 900; loss: 43333852.000; l2dist: 2.599\n",
      "tensor(38., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 60/200\n",
      "    step: 0; loss: 2604369740013481492480.000; l2dist: 0.000\n",
      "    step: 100; loss: 1333667283201874198528.000; l2dist: 1.842\n",
      "    step: 200; loss: 786751833303861428224.000; l2dist: 2.194\n",
      "    step: 300; loss: 543391981323920867328.000; l2dist: 2.414\n",
      "    step: 400; loss: 485373057302553165824.000; l2dist: 2.495\n",
      "    step: 500; loss: 460423326473153150976.000; l2dist: 2.544\n",
      "    step: 600; loss: 447480790384648388608.000; l2dist: 2.572\n",
      "    step: 700; loss: 440329320466988662784.000; l2dist: 2.588\n",
      "    step: 800; loss: 436047136093173186560.000; l2dist: 2.596\n",
      "    step: 900; loss: 433376888542235459584.000; l2dist: 2.599\n",
      "tensor(60., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 60/200\n"
     ]
    }
   ],
   "source": [
    "# Attack for L2 DkNN\n",
    "\n",
    "# attack = DKNNL2Attack()\n",
    "# attack = DKNNLinfAttack()\n",
    "from lib.dknn_attack_exp import DKNNExpAttack\n",
    "attack = DKNNExpAttack(knn)\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            x[begin:end], y[begin:end],\n",
    "            guide_layer=layers[0], m=2, binary_search_steps=10,\n",
    "            max_iterations=1000, learning_rate=1e-2,\n",
    "            initial_const=1e-1, random_start=False,\n",
    "            thres_steps=10, check_adv_steps=50, verbose=True,\n",
    "            max_linf=0.3)\n",
    "    return x_adv\n",
    "\n",
    "x_adv = attack_batch(x_test_sub[:num].cuda(), y_test_sub[:num], 200)\n",
    "# x_adv = attack_batch(x_test_sub[num-2:num-1].cuda(), y_test_sub[num-2:num-1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 7.592; l2dist: 0.000\n",
      "    step: 150; loss: 5.997; l2dist: 1.029\n",
      "    step: 300; loss: 5.585; l2dist: 1.400\n",
      "    step: 450; loss: 5.588; l2dist: 1.394\n",
      "    step: 600; loss: 5.583; l2dist: 1.405\n",
      "    step: 750; loss: 5.584; l2dist: 1.405\n",
      "    step: 900; loss: 5.585; l2dist: 1.404\n",
      "    step: 1050; loss: 5.587; l2dist: 1.395\n",
      "    step: 1200; loss: 5.584; l2dist: 1.404\n",
      "    step: 1350; loss: 5.586; l2dist: 1.405\n",
      "tensor(0., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 0/1\n",
      "    step: 0; loss: 75.919; l2dist: 0.000\n",
      "    step: 150; loss: 17.589; l2dist: 3.628\n",
      "    step: 300; loss: 15.533; l2dist: 2.641\n",
      "    step: 450; loss: 8.157; l2dist: 2.856\n",
      "    step: 600; loss: 7.948; l2dist: 2.798\n",
      "    step: 750; loss: 8.104; l2dist: 2.847\n",
      "    step: 900; loss: 7.841; l2dist: 2.800\n",
      "    step: 1050; loss: 7.986; l2dist: 2.826\n",
      "    step: 1200; loss: 7.870; l2dist: 2.805\n",
      "    step: 1350; loss: 7.879; l2dist: 2.807\n",
      "tensor(0., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 1/1\n",
      "    step: 0; loss: 759.191; l2dist: 0.000\n",
      "    step: 150; loss: 46.335; l2dist: 4.323\n",
      "    step: 300; loss: 16.761; l2dist: 4.094\n",
      "    step: 450; loss: 12.496; l2dist: 3.535\n",
      "    step: 600; loss: 9.702; l2dist: 3.115\n",
      "    step: 750; loss: 9.173; l2dist: 3.029\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-7099e7c389d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_adv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mx_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-7099e7c389d1>\u001b[0m in \u001b[0;36mattack_batch\u001b[0;34m(x, y, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minitial_const\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabort_early\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             thres=thres[i:i+1], a=0.1)\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_adv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-7fd4f24cb707>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, dknn, x_orig, label, guide_layer, m, binary_search_steps, max_iterations, learning_rate, initial_const, abort_early, max_linf, random_start, guide_mode, thres, a)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mreps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 loss, l2dist = self.loss_function(\n\u001b[0;32m--> 192\u001b[0;31m                     x, reps, guide_reps, coeff, dknn.layers, const, x_recon, device)\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-7fd4f24cb707>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(self, x, reps, guide_reps, coeff, layers, const, x_recon, device)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0madv_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# find L-2 norm squared of perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0ml2dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx_recon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;31m# total_loss is sum of squared perturbation norm and squared distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# of representations, multiplied by constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Attack for L2 DkNN\n",
    "\n",
    "attack = DKNNExpAttack()\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    for i in range(batch_size):\n",
    "        x_adv[i:i+1] = attack(\n",
    "            knn, x[i:i+1], y[i:i+1],\n",
    "            guide_layer=layers[0], m=4, binary_search_steps=15,\n",
    "            max_iterations=1500, learning_rate=1e-2, guide_mode=1,\n",
    "            initial_const=1e-1, abort_early=True, random_start=False,\n",
    "            thres=thres[i:i+1], a=0.1)\n",
    "    return x_adv\n",
    "\n",
    "x_adv = attack_batch(x_test_sub[:num].cuda(), y_test_sub[:num], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = reps[layer].view(batch_size, 1, -1);dist = ((rep - guide_reps[layer])**2).sum(2)\n",
    "fx = self.sigmoid((self.thres - dist).clamp(-80 / self.a, 80 / self.a), a=self.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.0325, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = knn.classify(x_adv)\n",
    "    ind = np.where(y_pred.argmax(1) == y_test_sub[:num].numpy())[0]\n",
    "    print(len(ind) / x_adv.size(0))\n",
    "(x_test_sub - x_adv.cpu()).view(num, -1).norm(2, 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (200) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4cb812a25389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx_test_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (200) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "thres = ((knn.x_train[knn.get_neighbors(x_test_sub)[0][1]].squeeze() - x_test_sub)**2).sum((1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.5435)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thres.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFTY = 1e20\n",
    "\n",
    "\n",
    "class DKNN_PGD(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dknn):\n",
    "        self.dknn = dknn\n",
    "        self.device = dknn.device\n",
    "        self.layers = dknn.layers\n",
    "        self.guide_reps = {}\n",
    "        self.thres = None\n",
    "        self.coeff = None\n",
    "\n",
    "    def __call__(self, x_orig, label, guide_layer, m, epsilon=0.1,\n",
    "                 max_epsilon=0.3, max_iterations=1000, num_restart=1,\n",
    "                 rand_start=True, thres_steps=100, check_adv_steps=100,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        x_orig is tensor (requires_grad=False)\n",
    "        \"\"\"\n",
    "\n",
    "        # make sure we run at least once\n",
    "        if num_restart < 1:\n",
    "            num_restart = 1\n",
    "\n",
    "        # if not using randomized start, no point in doing more than one start\n",
    "        if not rand_start:\n",
    "            num_restart = 1\n",
    "\n",
    "        label = label.cpu().numpy()\n",
    "        batch_size = x_orig.size(0)\n",
    "        min_, max_ = x_orig.min(), x_orig.max()\n",
    "        x_adv = x_orig.detach()\n",
    "        best_num_nn = np.zeros((batch_size, ))\n",
    "        \n",
    "        self.coeff = torch.zeros((x_orig.size(0), m))\n",
    "        self.coeff[:, :m // 2] += 1\n",
    "        self.coeff[:, m // 2:] -= 1\n",
    "        \n",
    "        for i in range(num_restart):\n",
    "\n",
    "            # initialize perturbation\n",
    "            delta = torch.zeros_like(x_adv)\n",
    "            if rand_start:\n",
    "                delta.uniform_(- max_epsilon * 0.1, max_epsilon * 0.1)\n",
    "            delta.requires_grad_()\n",
    "\n",
    "            for iteration in range(max_iterations):\n",
    "                x = torch.clamp(x_orig + delta, min_, max_)\n",
    "\n",
    "                # adaptively choose threshold and guide samples every\n",
    "                # <thres_steps> iterations\n",
    "                with torch.no_grad():\n",
    "                    if iteration % thres_steps == 0:\n",
    "                        thres = self.dknn.get_neighbors(x)[0][0][:, -1]\n",
    "                        self.thres = torch.tensor(thres).to(self.device).view(\n",
    "                            batch_size, 1)\n",
    "                        self.find_guide_samples(\n",
    "                            x, label, m=m, layer=guide_layer)\n",
    "\n",
    "                reps = self.dknn.get_activations(x, requires_grad=True)\n",
    "                loss = self.loss_function(reps)\n",
    "                loss.backward()\n",
    "                # perform update on delta\n",
    "                with torch.no_grad():\n",
    "                    delta -= epsilon * delta.grad.detach().sign()\n",
    "                    delta.clamp_(- max_epsilon, max_epsilon)\n",
    "\n",
    "                if (verbose and iteration % (np.ceil(max_iterations / 10)) == 0):\n",
    "                    print('    step: %d; loss: %.3f' %\n",
    "                          (iteration, loss.cpu().detach().numpy()))\n",
    "                \n",
    "                if ((iteration + 1) % check_adv_steps == 0 or\n",
    "                        iteration == max_iterations):\n",
    "                    with torch.no_grad():\n",
    "                        # check if x are adversarial. Only store adversarial examples\n",
    "                        # if they have a larger number of wrong neighbors than orevious\n",
    "                        is_adv, num_nn = self.check_adv(x, label)\n",
    "                        for j in range(batch_size):\n",
    "                            if is_adv[j] and num_nn[j] > best_num_nn[j]:\n",
    "                                x_adv[j] = x[j]\n",
    "                                best_num_nn[j] = num_nn[j]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                is_adv, _ = self.check_adv(x_adv, label)\n",
    "            if verbose:\n",
    "                print('number of successful adv: %d/%d' % (is_adv.sum(), batch_size))\n",
    "\n",
    "        return x_adv\n",
    "\n",
    "    def check_adv(self, x, label):\n",
    "        \"\"\"Check if label of <x> predicted by <dknn> matches with <label>\"\"\"\n",
    "        output = self.dknn.classify(x)\n",
    "        num_nn = output.max(1)\n",
    "        y_pred = output.argmax(1)\n",
    "        is_adv = (y_pred != label).astype(np.float32)\n",
    "        return is_adv, num_nn\n",
    "\n",
    "    def loss_function(self, reps):\n",
    "        \"\"\"Returns the loss averaged over the batch (first dimension of x) and\n",
    "        L-2 norm squared of the perturbation\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = reps[self.layers[0]].size(0)\n",
    "        adv_loss = torch.zeros(\n",
    "            (batch_size, len(self.layers)), device=self.device)\n",
    "        # find squared L-2 distance between original samples and their\n",
    "        # adversarial examples at each layer\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            rep = reps[layer].view(batch_size, 1, -1)\n",
    "            dist = ((rep - self.guide_reps[layer])**2).sum(2)\n",
    "            fx = self.thres - dist\n",
    "            Fx = torch.max(torch.tensor(0., device=self.device),\n",
    "                           self.coeff.to(self.device) * fx).sum(1)\n",
    "            Fx = (- self.coeff.to(self.device) * dist).sum(1)\n",
    "            adv_loss[:, l] = Fx\n",
    "\n",
    "        return adv_loss.mean()\n",
    "\n",
    "    def find_guide_samples(self, x, label, m=100, layer='relu1'):\n",
    "        \"\"\"Find k nearest neighbors to <x> that all have the same class but not\n",
    "        equal to <label>\n",
    "        \"\"\"\n",
    "        num_classes = self.dknn.num_classes\n",
    "        x_train = self.dknn.x_train\n",
    "        y_train = self.dknn.y_train\n",
    "        batch_size = x.size(0)\n",
    "        nn = torch.zeros((m, ) + x.size()).transpose(0, 1)\n",
    "        D, I = self.dknn.get_neighbors(\n",
    "            x, k=x_train.size(0), layers=[layer])[0]\n",
    "\n",
    "        for i, (d, ind) in enumerate(zip(D, I)):\n",
    "            mean_dist = np.zeros((num_classes, ))\n",
    "            for j in range(num_classes):\n",
    "                mean_dist[j] = np.mean(\n",
    "                    d[np.where(y_train[ind] == j)[0]][:m // 2])\n",
    "            mean_dist[label[i]] += INFTY\n",
    "            nearest_label = mean_dist.argmin()\n",
    "            nn_ind = np.where(y_train[ind] == nearest_label)[0][:m // 2]\n",
    "            nn[i, m // 2:] = x_train[ind[nn_ind]]\n",
    "            nn_ind = np.where(y_train[ind] == label[i])[0][:m // 2]\n",
    "            nn[i, :m // 2] = x_train[ind[nn_ind]]\n",
    "\n",
    "        # initialize self.guide_reps if empty\n",
    "        if not self.guide_reps:\n",
    "            guide_rep = self.dknn.get_activations(\n",
    "                nn[0], requires_grad=False)\n",
    "            for l in self.layers:\n",
    "                # set a zero tensor before filling it\n",
    "                size = (batch_size, ) + guide_rep[l].view(m, -1).size()\n",
    "                self.guide_reps[l] = torch.zeros(size, device=self.device)\n",
    "\n",
    "        # fill self.guide_reps\n",
    "        for i in range(batch_size):\n",
    "            guide_rep = self.dknn.get_activations(\n",
    "                nn[i], requires_grad=False)\n",
    "            self.guide_reps[layer][i] = guide_rep[layer].view(\n",
    "                m, -1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFTY = 1e20\n",
    "\n",
    "\n",
    "class DKNN_PGD(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dknn):\n",
    "        self.dknn = dknn\n",
    "        self.device = dknn.device\n",
    "        self.layers = dknn.layers\n",
    "        self.guide_reps = {}\n",
    "        self.thres = None\n",
    "        self.coeff = None\n",
    "\n",
    "    def __call__(self, x_orig, label, guide_layer, m, epsilon=0.1,\n",
    "                 max_epsilon=0.3, max_iterations=1000, num_restart=1,\n",
    "                 rand_start=True, thres_steps=100, check_adv_steps=100,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        x_orig is tensor (requires_grad=False)\n",
    "        \"\"\"\n",
    "\n",
    "        # make sure we run at least once\n",
    "        if num_restart < 1:\n",
    "            num_restart = 1\n",
    "\n",
    "        # if not using randomized start, no point in doing more than one start\n",
    "        if not rand_start:\n",
    "            num_restart = 1\n",
    "\n",
    "        label = label.cpu().numpy()\n",
    "        batch_size = x_orig.size(0)\n",
    "        min_, max_ = x_orig.min(), x_orig.max()\n",
    "        x_adv = x_orig.detach()\n",
    "        best_num_nn = np.zeros((batch_size, ))\n",
    "        \n",
    "        self.coeff = torch.zeros((x_orig.size(0), m))\n",
    "        self.coeff[:, :m // 2] += 1\n",
    "        self.coeff[:, m // 2:] -= 1\n",
    "        \n",
    "        for i in range(num_restart):\n",
    "\n",
    "            # initialize perturbation\n",
    "            delta = torch.zeros_like(x_adv)\n",
    "            if rand_start:\n",
    "                delta.uniform_(- max_epsilon * 0.1, max_epsilon * 0.1)\n",
    "            delta.requires_grad_()\n",
    "\n",
    "            for iteration in range(max_iterations):\n",
    "                x = torch.clamp(x_orig + delta, min_, max_)\n",
    "\n",
    "                # adaptively choose threshold and guide samples every\n",
    "                # <thres_steps> iterations\n",
    "                with torch.no_grad():\n",
    "                    if iteration % thres_steps == 0:\n",
    "                        thres = self.dknn.get_neighbors(x)[0][0][:, -1]\n",
    "                        self.thres = torch.tensor(thres).to(self.device).view(\n",
    "                            batch_size, 1)\n",
    "                        self.find_guide_samples(\n",
    "                            x, label, m=m, layer=guide_layer)\n",
    "\n",
    "                reps = self.dknn.get_activations(x, requires_grad=True)\n",
    "                loss = self.loss_function(reps)\n",
    "                loss.backward()\n",
    "                # perform update on delta\n",
    "                with torch.no_grad():\n",
    "#                     import pdb; pdb.set_trace()\n",
    "                    delta -= epsilon * delta.grad.detach().sign()\n",
    "#                     delta -= epsilon * delta.grad.detach()\n",
    "                    delta.clamp_(- max_epsilon, max_epsilon)\n",
    "\n",
    "                if (verbose and iteration % (np.ceil(max_iterations / 10)) == 0):\n",
    "                    print('    step: %d; loss: %.3f' %\n",
    "                          (iteration, loss.cpu().detach().numpy()))\n",
    "                \n",
    "                if ((iteration + 1) % check_adv_steps == 0 or\n",
    "                        iteration == max_iterations):\n",
    "                    with torch.no_grad():\n",
    "                        # check if x are adversarial. Only store adversarial examples\n",
    "                        # if they have a larger number of wrong neighbors than orevious\n",
    "                        is_adv, num_nn = self.check_adv(x, label)\n",
    "                        for j in range(batch_size):\n",
    "                            if is_adv[j] and num_nn[j] > best_num_nn[j]:\n",
    "                                x_adv[j] = x[j]\n",
    "                                best_num_nn[j] = num_nn[j]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                is_adv, _ = self.check_adv(x_adv, label)\n",
    "            if verbose:\n",
    "                print('number of successful adv: %d/%d' % (is_adv.sum(), batch_size))\n",
    "\n",
    "        return x_adv\n",
    "\n",
    "    def check_adv(self, x, label):\n",
    "        \"\"\"Check if label of <x> predicted by <dknn> matches with <label>\"\"\"\n",
    "        output = self.dknn.classify(x)\n",
    "        num_nn = output.max(1)\n",
    "        y_pred = output.argmax(1)\n",
    "        is_adv = (y_pred != label).astype(np.float32)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        return is_adv, num_nn\n",
    "\n",
    "    def loss_function(self, reps):\n",
    "        \"\"\"Returns the loss averaged over the batch (first dimension of x) and\n",
    "        L-2 norm squared of the perturbation\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = reps[self.layers[0]].size(0)\n",
    "        adv_loss = torch.zeros(\n",
    "            (batch_size, len(self.layers)), device=self.device)\n",
    "        # find squared L-2 distance between original samples and their\n",
    "        # adversarial examples at each layer\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            rep = reps[layer].view(batch_size, 1, -1)\n",
    "            dist = ((rep - self.guide_reps[layer])**2).sum(2)\n",
    "#             fx = self.thres - dist\n",
    "#             Fx = torch.max(torch.tensor(0., device=self.device),\n",
    "#                            self.coeff.to(self.device) * fx).sum(1)\n",
    "#             Fx = (- self.coeff.to(self.device) * dist).sum(1)\n",
    "            Fx = dist[:, 1]\n",
    "            adv_loss[:, l] = Fx\n",
    "\n",
    "        return adv_loss.mean()\n",
    "\n",
    "    def find_guide_samples(self, x, label, m=100, layer='relu1'):\n",
    "        \"\"\"Find k nearest neighbors to <x> that all have the same class but not\n",
    "        equal to <label>\n",
    "        \"\"\"\n",
    "        num_classes = self.dknn.num_classes\n",
    "        x_train = self.dknn.x_train\n",
    "        y_train = self.dknn.y_train\n",
    "        batch_size = x.size(0)\n",
    "        nn = torch.zeros((m, ) + x.size()).transpose(0, 1)\n",
    "        D, I = self.dknn.get_neighbors(\n",
    "            x, k=x_train.size(0), layers=[layer])[0]\n",
    "        y_pred = self.dknn.classify(x_train).argmax(1)\n",
    "        is_correct = y_pred == y_train.numpy()\n",
    "\n",
    "        for i, (d, ind) in enumerate(zip(D, I)):\n",
    "#             mean_dist = np.zeros((num_classes, ))\n",
    "#             for j in range(num_classes):\n",
    "#                 mean_dist[j] = np.mean(\n",
    "#                     d[np.where(y_train[ind] == j)[0]][:m // 2])\n",
    "#             mean_dist[label[i]] += INFTY\n",
    "#             nearest_label = mean_dist.argmin()\n",
    "#             nn_ind = np.where(y_train[ind] == nearest_label)[0][:m // 2]\n",
    "#             nn[i, m // 2:] = x_train[ind[nn_ind]]\n",
    "#             nn_ind = np.where(y_train[ind] == label[i])[0][:m // 2]\n",
    "#             nn[i, :m // 2] = x_train[ind[nn_ind]]\n",
    "            # find nearest sample that is correctly classified as j\n",
    "#             import pdb; pdb.set_trace()\n",
    "            is_not_label = y_train.numpy() != label[i]\n",
    "            idx = np.where(is_correct & is_not_label)[0]\n",
    "            nn_ind = (x_train - x[i].cpu())[idx].view(\n",
    "                idx.shape[0], -1).norm(2, 1).argmin()\n",
    "            nn[i, 1] = x_train[idx][nn_ind]\n",
    "\n",
    "        # initialize self.guide_reps if empty\n",
    "        if not self.guide_reps:\n",
    "            guide_rep = self.dknn.get_activations(\n",
    "                nn[0], requires_grad=False)\n",
    "            for l in self.layers:\n",
    "                # set a zero tensor before filling it\n",
    "                size = (batch_size, ) + guide_rep[l].view(m, -1).size()\n",
    "                self.guide_reps[l] = torch.zeros(size, device=self.device)\n",
    "\n",
    "        # fill self.guide_reps\n",
    "        for i in range(batch_size):\n",
    "            guide_rep = self.dknn.get_activations(\n",
    "                nn[i], requires_grad=False)\n",
    "            self.guide_reps[layer][i] = guide_rep[layer].view(\n",
    "                m, -1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 45.419\n",
      "    step: 100; loss: 34.209\n",
      "    step: 200; loss: 25.145\n",
      "    step: 300; loss: 17.951\n",
      "    step: 400; loss: 17.896\n",
      "    step: 500; loss: 17.885\n",
      "    step: 600; loss: 17.942\n",
      "    step: 700; loss: 17.890\n",
      "    step: 800; loss: 17.899\n",
      "    step: 900; loss: 17.923\n",
      "number of successful adv: 54/200\n"
     ]
    }
   ],
   "source": [
    "attack = DKNN_PGD(knn)\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            x[begin:end], y[begin:end],\n",
    "            guide_layer=layers[0], m=2, epsilon=0.001,\n",
    "            max_epsilon=0.3, max_iterations=1000, num_restart=10,\n",
    "            rand_start=False, thres_steps=1000, check_adv_steps=1000,\n",
    "            verbose=True)\n",
    "    return x_adv\n",
    "\n",
    "num = 200\n",
    "x_adv = attack_batch(x_test_sub[:num].cuda(), y_test_sub[:num], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
