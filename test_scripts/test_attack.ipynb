{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import foolbox\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from lib.dataset_utils import *\n",
    "from lib.mnist_model import *\n",
    "from lib.adv_model import *\n",
    "from lib.dknn_attack_v2 import DKNNAttackV2\n",
    "from lib.cwl2_attack import CWL2Attack\n",
    "from lib.dknn import DKNNL2\n",
    "from lib.utils import *\n",
    "from lib.lip_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsamples dataset and reduces to two classes (1 and 7)\n",
    "# Code adapted from https://github.com/yangarbiter/adversarial-nonparametrics\n",
    "\n",
    "num_samples = 2200\n",
    "num_val = 300 # can pick anything > 0, does not really get used here\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X, y), (_, _) = mnist.load_data()\n",
    "np.random.seed(2019)\n",
    "idx1 = np.random.choice(\n",
    "    np.where(y == 1)[0], num_samples // 2, replace=False)\n",
    "idx2 = np.random.choice(\n",
    "    np.where(y == 7)[0], num_samples // 2, replace=False)\n",
    "y[idx1] = 0\n",
    "y[idx2] = 1\n",
    "X = np.vstack((X[idx1], X[idx2])).astype(np.float32) / 255.\n",
    "y = np.concatenate((y[idx1], y[idx2]))\n",
    "\n",
    "idxs = np.arange(num_samples)\n",
    "np.random.shuffle(idxs)\n",
    "x_train = torch.tensor(X[idxs[:-200]])\n",
    "x_test = torch.tensor(X[idxs[-200:]])\n",
    "y_train = torch.tensor(y[idxs[:-200]])\n",
    "y_test = torch.tensor(y[idxs[-200:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_knn = KNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['identity']\n",
    "knn = DKNNL2(net_knn, x_train, y_train, \n",
    "             x_test, y_test, layers, \n",
    "             k=5, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = knn.classify(x_test_sub)\n",
    "    ind = np.where(y_pred.argmax(1) == y_test_sub.numpy())[0]\n",
    "    print((y_pred.argmax(1) == y_test_sub.numpy()).sum() / y_test_sub.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_batch(attack, x, y, init_mode, init_mode_k, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            x[begin:end], y[begin:end], 2, guide_layer='identity', m=6,\n",
    "            init_mode=init_mode, init_mode_k=init_mode_k,\n",
    "            binary_search_steps=10, max_iterations=1000, learning_rate=1e-2,\n",
    "            initial_const=1e-1, max_linf=None, random_start=False,\n",
    "            thres_steps=20, check_adv_steps=20, verbose=False)\n",
    "    return x_adv\n",
    "\n",
    "num = 200\n",
    "\n",
    "def full_eval(dknn):\n",
    "    with torch.no_grad():\n",
    "        y_pred = dknn.classify(x_test)\n",
    "        ind = np.where(y_pred.argmax(1) == y_test.numpy())[0]\n",
    "    print((y_pred.argmax(1) == y_test.numpy()).sum() / y_test.size(0))\n",
    "    \n",
    "    dist_all = np.zeros(num) + 1e9\n",
    "    attack = DKNNAttackV2(dknn)\n",
    "    \n",
    "    x_adv = attack_batch(\n",
    "        attack, x_test[:num].cuda(), y_test[:num], 1, 1, 200)\n",
    "    with torch.no_grad():\n",
    "        y_pred = dknn.classify(x_adv)\n",
    "        ind_adv = y_pred.argmax(1) != y_test[:num].numpy()\n",
    "        dist = (x_adv.cpu() - x_test[:num]).view(\n",
    "            num, -1).norm(2, 1).numpy()\n",
    "    for i in range(num):\n",
    "        if ind_adv[i] and (dist[i] < dist_all[i]):\n",
    "            dist_all[i] = dist[i]\n",
    "            \n",
    "    for k in range(1, 4):\n",
    "        x_adv = attack_batch(\n",
    "            attack, x_test[:num].cuda(), y_test[:num], 2, k, 200)\n",
    "        with torch.no_grad():\n",
    "            y_pred = dknn.classify(x_adv)\n",
    "            ind_adv = y_pred.argmax(1) != y_test[:num].numpy()\n",
    "            dist = (x_adv.cpu() - x_test[:num]).view(\n",
    "                num, -1).norm(2, 1).numpy()\n",
    "        for i in range(num):\n",
    "            if ind_adv[i] and (dist[i] < dist_all[i]):\n",
    "                dist_all[i] = dist[i]\n",
    "                \n",
    "    adv_acc = (dist_all == 1e9).mean()\n",
    "    print('adv accuracy: %.4f, mean dist: %.4f' % (\n",
    "        adv_acc, dist_all[dist_all < 1e9].mean()))\n",
    "    return dist_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "adv accuracy: 0.0000, mean dist: 3.0913\n",
      "284.73527669906616\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dist = full_eval(knn)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9384045471162654"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 0.046; l2dist: 0.000\n",
      "    step: 100; loss: 0.046; l2dist: 0.001\n",
      "binary step: 0; num successful adv: 2/200\n",
      "binary step: 0; num successful adv so far: 2/200\n",
      "    step: 0; loss: 0.462; l2dist: 0.000\n",
      "    step: 100; loss: 0.461; l2dist: 0.012\n",
      "    step: 200; loss: 0.461; l2dist: 0.017\n",
      "binary step: 1; num successful adv: 2/200\n",
      "binary step: 1; num successful adv so far: 2/200\n",
      "    step: 0; loss: 4.616; l2dist: 0.000\n",
      "    step: 100; loss: 4.581; l2dist: 0.132\n",
      "    step: 200; loss: 4.581; l2dist: 0.134\n",
      "binary step: 2; num successful adv: 3/200\n",
      "binary step: 2; num successful adv so far: 3/200\n",
      "    step: 0; loss: 46.030; l2dist: 0.000\n",
      "    step: 100; loss: 41.627; l2dist: 1.587\n",
      "    step: 200; loss: 41.554; l2dist: 1.625\n",
      "    step: 300; loss: 39.644; l2dist: 1.736\n",
      "    step: 400; loss: 38.801; l2dist: 1.949\n",
      "    step: 500; loss: 38.461; l2dist: 2.077\n",
      "    step: 600; loss: 38.309; l2dist: 2.132\n",
      "    step: 700; loss: 38.214; l2dist: 2.175\n",
      "    step: 800; loss: 38.140; l2dist: 2.213\n",
      "    step: 900; loss: 38.072; l2dist: 2.249\n",
      "binary step: 3; num successful adv: 60/200\n",
      "binary step: 3; num successful adv so far: 61/200\n",
      "    step: 0; loss: 336.516; l2dist: 0.000\n",
      "    step: 100; loss: 252.867; l2dist: 3.349\n",
      "    step: 200; loss: 210.854; l2dist: 3.692\n",
      "    step: 300; loss: 179.744; l2dist: 4.235\n",
      "    step: 400; loss: 177.945; l2dist: 4.344\n",
      "    step: 500; loss: 176.847; l2dist: 4.398\n",
      "    step: 600; loss: 175.732; l2dist: 4.429\n",
      "    step: 700; loss: 175.076; l2dist: 4.449\n",
      "    step: 800; loss: 175.051; l2dist: 4.464\n",
      "    step: 900; loss: 175.043; l2dist: 4.480\n",
      "binary step: 4; num successful adv: 155/200\n",
      "binary step: 4; num successful adv so far: 195/200\n",
      "    step: 0; loss: 301.908; l2dist: 0.000\n",
      "    step: 100; loss: 254.395; l2dist: 3.191\n",
      "    step: 200; loss: 224.371; l2dist: 3.407\n",
      "    step: 300; loss: 200.231; l2dist: 3.979\n",
      "    step: 400; loss: 176.433; l2dist: 4.128\n",
      "    step: 500; loss: 174.701; l2dist: 4.176\n",
      "    step: 600; loss: 174.420; l2dist: 4.216\n",
      "    step: 700; loss: 173.991; l2dist: 4.238\n",
      "    step: 800; loss: 173.574; l2dist: 4.264\n",
      "    step: 900; loss: 173.381; l2dist: 4.319\n",
      "binary step: 5; num successful adv: 133/200\n",
      "binary step: 5; num successful adv so far: 200/200\n",
      "    step: 0; loss: 207.983; l2dist: 0.000\n",
      "    step: 100; loss: 179.526; l2dist: 2.889\n",
      "    step: 200; loss: 164.767; l2dist: 3.037\n",
      "    step: 300; loss: 148.913; l2dist: 3.546\n",
      "    step: 400; loss: 147.091; l2dist: 3.748\n",
      "    step: 500; loss: 144.967; l2dist: 3.828\n",
      "    step: 600; loss: 134.146; l2dist: 3.870\n",
      "    step: 700; loss: 133.170; l2dist: 3.906\n",
      "    step: 800; loss: 133.126; l2dist: 3.914\n",
      "    step: 900; loss: 133.108; l2dist: 3.929\n",
      "binary step: 6; num successful adv: 130/200\n",
      "binary step: 6; num successful adv so far: 200/200\n",
      "    step: 0; loss: 166.207; l2dist: 0.000\n",
      "    step: 100; loss: 142.517; l2dist: 2.673\n",
      "    step: 200; loss: 133.223; l2dist: 2.787\n",
      "    step: 300; loss: 120.233; l2dist: 3.232\n",
      "    step: 400; loss: 118.809; l2dist: 3.412\n",
      "    step: 500; loss: 118.414; l2dist: 3.478\n",
      "    step: 600; loss: 118.180; l2dist: 3.503\n",
      "    step: 700; loss: 118.051; l2dist: 3.570\n",
      "    step: 800; loss: 112.343; l2dist: 3.683\n",
      "    step: 900; loss: 109.950; l2dist: 3.781\n",
      "binary step: 7; num successful adv: 116/200\n",
      "binary step: 7; num successful adv so far: 200/200\n",
      "    step: 0; loss: 147.973; l2dist: 0.000\n",
      "    step: 100; loss: 129.738; l2dist: 2.358\n",
      "    step: 200; loss: 123.120; l2dist: 2.484\n",
      "    step: 300; loss: 113.182; l2dist: 2.884\n",
      "    step: 400; loss: 111.643; l2dist: 3.083\n",
      "    step: 500; loss: 111.335; l2dist: 3.156\n",
      "    step: 600; loss: 111.079; l2dist: 3.231\n",
      "    step: 700; loss: 110.795; l2dist: 3.284\n",
      "    step: 800; loss: 110.444; l2dist: 3.403\n",
      "    step: 900; loss: 109.614; l2dist: 3.549\n",
      "binary step: 8; num successful adv: 110/200\n",
      "binary step: 8; num successful adv so far: 200/200\n",
      "    step: 0; loss: 142.295; l2dist: 0.000\n",
      "    step: 100; loss: 124.781; l2dist: 2.235\n",
      "    step: 200; loss: 119.217; l2dist: 2.329\n",
      "    step: 300; loss: 109.977; l2dist: 2.719\n",
      "    step: 400; loss: 108.601; l2dist: 2.909\n",
      "    step: 500; loss: 108.324; l2dist: 2.952\n",
      "    step: 600; loss: 108.188; l2dist: 2.997\n",
      "    step: 700; loss: 107.893; l2dist: 3.139\n",
      "    step: 800; loss: 107.549; l2dist: 3.230\n",
      "    step: 900; loss: 105.959; l2dist: 3.461\n",
      "binary step: 9; num successful adv: 105/200\n",
      "binary step: 9; num successful adv so far: 200/200\n",
      "    step: 0; loss: 138.450; l2dist: 0.000\n",
      "    step: 100; loss: 123.450; l2dist: 2.017\n",
      "    step: 200; loss: 117.407; l2dist: 2.161\n",
      "    step: 300; loss: 109.305; l2dist: 2.552\n",
      "    step: 400; loss: 107.517; l2dist: 2.751\n",
      "    step: 500; loss: 107.218; l2dist: 2.797\n",
      "    step: 600; loss: 107.068; l2dist: 2.857\n",
      "    step: 700; loss: 106.804; l2dist: 2.973\n",
      "    step: 800; loss: 106.410; l2dist: 3.084\n",
      "    step: 900; loss: 105.563; l2dist: 3.362\n",
      "binary step: 10; num successful adv: 98/200\n",
      "binary step: 10; num successful adv so far: 200/200\n",
      "    step: 0; loss: 137.636; l2dist: 0.000\n",
      "    step: 100; loss: 122.811; l2dist: 1.995\n",
      "    step: 200; loss: 118.139; l2dist: 2.089\n",
      "    step: 300; loss: 110.321; l2dist: 2.501\n",
      "    step: 400; loss: 108.968; l2dist: 2.700\n",
      "    step: 500; loss: 108.596; l2dist: 2.753\n",
      "    step: 600; loss: 108.439; l2dist: 2.807\n",
      "    step: 700; loss: 108.267; l2dist: 2.894\n",
      "    step: 800; loss: 107.953; l2dist: 3.030\n",
      "    step: 900; loss: 106.821; l2dist: 3.327\n",
      "binary step: 11; num successful adv: 105/200\n",
      "binary step: 11; num successful adv so far: 200/200\n",
      "    step: 0; loss: 137.157; l2dist: 0.000\n",
      "    step: 100; loss: 122.469; l2dist: 1.983\n",
      "    step: 200; loss: 117.892; l2dist: 2.085\n",
      "    step: 300; loss: 110.192; l2dist: 2.475\n",
      "    step: 400; loss: 108.825; l2dist: 2.677\n",
      "    step: 500; loss: 108.518; l2dist: 2.735\n",
      "    step: 600; loss: 108.361; l2dist: 2.780\n",
      "    step: 700; loss: 108.243; l2dist: 2.836\n",
      "    step: 800; loss: 107.841; l2dist: 2.967\n",
      "    step: 900; loss: 106.836; l2dist: 3.274\n",
      "binary step: 12; num successful adv: 108/200\n",
      "binary step: 12; num successful adv so far: 200/200\n",
      "    step: 0; loss: 136.890; l2dist: 0.000\n",
      "    step: 100; loss: 122.249; l2dist: 1.972\n",
      "    step: 200; loss: 117.749; l2dist: 2.064\n",
      "    step: 300; loss: 110.078; l2dist: 2.454\n",
      "    step: 400; loss: 108.745; l2dist: 2.655\n",
      "    step: 500; loss: 108.436; l2dist: 2.710\n",
      "    step: 600; loss: 108.241; l2dist: 2.779\n",
      "    step: 700; loss: 108.111; l2dist: 2.844\n",
      "    step: 800; loss: 107.935; l2dist: 2.938\n",
      "    step: 900; loss: 106.770; l2dist: 3.275\n",
      "binary step: 13; num successful adv: 100/200\n",
      "binary step: 13; num successful adv so far: 200/200\n",
      "    step: 0; loss: 137.332; l2dist: 0.000\n",
      "    step: 100; loss: 122.543; l2dist: 2.007\n",
      "    step: 200; loss: 117.985; l2dist: 2.110\n",
      "    step: 300; loss: 110.265; l2dist: 2.500\n",
      "    step: 400; loss: 108.879; l2dist: 2.701\n",
      "    step: 500; loss: 108.571; l2dist: 2.755\n",
      "    step: 600; loss: 108.415; l2dist: 2.804\n",
      "    step: 700; loss: 108.294; l2dist: 2.867\n",
      "    step: 800; loss: 108.108; l2dist: 2.968\n",
      "    step: 900; loss: 106.922; l2dist: 3.307\n",
      "binary step: 14; num successful adv: 200/200\n",
      "binary step: 14; num successful adv so far: 200/200\n"
     ]
    }
   ],
   "source": [
    "# Attack for L2 DkNN\n",
    "\n",
    "attack = DKNNL2Attack()\n",
    "# attack = DKNNLinfAttack()\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            knn, x[begin:end], y[begin:end],\n",
    "            guide_layer=layers[0], m=5, binary_search_steps=15,\n",
    "            max_iterations=1500, learning_rate=1e-2, guide_mode=2,\n",
    "            initial_const=1e-2, abort_early=True, random_start=False)\n",
    "    return x_adv\n",
    "\n",
    "num = 200\n",
    "x_adv = attack_batch(x_test_sub[:num].cuda(), y_test_sub[:num], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.9757, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = knn.classify(x_adv)\n",
    "    ind = np.where(y_pred.argmax(1) == y_test_sub[:num].numpy())[0]\n",
    "    print(len(ind) / x_adv.size(0))\n",
    "(x_test_sub - x_adv.cpu()).view(num, -1).norm(2, 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 9.961; l2dist: 0.000\n",
      "    step: 100; loss: 8.053; l2dist: 1.232\n",
      "    step: 200; loss: 8.048; l2dist: 1.235\n",
      "    step: 300; loss: 8.046; l2dist: 1.234\n",
      "    step: 400; loss: 7.850; l2dist: 1.271\n",
      "    step: 500; loss: 7.552; l2dist: 1.395\n",
      "    step: 600; loss: 7.440; l2dist: 1.444\n",
      "    step: 700; loss: 7.380; l2dist: 1.463\n",
      "    step: 800; loss: 7.336; l2dist: 1.482\n",
      "    step: 900; loss: 7.312; l2dist: 1.492\n",
      "binary step: 0; num successful adv: 5/200\n",
      "binary step: 0; num successful adv so far: 6/200\n",
      "    step: 0; loss: 99.356; l2dist: 0.000\n",
      "    step: 100; loss: 26.939; l2dist: 3.411\n",
      "    step: 200; loss: 21.338; l2dist: 3.496\n",
      "    step: 300; loss: 13.465; l2dist: 3.467\n",
      "    step: 400; loss: 12.395; l2dist: 3.356\n",
      "    step: 500; loss: 11.989; l2dist: 3.315\n",
      "    step: 600; loss: 11.904; l2dist: 3.303\n",
      "    step: 700; loss: 11.893; l2dist: 3.301\n",
      "    step: 800; loss: 11.940; l2dist: 3.310\n",
      "    step: 900; loss: 12.002; l2dist: 3.319\n",
      "binary step: 1; num successful adv: 147/200\n",
      "binary step: 1; num successful adv so far: 195/200\n",
      "    step: 0; loss: 299.503; l2dist: 0.000\n",
      "    step: 100; loss: 59.211; l2dist: 3.307\n",
      "    step: 200; loss: 20.069; l2dist: 3.487\n",
      "    step: 300; loss: 14.206; l2dist: 3.479\n",
      "    step: 400; loss: 13.547; l2dist: 3.447\n",
      "    step: 500; loss: 13.622; l2dist: 3.462\n",
      "    step: 600; loss: 13.885; l2dist: 3.501\n",
      "    step: 700; loss: 14.180; l2dist: 3.537\n",
      "    step: 800; loss: 14.505; l2dist: 3.571\n",
      "    step: 900; loss: 14.779; l2dist: 3.606\n",
      "binary step: 2; num successful adv: 124/200\n",
      "binary step: 2; num successful adv so far: 200/200\n",
      "    step: 0; loss: 183.355; l2dist: 0.000\n",
      "    step: 100; loss: 41.592; l2dist: 3.238\n",
      "    step: 200; loss: 19.727; l2dist: 3.413\n",
      "    step: 300; loss: 13.644; l2dist: 3.385\n",
      "    step: 400; loss: 12.533; l2dist: 3.323\n",
      "    step: 500; loss: 12.274; l2dist: 3.297\n",
      "    step: 600; loss: 12.233; l2dist: 3.295\n",
      "    step: 700; loss: 12.274; l2dist: 3.305\n",
      "    step: 800; loss: 12.399; l2dist: 3.313\n",
      "    step: 900; loss: 12.521; l2dist: 3.335\n",
      "binary step: 3; num successful adv: 114/200\n",
      "binary step: 3; num successful adv so far: 200/200\n",
      "    step: 0; loss: 127.166; l2dist: 0.000\n",
      "    step: 100; loss: 32.856; l2dist: 3.255\n",
      "    step: 200; loss: 20.034; l2dist: 3.401\n",
      "    step: 300; loss: 13.233; l2dist: 3.363\n",
      "    step: 400; loss: 12.137; l2dist: 3.280\n",
      "    step: 500; loss: 11.890; l2dist: 3.251\n",
      "    step: 600; loss: 11.824; l2dist: 3.249\n",
      "    step: 700; loss: 11.877; l2dist: 3.259\n",
      "    step: 800; loss: 12.014; l2dist: 3.275\n",
      "    step: 900; loss: 12.169; l2dist: 3.299\n",
      "binary step: 4; num successful adv: 115/200\n",
      "binary step: 4; num successful adv so far: 200/200\n",
      "    step: 0; loss: 100.339; l2dist: 0.000\n",
      "    step: 100; loss: 29.939; l2dist: 3.245\n",
      "    step: 200; loss: 20.120; l2dist: 3.378\n",
      "    step: 300; loss: 13.067; l2dist: 3.338\n",
      "    step: 400; loss: 11.913; l2dist: 3.255\n",
      "    step: 500; loss: 11.639; l2dist: 3.224\n",
      "    step: 600; loss: 11.517; l2dist: 3.210\n",
      "    step: 700; loss: 11.441; l2dist: 3.205\n",
      "    step: 800; loss: 11.490; l2dist: 3.210\n",
      "    step: 900; loss: 11.528; l2dist: 3.214\n",
      "binary step: 5; num successful adv: 110/200\n",
      "binary step: 5; num successful adv so far: 200/200\n",
      "    step: 0; loss: 86.609; l2dist: 0.000\n",
      "    step: 100; loss: 27.930; l2dist: 3.229\n",
      "    step: 200; loss: 20.346; l2dist: 3.360\n",
      "    step: 300; loss: 12.915; l2dist: 3.332\n",
      "    step: 400; loss: 11.814; l2dist: 3.239\n",
      "    step: 500; loss: 11.508; l2dist: 3.205\n",
      "    step: 600; loss: 11.375; l2dist: 3.191\n",
      "    step: 700; loss: 11.373; l2dist: 3.190\n",
      "    step: 800; loss: 11.380; l2dist: 3.191\n",
      "    step: 900; loss: 11.424; l2dist: 3.198\n",
      "binary step: 6; num successful adv: 116/200\n",
      "binary step: 6; num successful adv so far: 200/200\n",
      "    step: 0; loss: 80.661; l2dist: 0.000\n",
      "    step: 100; loss: 27.214; l2dist: 3.216\n",
      "    step: 200; loss: 20.304; l2dist: 3.352\n",
      "    step: 300; loss: 12.888; l2dist: 3.322\n",
      "    step: 400; loss: 11.799; l2dist: 3.236\n",
      "    step: 500; loss: 11.482; l2dist: 3.200\n",
      "    step: 600; loss: 11.391; l2dist: 3.187\n",
      "    step: 700; loss: 11.365; l2dist: 3.186\n",
      "    step: 800; loss: 11.331; l2dist: 3.182\n",
      "    step: 900; loss: 11.364; l2dist: 3.185\n",
      "binary step: 7; num successful adv: 95/200\n",
      "binary step: 7; num successful adv so far: 200/200\n",
      "    step: 0; loss: 80.419; l2dist: 0.000\n",
      "    step: 100; loss: 27.187; l2dist: 3.219\n",
      "    step: 200; loss: 20.438; l2dist: 3.355\n",
      "    step: 300; loss: 12.847; l2dist: 3.321\n",
      "    step: 400; loss: 11.728; l2dist: 3.227\n",
      "    step: 500; loss: 11.427; l2dist: 3.192\n",
      "    step: 600; loss: 11.343; l2dist: 3.183\n",
      "    step: 700; loss: 11.298; l2dist: 3.182\n",
      "    step: 800; loss: 11.290; l2dist: 3.183\n",
      "    step: 900; loss: 11.306; l2dist: 3.182\n",
      "binary step: 8; num successful adv: 103/200\n",
      "binary step: 8; num successful adv so far: 200/200\n",
      "    step: 0; loss: 81.729; l2dist: 0.000\n",
      "    step: 100; loss: 27.409; l2dist: 3.226\n",
      "    step: 200; loss: 20.448; l2dist: 3.359\n",
      "    step: 300; loss: 12.828; l2dist: 3.328\n",
      "    step: 400; loss: 11.731; l2dist: 3.234\n",
      "    step: 500; loss: 11.411; l2dist: 3.196\n",
      "    step: 600; loss: 11.337; l2dist: 3.188\n",
      "    step: 700; loss: 11.295; l2dist: 3.186\n",
      "    step: 800; loss: 11.292; l2dist: 3.183\n",
      "    step: 900; loss: 11.308; l2dist: 3.184\n",
      "binary step: 9; num successful adv: 200/200\n",
      "binary step: 9; num successful adv so far: 200/200\n"
     ]
    }
   ],
   "source": [
    "# Attack for L2 DkNN\n",
    "\n",
    "# attack = DKNNL2Attack()\n",
    "# attack = DKNNLinfAttack()\n",
    "from lib.dknn_attack_exp import DKNNExpAttack\n",
    "attack = DKNNExpAttack(knn)\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            x[begin:end], y[begin:end],\n",
    "            guide_layer=layers[0], m=6, binary_search_steps=10,\n",
    "            max_iterations=1000, learning_rate=1e-2,\n",
    "            initial_const=1e-1, random_start=False,\n",
    "            thres_steps=10, check_adv_steps=50, verbose=True,\n",
    "            max_linf=None)\n",
    "    return x_adv\n",
    "\n",
    "x_adv = attack_batch(x_test_sub[:num].cuda(), y_test_sub[:num], 200)\n",
    "# x_adv = attack_batch(x_test_sub[num-2:num-1].cuda(), y_test_sub[num-2:num-1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 7.592; l2dist: 0.000\n",
      "    step: 150; loss: 5.997; l2dist: 1.029\n",
      "    step: 300; loss: 5.585; l2dist: 1.400\n",
      "    step: 450; loss: 5.588; l2dist: 1.394\n",
      "    step: 600; loss: 5.583; l2dist: 1.405\n",
      "    step: 750; loss: 5.584; l2dist: 1.405\n",
      "    step: 900; loss: 5.585; l2dist: 1.404\n",
      "    step: 1050; loss: 5.587; l2dist: 1.395\n",
      "    step: 1200; loss: 5.584; l2dist: 1.404\n",
      "    step: 1350; loss: 5.586; l2dist: 1.405\n",
      "tensor(0., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 0/1\n",
      "    step: 0; loss: 75.919; l2dist: 0.000\n",
      "    step: 150; loss: 17.589; l2dist: 3.628\n",
      "    step: 300; loss: 15.533; l2dist: 2.641\n",
      "    step: 450; loss: 8.157; l2dist: 2.856\n",
      "    step: 600; loss: 7.948; l2dist: 2.798\n",
      "    step: 750; loss: 8.104; l2dist: 2.847\n",
      "    step: 900; loss: 7.841; l2dist: 2.800\n",
      "    step: 1050; loss: 7.986; l2dist: 2.826\n",
      "    step: 1200; loss: 7.870; l2dist: 2.805\n",
      "    step: 1350; loss: 7.879; l2dist: 2.807\n",
      "tensor(0., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 1/1\n",
      "    step: 0; loss: 759.191; l2dist: 0.000\n",
      "    step: 150; loss: 46.335; l2dist: 4.323\n",
      "    step: 300; loss: 16.761; l2dist: 4.094\n",
      "    step: 450; loss: 12.496; l2dist: 3.535\n",
      "    step: 600; loss: 9.702; l2dist: 3.115\n",
      "    step: 750; loss: 9.173; l2dist: 3.029\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-7099e7c389d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_adv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mx_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-7099e7c389d1>\u001b[0m in \u001b[0;36mattack_batch\u001b[0;34m(x, y, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minitial_const\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabort_early\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             thres=thres[i:i+1], a=0.1)\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_adv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-7fd4f24cb707>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, dknn, x_orig, label, guide_layer, m, binary_search_steps, max_iterations, learning_rate, initial_const, abort_early, max_linf, random_start, guide_mode, thres, a)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mreps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 loss, l2dist = self.loss_function(\n\u001b[0;32m--> 192\u001b[0;31m                     x, reps, guide_reps, coeff, dknn.layers, const, x_recon, device)\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-7fd4f24cb707>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(self, x, reps, guide_reps, coeff, layers, const, x_recon, device)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0madv_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# find L-2 norm squared of perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0ml2dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx_recon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;31m# total_loss is sum of squared perturbation norm and squared distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# of representations, multiplied by constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py36/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Attack for L2 DkNN\n",
    "\n",
    "attack = DKNNExpAttack()\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    for i in range(batch_size):\n",
    "        x_adv[i:i+1] = attack(\n",
    "            knn, x[i:i+1], y[i:i+1],\n",
    "            guide_layer=layers[0], m=4, binary_search_steps=15,\n",
    "            max_iterations=1500, learning_rate=1e-2, guide_mode=1,\n",
    "            initial_const=1e-1, abort_early=True, random_start=False,\n",
    "            thres=thres[i:i+1], a=0.1)\n",
    "    return x_adv\n",
    "\n",
    "x_adv = attack_batch(x_test_sub[:num].cuda(), y_test_sub[:num], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 3.188; l2dist: 0.000\n",
      "    step: 150; loss: 2.933; l2dist: 0.473\n",
      "    step: 300; loss: 2.930; l2dist: 0.474\n",
      "    step: 450; loss: 2.933; l2dist: 0.474\n",
      "    step: 600; loss: 2.931; l2dist: 0.473\n",
      "    step: 750; loss: 2.877; l2dist: 0.516\n",
      "    step: 900; loss: 2.839; l2dist: 0.557\n",
      "    step: 1050; loss: 2.831; l2dist: 0.569\n",
      "    step: 1200; loss: 2.823; l2dist: 0.573\n",
      "    step: 1350; loss: 2.821; l2dist: 0.578\n",
      "binary step: 0; num successful adv: 2/200\n",
      "binary step: 0; num successful adv so far: 3/200\n",
      "    step: 0; loss: 31.876; l2dist: 0.000\n",
      "    step: 150; loss: 15.769; l2dist: 2.569\n",
      "    step: 300; loss: 9.648; l2dist: 2.827\n",
      "    step: 450; loss: 9.046; l2dist: 2.802\n",
      "    step: 600; loss: 8.944; l2dist: 2.803\n",
      "    step: 750; loss: 8.984; l2dist: 2.806\n",
      "    step: 900; loss: 8.948; l2dist: 2.809\n",
      "    step: 1050; loss: 8.984; l2dist: 2.804\n",
      "    step: 1200; loss: 8.944; l2dist: 2.808\n",
      "    step: 1350; loss: 8.981; l2dist: 2.805\n",
      "binary step: 1; num successful adv: 101/200\n",
      "binary step: 1; num successful adv so far: 169/200\n",
      "    step: 0; loss: 186.758; l2dist: 0.000\n",
      "    step: 150; loss: 31.669; l2dist: 2.833\n",
      "    step: 300; loss: 12.121; l2dist: 2.837\n",
      "    step: 450; loss: 11.186; l2dist: 2.921\n",
      "    step: 600; loss: 11.087; l2dist: 2.917\n",
      "    step: 750; loss: 11.494; l2dist: 2.972\n",
      "    step: 900; loss: 11.490; l2dist: 2.972\n",
      "    step: 1050; loss: 11.763; l2dist: 3.005\n",
      "    step: 1200; loss: 11.626; l2dist: 2.990\n",
      "    step: 1350; loss: 11.836; l2dist: 3.016\n",
      "binary step: 2; num successful adv: 119/200\n",
      "binary step: 2; num successful adv so far: 200/200\n",
      "    step: 0; loss: 108.761; l2dist: 0.000\n",
      "    step: 150; loss: 25.319; l2dist: 2.914\n",
      "    step: 300; loss: 11.095; l2dist: 2.930\n",
      "    step: 450; loss: 10.242; l2dist: 2.920\n",
      "    step: 600; loss: 10.257; l2dist: 2.932\n",
      "    step: 750; loss: 10.468; l2dist: 2.959\n",
      "    step: 900; loss: 10.426; l2dist: 2.957\n",
      "    step: 1050; loss: 10.533; l2dist: 2.971\n",
      "    step: 1200; loss: 10.450; l2dist: 2.962\n",
      "    step: 1350; loss: 10.547; l2dist: 2.975\n",
      "binary step: 3; num successful adv: 117/200\n",
      "binary step: 3; num successful adv so far: 200/200\n",
      "    step: 0; loss: 69.603; l2dist: 0.000\n",
      "    step: 150; loss: 21.450; l2dist: 2.896\n",
      "    step: 300; loss: 10.403; l2dist: 2.919\n",
      "    step: 450; loss: 9.841; l2dist: 2.922\n",
      "    step: 600; loss: 9.749; l2dist: 2.917\n",
      "    step: 750; loss: 9.892; l2dist: 2.938\n",
      "    step: 900; loss: 9.802; l2dist: 2.929\n",
      "    step: 1050; loss: 9.897; l2dist: 2.942\n",
      "    step: 1200; loss: 9.805; l2dist: 2.931\n",
      "    step: 1350; loss: 9.901; l2dist: 2.941\n",
      "binary step: 4; num successful adv: 133/200\n",
      "binary step: 4; num successful adv so far: 200/200\n",
      "    step: 0; loss: 49.745; l2dist: 0.000\n",
      "    step: 150; loss: 18.902; l2dist: 2.811\n",
      "    step: 300; loss: 9.986; l2dist: 2.887\n",
      "    step: 450; loss: 9.512; l2dist: 2.881\n",
      "    step: 600; loss: 9.401; l2dist: 2.885\n",
      "    step: 750; loss: 9.487; l2dist: 2.886\n",
      "    step: 900; loss: 9.404; l2dist: 2.886\n",
      "    step: 1050; loss: 9.488; l2dist: 2.887\n",
      "    step: 1200; loss: 9.412; l2dist: 2.888\n",
      "    step: 1350; loss: 9.499; l2dist: 2.889\n",
      "binary step: 5; num successful adv: 146/200\n",
      "binary step: 5; num successful adv so far: 200/200\n",
      "    step: 0; loss: 40.556; l2dist: 0.000\n",
      "    step: 150; loss: 17.443; l2dist: 2.715\n",
      "    step: 300; loss: 9.734; l2dist: 2.856\n",
      "    step: 450; loss: 9.271; l2dist: 2.841\n",
      "    step: 600; loss: 9.164; l2dist: 2.847\n",
      "    step: 750; loss: 9.213; l2dist: 2.839\n",
      "    step: 900; loss: 9.149; l2dist: 2.845\n",
      "    step: 1050; loss: 9.211; l2dist: 2.841\n",
      "    step: 1200; loss: 9.155; l2dist: 2.845\n",
      "    step: 1350; loss: 9.208; l2dist: 2.840\n",
      "binary step: 6; num successful adv: 113/200\n",
      "binary step: 6; num successful adv so far: 200/200\n",
      "    step: 0; loss: 38.642; l2dist: 0.000\n",
      "    step: 150; loss: 16.920; l2dist: 2.668\n",
      "    step: 300; loss: 9.681; l2dist: 2.844\n",
      "    step: 450; loss: 9.136; l2dist: 2.821\n",
      "    step: 600; loss: 9.031; l2dist: 2.828\n",
      "    step: 750; loss: 9.085; l2dist: 2.822\n",
      "    step: 900; loss: 9.036; l2dist: 2.828\n",
      "    step: 1050; loss: 9.088; l2dist: 2.823\n",
      "    step: 1200; loss: 9.040; l2dist: 2.830\n",
      "    step: 1350; loss: 9.096; l2dist: 2.824\n",
      "binary step: 7; num successful adv: 108/200\n",
      "binary step: 7; num successful adv so far: 200/200\n",
      "    step: 0; loss: 37.817; l2dist: 0.000\n",
      "    step: 150; loss: 16.921; l2dist: 2.657\n",
      "    step: 300; loss: 9.670; l2dist: 2.842\n",
      "    step: 450; loss: 9.149; l2dist: 2.822\n",
      "    step: 600; loss: 9.053; l2dist: 2.833\n",
      "    step: 750; loss: 9.116; l2dist: 2.823\n",
      "    step: 900; loss: 9.063; l2dist: 2.832\n",
      "    step: 1050; loss: 9.117; l2dist: 2.825\n",
      "    step: 1200; loss: 9.061; l2dist: 2.834\n",
      "    step: 1350; loss: 9.121; l2dist: 2.823\n",
      "binary step: 8; num successful adv: 102/200\n",
      "binary step: 8; num successful adv so far: 200/200\n",
      "    step: 0; loss: 37.557; l2dist: 0.000\n",
      "    step: 150; loss: 16.820; l2dist: 2.653\n",
      "    step: 300; loss: 9.687; l2dist: 2.837\n",
      "    step: 450; loss: 9.140; l2dist: 2.821\n",
      "    step: 600; loss: 9.034; l2dist: 2.829\n",
      "    step: 750; loss: 9.085; l2dist: 2.818\n",
      "    step: 900; loss: 9.020; l2dist: 2.828\n",
      "    step: 1050; loss: 9.078; l2dist: 2.815\n",
      "    step: 1200; loss: 9.029; l2dist: 2.829\n",
      "    step: 1350; loss: 9.087; l2dist: 2.816\n",
      "binary step: 9; num successful adv: 118/200\n",
      "binary step: 9; num successful adv so far: 200/200\n",
      "    step: 0; loss: 37.355; l2dist: 0.000\n",
      "    step: 150; loss: 16.757; l2dist: 2.650\n",
      "    step: 300; loss: 9.661; l2dist: 2.838\n",
      "    step: 450; loss: 9.114; l2dist: 2.818\n",
      "    step: 600; loss: 9.024; l2dist: 2.826\n",
      "    step: 750; loss: 9.079; l2dist: 2.817\n",
      "    step: 900; loss: 9.024; l2dist: 2.829\n",
      "    step: 1050; loss: 9.073; l2dist: 2.817\n",
      "    step: 1200; loss: 9.026; l2dist: 2.829\n",
      "    step: 1350; loss: 9.063; l2dist: 2.816\n",
      "binary step: 10; num successful adv: 93/200\n",
      "binary step: 10; num successful adv so far: 200/200\n",
      "    step: 0; loss: 37.345; l2dist: 0.000\n",
      "    step: 150; loss: 16.788; l2dist: 2.650\n",
      "    step: 300; loss: 9.674; l2dist: 2.840\n",
      "    step: 450; loss: 9.119; l2dist: 2.821\n",
      "    step: 600; loss: 9.003; l2dist: 2.822\n",
      "    step: 750; loss: 9.042; l2dist: 2.815\n",
      "    step: 900; loss: 9.001; l2dist: 2.823\n",
      "    step: 1050; loss: 9.039; l2dist: 2.815\n",
      "    step: 1200; loss: 9.004; l2dist: 2.826\n",
      "    step: 1350; loss: 9.054; l2dist: 2.815\n",
      "binary step: 11; num successful adv: 105/200\n",
      "binary step: 11; num successful adv so far: 200/200\n",
      "    step: 0; loss: 37.296; l2dist: 0.000\n",
      "    step: 150; loss: 16.771; l2dist: 2.652\n",
      "    step: 300; loss: 9.651; l2dist: 2.835\n",
      "    step: 450; loss: 9.093; l2dist: 2.815\n",
      "    step: 600; loss: 8.983; l2dist: 2.822\n",
      "    step: 750; loss: 9.040; l2dist: 2.811\n",
      "    step: 900; loss: 8.993; l2dist: 2.824\n",
      "    step: 1050; loss: 9.061; l2dist: 2.815\n",
      "    step: 1200; loss: 8.998; l2dist: 2.825\n",
      "    step: 1350; loss: 9.043; l2dist: 2.811\n",
      "binary step: 12; num successful adv: 106/200\n",
      "binary step: 12; num successful adv so far: 200/200\n",
      "    step: 0; loss: 37.284; l2dist: 0.000\n",
      "    step: 150; loss: 16.769; l2dist: 2.651\n",
      "    step: 300; loss: 9.649; l2dist: 2.840\n",
      "    step: 450; loss: 9.117; l2dist: 2.815\n",
      "    step: 600; loss: 9.005; l2dist: 2.825\n",
      "    step: 750; loss: 9.072; l2dist: 2.815\n",
      "    step: 900; loss: 9.027; l2dist: 2.828\n",
      "    step: 1050; loss: 9.078; l2dist: 2.814\n",
      "    step: 1200; loss: 9.025; l2dist: 2.827\n",
      "    step: 1350; loss: 9.078; l2dist: 2.815\n",
      "binary step: 13; num successful adv: 104/200\n",
      "binary step: 13; num successful adv so far: 200/200\n",
      "    step: 0; loss: 37.323; l2dist: 0.000\n",
      "    step: 150; loss: 16.783; l2dist: 2.652\n",
      "    step: 300; loss: 9.661; l2dist: 2.841\n",
      "    step: 450; loss: 9.124; l2dist: 2.819\n",
      "    step: 600; loss: 9.011; l2dist: 2.828\n",
      "    step: 750; loss: 9.057; l2dist: 2.816\n",
      "    step: 900; loss: 9.031; l2dist: 2.832\n",
      "    step: 1050; loss: 9.069; l2dist: 2.816\n",
      "    step: 1200; loss: 9.022; l2dist: 2.830\n",
      "    step: 1350; loss: 9.074; l2dist: 2.818\n",
      "binary step: 14; num successful adv: 200/200\n",
      "binary step: 14; num successful adv so far: 200/200\n",
      "0.0\n",
      "    step: 0; loss: 47.471; l2dist: 6.650\n",
      "    step: 150; loss: 27.789; l2dist: 4.800\n",
      "    step: 300; loss: 6.396; l2dist: 2.099\n",
      "    step: 450; loss: 3.112; l2dist: 0.846\n",
      "    step: 600; loss: 2.925; l2dist: 0.655\n",
      "    step: 750; loss: 2.885; l2dist: 0.601\n",
      "    step: 900; loss: 2.866; l2dist: 0.589\n",
      "    step: 1050; loss: 2.860; l2dist: 0.588\n",
      "    step: 1200; loss: 2.848; l2dist: 0.590\n",
      "    step: 1350; loss: 2.845; l2dist: 0.592\n",
      "binary step: 0; num successful adv: 3/200\n",
      "binary step: 0; num successful adv so far: 131/200\n",
      "    step: 0; loss: 50.859; l2dist: 6.650\n",
      "    step: 150; loss: 33.617; l2dist: 5.349\n",
      "    step: 300; loss: 14.490; l2dist: 3.634\n",
      "    step: 450; loss: 12.703; l2dist: 3.390\n",
      "    step: 600; loss: 12.354; l2dist: 3.366\n",
      "    step: 750; loss: 12.284; l2dist: 3.321\n",
      "    step: 900; loss: 12.257; l2dist: 3.348\n",
      "    step: 1050; loss: 12.261; l2dist: 3.318\n",
      "    step: 1200; loss: 12.249; l2dist: 3.348\n",
      "    step: 1350; loss: 12.253; l2dist: 3.317\n",
      "binary step: 1; num successful adv: 136/200\n",
      "binary step: 1; num successful adv so far: 171/200\n",
      "    step: 0; loss: 72.330; l2dist: 6.650\n",
      "    step: 150; loss: 38.249; l2dist: 5.610\n",
      "    step: 300; loss: 16.981; l2dist: 3.748\n",
      "    step: 450; loss: 15.459; l2dist: 3.367\n",
      "    step: 600; loss: 15.355; l2dist: 3.343\n",
      "    step: 750; loss: 15.373; l2dist: 3.334\n",
      "    step: 900; loss: 15.326; l2dist: 3.328\n",
      "    step: 1050; loss: 15.363; l2dist: 3.329\n",
      "    step: 1200; loss: 15.321; l2dist: 3.328\n",
      "    step: 1350; loss: 15.352; l2dist: 3.328\n",
      "binary step: 2; num successful adv: 83/200\n",
      "binary step: 2; num successful adv so far: 200/200\n",
      "    step: 0; loss: 61.575; l2dist: 6.650\n",
      "    step: 150; loss: 37.321; l2dist: 5.602\n",
      "    step: 300; loss: 16.676; l2dist: 3.843\n",
      "    step: 450; loss: 15.124; l2dist: 3.549\n",
      "    step: 600; loss: 14.981; l2dist: 3.533\n",
      "    step: 750; loss: 14.981; l2dist: 3.514\n",
      "    step: 900; loss: 14.930; l2dist: 3.520\n",
      "    step: 1050; loss: 14.974; l2dist: 3.510\n",
      "    step: 1200; loss: 14.934; l2dist: 3.519\n",
      "    step: 1350; loss: 14.965; l2dist: 3.509\n",
      "binary step: 3; num successful adv: 110/200\n",
      "binary step: 3; num successful adv so far: 200/200\n",
      "    step: 0; loss: 56.131; l2dist: 6.650\n",
      "    step: 150; loss: 36.006; l2dist: 5.532\n",
      "    step: 300; loss: 15.760; l2dist: 3.779\n",
      "    step: 450; loss: 13.994; l2dist: 3.479\n",
      "    step: 600; loss: 13.771; l2dist: 3.456\n",
      "    step: 750; loss: 13.748; l2dist: 3.427\n",
      "    step: 900; loss: 13.697; l2dist: 3.440\n",
      "    step: 1050; loss: 13.720; l2dist: 3.421\n",
      "    step: 1200; loss: 13.680; l2dist: 3.438\n",
      "    step: 1350; loss: 13.710; l2dist: 3.418\n",
      "binary step: 4; num successful adv: 126/200\n",
      "binary step: 4; num successful adv so far: 200/200\n",
      "    step: 0; loss: 53.386; l2dist: 6.650\n",
      "    step: 150; loss: 34.776; l2dist: 5.444\n",
      "    step: 300; loss: 14.591; l2dist: 3.661\n",
      "    step: 450; loss: 12.651; l2dist: 3.341\n",
      "    step: 600; loss: 12.362; l2dist: 3.314\n",
      "    step: 750; loss: 12.313; l2dist: 3.277\n",
      "    step: 900; loss: 12.258; l2dist: 3.294\n",
      "    step: 1050; loss: 12.278; l2dist: 3.270\n",
      "    step: 1200; loss: 12.255; l2dist: 3.291\n",
      "    step: 1350; loss: 12.265; l2dist: 3.270\n",
      "binary step: 5; num successful adv: 127/200\n",
      "binary step: 5; num successful adv so far: 200/200\n",
      "    step: 0; loss: 52.084; l2dist: 6.650\n",
      "    step: 150; loss: 33.944; l2dist: 5.376\n",
      "    step: 300; loss: 13.784; l2dist: 3.571\n",
      "    step: 450; loss: 11.620; l2dist: 3.218\n",
      "    step: 600; loss: 11.317; l2dist: 3.189\n",
      "    step: 750; loss: 11.232; l2dist: 3.149\n",
      "    step: 900; loss: 11.170; l2dist: 3.168\n",
      "    step: 1050; loss: 11.167; l2dist: 3.142\n",
      "    step: 1200; loss: 11.153; l2dist: 3.164\n",
      "    step: 1350; loss: 11.166; l2dist: 3.139\n",
      "binary step: 6; num successful adv: 125/200\n",
      "binary step: 6; num successful adv so far: 200/200\n",
      "    step: 0; loss: 51.576; l2dist: 6.650\n",
      "    step: 150; loss: 33.544; l2dist: 5.340\n",
      "    step: 300; loss: 13.308; l2dist: 3.507\n",
      "    step: 450; loss: 11.115; l2dist: 3.157\n",
      "    step: 600; loss: 10.889; l2dist: 3.136\n",
      "    step: 750; loss: 10.805; l2dist: 3.095\n",
      "    step: 900; loss: 10.745; l2dist: 3.108\n",
      "    step: 1050; loss: 10.747; l2dist: 3.086\n",
      "    step: 1200; loss: 10.718; l2dist: 3.106\n",
      "    step: 1350; loss: 10.730; l2dist: 3.083\n",
      "binary step: 7; num successful adv: 103/200\n",
      "binary step: 7; num successful adv so far: 200/200\n",
      "    step: 0; loss: 51.421; l2dist: 6.650\n",
      "    step: 150; loss: 33.432; l2dist: 5.329\n",
      "    step: 300; loss: 13.154; l2dist: 3.487\n",
      "    step: 450; loss: 10.882; l2dist: 3.123\n",
      "    step: 600; loss: 10.640; l2dist: 3.104\n",
      "    step: 750; loss: 10.583; l2dist: 3.070\n",
      "    step: 900; loss: 10.543; l2dist: 3.085\n",
      "    step: 1050; loss: 10.549; l2dist: 3.065\n",
      "    step: 1200; loss: 10.533; l2dist: 3.084\n",
      "    step: 1350; loss: 10.543; l2dist: 3.061\n",
      "binary step: 8; num successful adv: 113/200\n",
      "binary step: 8; num successful adv so far: 200/200\n",
      "    step: 0; loss: 51.391; l2dist: 6.650\n",
      "    step: 150; loss: 33.391; l2dist: 5.326\n",
      "    step: 300; loss: 13.090; l2dist: 3.481\n",
      "    step: 450; loss: 10.854; l2dist: 3.123\n",
      "    step: 600; loss: 10.593; l2dist: 3.099\n",
      "    step: 750; loss: 10.504; l2dist: 3.063\n",
      "    step: 900; loss: 10.439; l2dist: 3.075\n",
      "    step: 1050; loss: 10.450; l2dist: 3.052\n",
      "    step: 1200; loss: 10.412; l2dist: 3.067\n",
      "    step: 1350; loss: 10.429; l2dist: 3.049\n",
      "binary step: 9; num successful adv: 106/200\n",
      "binary step: 9; num successful adv so far: 200/200\n",
      "    step: 0; loss: 51.364; l2dist: 6.650\n",
      "    step: 150; loss: 33.384; l2dist: 5.324\n",
      "    step: 300; loss: 13.058; l2dist: 3.477\n",
      "    step: 450; loss: 10.799; l2dist: 3.118\n",
      "    step: 600; loss: 10.516; l2dist: 3.083\n",
      "    step: 750; loss: 10.405; l2dist: 3.046\n",
      "    step: 900; loss: 10.329; l2dist: 3.058\n",
      "    step: 1050; loss: 10.313; l2dist: 3.034\n",
      "    step: 1200; loss: 10.274; l2dist: 3.048\n",
      "    step: 1350; loss: 10.299; l2dist: 3.031\n",
      "binary step: 10; num successful adv: 105/200\n",
      "binary step: 10; num successful adv so far: 200/200\n",
      "    step: 0; loss: 51.366; l2dist: 6.650\n",
      "    step: 150; loss: 33.385; l2dist: 5.324\n",
      "    step: 300; loss: 13.043; l2dist: 3.475\n",
      "    step: 450; loss: 10.791; l2dist: 3.118\n",
      "    step: 600; loss: 10.465; l2dist: 3.080\n",
      "    step: 750; loss: 10.378; l2dist: 3.043\n",
      "    step: 900; loss: 10.317; l2dist: 3.055\n",
      "    step: 1050; loss: 10.321; l2dist: 3.033\n",
      "    step: 1200; loss: 10.294; l2dist: 3.048\n",
      "    step: 1350; loss: 10.295; l2dist: 3.032\n",
      "binary step: 11; num successful adv: 103/200\n",
      "binary step: 11; num successful adv so far: 200/200\n",
      "    step: 0; loss: 51.368; l2dist: 6.650\n",
      "    step: 150; loss: 33.385; l2dist: 5.325\n",
      "    step: 300; loss: 13.042; l2dist: 3.475\n",
      "    step: 450; loss: 10.730; l2dist: 3.110\n",
      "    step: 600; loss: 10.447; l2dist: 3.082\n",
      "    step: 750; loss: 10.374; l2dist: 3.045\n",
      "    step: 900; loss: 10.306; l2dist: 3.059\n",
      "    step: 1050; loss: 10.309; l2dist: 3.034\n",
      "    step: 1200; loss: 10.269; l2dist: 3.048\n",
      "    step: 1350; loss: 10.292; l2dist: 3.031\n",
      "binary step: 12; num successful adv: 106/200\n",
      "binary step: 12; num successful adv so far: 200/200\n",
      "    step: 0; loss: 51.367; l2dist: 6.650\n",
      "    step: 150; loss: 33.372; l2dist: 5.325\n",
      "    step: 300; loss: 13.029; l2dist: 3.473\n",
      "    step: 450; loss: 10.752; l2dist: 3.115\n",
      "    step: 600; loss: 10.416; l2dist: 3.079\n",
      "    step: 750; loss: 10.319; l2dist: 3.039\n",
      "    step: 900; loss: 10.250; l2dist: 3.049\n",
      "    step: 1050; loss: 10.250; l2dist: 3.029\n",
      "    step: 1200; loss: 10.218; l2dist: 3.042\n",
      "    step: 1350; loss: 10.235; l2dist: 3.024\n",
      "binary step: 13; num successful adv: 105/200\n",
      "binary step: 13; num successful adv so far: 200/200\n",
      "    step: 0; loss: 51.374; l2dist: 6.650\n",
      "    step: 150; loss: 33.372; l2dist: 5.325\n",
      "    step: 300; loss: 13.038; l2dist: 3.475\n",
      "    step: 450; loss: 10.772; l2dist: 3.114\n",
      "    step: 600; loss: 10.472; l2dist: 3.086\n",
      "    step: 750; loss: 10.389; l2dist: 3.046\n",
      "    step: 900; loss: 10.326; l2dist: 3.063\n",
      "    step: 1050; loss: 10.324; l2dist: 3.036\n",
      "    step: 1200; loss: 10.298; l2dist: 3.056\n",
      "    step: 1350; loss: 10.320; l2dist: 3.033\n",
      "binary step: 14; num successful adv: 200/200\n",
      "binary step: 14; num successful adv so far: 200/200\n",
      "0.0\n",
      "tensor(2.7274, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from lib.dknn_attack_exp import DKNNExpAttack\n",
    "attack = DKNNExpAttack(knn)\n",
    "\n",
    "def attack_batch(x, y, batch_size, init_mode):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = int(np.ceil(total_num / batch_size))\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            x[begin:end], y[begin:end],\n",
    "            guide_layer=layers[0], m=2, init_mode=init_mode, \n",
    "            binary_search_steps=15, max_iterations=1500, learning_rate=1e-2,\n",
    "            initial_const=1e-1, random_start=False,\n",
    "            thres_steps=20, check_adv_steps=20, verbose=True,\n",
    "            max_linf=None)\n",
    "    return x_adv\n",
    "\n",
    "num = 200\n",
    "x_adv0 = attack_batch(x_test_sub[:num].cuda(), y_test_sub[:num], 200, 0)\n",
    "with torch.no_grad():\n",
    "    y_pred = knn.classify(x_adv0)\n",
    "    ind_adv = np.where(y_pred.argmax(1) == y_test_sub[:num].numpy())[0]\n",
    "    print(len(ind_adv) / y_pred.shape[0])\n",
    "pert0 = (x_test_sub - x_adv0.cpu()).view(num, -1).norm(2, 1)\n",
    "\n",
    "x_adv1 = attack_batch(x_test_sub[:num].cuda(), y_test_sub[:num], 200, 1)\n",
    "with torch.no_grad():\n",
    "    y_pred = knn.classify(x_adv1)\n",
    "    ind_adv = np.where(y_pred.argmax(1) == y_test_sub[:num].numpy())[0]\n",
    "    print(len(ind_adv) / y_pred.shape[0])\n",
    "pert1 = (x_test_sub - x_adv1.cpu()).view(num, -1).norm(2, 1)\n",
    "print(torch.min(pert0, pert1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = reps[layer].view(batch_size, 1, -1);dist = ((rep - guide_reps[layer])**2).sum(2)\n",
    "fx = self.sigmoid((self.thres - dist).clamp(-80 / self.a, 80 / self.a), a=self.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.1310, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = knn.classify(x_adv)\n",
    "    ind = np.where(y_pred.argmax(1) == y_test_sub[:num].numpy())[0]\n",
    "    print(len(ind) / x_adv.size(0))\n",
    "(x_test_sub - x_adv.cpu()).view(num, -1).norm(2, 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (200) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4cb812a25389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx_test_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (200) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "thres = ((knn.x_train[knn.get_neighbors(x_test_sub)[0][1]].squeeze() - x_test_sub)**2).sum((1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.5435)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thres.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFTY = 1e20\n",
    "\n",
    "\n",
    "class DKNN_PGD(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dknn):\n",
    "        self.dknn = dknn\n",
    "        self.device = dknn.device\n",
    "        self.layers = dknn.layers\n",
    "        self.guide_reps = {}\n",
    "        self.thres = None\n",
    "        self.coeff = None\n",
    "\n",
    "    def __call__(self, x_orig, label, guide_layer, m, epsilon=0.1,\n",
    "                 max_epsilon=0.3, max_iterations=1000, num_restart=1,\n",
    "                 rand_start=True, thres_steps=100, check_adv_steps=100,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        x_orig is tensor (requires_grad=False)\n",
    "        \"\"\"\n",
    "\n",
    "        # make sure we run at least once\n",
    "        if num_restart < 1:\n",
    "            num_restart = 1\n",
    "\n",
    "        # if not using randomized start, no point in doing more than one start\n",
    "        if not rand_start:\n",
    "            num_restart = 1\n",
    "\n",
    "        label = label.cpu().numpy()\n",
    "        batch_size = x_orig.size(0)\n",
    "        min_, max_ = x_orig.min(), x_orig.max()\n",
    "        x_adv = x_orig.detach()\n",
    "        best_num_nn = np.zeros((batch_size, ))\n",
    "        \n",
    "        self.coeff = torch.zeros((x_orig.size(0), m))\n",
    "        self.coeff[:, :m // 2] += 1\n",
    "        self.coeff[:, m // 2:] -= 1\n",
    "        \n",
    "        for i in range(num_restart):\n",
    "\n",
    "            # initialize perturbation\n",
    "            delta = torch.zeros_like(x_adv)\n",
    "            if rand_start:\n",
    "                delta.uniform_(- max_epsilon * 0.1, max_epsilon * 0.1)\n",
    "            delta.requires_grad_()\n",
    "\n",
    "            for iteration in range(max_iterations):\n",
    "                x = torch.clamp(x_orig + delta, min_, max_)\n",
    "\n",
    "                # adaptively choose threshold and guide samples every\n",
    "                # <thres_steps> iterations\n",
    "                with torch.no_grad():\n",
    "                    if iteration % thres_steps == 0:\n",
    "                        thres = self.dknn.get_neighbors(x)[0][0][:, -1]\n",
    "                        self.thres = torch.tensor(thres).to(self.device).view(\n",
    "                            batch_size, 1)\n",
    "                        self.find_guide_samples(\n",
    "                            x, label, m=m, layer=guide_layer)\n",
    "\n",
    "                reps = self.dknn.get_activations(x, requires_grad=True)\n",
    "                loss = self.loss_function(reps)\n",
    "                loss.backward()\n",
    "                # perform update on delta\n",
    "                with torch.no_grad():\n",
    "                    delta -= epsilon * delta.grad.detach().sign()\n",
    "                    delta.clamp_(- max_epsilon, max_epsilon)\n",
    "\n",
    "                if (verbose and iteration % (np.ceil(max_iterations / 10)) == 0):\n",
    "                    print('    step: %d; loss: %.3f' %\n",
    "                          (iteration, loss.cpu().detach().numpy()))\n",
    "                \n",
    "                if ((iteration + 1) % check_adv_steps == 0 or\n",
    "                        iteration == max_iterations):\n",
    "                    with torch.no_grad():\n",
    "                        # check if x are adversarial. Only store adversarial examples\n",
    "                        # if they have a larger number of wrong neighbors than orevious\n",
    "                        is_adv, num_nn = self.check_adv(x, label)\n",
    "                        for j in range(batch_size):\n",
    "                            if is_adv[j] and num_nn[j] > best_num_nn[j]:\n",
    "                                x_adv[j] = x[j]\n",
    "                                best_num_nn[j] = num_nn[j]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                is_adv, _ = self.check_adv(x_adv, label)\n",
    "            if verbose:\n",
    "                print('number of successful adv: %d/%d' % (is_adv.sum(), batch_size))\n",
    "\n",
    "        return x_adv\n",
    "\n",
    "    def check_adv(self, x, label):\n",
    "        \"\"\"Check if label of <x> predicted by <dknn> matches with <label>\"\"\"\n",
    "        output = self.dknn.classify(x)\n",
    "        num_nn = output.max(1)\n",
    "        y_pred = output.argmax(1)\n",
    "        is_adv = (y_pred != label).astype(np.float32)\n",
    "        return is_adv, num_nn\n",
    "\n",
    "    def loss_function(self, reps):\n",
    "        \"\"\"Returns the loss averaged over the batch (first dimension of x) and\n",
    "        L-2 norm squared of the perturbation\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = reps[self.layers[0]].size(0)\n",
    "        adv_loss = torch.zeros(\n",
    "            (batch_size, len(self.layers)), device=self.device)\n",
    "        # find squared L-2 distance between original samples and their\n",
    "        # adversarial examples at each layer\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            rep = reps[layer].view(batch_size, 1, -1)\n",
    "            dist = ((rep - self.guide_reps[layer])**2).sum(2)\n",
    "            fx = self.thres - dist\n",
    "            Fx = torch.max(torch.tensor(0., device=self.device),\n",
    "                           self.coeff.to(self.device) * fx).sum(1)\n",
    "            Fx = (- self.coeff.to(self.device) * dist).sum(1)\n",
    "            adv_loss[:, l] = Fx\n",
    "\n",
    "        return adv_loss.mean()\n",
    "\n",
    "    def find_guide_samples(self, x, label, m=100, layer='relu1'):\n",
    "        \"\"\"Find k nearest neighbors to <x> that all have the same class but not\n",
    "        equal to <label>\n",
    "        \"\"\"\n",
    "        num_classes = self.dknn.num_classes\n",
    "        x_train = self.dknn.x_train\n",
    "        y_train = self.dknn.y_train\n",
    "        batch_size = x.size(0)\n",
    "        nn = torch.zeros((m, ) + x.size()).transpose(0, 1)\n",
    "        D, I = self.dknn.get_neighbors(\n",
    "            x, k=x_train.size(0), layers=[layer])[0]\n",
    "\n",
    "        for i, (d, ind) in enumerate(zip(D, I)):\n",
    "            mean_dist = np.zeros((num_classes, ))\n",
    "            for j in range(num_classes):\n",
    "                mean_dist[j] = np.mean(\n",
    "                    d[np.where(y_train[ind] == j)[0]][:m // 2])\n",
    "            mean_dist[label[i]] += INFTY\n",
    "            nearest_label = mean_dist.argmin()\n",
    "            nn_ind = np.where(y_train[ind] == nearest_label)[0][:m // 2]\n",
    "            nn[i, m // 2:] = x_train[ind[nn_ind]]\n",
    "            nn_ind = np.where(y_train[ind] == label[i])[0][:m // 2]\n",
    "            nn[i, :m // 2] = x_train[ind[nn_ind]]\n",
    "\n",
    "        # initialize self.guide_reps if empty\n",
    "        if not self.guide_reps:\n",
    "            guide_rep = self.dknn.get_activations(\n",
    "                nn[0], requires_grad=False)\n",
    "            for l in self.layers:\n",
    "                # set a zero tensor before filling it\n",
    "                size = (batch_size, ) + guide_rep[l].view(m, -1).size()\n",
    "                self.guide_reps[l] = torch.zeros(size, device=self.device)\n",
    "\n",
    "        # fill self.guide_reps\n",
    "        for i in range(batch_size):\n",
    "            guide_rep = self.dknn.get_activations(\n",
    "                nn[i], requires_grad=False)\n",
    "            self.guide_reps[layer][i] = guide_rep[layer].view(\n",
    "                m, -1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFTY = 1e20\n",
    "\n",
    "\n",
    "class DKNN_PGD(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dknn):\n",
    "        self.dknn = dknn\n",
    "        self.device = dknn.device\n",
    "        self.layers = dknn.layers\n",
    "        self.guide_reps = {}\n",
    "        self.thres = None\n",
    "        self.coeff = None\n",
    "\n",
    "    def __call__(self, x_orig, label, guide_layer, m, epsilon=0.1,\n",
    "                 max_epsilon=0.3, max_iterations=1000, num_restart=1,\n",
    "                 rand_start=True, thres_steps=100, check_adv_steps=100,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        x_orig is tensor (requires_grad=False)\n",
    "        \"\"\"\n",
    "\n",
    "        # make sure we run at least once\n",
    "        if num_restart < 1:\n",
    "            num_restart = 1\n",
    "\n",
    "        # if not using randomized start, no point in doing more than one start\n",
    "        if not rand_start:\n",
    "            num_restart = 1\n",
    "\n",
    "        label = label.cpu().numpy()\n",
    "        batch_size = x_orig.size(0)\n",
    "        min_, max_ = x_orig.min(), x_orig.max()\n",
    "        x_adv = x_orig.detach()\n",
    "        best_num_nn = np.zeros((batch_size, ))\n",
    "        \n",
    "        self.coeff = torch.zeros((x_orig.size(0), m))\n",
    "        self.coeff[:, :m // 2] += 1\n",
    "        self.coeff[:, m // 2:] -= 1\n",
    "        \n",
    "        for i in range(num_restart):\n",
    "\n",
    "            # initialize perturbation\n",
    "            delta = torch.zeros_like(x_adv)\n",
    "            if rand_start:\n",
    "                delta.uniform_(- max_epsilon * 0.1, max_epsilon * 0.1)\n",
    "            delta.requires_grad_()\n",
    "\n",
    "            for iteration in range(max_iterations):\n",
    "                x = torch.clamp(x_orig + delta, min_, max_)\n",
    "\n",
    "                # adaptively choose threshold and guide samples every\n",
    "                # <thres_steps> iterations\n",
    "                with torch.no_grad():\n",
    "                    if iteration % thres_steps == 0:\n",
    "                        thres = self.dknn.get_neighbors(x)[0][0][:, -1]\n",
    "                        self.thres = torch.tensor(thres).to(self.device).view(\n",
    "                            batch_size, 1)\n",
    "                        self.find_guide_samples(\n",
    "                            x, label, m=m, layer=guide_layer)\n",
    "\n",
    "                reps = self.dknn.get_activations(x, requires_grad=True)\n",
    "                loss = self.loss_function(reps)\n",
    "                loss.backward()\n",
    "                # perform update on delta\n",
    "                with torch.no_grad():\n",
    "#                     import pdb; pdb.set_trace()\n",
    "                    delta -= epsilon * delta.grad.detach().sign()\n",
    "#                     delta -= epsilon * delta.grad.detach()\n",
    "                    delta.clamp_(- max_epsilon, max_epsilon)\n",
    "\n",
    "                if (verbose and iteration % (np.ceil(max_iterations / 10)) == 0):\n",
    "                    print('    step: %d; loss: %.3f' %\n",
    "                          (iteration, loss.cpu().detach().numpy()))\n",
    "                \n",
    "                if ((iteration + 1) % check_adv_steps == 0 or\n",
    "                        iteration == max_iterations):\n",
    "                    with torch.no_grad():\n",
    "                        # check if x are adversarial. Only store adversarial examples\n",
    "                        # if they have a larger number of wrong neighbors than orevious\n",
    "                        is_adv, num_nn = self.check_adv(x, label)\n",
    "                        for j in range(batch_size):\n",
    "                            if is_adv[j] and num_nn[j] > best_num_nn[j]:\n",
    "                                x_adv[j] = x[j]\n",
    "                                best_num_nn[j] = num_nn[j]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                is_adv, _ = self.check_adv(x_adv, label)\n",
    "            if verbose:\n",
    "                print('number of successful adv: %d/%d' % (is_adv.sum(), batch_size))\n",
    "\n",
    "        return x_adv\n",
    "\n",
    "    def check_adv(self, x, label):\n",
    "        \"\"\"Check if label of <x> predicted by <dknn> matches with <label>\"\"\"\n",
    "        output = self.dknn.classify(x)\n",
    "        num_nn = output.max(1)\n",
    "        y_pred = output.argmax(1)\n",
    "        is_adv = (y_pred != label).astype(np.float32)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        return is_adv, num_nn\n",
    "\n",
    "    def loss_function(self, reps):\n",
    "        \"\"\"Returns the loss averaged over the batch (first dimension of x) and\n",
    "        L-2 norm squared of the perturbation\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = reps[self.layers[0]].size(0)\n",
    "        adv_loss = torch.zeros(\n",
    "            (batch_size, len(self.layers)), device=self.device)\n",
    "        # find squared L-2 distance between original samples and their\n",
    "        # adversarial examples at each layer\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            rep = reps[layer].view(batch_size, 1, -1)\n",
    "            dist = ((rep - self.guide_reps[layer])**2).sum(2)\n",
    "#             fx = self.thres - dist\n",
    "#             Fx = torch.max(torch.tensor(0., device=self.device),\n",
    "#                            self.coeff.to(self.device) * fx).sum(1)\n",
    "#             Fx = (- self.coeff.to(self.device) * dist).sum(1)\n",
    "            Fx = dist[:, 1]\n",
    "            adv_loss[:, l] = Fx\n",
    "\n",
    "        return adv_loss.mean()\n",
    "\n",
    "    def find_guide_samples(self, x, label, m=100, layer='relu1'):\n",
    "        \"\"\"Find k nearest neighbors to <x> that all have the same class but not\n",
    "        equal to <label>\n",
    "        \"\"\"\n",
    "        num_classes = self.dknn.num_classes\n",
    "        x_train = self.dknn.x_train\n",
    "        y_train = self.dknn.y_train\n",
    "        batch_size = x.size(0)\n",
    "        nn = torch.zeros((m, ) + x.size()).transpose(0, 1)\n",
    "        D, I = self.dknn.get_neighbors(\n",
    "            x, k=x_train.size(0), layers=[layer])[0]\n",
    "        y_pred = self.dknn.classify(x_train).argmax(1)\n",
    "        is_correct = y_pred == y_train.numpy()\n",
    "\n",
    "        for i, (d, ind) in enumerate(zip(D, I)):\n",
    "#             mean_dist = np.zeros((num_classes, ))\n",
    "#             for j in range(num_classes):\n",
    "#                 mean_dist[j] = np.mean(\n",
    "#                     d[np.where(y_train[ind] == j)[0]][:m // 2])\n",
    "#             mean_dist[label[i]] += INFTY\n",
    "#             nearest_label = mean_dist.argmin()\n",
    "#             nn_ind = np.where(y_train[ind] == nearest_label)[0][:m // 2]\n",
    "#             nn[i, m // 2:] = x_train[ind[nn_ind]]\n",
    "#             nn_ind = np.where(y_train[ind] == label[i])[0][:m // 2]\n",
    "#             nn[i, :m // 2] = x_train[ind[nn_ind]]\n",
    "            # find nearest sample that is correctly classified as j\n",
    "#             import pdb; pdb.set_trace()\n",
    "            is_not_label = y_train.numpy() != label[i]\n",
    "            idx = np.where(is_correct & is_not_label)[0]\n",
    "            nn_ind = (x_train - x[i].cpu())[idx].view(\n",
    "                idx.shape[0], -1).norm(2, 1).argmin()\n",
    "            nn[i, 1] = x_train[idx][nn_ind]\n",
    "\n",
    "        # initialize self.guide_reps if empty\n",
    "        if not self.guide_reps:\n",
    "            guide_rep = self.dknn.get_activations(\n",
    "                nn[0], requires_grad=False)\n",
    "            for l in self.layers:\n",
    "                # set a zero tensor before filling it\n",
    "                size = (batch_size, ) + guide_rep[l].view(m, -1).size()\n",
    "                self.guide_reps[l] = torch.zeros(size, device=self.device)\n",
    "\n",
    "        # fill self.guide_reps\n",
    "        for i in range(batch_size):\n",
    "            guide_rep = self.dknn.get_activations(\n",
    "                nn[i], requires_grad=False)\n",
    "            self.guide_reps[layer][i] = guide_rep[layer].view(\n",
    "                m, -1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 45.419\n",
      "    step: 100; loss: 34.209\n",
      "    step: 200; loss: 25.145\n",
      "    step: 300; loss: 17.951\n",
      "    step: 400; loss: 17.896\n",
      "    step: 500; loss: 17.885\n",
      "    step: 600; loss: 17.942\n",
      "    step: 700; loss: 17.890\n",
      "    step: 800; loss: 17.899\n",
      "    step: 900; loss: 17.923\n",
      "number of successful adv: 54/200\n"
     ]
    }
   ],
   "source": [
    "attack = DKNN_PGD(knn)\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            x[begin:end], y[begin:end],\n",
    "            guide_layer=layers[0], m=2, epsilon=0.001,\n",
    "            max_epsilon=0.3, max_iterations=1000, num_restart=10,\n",
    "            rand_start=False, thres_steps=1000, check_adv_steps=1000,\n",
    "            verbose=True)\n",
    "    return x_adv\n",
    "\n",
    "num = 200\n",
    "x_adv = attack_batch(x_test_sub[:num].cuda(), y_test_sub[:num], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
