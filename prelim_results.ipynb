{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import foolbox\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from lib.dataset_utils import *\n",
    "from lib.mnist_model import *\n",
    "from lib.adv_model import *\n",
    "from lib.dknn_attack import DKNNAttack\n",
    "from lib.cwl2_attack import CWL2Attack\n",
    "from lib.dknn import DKNN, DKNNL2\n",
    "from lib.utils import *\n",
    "from lib.lip_model import *\n",
    "from lib.knn import KNNL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = 2\n",
    "\n",
    "# model_name = 'train_mnist_exp%d.h5' % exp_id\n",
    "# net = BasicModel()\n",
    "\n",
    "# model_name = 'train_mnist_snnl_exp%d.h5' % exp_id\n",
    "# net = SNNLModel(train_it=True)\n",
    "\n",
    "# model_name = 'train_mnist_hidden_mixup_exp%d.h5' % exp_id\n",
    "# net = HiddenMixupModel()\n",
    "\n",
    "# model_name = 'train_mnist_vae_exp%d.h5' % exp_id\n",
    "# net = VAE((1, 28, 28), num_classes=10, latent_dim=20)\n",
    "# net = VAE2((1, 28, 28), num_classes=10, latent_dim=1000)\n",
    "\n",
    "# model_name = 'train_mnist_cav_exp%d.h5' % exp_id\n",
    "# net = ClassAuxVAE((1, 28, 28), num_classes=10, latent_dim=20)\n",
    "\n",
    "# model_name = 'lip_mnist_exp%d.h5' % exp_id\n",
    "# net = LipschitzModel()\n",
    "\n",
    "# model_name = 'dist_mnist_exp%d.h5' % exp_id\n",
    "# net = NeighborModel()\n",
    "\n",
    "model_name = 'adv_mnist_exp%d.h5' % exp_id\n",
    "basic_net = BasicModel()\n",
    "# basic_net = BasicModelV2()\n",
    "config = {'epsilon': 0.3,\n",
    "              'num_steps': 40,\n",
    "              'step_size': 0.01,\n",
    "              'random_start': True,\n",
    "              'loss_func': 'xent'}\n",
    "net = PGDModel(basic_net, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicModel(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(8, 8), stride=(2, 2), padding=(3, 3))\n",
       "  (relu1): ReLU(inplace)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(6, 6), stride=(2, 2), padding=(3, 3))\n",
       "  (relu2): ReLU(inplace)\n",
       "  (conv3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu3): ReLU(inplace)\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set all random seeds\n",
    "seed = 2019\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set up model directory\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "net = net.to(device)\n",
    "# if device == 'cuda':\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "#     cudnn.benchmark = True\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "# net = net.module\n",
    "net = net.basic_net\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test) = load_mnist_all(\n",
    "    '/data', val_size=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net(x_test.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred.argmax(1).cpu() == y_test).sum().numpy() / y_test.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.to(device)\n",
    "# x_test = x_test.to(device)\n",
    "# x_valid = x_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# layers = ['relu1', 'relu2', 'relu3', 'fc']\n",
    "# layers = ['relu1', 'relu2', 'relu3']\n",
    "layers = ['relu3']\n",
    "# layers = ['fc2']\n",
    "# layers = ['en_conv3']\n",
    "# layers = ['en_mu']\n",
    "# layers = ['maxpool1', 'maxpool2', 'relu3', 'fc2']\n",
    "# layers = ['maxpool2']\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     dknn = DKNN(net, x_train, y_train, x_valid, y_valid, layers, \n",
    "#                 k=75, num_classes=10)\n",
    "#     y_pred = dknn.classify(x_test)\n",
    "    \n",
    "\n",
    "dknn = DKNNL2(net, x_train, y_train, x_valid, y_valid, layers, \n",
    "              k=75, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9726\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = dknn.classify(x_test)\n",
    "    print((y_pred.argmax(1) == y_test.numpy()).sum() / y_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred = dknn.credibility(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 981., 1057.,  890.,  642.,  523., 1019.,    0.,    0.,    0.,\n",
       "        4888.]),\n",
       " array([0.02366667, 0.1213    , 0.21893333, 0.31656667, 0.4142    ,\n",
       "        0.51183333, 0.60946667, 0.7071    , 0.80473333, 0.90236667,\n",
       "        1.        ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEGVJREFUeJzt3X2snvVdx/H3Z3QP6uaAtRDSFotZl4wtcSMnULNEt7GUwgzlDzBdnHSksclEM3XRMf0DhZGARjHEPViFrCxugNNJs6HY8JCpEUaRjfEg6RlDaCBrZ0t1IUNhX/+4f50Hdk7Pddpz7sPp7/1KTu7r+l6/+75/X87hfM71cF9NVSFJ6s+rFnsCkqTFYQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOrVssSdwOMuXL681a9Ys9jQkaUm5//77v1tVK2Yb94oOgDVr1rBr167FnoYkLSlJ/mPIuEGHgJI8keSbSb6eZFernZhkZ5Ld7fGEVk+S65JMJnkwyRlTXmdzG787yeYjaUySND/mcg7gPVX1jqqaaOuXAXdU1VrgjrYOcC6wtn1tBT4No8AALgfOAs4ELj8UGpKk8Tuak8Abge1teTtwwZT6jTVyD3B8klOAc4CdVbW/qg4AO4ENR/H+kqSjMDQACvjHJPcn2dpqJ1fVMwDt8aRWXwk8NeW5e1ptprokaREMPQn8rqp6OslJwM4k/36YsZmmVoepv/TJo4DZCnDqqacOnJ4kaa4G7QFU1dPtcS/wJUbH8L/TDu3QHve24XuA1VOevgp4+jD1l7/XtqqaqKqJFStmvYpJknSEZg2AJD+R5A2HloH1wEPADuDQlTybgVvb8g7g4nY10DrgYDtEdDuwPskJ7eTv+laTJC2CIYeATga+lOTQ+M9X1T8kuQ+4JckW4Engojb+NuA8YBJ4DrgEoKr2J7kSuK+Nu6Kq9s9bJ5KkOckr+d8EnpiYKD8IJklzk+T+KZfsz+gV/UlgSVpMay77yqK99xNXv3/B38ObwUlSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcGB0CS45I8kOTLbf20JPcm2Z3k5iSvafXXtvXJtn3NlNf4eKs/luSc+W5GkjTcXPYAPgI8OmX9GuDaqloLHAC2tPoW4EBVvRm4to0jyenAJuBtwAbgU0mOO7rpS5KO1KAASLIKeD/wl209wHuBL7Yh24EL2vLGtk7bfnYbvxG4qaqer6pvA5PAmfPRhCRp7obuAfwp8DvAD9r6m4Bnq+qFtr4HWNmWVwJPAbTtB9v4H9anec4PJdmaZFeSXfv27ZtDK5KkuZg1AJL8ArC3qu6fWp5maM2y7XDP+f9C1baqmqiqiRUrVsw2PUnSEVo2YMy7gPOTnAe8DvhJRnsExydZ1v7KXwU83cbvAVYDe5IsA94I7J9SP2TqcyRJYzbrHkBVfbyqVlXVGkYnce+sql8C7gIubMM2A7e25R1tnbb9zqqqVt/UrhI6DVgLfG3eOpEkzcmQPYCZfAy4KckngAeA61v9euBzSSYZ/eW/CaCqHk5yC/AI8AJwaVW9eBTvL0k6CnMKgKq6G7i7LT/ONFfxVNX3gYtmeP5VwFVznaQkaf75SWBJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASerUrAGQ5HVJvpbkG0keTvIHrX5aknuT7E5yc5LXtPpr2/pk275mymt9vNUfS3LOQjUlSZrdkD2A54H3VtXPAO8ANiRZB1wDXFtVa4EDwJY2fgtwoKreDFzbxpHkdGAT8DZgA/CpJMfNZzOSpOFmDYAa+V5bfXX7KuC9wBdbfTtwQVve2NZp289Okla/qaqer6pvA5PAmfPShSRpzgadA0hyXJKvA3uBncC3gGer6oU2ZA+wsi2vBJ4CaNsPAm+aWp/mOZKkMRsUAFX1YlW9A1jF6K/2t043rD1mhm0z1V8iydYku5Ls2rdv35DpSZKOwJyuAqqqZ4G7gXXA8UmWtU2rgKfb8h5gNUDb/kZg/9T6NM+Z+h7bqmqiqiZWrFgxl+lJkuZgyFVAK5Ic35Z/DHgf8ChwF3BhG7YZuLUt72jrtO13VlW1+qZ2ldBpwFrga/PViCRpbpbNPoRTgO3tip1XAbdU1ZeTPALclOQTwAPA9W389cDnkkwy+st/E0BVPZzkFuAR4AXg0qp6cX7bkSQNNWsAVNWDwDunqT/ONFfxVNX3gYtmeK2rgKvmPk1J0nzzk8CS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktSpWQMgyeokdyV5NMnDST7S6icm2Zlkd3s8odWT5Lokk0keTHLGlNfa3MbvTrJ54dqSJM1myB7AC8BHq+qtwDrg0iSnA5cBd1TVWuCOtg5wLrC2fW0FPg2jwAAuB84CzgQuPxQakqTxmzUAquqZqvq3tvzfwKPASmAjsL0N2w5c0JY3AjfWyD3A8UlOAc4BdlbV/qo6AOwENsxrN5KkweZ0DiDJGuCdwL3AyVX1DIxCAjipDVsJPDXlaXtabaa6JGkRDA6AJK8H/gb4jar6r8MNnaZWh6m//H22JtmVZNe+ffuGTk+SNEeDAiDJqxn98v+rqvrbVv5OO7RDe9zb6nuA1VOevgp4+jD1l6iqbVU1UVUTK1asmEsvkqQ5GHIVUIDrgUer6k+mbNoBHLqSZzNw65T6xe1qoHXAwXaI6HZgfZIT2snf9a0mSVoEywaMeRfwy8A3k3y91X4XuBq4JckW4EngorbtNuA8YBJ4DrgEoKr2J7kSuK+Nu6Kq9s9LF5KkOZs1AKrqn5n++D3A2dOML+DSGV7rBuCGuUxQkrQw/CSwJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE7NGgBJbkiyN8lDU2onJtmZZHd7PKHVk+S6JJNJHkxyxpTnbG7jdyfZvDDtSJKGWjZgzGeBPwNunFK7DLijqq5Ocllb/xhwLrC2fZ0FfBo4K8mJwOXABFDA/Ul2VNWB+WrklWTNZV9ZlPd94ur3L8r7SlqaZt0DqKqvAvtfVt4IbG/L24ELptRvrJF7gOOTnAKcA+ysqv3tl/5OYMN8NCBJOjJD9gCmc3JVPQNQVc8kOanVVwJPTRm3p9Vmqv+IJFuBrQCnnnrqEU5PWnju6WmpO9IAmEmmqdVh6j9arNoGbAOYmJiYdsxQi/U/qCQtBUd6FdB32qEd2uPeVt8DrJ4ybhXw9GHqkqRFcqQBsAM4dCXPZuDWKfWL29VA64CD7VDR7cD6JCe0K4bWt5okaZHMeggoyReAdwPLk+xhdDXP1cAtSbYATwIXteG3AecBk8BzwCUAVbU/yZXAfW3cFVX18hPLkqQxmjUAquoDM2w6e5qxBVw6w+vcANwwp9lpThbznIcnJqWlx08CS1KnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASerUfN8OWp3y3vjS0uMegCR1ygCQpE4ZAJLUKQNAkjrlSWAtaf67z9KRcw9AkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOjX2AEiyIcljSSaTXDbu95ckjYw1AJIcB3wSOBc4HfhAktPHOQdJ0si49wDOBCar6vGq+h/gJmDjmOcgSWL8AbASeGrK+p5WkySN2bIxv1+mqdVLBiRbga1t9XtJHjvM6y0HvjtPc1tqeu29+75zzSLPZPy6/J7nmqPq+6eGDBp3AOwBVk9ZXwU8PXVAVW0Dtg15sSS7qmpi/qa3dPTau333p9fex9H3uA8B3QesTXJaktcAm4AdY56DJIkx7wFU1QtJfg24HTgOuKGqHh7nHCRJI+M+BERV3QbcNk8vN+hQ0TGq197tuz+99r7gfaeqZh8lSTrmeCsISerUkgiA2W4fkeS1SW5u2+9Nsmb8s5x/A/r+rSSPJHkwyR1JBl36tRQMvWVIkguTVJJj4iqRIX0n+cX2fX84yefHPceFMOBn/dQkdyV5oP28n7cY85xvSW5IsjfJQzNsT5Lr2n+XB5OcMa8TqKpX9Bejk8XfAn4aeA3wDeD0l435VeAzbXkTcPNiz3tMfb8H+PG2/OFjoe+hvbdxbwC+CtwDTCz2vMf0PV8LPACc0NZPWux5j6nvbcCH2/LpwBOLPe956v3ngDOAh2bYfh7w94w+Q7UOuHc+338p7AEMuX3ERmB7W/4icHaS6T50tpTM2ndV3VVVz7XVexh9ruJYMPSWIVcCfwh8f5yTW0BD+v4V4JNVdQCgqvaOeY4LYUjfBfxkW34jL/v80FJVVV8F9h9myEbgxhq5Bzg+ySnz9f5LIQCG3D7ih2Oq6gXgIPCmscxu4cz1thlbGP2lcCyYtfck7wRWV9WXxzmxBTbke/4W4C1J/iXJPUk2jG12C2dI378PfDDJHkZXEf76eKa26Bb09jljvwz0CMx6+4iBY5aawT0l+SAwAfz8gs5ofA7be5JXAdcCHxrXhMZkyPd8GaPDQO9mtMf3T0neXlXPLvDcFtKQvj8AfLaq/jjJzwKfa33/YOGnt6gW9HfbUtgDmPX2EVPHJFnGaBfxcLtVS8GQvknyPuD3gPOr6vkxzW2hzdb7G4C3A3cneYLRsdEdx8CJ4KE/67dW1f9W1beBxxgFwlI2pO8twC0AVfWvwOsY3SPoWDfo98CRWgoBMOT2ETuAzW35QuDOamdQlrBZ+26HQf6c0S//Y+FY8CGH7b2qDlbV8qpaU1VrGJ3/OL+qdi3OdOfNkJ/1v2N08p8kyxkdEnp8rLOcf0P6fhI4GyDJWxkFwL6xznJx7AAublcDrQMOVtUz8/Xir/hDQDXD7SOSXAHsqqodwPWMdgknGf3lv2nxZjw/Bvb9R8Drgb9u57yfrKrzF23S82Rg78ecgX3fDqxP8gjwIvDbVfWfizfrozew748Cf5HkNxkdAvnQMfBHHkm+wOhw3vJ2fuNy4NUAVfUZRuc7zgMmgeeAS+b1/Y+B/4aSpCOwFA4BSZIWgAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKn/g+abYj5yvny4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = np.argmax(y_pred, 1) == y_test.numpy()\n",
    "num_correct_by_cred = np.zeros((10, ))\n",
    "num_cred = np.zeros((10, ))\n",
    "for i in np.arange(10):\n",
    "    ind = (cred > i * 0.1) & (cred <= i* 0.1 + 0.1)\n",
    "    num_cred[i] = np.sum(ind)\n",
    "    num_correct_by_cred[i] = np.sum(correct[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(np.arange(10) * 0.1, num_cred, width=0.05)\n",
    "ax.bar(np.arange(10) * 0.1 + 0.05, num_correct_by_cred, width=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.52764613, 0.83124478, 0.94539249, 0.98878343, 0.99498495,\n",
       "              nan,        nan,        nan,        nan, 0.99922103])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct_by_cred / num_cred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17333333333333334"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dknn.A.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = dknn.get_neighbors(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72602546\n",
      "0.6874378\n",
      "0.7105881\n",
      "0.94844836\n"
     ]
    }
   ],
   "source": [
    "for (D, I) in nn:\n",
    "    print(D[-1].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGD Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_np = x_test.cpu().numpy()\n",
    "y_test_np = y_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foolbox.models.PyTorchModel(net, bounds=(0, 1), num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = foolbox.criteria.Misclassification()\n",
    "distance = foolbox.distances.Linfinity\n",
    "\n",
    "attack = foolbox.attacks.RandomPGD(\n",
    "    model, criterion=criterion, distance=distance)\n",
    "\n",
    "def attack_wrap(x, y):\n",
    "    return attack(x, y, binary_search=False, epsilon=0.3, \n",
    "                  stepsize=0.01, iterations=300, \n",
    "                  random_start=True, return_early=True)\n",
    "\n",
    "x_adv = np.zeros_like(x_test_np)\n",
    "for i, (x, y) in enumerate(zip(x_test_np, y_test_np)):\n",
    "    x_adv[i] = attack_wrap(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_adv = torch.tensor(x_adv).to(device)\n",
    "    y_pred = net(x_adv).detach().cpu().numpy()\n",
    "np.mean(np.argmax(y_pred, 1) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of successful adv: 820/10000\n"
     ]
    }
   ],
   "source": [
    "from lib.pgd_attack import PGDAttack\n",
    "\n",
    "attack = PGDAttack()\n",
    "x_adv = attack(net, x_test.cuda(), y_test.to(device),\n",
    "               targeted=False, epsilon=0.01, max_epsilon=0.3,\n",
    "               max_iterations=300, num_restart=10, rand_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9180"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = net(x_adv)\n",
    "(y_pred.argmax(1).cpu() == y_test).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dknn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5736a610097b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dknn' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = dknn.classify(x_adv.cpu())\n",
    "(y_pred.argmax(1) == y_test.numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9.63e+03, 2.96e+02, 5.70e+01, 7.00e+00, 5.00e+00, 1.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 4.00e+00]),\n",
       " array([0.00383333, 0.10345   , 0.20306667, 0.30268333, 0.4023    ,\n",
       "        0.50191667, 0.60153333, 0.70115   , 0.80076667, 0.90038333,\n",
       "        1.        ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cred = dknn.credibility(y_pred)\n",
    "plt.hist(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9091"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DkNN Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 14.109; l2dist: 0.000\n",
      "    step: 50; loss: 10.948; l2dist: 1.190\n",
      "    step: 100; loss: 9.823; l2dist: 1.287\n",
      "    step: 150; loss: 9.276; l2dist: 1.311\n",
      "    step: 200; loss: 9.182; l2dist: 1.309\n",
      "    step: 250; loss: 9.152; l2dist: 1.307\n",
      "    step: 300; loss: 9.139; l2dist: 1.306\n",
      "    step: 350; loss: 9.132; l2dist: 1.306\n",
      "    step: 400; loss: 9.128; l2dist: 1.306\n",
      "    step: 450; loss: 9.125; l2dist: 1.306\n",
      "tensor(25., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 25/100\n",
      "    step: 0; loss: 117.042; l2dist: 0.000\n",
      "    step: 50; loss: 62.943; l2dist: 2.664\n",
      "    step: 100; loss: 53.316; l2dist: 2.150\n",
      "    step: 150; loss: 52.207; l2dist: 1.985\n",
      "    step: 200; loss: 51.862; l2dist: 1.933\n",
      "    step: 250; loss: 51.742; l2dist: 1.913\n",
      "    step: 300; loss: 51.684; l2dist: 1.904\n",
      "    step: 350; loss: 51.645; l2dist: 1.897\n",
      "    step: 400; loss: 51.629; l2dist: 1.894\n",
      "    step: 450; loss: 51.622; l2dist: 1.893\n",
      "tensor(71., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 81/100\n",
      "    step: 0; loss: 359.826; l2dist: 0.000\n",
      "    step: 50; loss: 178.262; l2dist: 2.591\n",
      "    step: 100; loss: 169.239; l2dist: 2.290\n",
      "    step: 150; loss: 168.122; l2dist: 2.142\n",
      "    step: 200; loss: 167.539; l2dist: 2.064\n",
      "    step: 250; loss: 167.174; l2dist: 2.013\n",
      "    step: 300; loss: 166.905; l2dist: 1.974\n",
      "    step: 350; loss: 166.697; l2dist: 1.942\n",
      "    step: 400; loss: 166.516; l2dist: 1.915\n",
      "    step: 450; loss: 166.375; l2dist: 1.894\n",
      "tensor(73., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 86/100\n",
      "    step: 0; loss: 2232.470; l2dist: 0.000\n",
      "    step: 50; loss: 1089.737; l2dist: 2.360\n",
      "    step: 100; loss: 1071.281; l2dist: 2.238\n",
      "    step: 150; loss: 1070.556; l2dist: 2.157\n",
      "    step: 200; loss: 1070.239; l2dist: 2.118\n",
      "    step: 250; loss: 1069.995; l2dist: 2.088\n",
      "    step: 300; loss: 1069.800; l2dist: 2.066\n",
      "    step: 350; loss: 1069.641; l2dist: 2.048\n",
      "    step: 400; loss: 1069.489; l2dist: 2.030\n",
      "    step: 450; loss: 1069.332; l2dist: 2.012\n",
      "tensor(69., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 86/100\n",
      "    step: 0; loss: 21529.770; l2dist: 0.000\n",
      "    step: 50; loss: 10450.059; l2dist: 2.206\n",
      "    step: 100; loss: 10317.765; l2dist: 2.142\n",
      "    step: 150; loss: 10316.764; l2dist: 2.082\n",
      "tensor(45., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 86/100\n",
      "    step: 0; loss: 214765.375; l2dist: 0.000\n",
      "    step: 50; loss: 104169.078; l2dist: 2.168\n",
      "    step: 100; loss: 102891.961; l2dist: 2.111\n",
      "    step: 150; loss: 102887.398; l2dist: 2.061\n",
      "tensor(42., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 86/100\n",
      "    step: 0; loss: 2147235.000; l2dist: 0.000\n",
      "    step: 50; loss: 1041406.312; l2dist: 2.159\n",
      "    step: 100; loss: 1028681.688; l2dist: 2.100\n",
      "    step: 150; loss: 1028641.125; l2dist: 2.056\n",
      "tensor(46., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 86/100\n",
      "    step: 0; loss: 21471984.000; l2dist: 0.000\n",
      "    step: 50; loss: 10413805.000; l2dist: 2.151\n",
      "    step: 100; loss: 10286603.000; l2dist: 2.094\n",
      "    step: 150; loss: 10286200.000; l2dist: 2.051\n",
      "tensor(42., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 86/100\n",
      "    step: 0; loss: 214719504.000; l2dist: 0.000\n",
      "    step: 50; loss: 104137800.000; l2dist: 2.150\n",
      "    step: 100; loss: 102865816.000; l2dist: 2.091\n",
      "    step: 150; loss: 102861792.000; l2dist: 2.050\n",
      "tensor(32., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 86/100\n",
      "    step: 0; loss: 214719455381936600975933440.000; l2dist: 0.000\n",
      "tensor(6., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 86/100\n",
      "    step: 0; loss: 12.825; l2dist: 0.000\n",
      "    step: 50; loss: 10.053; l2dist: 1.120\n",
      "    step: 100; loss: 9.180; l2dist: 1.207\n",
      "    step: 150; loss: 8.614; l2dist: 1.239\n",
      "    step: 200; loss: 8.516; l2dist: 1.240\n",
      "    step: 250; loss: 8.486; l2dist: 1.239\n",
      "    step: 300; loss: 8.473; l2dist: 1.238\n",
      "    step: 350; loss: 8.466; l2dist: 1.238\n",
      "    step: 400; loss: 8.462; l2dist: 1.237\n",
      "    step: 450; loss: 8.460; l2dist: 1.237\n",
      "tensor(28., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 28/100\n",
      "    step: 0; loss: 105.133; l2dist: 0.000\n",
      "    step: 50; loss: 58.516; l2dist: 2.464\n",
      "    step: 100; loss: 49.310; l2dist: 1.978\n",
      "    step: 150; loss: 48.371; l2dist: 1.828\n",
      "    step: 200; loss: 48.100; l2dist: 1.789\n",
      "    step: 250; loss: 48.003; l2dist: 1.774\n",
      "    step: 300; loss: 47.963; l2dist: 1.768\n",
      "    step: 350; loss: 47.944; l2dist: 1.765\n",
      "    step: 400; loss: 47.932; l2dist: 1.763\n",
      "    step: 450; loss: 47.925; l2dist: 1.762\n",
      "tensor(73., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 84/100\n",
      "    step: 0; loss: 273.665; l2dist: 0.000\n",
      "    step: 50; loss: 141.760; l2dist: 2.339\n",
      "    step: 100; loss: 131.920; l2dist: 2.029\n",
      "    step: 150; loss: 131.007; l2dist: 1.901\n",
      "    step: 200; loss: 130.585; l2dist: 1.841\n",
      "    step: 250; loss: 130.350; l2dist: 1.808\n",
      "    step: 300; loss: 130.188; l2dist: 1.783\n",
      "    step: 350; loss: 130.053; l2dist: 1.762\n",
      "    step: 400; loss: 129.942; l2dist: 1.745\n",
      "    step: 450; loss: 129.868; l2dist: 1.733\n",
      "tensor(78., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 90/100\n",
      "    step: 0; loss: 1435.423; l2dist: 0.000\n",
      "    step: 50; loss: 714.714; l2dist: 2.098\n",
      "    step: 100; loss: 688.749; l2dist: 1.959\n",
      "    step: 150; loss: 688.047; l2dist: 1.877\n",
      "    step: 200; loss: 687.762; l2dist: 1.840\n",
      "    step: 250; loss: 687.554; l2dist: 1.812\n",
      "    step: 300; loss: 687.400; l2dist: 1.791\n",
      "    step: 350; loss: 687.272; l2dist: 1.775\n",
      "    step: 400; loss: 687.164; l2dist: 1.760\n",
      "    step: 450; loss: 687.077; l2dist: 1.748\n",
      "tensor(64., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 90/100\n",
      "    step: 0; loss: 13600.114; l2dist: 0.000\n",
      "    step: 50; loss: 6702.262; l2dist: 1.965\n",
      "    step: 100; loss: 6498.575; l2dist: 1.900\n",
      "    step: 150; loss: 6497.645; l2dist: 1.830\n",
      "    step: 200; loss: 6497.451; l2dist: 1.806\n",
      "tensor(49., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 90/100\n",
      "    step: 0; loss: 135405.562; l2dist: 0.000\n",
      "    step: 50; loss: 66651.727; l2dist: 1.929\n",
      "    step: 100; loss: 64666.219; l2dist: 1.867\n",
      "    step: 150; loss: 64662.070; l2dist: 1.808\n",
      "tensor(42., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 90/100\n",
      "    step: 0; loss: 1353566.375; l2dist: 0.000\n",
      "    step: 50; loss: 666195.375; l2dist: 1.921\n",
      "    step: 100; loss: 646392.125; l2dist: 1.858\n",
      "    step: 150; loss: 646355.688; l2dist: 1.804\n",
      "tensor(44., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 90/100\n",
      "    step: 0; loss: 13535205.000; l2dist: 0.000\n",
      "    step: 50; loss: 6661644.500; l2dist: 1.917\n",
      "    step: 100; loss: 6463666.500; l2dist: 1.857\n",
      "    step: 150; loss: 6463306.500; l2dist: 1.804\n",
      "tensor(37., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 90/100\n",
      "    step: 0; loss: 135351600.000; l2dist: 0.000\n",
      "    step: 50; loss: 66616144.000; l2dist: 1.917\n",
      "    step: 100; loss: 64636404.000; l2dist: 1.858\n",
      "    step: 150; loss: 64632804.000; l2dist: 1.804\n",
      "tensor(43., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 90/100\n",
      "    step: 0; loss: 135351545794806085637373952.000; l2dist: 0.000\n",
      "tensor(7., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 90/100\n",
      "    step: 0; loss: 13.403; l2dist: 0.000\n",
      "    step: 50; loss: 10.483; l2dist: 1.156\n",
      "    step: 100; loss: 9.555; l2dist: 1.259\n",
      "    step: 150; loss: 8.976; l2dist: 1.281\n",
      "    step: 200; loss: 8.880; l2dist: 1.281\n",
      "    step: 250; loss: 8.852; l2dist: 1.280\n",
      "    step: 300; loss: 8.839; l2dist: 1.279\n",
      "    step: 350; loss: 8.833; l2dist: 1.279\n",
      "    step: 400; loss: 8.829; l2dist: 1.279\n",
      "    step: 450; loss: 8.826; l2dist: 1.279\n",
      "tensor(30., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 30/100\n",
      "    step: 0; loss: 102.307; l2dist: 0.000\n",
      "    step: 50; loss: 56.855; l2dist: 2.474\n",
      "    step: 100; loss: 48.162; l2dist: 1.998\n",
      "    step: 150; loss: 47.273; l2dist: 1.863\n",
      "    step: 200; loss: 46.979; l2dist: 1.813\n",
      "    step: 250; loss: 46.872; l2dist: 1.795\n",
      "    step: 300; loss: 46.824; l2dist: 1.786\n",
      "    step: 350; loss: 46.801; l2dist: 1.782\n",
      "    step: 400; loss: 46.789; l2dist: 1.780\n",
      "    step: 450; loss: 46.783; l2dist: 1.779\n",
      "tensor(71., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 86/100\n",
      "    step: 0; loss: 245.150; l2dist: 0.000\n",
      "    step: 50; loss: 126.886; l2dist: 2.375\n",
      "    step: 100; loss: 120.371; l2dist: 2.082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 119.403; l2dist: 1.939\n",
      "    step: 200; loss: 118.960; l2dist: 1.876\n",
      "    step: 250; loss: 118.673; l2dist: 1.835\n",
      "    step: 300; loss: 118.474; l2dist: 1.805\n",
      "    step: 350; loss: 118.343; l2dist: 1.785\n",
      "    step: 400; loss: 118.234; l2dist: 1.767\n",
      "    step: 450; loss: 118.167; l2dist: 1.756\n",
      "tensor(69., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 87/100\n",
      "    step: 0; loss: 1862.874; l2dist: 0.000\n",
      "    step: 50; loss: 916.313; l2dist: 2.168\n",
      "    step: 100; loss: 908.368; l2dist: 2.058\n",
      "    step: 150; loss: 907.772; l2dist: 1.985\n",
      "    step: 200; loss: 907.536; l2dist: 1.956\n",
      "    step: 250; loss: 907.359; l2dist: 1.936\n",
      "    step: 300; loss: 907.189; l2dist: 1.918\n",
      "    step: 350; loss: 907.037; l2dist: 1.901\n",
      "    step: 400; loss: 906.902; l2dist: 1.885\n",
      "    step: 450; loss: 906.775; l2dist: 1.870\n",
      "tensor(70., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 88/100\n",
      "    step: 0; loss: 17338.039; l2dist: 0.000\n",
      "    step: 50; loss: 8503.896; l2dist: 2.033\n",
      "    step: 100; loss: 8462.479; l2dist: 1.988\n",
      "    step: 150; loss: 8461.827; l2dist: 1.930\n",
      "tensor(40., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 88/100\n",
      "    step: 0; loss: 172586.875; l2dist: 0.000\n",
      "    step: 50; loss: 84603.781; l2dist: 2.028\n",
      "    step: 100; loss: 84220.336; l2dist: 1.982\n",
      "    step: 150; loss: 84218.297; l2dist: 1.931\n",
      "tensor(41., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 88/100\n",
      "    step: 0; loss: 1725294.375; l2dist: 0.000\n",
      "    step: 50; loss: 845698.250; l2dist: 2.012\n",
      "    step: 100; loss: 841894.125; l2dist: 1.978\n",
      "    step: 150; loss: 841878.250; l2dist: 1.924\n",
      "tensor(50., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 88/100\n",
      "    step: 0; loss: 17252250.000; l2dist: 0.000\n",
      "    step: 50; loss: 8456591.000; l2dist: 2.005\n",
      "    step: 100; loss: 8418580.000; l2dist: 1.974\n",
      "    step: 150; loss: 8418424.000; l2dist: 1.921\n",
      "tensor(29., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 88/100\n",
      "    step: 0; loss: 172521856.000; l2dist: 0.000\n",
      "    step: 50; loss: 84565536.000; l2dist: 2.012\n",
      "    step: 100; loss: 84185448.000; l2dist: 1.977\n",
      "    step: 150; loss: 84183912.000; l2dist: 1.924\n",
      "tensor(44., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 88/100\n",
      "    step: 0; loss: 172521790443563053272268800.000; l2dist: 0.000\n",
      "tensor(10., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 88/100\n",
      "    step: 0; loss: 12.332; l2dist: 0.000\n",
      "    step: 50; loss: 9.619; l2dist: 1.097\n",
      "    step: 100; loss: 8.886; l2dist: 1.192\n",
      "    step: 150; loss: 8.284; l2dist: 1.219\n",
      "    step: 200; loss: 8.193; l2dist: 1.217\n",
      "    step: 250; loss: 8.163; l2dist: 1.216\n",
      "    step: 300; loss: 8.150; l2dist: 1.216\n",
      "    step: 350; loss: 8.144; l2dist: 1.216\n",
      "    step: 400; loss: 8.140; l2dist: 1.215\n",
      "    step: 450; loss: 8.138; l2dist: 1.215\n",
      "tensor(32., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 95.903; l2dist: 0.000\n",
      "    step: 50; loss: 53.270; l2dist: 2.335\n",
      "    step: 100; loss: 44.893; l2dist: 1.906\n",
      "    step: 150; loss: 44.060; l2dist: 1.765\n",
      "    step: 200; loss: 43.841; l2dist: 1.732\n",
      "    step: 250; loss: 43.744; l2dist: 1.716\n",
      "    step: 300; loss: 43.701; l2dist: 1.710\n",
      "    step: 350; loss: 43.680; l2dist: 1.706\n",
      "    step: 400; loss: 43.671; l2dist: 1.705\n",
      "    step: 450; loss: 43.664; l2dist: 1.703\n",
      "tensor(79., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 88/100\n",
      "    step: 0; loss: 194.574; l2dist: 0.000\n",
      "    step: 50; loss: 103.461; l2dist: 2.188\n",
      "    step: 100; loss: 97.391; l2dist: 1.900\n",
      "    step: 150; loss: 96.639; l2dist: 1.786\n",
      "    step: 200; loss: 96.246; l2dist: 1.729\n",
      "    step: 250; loss: 95.993; l2dist: 1.691\n",
      "    step: 300; loss: 95.856; l2dist: 1.669\n",
      "    step: 350; loss: 95.765; l2dist: 1.654\n",
      "    step: 400; loss: 95.683; l2dist: 1.639\n",
      "    step: 450; loss: 95.628; l2dist: 1.629\n",
      "tensor(79., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 93/100\n",
      "    step: 0; loss: 918.531; l2dist: 0.000\n",
      "    step: 50; loss: 470.640; l2dist: 1.938\n",
      "    step: 100; loss: 465.099; l2dist: 1.808\n",
      "    step: 150; loss: 464.518; l2dist: 1.728\n",
      "    step: 200; loss: 464.278; l2dist: 1.695\n",
      "    step: 250; loss: 464.135; l2dist: 1.676\n",
      "    step: 300; loss: 464.019; l2dist: 1.663\n",
      "    step: 350; loss: 463.918; l2dist: 1.652\n",
      "    step: 400; loss: 463.834; l2dist: 1.643\n",
      "    step: 450; loss: 463.758; l2dist: 1.634\n",
      "tensor(72., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 94/100\n",
      "    step: 0; loss: 7129.562; l2dist: 0.000\n",
      "    step: 50; loss: 3682.152; l2dist: 1.793\n",
      "    step: 100; loss: 3666.417; l2dist: 1.718\n",
      "    step: 150; loss: 3665.806; l2dist: 1.653\n",
      "    step: 200; loss: 3665.648; l2dist: 1.630\n",
      "tensor(53., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 94/100\n",
      "    step: 0; loss: 70040.039; l2dist: 0.000\n",
      "    step: 50; loss: 36169.027; l2dist: 1.764\n",
      "    step: 100; loss: 36042.402; l2dist: 1.686\n",
      "    step: 150; loss: 36041.148; l2dist: 1.631\n",
      "tensor(44., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 94/100\n",
      "    step: 0; loss: 699487.500; l2dist: 0.000\n",
      "    step: 50; loss: 361198.219; l2dist: 1.765\n",
      "    step: 100; loss: 359961.469; l2dist: 1.686\n",
      "    step: 150; loss: 359953.562; l2dist: 1.632\n",
      "tensor(44., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 94/100\n",
      "    step: 0; loss: 6994166.000; l2dist: 0.000\n",
      "    step: 50; loss: 3611584.500; l2dist: 1.766\n",
      "    step: 100; loss: 3599245.750; l2dist: 1.685\n",
      "    step: 150; loss: 3599170.750; l2dist: 1.635\n",
      "tensor(45., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 94/100\n",
      "    step: 0; loss: 69940864.000; l2dist: 0.000\n",
      "    step: 50; loss: 36115404.000; l2dist: 1.769\n",
      "    step: 100; loss: 35992048.000; l2dist: 1.690\n",
      "    step: 150; loss: 35991304.000; l2dist: 1.637\n",
      "tensor(44., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 94/100\n",
      "    step: 0; loss: 69940774815237543823409152.000; l2dist: 0.000\n",
      "tensor(16., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 94/100\n",
      "    step: 0; loss: 13.668; l2dist: 0.000\n",
      "    step: 50; loss: 10.682; l2dist: 1.163\n",
      "    step: 100; loss: 9.640; l2dist: 1.246\n",
      "    step: 150; loss: 9.058; l2dist: 1.278\n",
      "    step: 200; loss: 8.961; l2dist: 1.278\n",
      "    step: 250; loss: 8.931; l2dist: 1.277\n",
      "    step: 300; loss: 8.918; l2dist: 1.276\n",
      "    step: 350; loss: 8.911; l2dist: 1.276\n",
      "    step: 400; loss: 8.907; l2dist: 1.276\n",
      "    step: 450; loss: 8.905; l2dist: 1.276\n",
      "tensor(33., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 33/100\n",
      "    step: 0; loss: 102.328; l2dist: 0.000\n",
      "    step: 50; loss: 55.881; l2dist: 2.335\n",
      "    step: 100; loss: 46.278; l2dist: 1.980\n",
      "    step: 150; loss: 45.262; l2dist: 1.845\n",
      "    step: 200; loss: 44.914; l2dist: 1.794\n",
      "    step: 250; loss: 44.781; l2dist: 1.774\n",
      "    step: 300; loss: 44.708; l2dist: 1.763\n",
      "    step: 350; loss: 44.670; l2dist: 1.758\n",
      "    step: 400; loss: 44.651; l2dist: 1.755\n",
      "    step: 450; loss: 44.641; l2dist: 1.753\n",
      "tensor(75., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 88/100\n",
      "    step: 0; loss: 204.692; l2dist: 0.000\n",
      "    step: 50; loss: 107.460; l2dist: 2.177\n",
      "    step: 100; loss: 99.824; l2dist: 1.943\n",
      "    step: 150; loss: 98.970; l2dist: 1.824\n",
      "    step: 200; loss: 98.617; l2dist: 1.777\n",
      "    step: 250; loss: 98.403; l2dist: 1.746\n",
      "    step: 300; loss: 98.283; l2dist: 1.729\n",
      "    step: 350; loss: 98.192; l2dist: 1.716\n",
      "    step: 400; loss: 98.112; l2dist: 1.704\n",
      "    step: 450; loss: 98.059; l2dist: 1.696\n",
      "tensor(74., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 92/100\n",
      "    step: 0; loss: 1225.264; l2dist: 0.000\n",
      "    step: 50; loss: 594.365; l2dist: 1.962\n",
      "    step: 100; loss: 580.467; l2dist: 1.846\n",
      "    step: 150; loss: 579.804; l2dist: 1.771\n",
      "    step: 200; loss: 579.616; l2dist: 1.744\n",
      "    step: 250; loss: 579.502; l2dist: 1.729\n",
      "    step: 300; loss: 579.419; l2dist: 1.719\n",
      "    step: 350; loss: 579.354; l2dist: 1.712\n",
      "    step: 400; loss: 579.299; l2dist: 1.705\n",
      "tensor(73., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 92/100\n",
      "    step: 0; loss: 11736.792; l2dist: 0.000\n",
      "    step: 50; loss: 5614.220; l2dist: 1.812\n",
      "    step: 100; loss: 5522.464; l2dist: 1.762\n",
      "    step: 150; loss: 5521.625; l2dist: 1.699\n",
      "    step: 200; loss: 5521.490; l2dist: 1.680\n",
      "tensor(51., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 92/100\n",
      "    step: 0; loss: 116996.977; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 55886.492; l2dist: 1.786\n",
      "    step: 100; loss: 55009.500; l2dist: 1.727\n",
      "    step: 150; loss: 55006.242; l2dist: 1.677\n",
      "tensor(46., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 92/100\n",
      "    step: 0; loss: 1169630.375; l2dist: 0.000\n",
      "    step: 50; loss: 558626.812; l2dist: 1.774\n",
      "    step: 100; loss: 549897.188; l2dist: 1.714\n",
      "    step: 150; loss: 549869.562; l2dist: 1.671\n",
      "tensor(41., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 92/100\n",
      "    step: 0; loss: 11695959.000; l2dist: 0.000\n",
      "    step: 50; loss: 5586028.500; l2dist: 1.772\n",
      "    step: 100; loss: 5498772.500; l2dist: 1.714\n",
      "    step: 150; loss: 5498501.000; l2dist: 1.671\n",
      "tensor(43., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 92/100\n",
      "    step: 0; loss: 116959248.000; l2dist: 0.000\n",
      "    step: 50; loss: 55860044.000; l2dist: 1.771\n",
      "    step: 100; loss: 54987528.000; l2dist: 1.717\n",
      "    step: 150; loss: 54984816.000; l2dist: 1.670\n",
      "tensor(43., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 92/100\n",
      "    step: 0; loss: 116959201169369903489089536.000; l2dist: 0.000\n",
      "tensor(9., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 92/100\n",
      "    step: 0; loss: 13.273; l2dist: 0.000\n",
      "    step: 50; loss: 10.305; l2dist: 1.154\n",
      "    step: 100; loss: 9.477; l2dist: 1.238\n",
      "    step: 150; loss: 8.962; l2dist: 1.250\n",
      "    step: 200; loss: 8.871; l2dist: 1.250\n",
      "    step: 250; loss: 8.845; l2dist: 1.249\n",
      "    step: 300; loss: 8.831; l2dist: 1.249\n",
      "    step: 350; loss: 8.825; l2dist: 1.248\n",
      "    step: 400; loss: 8.821; l2dist: 1.248\n",
      "    step: 450; loss: 8.819; l2dist: 1.248\n",
      "tensor(36., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 36/100\n",
      "    step: 0; loss: 101.566; l2dist: 0.000\n",
      "    step: 50; loss: 55.080; l2dist: 2.335\n",
      "    step: 100; loss: 48.520; l2dist: 1.883\n",
      "    step: 150; loss: 47.684; l2dist: 1.774\n",
      "    step: 200; loss: 47.420; l2dist: 1.739\n",
      "    step: 250; loss: 47.320; l2dist: 1.725\n",
      "    step: 300; loss: 47.276; l2dist: 1.719\n",
      "    step: 350; loss: 47.256; l2dist: 1.716\n",
      "    step: 400; loss: 47.246; l2dist: 1.715\n",
      "    step: 450; loss: 47.241; l2dist: 1.714\n",
      "tensor(70., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 78/100\n",
      "    step: 0; loss: 411.289; l2dist: 0.000\n",
      "    step: 50; loss: 197.876; l2dist: 2.355\n",
      "    step: 100; loss: 192.800; l2dist: 2.074\n",
      "    step: 150; loss: 191.668; l2dist: 1.932\n",
      "    step: 200; loss: 191.092; l2dist: 1.862\n",
      "    step: 250; loss: 190.692; l2dist: 1.811\n",
      "    step: 300; loss: 190.418; l2dist: 1.774\n",
      "    step: 350; loss: 190.176; l2dist: 1.741\n",
      "    step: 400; loss: 190.003; l2dist: 1.716\n",
      "    step: 450; loss: 189.883; l2dist: 1.698\n",
      "tensor(69., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 85/100\n",
      "    step: 0; loss: 2502.768; l2dist: 0.000\n",
      "    step: 50; loss: 1149.712; l2dist: 2.198\n",
      "    step: 100; loss: 1142.914; l2dist: 2.052\n",
      "    step: 150; loss: 1142.218; l2dist: 1.972\n",
      "    step: 200; loss: 1141.862; l2dist: 1.927\n",
      "    step: 250; loss: 1141.612; l2dist: 1.897\n",
      "    step: 300; loss: 1141.407; l2dist: 1.874\n",
      "    step: 350; loss: 1141.216; l2dist: 1.852\n",
      "    step: 400; loss: 1141.047; l2dist: 1.832\n",
      "    step: 450; loss: 1140.894; l2dist: 1.814\n",
      "tensor(62., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 86/100\n",
      "    step: 0; loss: 21995.230; l2dist: 0.000\n",
      "    step: 50; loss: 10148.924; l2dist: 2.093\n",
      "    step: 100; loss: 10114.413; l2dist: 1.978\n",
      "    step: 150; loss: 10113.763; l2dist: 1.915\n",
      "tensor(52., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 86/100\n",
      "    step: 0; loss: 218188.156; l2dist: 0.000\n",
      "    step: 50; loss: 100668.891; l2dist: 2.039\n",
      "    step: 100; loss: 100350.898; l2dist: 1.926\n",
      "    step: 150; loss: 100349.695; l2dist: 1.881\n",
      "tensor(51., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 86/100\n",
      "    step: 0; loss: 2180747.750; l2dist: 0.000\n",
      "    step: 50; loss: 1006130.375; l2dist: 2.019\n",
      "    step: 100; loss: 1002975.812; l2dist: 1.916\n",
      "    step: 150; loss: 1002968.688; l2dist: 1.868\n",
      "tensor(42., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 86/100\n",
      "    step: 0; loss: 21806632.000; l2dist: 0.000\n",
      "    step: 50; loss: 10060861.000; l2dist: 2.011\n",
      "    step: 100; loss: 10029341.000; l2dist: 1.910\n",
      "    step: 150; loss: 10029273.000; l2dist: 1.864\n",
      "tensor(41., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 86/100\n",
      "    step: 0; loss: 218065584.000; l2dist: 0.000\n",
      "    step: 50; loss: 100608224.000; l2dist: 2.012\n",
      "    step: 100; loss: 100293024.000; l2dist: 1.907\n",
      "    step: 150; loss: 100292384.000; l2dist: 1.864\n",
      "tensor(42., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 86/100\n",
      "    step: 0; loss: 218065510289466776543559680.000; l2dist: 0.000\n",
      "tensor(14., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 86/100\n",
      "    step: 0; loss: 12.695; l2dist: 0.000\n",
      "    step: 50; loss: 9.941; l2dist: 1.147\n",
      "    step: 100; loss: 9.205; l2dist: 1.216\n",
      "    step: 150; loss: 8.642; l2dist: 1.243\n",
      "    step: 200; loss: 8.553; l2dist: 1.243\n",
      "    step: 250; loss: 8.526; l2dist: 1.241\n",
      "    step: 300; loss: 8.514; l2dist: 1.241\n",
      "    step: 350; loss: 8.508; l2dist: 1.240\n",
      "    step: 400; loss: 8.505; l2dist: 1.240\n",
      "    step: 450; loss: 8.502; l2dist: 1.240\n",
      "tensor(32., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 32/100\n",
      "    step: 0; loss: 95.479; l2dist: 0.000\n",
      "    step: 50; loss: 53.613; l2dist: 2.381\n",
      "    step: 100; loss: 47.093; l2dist: 1.911\n",
      "    step: 150; loss: 46.274; l2dist: 1.784\n",
      "    step: 200; loss: 46.021; l2dist: 1.746\n",
      "    step: 250; loss: 45.945; l2dist: 1.733\n",
      "    step: 300; loss: 45.911; l2dist: 1.728\n",
      "    step: 350; loss: 45.894; l2dist: 1.725\n",
      "    step: 400; loss: 45.887; l2dist: 1.724\n",
      "    step: 450; loss: 45.882; l2dist: 1.723\n",
      "tensor(76., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 84/100\n",
      "    step: 0; loss: 275.172; l2dist: 0.000\n",
      "    step: 50; loss: 148.665; l2dist: 2.261\n",
      "    step: 100; loss: 142.562; l2dist: 1.952\n",
      "    step: 150; loss: 141.733; l2dist: 1.829\n",
      "    step: 200; loss: 141.314; l2dist: 1.776\n",
      "    step: 250; loss: 141.040; l2dist: 1.739\n",
      "    step: 300; loss: 140.843; l2dist: 1.711\n",
      "    step: 350; loss: 140.718; l2dist: 1.692\n",
      "    step: 400; loss: 140.614; l2dist: 1.676\n",
      "    step: 450; loss: 140.551; l2dist: 1.666\n",
      "tensor(68., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 87/100\n",
      "    step: 0; loss: 2038.274; l2dist: 0.000\n",
      "    step: 50; loss: 1042.459; l2dist: 2.068\n",
      "    step: 100; loss: 1029.871; l2dist: 1.921\n",
      "    step: 150; loss: 1029.356; l2dist: 1.856\n",
      "    step: 200; loss: 1029.139; l2dist: 1.831\n",
      "    step: 250; loss: 1028.985; l2dist: 1.814\n",
      "    step: 300; loss: 1028.853; l2dist: 1.800\n",
      "    step: 350; loss: 1028.729; l2dist: 1.786\n",
      "    step: 400; loss: 1028.611; l2dist: 1.772\n",
      "    step: 450; loss: 1028.496; l2dist: 1.758\n",
      "tensor(65., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 87/100\n",
      "    step: 0; loss: 19940.135; l2dist: 0.000\n",
      "    step: 50; loss: 10117.446; l2dist: 1.916\n",
      "    step: 100; loss: 10029.507; l2dist: 1.852\n",
      "    step: 150; loss: 10028.810; l2dist: 1.803\n",
      "tensor(44., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 87/100\n",
      "    step: 0; loss: 199032.391; l2dist: 0.000\n",
      "    step: 50; loss: 100902.375; l2dist: 1.895\n",
      "    step: 100; loss: 100057.078; l2dist: 1.838\n",
      "    step: 150; loss: 100053.930; l2dist: 1.797\n",
      "tensor(42., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 87/100\n",
      "    step: 0; loss: 1989998.250; l2dist: 0.000\n",
      "    step: 50; loss: 1008773.875; l2dist: 1.882\n",
      "    step: 100; loss: 1000355.312; l2dist: 1.827\n",
      "    step: 150; loss: 1000327.312; l2dist: 1.793\n",
      "tensor(40., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 87/100\n",
      "    step: 0; loss: 19899660.000; l2dist: 0.000\n",
      "    step: 50; loss: 10087492.000; l2dist: 1.876\n",
      "    step: 100; loss: 10003341.000; l2dist: 1.822\n",
      "    step: 150; loss: 10003064.000; l2dist: 1.789\n",
      "tensor(37., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 87/100\n",
      "    step: 0; loss: 198996288.000; l2dist: 0.000\n",
      "    step: 50; loss: 100874664.000; l2dist: 1.878\n",
      "    step: 100; loss: 100033208.000; l2dist: 1.826\n",
      "    step: 150; loss: 100030440.000; l2dist: 1.790\n",
      "tensor(34., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 87/100\n",
      "    step: 0; loss: 198996262390245822398726144.000; l2dist: 0.000\n",
      "tensor(9., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 13.707; l2dist: 0.000\n",
      "    step: 50; loss: 10.731; l2dist: 1.142\n",
      "    step: 100; loss: 9.641; l2dist: 1.242\n",
      "    step: 150; loss: 9.016; l2dist: 1.277\n",
      "    step: 200; loss: 8.921; l2dist: 1.277\n",
      "    step: 250; loss: 8.893; l2dist: 1.275\n",
      "    step: 300; loss: 8.881; l2dist: 1.275\n",
      "    step: 350; loss: 8.874; l2dist: 1.274\n",
      "    step: 400; loss: 8.870; l2dist: 1.274\n",
      "    step: 450; loss: 8.868; l2dist: 1.274\n",
      "tensor(26., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 26/100\n",
      "    step: 0; loss: 113.762; l2dist: 0.000\n",
      "    step: 50; loss: 62.509; l2dist: 2.574\n",
      "    step: 100; loss: 51.873; l2dist: 2.078\n",
      "    step: 150; loss: 50.842; l2dist: 1.928\n",
      "    step: 200; loss: 50.480; l2dist: 1.872\n",
      "    step: 250; loss: 50.333; l2dist: 1.850\n",
      "    step: 300; loss: 50.275; l2dist: 1.840\n",
      "    step: 350; loss: 50.246; l2dist: 1.836\n",
      "    step: 400; loss: 50.228; l2dist: 1.833\n",
      "    step: 450; loss: 50.219; l2dist: 1.832\n",
      "tensor(77., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 85/100\n",
      "    step: 0; loss: 276.778; l2dist: 0.000\n",
      "    step: 50; loss: 149.449; l2dist: 2.391\n",
      "    step: 100; loss: 132.013; l2dist: 2.068\n",
      "    step: 150; loss: 131.062; l2dist: 1.921\n",
      "    step: 200; loss: 130.698; l2dist: 1.869\n",
      "    step: 250; loss: 130.474; l2dist: 1.836\n",
      "    step: 300; loss: 130.338; l2dist: 1.816\n",
      "    step: 350; loss: 130.245; l2dist: 1.802\n",
      "    step: 400; loss: 130.179; l2dist: 1.791\n",
      "    step: 450; loss: 130.123; l2dist: 1.783\n",
      "tensor(80., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 91/100\n",
      "    step: 0; loss: 1399.110; l2dist: 0.000\n",
      "    step: 50; loss: 707.216; l2dist: 2.114\n",
      "    step: 100; loss: 685.139; l2dist: 1.954\n",
      "    step: 150; loss: 684.502; l2dist: 1.867\n",
      "    step: 200; loss: 684.288; l2dist: 1.835\n",
      "    step: 250; loss: 684.137; l2dist: 1.815\n",
      "    step: 300; loss: 684.025; l2dist: 1.801\n",
      "    step: 350; loss: 683.940; l2dist: 1.790\n",
      "    step: 400; loss: 683.878; l2dist: 1.782\n",
      "tensor(63., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 92/100\n",
      "    step: 0; loss: 12674.857; l2dist: 0.000\n",
      "    step: 50; loss: 6167.500; l2dist: 1.991\n",
      "    step: 100; loss: 6048.231; l2dist: 1.900\n",
      "    step: 150; loss: 6047.344; l2dist: 1.822\n",
      "    step: 200; loss: 6047.181; l2dist: 1.800\n",
      "tensor(50., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 92/100\n",
      "    step: 0; loss: 125836.938; l2dist: 0.000\n",
      "    step: 50; loss: 61050.445; l2dist: 1.976\n",
      "    step: 100; loss: 59934.148; l2dist: 1.879\n",
      "    step: 150; loss: 59930.719; l2dist: 1.811\n",
      "tensor(44., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 92/100\n",
      "    step: 0; loss: 1257440.250; l2dist: 0.000\n",
      "    step: 50; loss: 609841.938; l2dist: 1.971\n",
      "    step: 100; loss: 598745.188; l2dist: 1.877\n",
      "    step: 150; loss: 598716.250; l2dist: 1.810\n",
      "tensor(52., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 92/100\n",
      "    step: 0; loss: 12573437.000; l2dist: 0.000\n",
      "    step: 50; loss: 6097719.500; l2dist: 1.966\n",
      "    step: 100; loss: 5986813.500; l2dist: 1.875\n",
      "    step: 150; loss: 5986528.500; l2dist: 1.807\n",
      "tensor(40., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 92/100\n",
      "    step: 0; loss: 125733384.000; l2dist: 0.000\n",
      "    step: 50; loss: 60976476.000; l2dist: 1.969\n",
      "    step: 100; loss: 59867464.000; l2dist: 1.875\n",
      "    step: 150; loss: 59864624.000; l2dist: 1.809\n",
      "tensor(50., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 92/100\n",
      "    step: 0; loss: 125733256637449298893602816.000; l2dist: 0.000\n",
      "tensor(12., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 92/100\n",
      "    step: 0; loss: 14.465; l2dist: 0.000\n",
      "    step: 50; loss: 11.345; l2dist: 1.203\n",
      "    step: 100; loss: 10.214; l2dist: 1.331\n",
      "    step: 150; loss: 9.616; l2dist: 1.330\n",
      "    step: 200; loss: 9.530; l2dist: 1.329\n",
      "    step: 250; loss: 9.502; l2dist: 1.328\n",
      "    step: 300; loss: 9.490; l2dist: 1.327\n",
      "    step: 350; loss: 9.483; l2dist: 1.327\n",
      "    step: 400; loss: 9.479; l2dist: 1.327\n",
      "    step: 450; loss: 9.477; l2dist: 1.326\n",
      "tensor(31., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 31/100\n",
      "    step: 0; loss: 107.727; l2dist: 0.000\n",
      "    step: 50; loss: 59.283; l2dist: 2.513\n",
      "    step: 100; loss: 51.189; l2dist: 2.024\n",
      "    step: 150; loss: 50.113; l2dist: 1.883\n",
      "    step: 200; loss: 49.805; l2dist: 1.830\n",
      "    step: 250; loss: 49.713; l2dist: 1.814\n",
      "    step: 300; loss: 49.672; l2dist: 1.807\n",
      "    step: 350; loss: 49.651; l2dist: 1.803\n",
      "    step: 400; loss: 49.639; l2dist: 1.801\n",
      "    step: 450; loss: 49.633; l2dist: 1.800\n",
      "tensor(63., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 77/100\n",
      "    step: 0; loss: 414.334; l2dist: 0.000\n",
      "    step: 50; loss: 211.302; l2dist: 2.536\n",
      "    step: 100; loss: 201.205; l2dist: 2.240\n",
      "    step: 150; loss: 200.078; l2dist: 2.102\n",
      "    step: 200; loss: 199.538; l2dist: 2.033\n",
      "    step: 250; loss: 199.075; l2dist: 1.971\n",
      "    step: 300; loss: 198.754; l2dist: 1.926\n",
      "    step: 350; loss: 198.545; l2dist: 1.894\n",
      "    step: 400; loss: 198.390; l2dist: 1.870\n",
      "    step: 450; loss: 198.250; l2dist: 1.848\n",
      "tensor(73., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 89/100\n",
      "    step: 0; loss: 2045.154; l2dist: 0.000\n",
      "    step: 50; loss: 984.736; l2dist: 2.304\n",
      "    step: 100; loss: 968.240; l2dist: 2.166\n",
      "    step: 150; loss: 967.453; l2dist: 2.093\n",
      "    step: 200; loss: 967.091; l2dist: 2.045\n",
      "    step: 250; loss: 966.881; l2dist: 2.017\n",
      "    step: 300; loss: 966.705; l2dist: 1.994\n",
      "    step: 350; loss: 966.565; l2dist: 1.976\n",
      "    step: 400; loss: 966.448; l2dist: 1.961\n",
      "    step: 450; loss: 966.326; l2dist: 1.946\n",
      "tensor(64., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 89/100\n",
      "    step: 0; loss: 19285.000; l2dist: 0.000\n",
      "    step: 50; loss: 9173.229; l2dist: 2.167\n",
      "    step: 100; loss: 9070.261; l2dist: 2.079\n",
      "    step: 150; loss: 9069.116; l2dist: 2.020\n",
      "    step: 200; loss: 9068.888; l2dist: 1.990\n",
      "tensor(49., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 89/100\n",
      "    step: 0; loss: 192032.609; l2dist: 0.000\n",
      "    step: 50; loss: 91243.328; l2dist: 2.125\n",
      "    step: 100; loss: 90266.727; l2dist: 2.045\n",
      "    step: 150; loss: 90261.188; l2dist: 1.991\n",
      "tensor(46., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 89/100\n",
      "    step: 0; loss: 1919656.375; l2dist: 0.000\n",
      "    step: 50; loss: 912019.438; l2dist: 2.098\n",
      "    step: 100; loss: 902305.125; l2dist: 2.025\n",
      "    step: 150; loss: 902255.312; l2dist: 1.980\n",
      "tensor(43., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 89/100\n",
      "    step: 0; loss: 19195986.000; l2dist: 0.000\n",
      "    step: 50; loss: 9119824.000; l2dist: 2.089\n",
      "    step: 100; loss: 9022733.000; l2dist: 2.020\n",
      "    step: 150; loss: 9022240.000; l2dist: 1.975\n",
      "tensor(43., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 89/100\n",
      "    step: 0; loss: 191959280.000; l2dist: 0.000\n",
      "    step: 50; loss: 91197888.000; l2dist: 2.087\n",
      "    step: 100; loss: 90227016.000; l2dist: 2.016\n",
      "    step: 150; loss: 90222096.000; l2dist: 1.970\n",
      "tensor(47., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 89/100\n",
      "    step: 0; loss: 191959216907751176357806080.000; l2dist: 0.000\n",
      "tensor(6., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 89/100\n",
      "    step: 0; loss: 13.666; l2dist: 0.000\n",
      "    step: 50; loss: 10.549; l2dist: 1.165\n",
      "    step: 100; loss: 9.587; l2dist: 1.254\n",
      "    step: 150; loss: 8.998; l2dist: 1.292\n",
      "    step: 200; loss: 8.904; l2dist: 1.291\n",
      "    step: 250; loss: 8.875; l2dist: 1.289\n",
      "    step: 300; loss: 8.862; l2dist: 1.288\n",
      "    step: 350; loss: 8.855; l2dist: 1.288\n",
      "    step: 400; loss: 8.851; l2dist: 1.288\n",
      "    step: 450; loss: 8.849; l2dist: 1.287\n",
      "tensor(33., device='cuda:0')\n",
      "binary step: 0; number of successful adv: 33/100\n",
      "    step: 0; loss: 102.549; l2dist: 0.000\n",
      "    step: 50; loss: 54.545; l2dist: 2.446\n",
      "    step: 100; loss: 46.323; l2dist: 1.965\n",
      "    step: 150; loss: 45.332; l2dist: 1.822\n",
      "    step: 200; loss: 45.050; l2dist: 1.783\n",
      "    step: 250; loss: 44.934; l2dist: 1.765\n",
      "    step: 300; loss: 44.885; l2dist: 1.756\n",
      "    step: 350; loss: 44.860; l2dist: 1.751\n",
      "    step: 400; loss: 44.850; l2dist: 1.750\n",
      "    step: 450; loss: 44.843; l2dist: 1.748\n",
      "tensor(73., device='cuda:0')\n",
      "binary step: 1; number of successful adv: 83/100\n",
      "    step: 0; loss: 315.383; l2dist: 0.000\n",
      "    step: 50; loss: 144.837; l2dist: 2.354\n",
      "    step: 100; loss: 138.603; l2dist: 2.074\n",
      "    step: 150; loss: 137.571; l2dist: 1.942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 200; loss: 137.036; l2dist: 1.876\n",
      "    step: 250; loss: 136.689; l2dist: 1.833\n",
      "    step: 300; loss: 136.447; l2dist: 1.799\n",
      "    step: 350; loss: 136.253; l2dist: 1.772\n",
      "    step: 400; loss: 136.077; l2dist: 1.746\n",
      "    step: 450; loss: 135.932; l2dist: 1.725\n",
      "tensor(78., device='cuda:0')\n",
      "binary step: 2; number of successful adv: 91/100\n",
      "    step: 0; loss: 1411.227; l2dist: 0.000\n",
      "    step: 50; loss: 662.034; l2dist: 2.111\n",
      "    step: 100; loss: 653.267; l2dist: 1.962\n",
      "    step: 150; loss: 652.532; l2dist: 1.879\n",
      "    step: 200; loss: 652.208; l2dist: 1.839\n",
      "    step: 250; loss: 651.990; l2dist: 1.814\n",
      "    step: 300; loss: 651.786; l2dist: 1.790\n",
      "    step: 350; loss: 651.632; l2dist: 1.770\n",
      "    step: 400; loss: 651.506; l2dist: 1.755\n",
      "    step: 450; loss: 651.406; l2dist: 1.743\n",
      "tensor(70., device='cuda:0')\n",
      "binary step: 3; number of successful adv: 91/100\n",
      "    step: 0; loss: 13120.391; l2dist: 0.000\n",
      "    step: 50; loss: 6135.010; l2dist: 1.969\n",
      "    step: 100; loss: 6085.607; l2dist: 1.868\n",
      "    step: 150; loss: 6084.874; l2dist: 1.796\n",
      "    step: 200; loss: 6084.634; l2dist: 1.768\n",
      "tensor(49., device='cuda:0')\n",
      "binary step: 4; number of successful adv: 91/100\n",
      "    step: 0; loss: 130555.477; l2dist: 0.000\n",
      "    step: 50; loss: 61002.582; l2dist: 1.956\n",
      "    step: 100; loss: 60541.547; l2dist: 1.831\n",
      "    step: 150; loss: 60539.719; l2dist: 1.773\n",
      "tensor(46., device='cuda:0')\n",
      "binary step: 5; number of successful adv: 91/100\n",
      "    step: 0; loss: 1305050.000; l2dist: 0.000\n",
      "    step: 50; loss: 609733.750; l2dist: 1.937\n",
      "    step: 100; loss: 605158.000; l2dist: 1.815\n",
      "    step: 150; loss: 605144.812; l2dist: 1.760\n",
      "tensor(49., device='cuda:0')\n",
      "binary step: 6; number of successful adv: 91/100\n",
      "    step: 0; loss: 13050046.000; l2dist: 0.000\n",
      "    step: 50; loss: 6097068.000; l2dist: 1.925\n",
      "    step: 100; loss: 6051343.000; l2dist: 1.807\n",
      "    step: 150; loss: 6051218.000; l2dist: 1.756\n",
      "tensor(38., device='cuda:0')\n",
      "binary step: 7; number of successful adv: 91/100\n",
      "    step: 0; loss: 130500032.000; l2dist: 0.000\n",
      "    step: 50; loss: 60970424.000; l2dist: 1.927\n",
      "    step: 100; loss: 60513212.000; l2dist: 1.809\n",
      "    step: 150; loss: 60511952.000; l2dist: 1.757\n",
      "tensor(48., device='cuda:0')\n",
      "binary step: 8; number of successful adv: 91/100\n",
      "    step: 0; loss: 130499996763188252433711104.000; l2dist: 0.000\n",
      "tensor(11., device='cuda:0')\n",
      "binary step: 9; number of successful adv: 91/100\n"
     ]
    }
   ],
   "source": [
    "# attack = DKNNAttack()\n",
    "\n",
    "from lib.dknn_attack_l2 import DKNNL2Attack\n",
    "attack = DKNNL2Attack()\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            dknn, x[begin:end], y[begin:end],\n",
    "            guide_layer=layers[0], m=100, binary_search_steps=10,\n",
    "            max_iterations=500, learning_rate=1e-1,\n",
    "            initial_const=1e-1, abort_early=True)\n",
    "    return x_adv\n",
    "\n",
    "num = 1000\n",
    "x_adv = attack_batch(x_test[:num].cuda(), y_test[:num], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.105"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dknn.classify(x_adv)\n",
    "(y_pred.argmax(1) == y_test[:num].numpy()).sum() / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([864.,  26.,  13.,  12.,   9.,  16.,   0.,   0.,   0.,  60.]),\n",
       " array([0.0365 , 0.13285, 0.2292 , 0.32555, 0.4219 , 0.51825, 0.6146 ,\n",
       "        0.71095, 0.8073 , 0.90365, 1.     ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cred = dknn.credibility(y_pred)\n",
    "plt.hist(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADQNJREFUeJzt3W+MVfWdx/HPZylNjPQBWLHEgnQb3bgaAzoaE3AzamxYbYKN1NQHGzbZMH2AZps0ZA1PypMmjemfrU9IpikpJtSWhFbRGBeDGylRGwejBYpQICzMgkAzJgUT0yDfPphDO8W5v3u5/84dv+9XQube8z1/vrnhM+ecOefcnyNCAPL5h7obAFAPwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+IKnP9HNjtrmdEOixiHAr83W057e9wvZB24dtP9nJugD0l9u9t9/2LEmHJD0gaVzSW5Iei4jfF5Zhzw/0WD/2/HdJOhwRRyPiz5J+IWllB+sD0EedhP96SSemvB+vpv0d2yO2x2yPdbAtAF3WyR/8pju0+MRhfUSMShqVOOwHBkkne/5xSQunvP+ipJOdtQOgXzoJ/1uSbrT9JduflfQNSdu70xaAXmv7sD8iLth+XNL/SJolaVNE7O9aZwB6qu1LfW1tjHN+oOf6cpMPgJmL8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaTaHqJbkmwfk3RO0seSLkTEUDeaAtB7HYW/cm9E/LEL6wHQRxz2A0l1Gv6QtMP2Htsj3WgIQH90eti/LCJO2p4v6RXb70XErqkzVL8U+MUADBhHRHdWZG+QdD4ivl+YpzsbA9BQRLiV+do+7Ld9te3PXXot6SuS9rW7PgD91clh/3WSfm370np+HhEvd6UrAD3XtcP+ljbGYT/Qcz0/7AcwsxF+ICnCDyRF+IGkCD+QFOEHkurGU30prFq1qmFtzZo1xWVPnjxZrH/00UfF+pYtW4r1999/v2Ht8OHDxWWRF3t+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iKR3pbdPTo0Ya1xYsX96+RaZw7d65hbf/+/X3sZLCMj483rD311FPFZcfGxrrdTt/wSC+AIsIPJEX4gaQIP5AU4QeSIvxAUoQfSIrn+VtUemb/tttuKy574MCBYv3mm28u1m+//fZifXh4uGHt7rvvLi574sSJYn3hwoXFeicuXLhQrJ89e7ZYX7BgQdvbPn78eLE+k6/zt4o9P5AU4QeSIvxAUoQfSIrwA0kRfiApwg8k1fR5ftubJH1V0pmIuLWaNk/SLyUtlnRM0qMR8UHTjc3g5/kH2dy5cxvWlixZUlx2z549xfqdd97ZVk+taDZewaFDh4r1ZvdPzJs3r2Ft7dq1xWU3btxYrA+ybj7P/zNJKy6b9qSknRFxo6Sd1XsAM0jT8EfELkkTl01eKWlz9XqzpIe73BeAHmv3nP+6iDglSdXP+d1rCUA/9PzeftsjkkZ6vR0AV6bdPf9p2wskqfp5ptGMETEaEUMRMdTmtgD0QLvh3y5pdfV6taTnu9MOgH5pGn7bz0p6Q9I/2R63/R+SvifpAdt/kPRA9R7ADML39mNgPfLII8X61q1bi/V9+/Y1rN17773FZScmLr/ANXPwvf0Aigg/kBThB5Ii/EBShB9IivADSXGpD7WZP7/8SMjevXs7Wn7VqlUNa9u2bSsuO5NxqQ9AEeEHkiL8QFKEH0iK8ANJEX4gKcIPJMUQ3ahNs6/Pvvbaa4v1Dz4of1v8wYMHr7inTNjzA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBSPM+Pnlq2bFnD2quvvlpcdvbs2cX68PBwsb5r165i/dOK5/kBFBF+ICnCDyRF+IGkCD+QFOEHkiL8QFJNn+e3vUnSVyWdiYhbq2kbJK2RdLaabX1EvNSrJjFzPfjggw1rza7j79y5s1h/44032uoJk1rZ8/9M0opppv8oIpZU/wg+MMM0DX9E7JI00YdeAPRRJ+f8j9v+ne1Ntud2rSMAfdFu+DdK+rKkJZJOSfpBoxltj9gesz3W5rYA9EBb4Y+I0xHxcURclPQTSXcV5h2NiKGIGGq3SQDd11b4bS+Y8vZrkvZ1px0A/dLKpb5nJQ1L+rztcUnfkTRse4mkkHRM0jd72COAHuB5fnTkqquuKtZ3797dsHbLLbcUl73vvvuK9ddff71Yz4rn+QEUEX4gKcIPJEX4gaQIP5AU4QeSYohudGTdunXF+tKlSxvWXn755eKyXMrrLfb8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AUj/Si6KGHHirWn3vuuWL9ww8/bFhbsWK6L4X+mzfffLNYx/R4pBdAEeEHkiL8QFKEH0iK8ANJEX4gKcIPJMXz/Mldc801xfrTTz9drM+aNatYf+mlxgM4cx2/Xuz5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCpps/z214o6RlJX5B0UdJoRPzY9jxJv5S0WNIxSY9GxAdN1sXz/H3W7Dp8s2vtd9xxR7F+5MiRYr30zH6zZdGebj7Pf0HStyPiZkl3S1pr+58lPSlpZ0TcKGln9R7ADNE0/BFxKiLerl6fk3RA0vWSVkraXM22WdLDvWoSQPdd0Tm/7cWSlkr6raTrIuKUNPkLQtL8bjcHoHdavrff9hxJ2yR9KyL+ZLd0WiHbI5JG2msPQK+0tOe3PVuTwd8SEb+qJp+2vaCqL5B0ZrplI2I0IoYiYqgbDQPojqbh9+Qu/qeSDkTED6eUtktaXb1eLen57rcHoFdaudS3XNJvJO3V5KU+SVqvyfP+rZIWSTou6esRMdFkXVzq67ObbrqpWH/vvfc6Wv/KlSuL9RdeeKGj9ePKtXqpr+k5f0TsltRoZfdfSVMABgd3+AFJEX4gKcIPJEX4gaQIP5AU4QeS4qu7PwVuuOGGhrUdO3Z0tO5169YV6y+++GJH60d92PMDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFJc5/8UGBlp/C1pixYt6mjdr732WrHe7PsgMLjY8wNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUlznnwGWL19erD/xxBN96gSfJuz5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCpptf5bS+U9IykL0i6KGk0In5se4OkNZLOVrOuj4iXetVoZvfcc0+xPmfOnLbXfeTIkWL9/Pnzba8bg62Vm3wuSPp2RLxt+3OS9th+par9KCK+37v2APRK0/BHxClJp6rX52wfkHR9rxsD0FtXdM5ve7GkpZJ+W0163PbvbG+yPbfBMiO2x2yPddQpgK5qOfy250jaJulbEfEnSRslfVnSEk0eGfxguuUiYjQihiJiqAv9AuiSlsJve7Ymg78lIn4lSRFxOiI+joiLkn4i6a7etQmg25qG37Yl/VTSgYj44ZTpC6bM9jVJ+7rfHoBeaeWv/csk/Zukvbbfqaatl/SY7SWSQtIxSd/sSYfoyLvvvlus33///cX6xMREN9vBAGnlr/27JXmaEtf0gRmMO/yApAg/kBThB5Ii/EBShB9IivADSbmfQyzbZjxnoMciYrpL85/Anh9IivADSRF+ICnCDyRF+IGkCD+QFOEHkur3EN1/lPR/U95/vpo2iAa1t0HtS6K3dnWztxtanbGvN/l8YuP22KB+t9+g9jaofUn01q66euOwH0iK8ANJ1R3+0Zq3XzKovQ1qXxK9tauW3mo95wdQn7r3/ABqUkv4ba+wfdD2YdtP1tFDI7aP2d5r+526hxirhkE7Y3vflGnzbL9i+w/Vz2mHSauptw22/7/67N6x/WBNvS20/b+2D9jeb/s/q+m1fnaFvmr53Pp+2G97lqRDkh6QNC7pLUmPRcTv+9pIA7aPSRqKiNqvCdv+F0nnJT0TEbdW056SNBER36t+cc6NiP8akN42SDpf98jN1YAyC6aOLC3pYUn/rho/u0Jfj6qGz62OPf9dkg5HxNGI+LOkX0haWUMfAy8idkm6fNSMlZI2V683a/I/T9816G0gRMSpiHi7en1O0qWRpWv97Ap91aKO8F8v6cSU9+MarCG/Q9IO23tsj9TdzDSuq4ZNvzR8+vya+7lc05Gb++mykaUH5rNrZ8Trbqsj/NN9xdAgXXJYFhG3S/pXSWurw1u0pqWRm/tlmpGlB0K7I153Wx3hH5e0cMr7L0o6WUMf04qIk9XPM5J+rcEbffj0pUFSq59nau7nrwZp5ObpRpbWAHx2gzTidR3hf0vSjba/ZPuzkr4haXsNfXyC7aurP8TI9tWSvqLBG314u6TV1evVkp6vsZe/MygjNzcaWVo1f3aDNuJ1LTf5VJcy/lvSLEmbIuK7fW9iGrb/UZN7e2nyicef19mb7WclDWvyqa/Tkr4j6TlJWyUtknRc0tcjou9/eGvQ27AmD13/OnLzpXPsPve2XNJvJO2VdLGavF6T59e1fXaFvh5TDZ8bd/gBSXGHH5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpP4CIJjqosJxHysAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    plt.imshow(x_adv[i].cpu().detach().numpy().squeeze(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6346755"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_clean = dknn.classify(x_test[:num])\n",
    "ind = (y_clean.argmax(1) == y_test[:num].numpy()) & (y_pred.argmax(1) != y_test[:num].numpy())\n",
    "np.mean(np.sqrt(np.sum((x_adv.cpu().detach().numpy()[ind] - x_test.numpy()[:num][ind])**2, (1, 2, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7690771 , 1.5727358 , 2.7503514 , 1.2389098 , 1.6135488 ,\n",
       "       1.6507565 , 1.6114699 , 2.6745708 , 1.6988411 , 1.1849897 ,\n",
       "       1.7280196 , 1.073701  , 1.117185  , 3.6193776 , 0.20475903,\n",
       "       1.5696911 , 1.6756727 , 1.5497293 , 1.531895  , 1.5029132 ,\n",
       "       0.83050066, 3.8512247 , 1.3603473 , 3.219409  , 1.1694777 ,\n",
       "       1.1214588 , 1.0054519 , 2.2142687 , 1.3744861 , 0.579396  ,\n",
       "       1.5447656 , 1.6869639 , 1.4680444 , 0.34286806, 1.617254  ,\n",
       "       1.2845362 , 1.202461  , 0.96131426, 1.6332853 , 1.7632225 ,\n",
       "       1.8584356 , 1.392422  , 1.4776517 , 0.379419  , 1.6418229 ,\n",
       "       2.1015382 , 2.7902954 , 1.4630202 , 1.5281005 , 1.3712598 ,\n",
       "       2.9001687 , 0.70672876, 0.14354603, 1.7249051 , 0.65658015,\n",
       "       0.16869728, 1.1320397 , 2.081553  , 2.0443232 , 4.4420156 ,\n",
       "       1.3115183 , 1.314355  , 1.9395772 , 3.973562  , 0.6264785 ,\n",
       "       1.695305  , 3.785954  , 1.2417407 , 0.72902596, 3.0008333 ,\n",
       "       2.653672  , 2.2764254 , 1.5827845 , 2.8872643 , 0.50104076,\n",
       "       2.195806  , 1.468345  , 1.736231  , 0.59822166, 1.5895675 ,\n",
       "       1.253771  , 2.399162  , 2.5807765 , 1.3474916 , 1.9274796 ,\n",
       "       1.2420107 , 1.0607709 , 1.1742638 , 1.420456  , 0.17367817,\n",
       "       0.8649887 , 3.0135713 , 0.85285723, 0.6238036 , 1.8817921 ,\n",
       "       0.71288496, 1.3076894 , 2.044163  , 1.216827  , 0.8485376 ,\n",
       "       1.8642726 , 2.2842288 , 3.29524   , 2.8043323 , 1.6229379 ,\n",
       "       1.62139   , 2.9486117 , 1.1736972 , 2.4461105 , 0.572552  ,\n",
       "       1.3843877 , 0.27421874, 1.3827742 , 0.0275646 , 1.2788595 ,\n",
       "       1.9584606 , 2.5496275 , 2.3035955 , 1.171858  , 0.4144176 ,\n",
       "       1.5977044 , 1.2360696 , 1.0614456 , 2.7983818 , 2.1653209 ,\n",
       "       2.3087723 , 1.1201648 , 0.27843282, 1.8833951 , 1.8368257 ,\n",
       "       2.5361009 , 2.6955853 , 0.9877332 , 1.5758107 , 0.6307297 ,\n",
       "       0.21035546, 1.3660717 , 0.97279924, 0.6592657 , 1.4890934 ,\n",
       "       2.0492377 , 1.6921487 , 1.0801578 , 1.5608746 , 2.6246414 ,\n",
       "       1.2796942 , 2.8139346 , 0.50099933, 3.582763  , 1.083053  ,\n",
       "       1.2757261 , 1.9776108 , 0.8503263 , 3.1703382 , 1.1804453 ,\n",
       "       1.4588034 , 1.0464561 , 2.3894212 , 2.1740217 , 1.5757399 ,\n",
       "       2.3874304 , 1.1659182 , 1.799684  , 2.4519691 , 2.4914095 ,\n",
       "       1.3632177 , 1.4096706 , 1.7976866 , 4.382721  , 2.7538135 ,\n",
       "       2.821192  , 1.5739121 , 0.9097748 , 1.6181465 , 1.183876  ,\n",
       "       2.150998  , 2.3448946 , 1.390146  , 2.1172514 , 1.1769048 ,\n",
       "       1.3166126 , 1.568746  , 0.41310287, 1.4403062 , 1.2312055 ,\n",
       "       1.4310101 , 1.2255199 , 1.1158777 , 1.1189715 , 2.2274706 ,\n",
       "       1.6657848 , 1.8751297 , 1.5563223 , 0.81624097, 2.082359  ,\n",
       "       0.8606614 , 0.22924396, 2.8989072 , 2.1285784 , 1.4080226 ,\n",
       "       1.2720423 , 1.5257595 , 1.0458648 , 1.9141167 , 1.1604652 ,\n",
       "       1.5902158 , 0.8312023 , 2.4548464 , 1.9829696 , 0.8923977 ,\n",
       "       1.4979806 , 1.6298871 , 0.49032146, 1.9598677 , 1.7355742 ,\n",
       "       2.0456514 , 1.2699902 , 1.3288062 , 1.6172287 , 0.9217008 ,\n",
       "       1.3103927 , 1.615647  , 2.1798995 , 2.8989642 , 1.3481541 ,\n",
       "       1.2503638 , 1.43819   , 0.9225827 , 1.1240923 , 3.0148723 ,\n",
       "       1.8577667 , 2.684843  , 1.3083903 , 1.7790079 , 2.535333  ,\n",
       "       2.4111376 , 0.8711966 , 4.0060916 , 2.279077  , 1.4309493 ,\n",
       "       0.5862731 , 1.507293  , 1.5174232 , 0.45190936, 0.38534904,\n",
       "       2.569451  , 1.8448906 , 0.49202058, 2.1496854 , 4.26615   ,\n",
       "       2.8038924 , 1.5302551 , 2.1963327 , 1.6134162 , 2.046745  ,\n",
       "       1.8139042 , 1.492159  , 1.6104118 , 0.8071398 , 3.4058058 ,\n",
       "       2.1573849 , 1.8176101 , 1.5185018 , 0.4907558 , 1.8782644 ,\n",
       "       1.2604771 , 1.4847286 , 2.1453552 , 1.9277999 , 1.8784378 ,\n",
       "       1.5268748 , 0.42876348, 1.9867822 , 1.7237818 , 0.9659674 ,\n",
       "       1.6395341 , 1.6345764 , 1.5768218 , 3.1316786 , 1.0379158 ,\n",
       "       2.4302497 , 1.2158661 , 1.6120286 , 4.919554  , 2.575137  ,\n",
       "       1.5616231 , 1.8848553 , 1.9198173 , 1.5070149 , 0.59190637,\n",
       "       1.8617967 , 1.506396  , 1.3971101 , 1.3148829 , 2.1784754 ,\n",
       "       1.4370937 , 1.7933607 , 1.4565617 , 1.6187027 , 1.6771487 ,\n",
       "       2.159775  , 0.8160609 , 1.4780773 , 3.4191632 , 1.4693245 ,\n",
       "       1.0291431 , 1.3803989 , 1.3432884 , 0.39333946, 1.652033  ,\n",
       "       2.080631  , 2.006853  , 1.3163167 , 0.72707266, 1.5023466 ,\n",
       "       1.7191781 , 1.5216159 , 2.007744  , 1.9959482 , 1.675582  ,\n",
       "       1.7026742 , 0.98899204, 1.851238  , 0.84671074, 3.2517114 ,\n",
       "       1.4143621 , 2.0199308 , 0.96597344, 1.3015662 , 3.169384  ,\n",
       "       1.5266299 , 1.6584232 , 1.5856628 , 1.5068017 , 2.194629  ,\n",
       "       1.366195  , 0.72961444, 0.65638244, 1.147985  , 2.021531  ,\n",
       "       0.9777039 , 1.2945788 , 1.3985897 , 0.6825149 , 1.3045317 ,\n",
       "       1.3727348 , 1.1634436 , 2.290856  , 0.06952491, 1.3141358 ,\n",
       "       2.2848089 , 1.1235224 , 5.004341  , 2.5426037 , 2.2492006 ,\n",
       "       0.42285365, 0.30693027, 3.0693824 , 1.4196547 , 1.0899699 ,\n",
       "       2.6375206 , 1.6556469 , 1.3947968 , 1.4162948 , 1.712886  ,\n",
       "       0.06026939, 1.789254  , 2.4638116 , 0.29241002, 1.6562    ,\n",
       "       2.943118  , 3.4750862 , 0.01710702, 2.4245255 , 1.7496834 ,\n",
       "       1.4583173 , 0.9505685 , 2.2099452 , 0.8772709 , 1.5548596 ,\n",
       "       1.5260772 , 1.3704145 , 2.9611354 , 1.5689032 , 2.1508534 ,\n",
       "       1.4705797 , 1.6442173 , 1.5974029 , 1.5061488 , 0.94498473,\n",
       "       2.0796568 , 0.54261917, 1.0653186 , 1.1160777 , 1.5047798 ,\n",
       "       0.96676016, 1.7330216 , 3.2164142 , 0.09951383, 0.76885223,\n",
       "       1.6127341 , 0.970417  , 2.4865384 , 4.3659444 , 1.1322664 ,\n",
       "       1.2282122 , 1.2525387 , 1.4092263 , 1.2131181 , 0.77036595,\n",
       "       0.02345105, 2.2447016 , 2.9174275 , 2.2923827 , 0.48842534,\n",
       "       0.8784619 , 2.2157865 , 2.0766392 , 0.81992155, 1.5344929 ,\n",
       "       0.54678565, 1.133792  , 0.757348  , 3.32433   , 2.247202  ,\n",
       "       2.0627985 , 1.5025805 , 0.8400878 , 1.8143874 , 1.8087684 ,\n",
       "       1.5362809 , 0.11493555, 2.4096518 , 1.251836  , 1.66864   ,\n",
       "       0.2514801 , 2.5758471 , 1.7505811 , 1.2099819 , 0.84046113,\n",
       "       1.9245665 , 0.49727958, 1.9789875 , 0.5423367 , 3.0433817 ,\n",
       "       1.0984727 , 2.886558  , 1.58216   , 2.1987863 , 1.2509154 ,\n",
       "       1.7470629 , 0.71369976, 1.2834586 , 2.0223975 , 1.5804676 ,\n",
       "       4.733744  , 2.5184498 , 0.3644747 , 1.5048466 , 2.4722793 ,\n",
       "       0.0151635 , 0.4284101 , 1.458909  , 2.2762322 , 1.8537344 ,\n",
       "       0.94146913, 2.6679418 , 3.3520832 , 1.9944358 , 2.2450871 ,\n",
       "       0.37598038, 1.635943  , 1.0923388 , 1.8376542 , 2.2926152 ,\n",
       "       2.50034   , 3.1564057 , 0.53040445, 1.4219126 , 0.4119538 ,\n",
       "       0.23696113, 1.7091867 , 2.004867  , 0.583767  , 0.5385566 ,\n",
       "       2.6059108 , 1.2288632 , 0.884928  , 1.4254961 , 1.4081922 ,\n",
       "       2.1211514 , 2.0283647 , 1.5807055 , 1.4159237 , 0.20897536,\n",
       "       2.284829  , 1.2418396 , 0.6019787 , 2.1375186 , 1.8300776 ,\n",
       "       2.010996  , 1.0141667 , 0.86418676, 0.5851167 , 1.5907141 ,\n",
       "       0.7645736 , 1.7106862 , 2.317221  , 2.779648  , 1.4601697 ,\n",
       "       2.541476  , 1.2439914 , 1.7778863 , 2.294864  , 1.6995568 ,\n",
       "       1.4331648 , 0.9220635 , 1.3788389 , 2.4984694 , 1.5820199 ,\n",
       "       0.521077  , 1.4654105 , 1.4062809 , 1.6681406 , 1.7645408 ,\n",
       "       2.517859  , 1.1326581 , 0.9265332 , 1.1085967 , 1.22596   ,\n",
       "       1.7064701 , 1.4900403 , 0.3078062 , 0.54248905, 1.5867181 ,\n",
       "       1.7291739 , 2.6208334 , 1.5177407 , 0.9807348 , 1.9055173 ,\n",
       "       2.261632  , 1.1374973 , 1.6072792 , 1.8788278 , 0.21823432,\n",
       "       2.554861  , 0.82780325, 1.5576645 , 1.059002  , 2.0019882 ,\n",
       "       1.1845269 , 0.65635437, 1.1021986 , 3.6783667 , 1.865692  ,\n",
       "       2.116018  , 1.5937148 , 1.7472187 , 1.8402282 , 2.1163156 ,\n",
       "       1.5498312 , 2.0907626 , 0.5476052 , 1.32061   , 1.2156297 ,\n",
       "       3.4459274 , 2.3004065 , 2.0206437 , 1.7113389 , 1.9794114 ,\n",
       "       1.6579012 , 0.5226842 , 1.2213417 , 0.5013227 , 1.5663605 ,\n",
       "       2.077136  , 2.8839943 , 3.0500333 , 1.2120163 , 1.5274029 ,\n",
       "       1.4278799 , 3.4182143 , 2.3131502 , 0.33748838, 1.7551876 ,\n",
       "       3.471508  , 1.4391288 , 1.4588808 , 1.2149384 , 2.566642  ,\n",
       "       1.3751546 , 2.2218184 , 1.0724928 , 0.73079073, 2.1412103 ,\n",
       "       1.5272259 , 1.3616402 , 1.6446394 , 1.2148848 , 1.259624  ,\n",
       "       2.2498627 , 2.3021064 , 1.522921  , 2.0548122 , 1.5613422 ,\n",
       "       1.5224253 , 1.245902  , 1.6433756 , 2.7658284 , 1.9288568 ,\n",
       "       0.7768782 , 1.0707307 , 1.9784288 , 2.856062  , 1.3958158 ,\n",
       "       1.6647933 , 0.8197457 , 1.3080217 , 1.8964634 , 1.5060523 ,\n",
       "       2.2991247 , 1.5282164 , 1.489331  , 0.9104565 , 1.3388771 ,\n",
       "       4.6163425 , 2.1070554 , 2.5082166 , 1.3399181 , 3.3706057 ,\n",
       "       0.3873869 , 1.3800561 , 0.8603596 , 2.0575337 , 2.407905  ,\n",
       "       1.1746871 , 1.2108705 , 2.4449894 , 1.5426803 , 3.2408752 ,\n",
       "       2.2066026 , 1.1917725 , 1.1261123 , 1.2644563 , 2.338895  ,\n",
       "       2.4561508 , 1.7803862 , 1.373101  , 1.906777  , 1.4530723 ,\n",
       "       0.04498111, 1.2422736 , 0.90305704, 0.88265413, 1.5345497 ,\n",
       "       0.18834801, 2.1403244 , 1.6220591 , 1.6610599 , 1.2916903 ,\n",
       "       0.5990729 , 1.7832093 , 1.2130985 , 1.3720354 , 1.2690786 ,\n",
       "       2.3199525 , 1.9716022 , 0.9638829 , 1.9349359 , 1.3931115 ,\n",
       "       1.8201532 , 1.6030116 , 2.0086277 , 1.1228442 , 2.8070457 ,\n",
       "       1.9678994 , 1.3681957 , 1.9322734 , 1.039139  , 1.7201589 ,\n",
       "       0.89914376, 1.6729124 , 2.7470036 , 0.7417702 , 1.8428811 ,\n",
       "       1.7271024 , 1.722516  , 2.0689447 , 1.1054903 , 3.957941  ,\n",
       "       1.520549  , 2.806292  , 1.4099163 , 1.7631351 , 2.3002224 ,\n",
       "       0.77991277, 0.9837141 , 1.149155  , 0.36352706, 0.4735256 ,\n",
       "       0.8964635 , 1.8128003 , 2.7327907 , 2.9297762 , 2.8917763 ,\n",
       "       1.4089168 , 0.57750815, 1.8202013 , 2.8670301 , 1.703131  ,\n",
       "       1.9798272 , 2.5690646 , 0.7349921 , 2.797301  , 1.4797684 ,\n",
       "       0.05383288, 1.8054584 , 0.957527  , 3.2969012 , 2.5753624 ,\n",
       "       3.6535962 , 1.5626489 , 1.5852522 , 2.1860101 , 1.5934908 ,\n",
       "       1.6963418 , 2.4941983 , 3.317441  , 1.1268992 , 1.3398033 ,\n",
       "       1.0704423 , 2.7530785 , 2.1394885 , 1.6478498 , 0.07621299,\n",
       "       1.3083007 , 2.5955858 , 0.91862994, 4.452967  , 2.36504   ,\n",
       "       1.0326035 , 1.361232  , 0.65085   , 0.92821085, 2.020871  ,\n",
       "       1.8844362 , 1.2483375 , 0.9479663 , 2.935703  , 0.64579767,\n",
       "       2.2378569 , 0.949608  , 2.6784308 , 0.89858353, 1.3312467 ,\n",
       "       1.3204893 , 0.4188678 , 0.98604906, 1.174993  , 1.1430627 ,\n",
       "       2.164461  , 2.430578  , 1.3182204 , 1.2809242 , 1.6145313 ,\n",
       "       1.4813478 , 1.5206171 , 0.49967238, 0.8387291 , 2.080369  ,\n",
       "       3.4004278 , 1.6765906 , 2.1162243 , 0.5519269 , 0.58993685,\n",
       "       1.4335966 , 1.185346  , 1.3644625 , 2.0989726 , 2.7198052 ,\n",
       "       1.5500408 , 1.4326553 , 1.9048964 , 1.4257866 , 1.9502375 ,\n",
       "       0.40920565, 0.37066755, 1.112922  , 2.0380712 , 0.83802307,\n",
       "       1.4802135 , 2.5059812 , 4.0031133 , 0.22413632, 0.76344025],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.sum((x_adv.cpu().detach().numpy()[ind] - x_test.numpy()[:num][ind])**2, (1, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ours: [2.1830862, 4.239626, 2.6917257, 3.8731990, 1.4814271, 2.63134,\n",
    "#        2.0538194, 1.270368, 2.9351923, 2.4314828]\n",
    "# bd2:  [2.9166877, 2.957195, 2.9477980, 2.8653080, 1.2530060, 2.33518, \n",
    "#        1.6536615, 1.168170, 1.6185884, 2.3803349]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CW L2 Attack\n",
    "\n",
    "without DkNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: 121.171; l2dist: 0.000\n",
      "    step: 50; loss: 14.259; l2dist: 3.370\n",
      "    step: 100; loss: 5.832; l2dist: 2.329\n",
      "    step: 150; loss: 3.887; l2dist: 1.881\n",
      "    step: 200; loss: 3.105; l2dist: 1.666\n",
      "    step: 250; loss: 2.774; l2dist: 1.574\n",
      "    step: 300; loss: 2.581; l2dist: 1.513\n",
      "    step: 350; loss: 2.525; l2dist: 1.488\n",
      "    step: 400; loss: 2.486; l2dist: 1.473\n",
      "    step: 450; loss: 2.449; l2dist: 1.461\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 75.129; l2dist: 0.000\n",
      "    step: 50; loss: 10.528; l2dist: 2.859\n",
      "    step: 100; loss: 4.196; l2dist: 1.949\n",
      "    step: 150; loss: 3.019; l2dist: 1.639\n",
      "    step: 200; loss: 2.641; l2dist: 1.526\n",
      "    step: 250; loss: 2.478; l2dist: 1.469\n",
      "    step: 300; loss: 2.393; l2dist: 1.444\n",
      "    step: 350; loss: 2.319; l2dist: 1.425\n",
      "    step: 400; loss: 2.340; l2dist: 1.432\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.686; l2dist: 0.000\n",
      "    step: 50; loss: 8.584; l2dist: 2.540\n",
      "    step: 100; loss: 3.611; l2dist: 1.794\n",
      "    step: 150; loss: 2.719; l2dist: 1.548\n",
      "    step: 200; loss: 2.470; l2dist: 1.472\n",
      "    step: 250; loss: 2.363; l2dist: 1.433\n",
      "    step: 300; loss: 2.287; l2dist: 1.416\n",
      "    step: 350; loss: 2.289; l2dist: 1.408\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.072; l2dist: 0.000\n",
      "    step: 50; loss: 7.524; l2dist: 2.317\n",
      "    step: 100; loss: 3.475; l2dist: 1.732\n",
      "    step: 150; loss: 2.640; l2dist: 1.503\n",
      "    step: 200; loss: 2.406; l2dist: 1.442\n",
      "    step: 250; loss: 2.313; l2dist: 1.414\n",
      "    step: 300; loss: 2.268; l2dist: 1.404\n",
      "    step: 350; loss: 2.253; l2dist: 1.404\n",
      "    step: 400; loss: 2.235; l2dist: 1.394\n",
      "    step: 450; loss: 2.222; l2dist: 1.388\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.327; l2dist: 0.000\n",
      "    step: 50; loss: 6.720; l2dist: 2.073\n",
      "    step: 100; loss: 3.511; l2dist: 1.676\n",
      "    step: 150; loss: 2.627; l2dist: 1.487\n",
      "    step: 200; loss: 2.417; l2dist: 1.428\n",
      "    step: 250; loss: 2.314; l2dist: 1.404\n",
      "    step: 300; loss: 2.291; l2dist: 1.396\n",
      "    step: 350; loss: 2.278; l2dist: 1.384\n",
      "    step: 400; loss: 2.252; l2dist: 1.385\n",
      "    step: 450; loss: 2.229; l2dist: 1.384\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.244; l2dist: 0.000\n",
      "    step: 50; loss: 6.437; l2dist: 2.006\n",
      "    step: 100; loss: 3.453; l2dist: 1.617\n",
      "    step: 150; loss: 2.603; l2dist: 1.451\n",
      "    step: 200; loss: 2.391; l2dist: 1.407\n",
      "    step: 250; loss: 2.305; l2dist: 1.383\n",
      "    step: 300; loss: 2.247; l2dist: 1.370\n",
      "    step: 350; loss: 2.225; l2dist: 1.368\n",
      "    step: 400; loss: 2.229; l2dist: 1.366\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.416; l2dist: 0.000\n",
      "    step: 50; loss: 6.415; l2dist: 1.988\n",
      "    step: 100; loss: 3.436; l2dist: 1.607\n",
      "    step: 150; loss: 2.577; l2dist: 1.431\n",
      "    step: 200; loss: 2.369; l2dist: 1.386\n",
      "    step: 250; loss: 2.280; l2dist: 1.368\n",
      "    step: 300; loss: 2.246; l2dist: 1.355\n",
      "    step: 350; loss: 2.223; l2dist: 1.353\n",
      "    step: 400; loss: 2.212; l2dist: 1.349\n",
      "    step: 450; loss: 2.210; l2dist: 1.346\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.906; l2dist: 0.000\n",
      "    step: 50; loss: 6.411; l2dist: 1.981\n",
      "    step: 100; loss: 3.455; l2dist: 1.610\n",
      "    step: 150; loss: 2.578; l2dist: 1.442\n",
      "    step: 200; loss: 2.368; l2dist: 1.393\n",
      "    step: 250; loss: 2.292; l2dist: 1.373\n",
      "    step: 300; loss: 2.253; l2dist: 1.359\n",
      "    step: 350; loss: 2.240; l2dist: 1.354\n",
      "    step: 400; loss: 2.213; l2dist: 1.360\n",
      "    step: 450; loss: 2.213; l2dist: 1.351\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.751; l2dist: 0.000\n",
      "    step: 50; loss: 6.410; l2dist: 1.980\n",
      "    step: 100; loss: 3.449; l2dist: 1.608\n",
      "    step: 150; loss: 2.574; l2dist: 1.442\n",
      "    step: 200; loss: 2.367; l2dist: 1.402\n",
      "    step: 250; loss: 2.290; l2dist: 1.383\n",
      "    step: 300; loss: 2.247; l2dist: 1.364\n",
      "    step: 350; loss: 2.233; l2dist: 1.365\n",
      "    step: 400; loss: 2.218; l2dist: 1.365\n",
      "    step: 450; loss: 2.207; l2dist: 1.356\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.893; l2dist: 0.000\n",
      "    step: 50; loss: 6.454; l2dist: 1.990\n",
      "    step: 100; loss: 3.469; l2dist: 1.623\n",
      "    step: 150; loss: 2.591; l2dist: 1.448\n",
      "    step: 200; loss: 2.366; l2dist: 1.401\n",
      "    step: 250; loss: 2.282; l2dist: 1.385\n",
      "    step: 300; loss: 2.247; l2dist: 1.369\n",
      "    step: 350; loss: 2.240; l2dist: 1.365\n",
      "    step: 400; loss: 2.238; l2dist: 1.361\n",
      "    step: 450; loss: 2.210; l2dist: 1.364\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 128.587; l2dist: 0.000\n",
      "    step: 50; loss: 15.144; l2dist: 3.476\n",
      "    step: 100; loss: 6.006; l2dist: 2.370\n",
      "    step: 150; loss: 4.000; l2dist: 1.916\n",
      "    step: 200; loss: 3.227; l2dist: 1.710\n",
      "    step: 250; loss: 2.866; l2dist: 1.611\n",
      "    step: 300; loss: 2.705; l2dist: 1.551\n",
      "    step: 350; loss: 2.644; l2dist: 1.540\n",
      "    step: 400; loss: 2.548; l2dist: 1.513\n",
      "    step: 450; loss: 2.526; l2dist: 1.508\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 115.593; l2dist: 0.000\n",
      "    step: 50; loss: 12.955; l2dist: 3.190\n",
      "    step: 100; loss: 5.268; l2dist: 2.167\n",
      "    step: 150; loss: 3.650; l2dist: 1.783\n",
      "    step: 200; loss: 3.030; l2dist: 1.629\n",
      "    step: 250; loss: 2.705; l2dist: 1.550\n",
      "    step: 300; loss: 2.592; l2dist: 1.506\n",
      "    step: 350; loss: 2.495; l2dist: 1.491\n",
      "    step: 400; loss: 2.385; l2dist: 1.463\n",
      "    step: 450; loss: 2.353; l2dist: 1.462\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.682; l2dist: 0.000\n",
      "    step: 50; loss: 10.310; l2dist: 2.798\n",
      "    step: 100; loss: 4.111; l2dist: 1.912\n",
      "    step: 150; loss: 3.010; l2dist: 1.631\n",
      "    step: 200; loss: 2.623; l2dist: 1.536\n",
      "    step: 250; loss: 2.485; l2dist: 1.481\n",
      "    step: 300; loss: 2.336; l2dist: 1.448\n",
      "    step: 350; loss: 2.313; l2dist: 1.442\n",
      "    step: 400; loss: 2.278; l2dist: 1.424\n",
      "    step: 450; loss: 2.257; l2dist: 1.421\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.262; l2dist: 0.000\n",
      "    step: 50; loss: 8.787; l2dist: 2.519\n",
      "    step: 100; loss: 3.680; l2dist: 1.812\n",
      "    step: 150; loss: 2.724; l2dist: 1.567\n",
      "    step: 200; loss: 2.439; l2dist: 1.482\n",
      "    step: 250; loss: 2.346; l2dist: 1.446\n",
      "    step: 300; loss: 2.265; l2dist: 1.423\n",
      "    step: 350; loss: 2.250; l2dist: 1.416\n",
      "    step: 400; loss: 2.290; l2dist: 1.431\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.017; l2dist: 0.000\n",
      "    step: 50; loss: 7.824; l2dist: 2.299\n",
      "    step: 100; loss: 3.496; l2dist: 1.715\n",
      "    step: 150; loss: 2.634; l2dist: 1.512\n",
      "    step: 200; loss: 2.393; l2dist: 1.448\n",
      "    step: 250; loss: 2.302; l2dist: 1.420\n",
      "    step: 300; loss: 2.269; l2dist: 1.410\n",
      "    step: 350; loss: 2.240; l2dist: 1.403\n",
      "    step: 400; loss: 2.229; l2dist: 1.392\n",
      "    step: 450; loss: 2.221; l2dist: 1.395\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.538; l2dist: 0.000\n",
      "    step: 50; loss: 7.378; l2dist: 2.198\n",
      "    step: 100; loss: 3.336; l2dist: 1.637\n",
      "    step: 150; loss: 2.556; l2dist: 1.452\n",
      "    step: 200; loss: 2.327; l2dist: 1.398\n",
      "    step: 250; loss: 2.266; l2dist: 1.377\n",
      "    step: 300; loss: 2.206; l2dist: 1.372\n",
      "    step: 350; loss: 2.179; l2dist: 1.361\n",
      "    step: 400; loss: 2.156; l2dist: 1.358\n",
      "    step: 450; loss: 2.161; l2dist: 1.363\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.276; l2dist: 0.000\n",
      "    step: 50; loss: 7.214; l2dist: 2.164\n",
      "    step: 100; loss: 3.293; l2dist: 1.615\n",
      "    step: 150; loss: 2.536; l2dist: 1.443\n",
      "    step: 200; loss: 2.317; l2dist: 1.385\n",
      "    step: 250; loss: 2.234; l2dist: 1.362\n",
      "    step: 300; loss: 2.197; l2dist: 1.355\n",
      "    step: 350; loss: 2.168; l2dist: 1.353\n",
      "    step: 400; loss: 2.160; l2dist: 1.346\n",
      "    step: 450; loss: 2.150; l2dist: 1.348\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.289; l2dist: 0.000\n",
      "    step: 50; loss: 7.137; l2dist: 2.151\n",
      "    step: 100; loss: 3.302; l2dist: 1.628\n",
      "    step: 150; loss: 2.536; l2dist: 1.455\n",
      "    step: 200; loss: 2.313; l2dist: 1.398\n",
      "    step: 250; loss: 2.234; l2dist: 1.374\n",
      "    step: 300; loss: 2.210; l2dist: 1.365\n",
      "    step: 350; loss: 2.181; l2dist: 1.361\n",
      "    step: 400; loss: 2.155; l2dist: 1.353\n",
      "    step: 450; loss: 2.147; l2dist: 1.354\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.820; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 7.134; l2dist: 2.149\n",
      "    step: 100; loss: 3.299; l2dist: 1.621\n",
      "    step: 150; loss: 2.539; l2dist: 1.448\n",
      "    step: 200; loss: 2.314; l2dist: 1.398\n",
      "    step: 250; loss: 2.240; l2dist: 1.374\n",
      "    step: 300; loss: 2.193; l2dist: 1.367\n",
      "    step: 350; loss: 2.179; l2dist: 1.357\n",
      "    step: 400; loss: 2.167; l2dist: 1.357\n",
      "    step: 450; loss: 2.161; l2dist: 1.359\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.044; l2dist: 0.000\n",
      "    step: 50; loss: 7.187; l2dist: 2.160\n",
      "    step: 100; loss: 3.313; l2dist: 1.630\n",
      "    step: 150; loss: 2.546; l2dist: 1.454\n",
      "    step: 200; loss: 2.330; l2dist: 1.401\n",
      "    step: 250; loss: 2.244; l2dist: 1.386\n",
      "    step: 300; loss: 2.217; l2dist: 1.376\n",
      "    step: 350; loss: 2.177; l2dist: 1.369\n",
      "    step: 400; loss: 2.167; l2dist: 1.370\n",
      "    step: 450; loss: 2.162; l2dist: 1.366\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.501; l2dist: 0.000\n",
      "    step: 50; loss: 13.032; l2dist: 3.298\n",
      "    step: 100; loss: 5.212; l2dist: 2.202\n",
      "    step: 150; loss: 3.483; l2dist: 1.773\n",
      "    step: 200; loss: 2.797; l2dist: 1.580\n",
      "    step: 250; loss: 2.561; l2dist: 1.504\n",
      "    step: 300; loss: 2.363; l2dist: 1.437\n",
      "    step: 350; loss: 2.303; l2dist: 1.419\n",
      "    step: 400; loss: 2.223; l2dist: 1.393\n",
      "    step: 450; loss: 2.219; l2dist: 1.387\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 138.063; l2dist: 0.000\n",
      "    step: 50; loss: 12.945; l2dist: 3.250\n",
      "    step: 100; loss: 5.648; l2dist: 2.225\n",
      "    step: 150; loss: 3.782; l2dist: 1.796\n",
      "    step: 200; loss: 2.994; l2dist: 1.593\n",
      "    step: 250; loss: 2.676; l2dist: 1.500\n",
      "    step: 300; loss: 2.450; l2dist: 1.435\n",
      "    step: 350; loss: 2.348; l2dist: 1.413\n",
      "    step: 400; loss: 2.241; l2dist: 1.392\n",
      "    step: 450; loss: 2.208; l2dist: 1.382\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.534; l2dist: 0.000\n",
      "    step: 50; loss: 9.589; l2dist: 2.744\n",
      "    step: 100; loss: 4.016; l2dist: 1.864\n",
      "    step: 150; loss: 2.822; l2dist: 1.553\n",
      "    step: 200; loss: 2.442; l2dist: 1.442\n",
      "    step: 250; loss: 2.264; l2dist: 1.396\n",
      "    step: 300; loss: 2.176; l2dist: 1.365\n",
      "    step: 350; loss: 2.129; l2dist: 1.354\n",
      "    step: 400; loss: 2.054; l2dist: 1.332\n",
      "    step: 450; loss: 2.064; l2dist: 1.339\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.259; l2dist: 0.000\n",
      "    step: 50; loss: 7.857; l2dist: 2.365\n",
      "    step: 100; loss: 3.365; l2dist: 1.694\n",
      "    step: 150; loss: 2.478; l2dist: 1.458\n",
      "    step: 200; loss: 2.227; l2dist: 1.376\n",
      "    step: 250; loss: 2.075; l2dist: 1.339\n",
      "    step: 300; loss: 2.048; l2dist: 1.322\n",
      "    step: 350; loss: 1.992; l2dist: 1.310\n",
      "    step: 400; loss: 1.954; l2dist: 1.298\n",
      "    step: 450; loss: 1.951; l2dist: 1.292\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.873; l2dist: 0.000\n",
      "    step: 50; loss: 7.000; l2dist: 2.136\n",
      "    step: 100; loss: 3.196; l2dist: 1.596\n",
      "    step: 150; loss: 2.386; l2dist: 1.412\n",
      "    step: 200; loss: 2.161; l2dist: 1.352\n",
      "    step: 250; loss: 2.046; l2dist: 1.320\n",
      "    step: 300; loss: 1.997; l2dist: 1.309\n",
      "    step: 350; loss: 1.981; l2dist: 1.301\n",
      "    step: 400; loss: 1.957; l2dist: 1.293\n",
      "    step: 450; loss: 1.944; l2dist: 1.282\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.033; l2dist: 0.000\n",
      "    step: 50; loss: 6.417; l2dist: 1.990\n",
      "    step: 100; loss: 3.084; l2dist: 1.529\n",
      "    step: 150; loss: 2.325; l2dist: 1.355\n",
      "    step: 200; loss: 2.113; l2dist: 1.304\n",
      "    step: 250; loss: 2.015; l2dist: 1.280\n",
      "    step: 300; loss: 1.975; l2dist: 1.265\n",
      "    step: 350; loss: 1.962; l2dist: 1.264\n",
      "    step: 400; loss: 1.947; l2dist: 1.257\n",
      "    step: 450; loss: 1.933; l2dist: 1.254\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.796; l2dist: 0.000\n",
      "    step: 50; loss: 6.367; l2dist: 1.981\n",
      "    step: 100; loss: 3.073; l2dist: 1.537\n",
      "    step: 150; loss: 2.313; l2dist: 1.362\n",
      "    step: 200; loss: 2.118; l2dist: 1.322\n",
      "    step: 250; loss: 2.019; l2dist: 1.291\n",
      "    step: 300; loss: 1.986; l2dist: 1.275\n",
      "    step: 350; loss: 1.963; l2dist: 1.263\n",
      "    step: 400; loss: 1.944; l2dist: 1.264\n",
      "    step: 450; loss: 1.922; l2dist: 1.258\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.357; l2dist: 0.000\n",
      "    step: 50; loss: 6.253; l2dist: 1.949\n",
      "    step: 100; loss: 3.060; l2dist: 1.523\n",
      "    step: 150; loss: 2.313; l2dist: 1.358\n",
      "    step: 200; loss: 2.100; l2dist: 1.304\n",
      "    step: 250; loss: 2.012; l2dist: 1.284\n",
      "    step: 300; loss: 1.979; l2dist: 1.279\n",
      "    step: 350; loss: 1.951; l2dist: 1.266\n",
      "    step: 400; loss: 1.969; l2dist: 1.278\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.652; l2dist: 0.000\n",
      "    step: 50; loss: 6.212; l2dist: 1.936\n",
      "    step: 100; loss: 3.071; l2dist: 1.522\n",
      "    step: 150; loss: 2.309; l2dist: 1.360\n",
      "    step: 200; loss: 2.117; l2dist: 1.325\n",
      "    step: 250; loss: 2.029; l2dist: 1.288\n",
      "    step: 300; loss: 1.973; l2dist: 1.281\n",
      "    step: 350; loss: 1.969; l2dist: 1.275\n",
      "    step: 400; loss: 1.945; l2dist: 1.276\n",
      "    step: 450; loss: 1.934; l2dist: 1.267\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.081; l2dist: 0.000\n",
      "    step: 50; loss: 6.265; l2dist: 1.957\n",
      "    step: 100; loss: 3.080; l2dist: 1.531\n",
      "    step: 150; loss: 2.316; l2dist: 1.367\n",
      "    step: 200; loss: 2.103; l2dist: 1.319\n",
      "    step: 250; loss: 2.027; l2dist: 1.300\n",
      "    step: 300; loss: 1.980; l2dist: 1.285\n",
      "    step: 350; loss: 1.959; l2dist: 1.277\n",
      "    step: 400; loss: 1.955; l2dist: 1.275\n",
      "    step: 450; loss: 1.943; l2dist: 1.273\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.186; l2dist: 0.000\n",
      "    step: 50; loss: 13.011; l2dist: 3.284\n",
      "    step: 100; loss: 5.313; l2dist: 2.224\n",
      "    step: 150; loss: 3.428; l2dist: 1.778\n",
      "    step: 200; loss: 2.725; l2dist: 1.584\n",
      "    step: 250; loss: 2.419; l2dist: 1.485\n",
      "    step: 300; loss: 2.266; l2dist: 1.441\n",
      "    step: 350; loss: 2.154; l2dist: 1.401\n",
      "    step: 400; loss: 2.091; l2dist: 1.381\n",
      "    step: 450; loss: 2.046; l2dist: 1.365\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 85.868; l2dist: 0.000\n",
      "    step: 50; loss: 10.552; l2dist: 2.897\n",
      "    step: 100; loss: 4.082; l2dist: 1.913\n",
      "    step: 150; loss: 2.826; l2dist: 1.595\n",
      "    step: 200; loss: 2.342; l2dist: 1.454\n",
      "    step: 250; loss: 2.151; l2dist: 1.398\n",
      "    step: 300; loss: 2.018; l2dist: 1.351\n",
      "    step: 350; loss: 2.001; l2dist: 1.342\n",
      "    step: 400; loss: 1.954; l2dist: 1.335\n",
      "    step: 450; loss: 1.958; l2dist: 1.331\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.760; l2dist: 0.000\n",
      "    step: 50; loss: 8.506; l2dist: 2.533\n",
      "    step: 100; loss: 3.282; l2dist: 1.724\n",
      "    step: 150; loss: 2.348; l2dist: 1.457\n",
      "    step: 200; loss: 2.085; l2dist: 1.373\n",
      "    step: 250; loss: 1.971; l2dist: 1.330\n",
      "    step: 300; loss: 1.917; l2dist: 1.322\n",
      "    step: 350; loss: 1.881; l2dist: 1.310\n",
      "    step: 400; loss: 1.867; l2dist: 1.307\n",
      "    step: 450; loss: 1.853; l2dist: 1.295\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.349; l2dist: 0.000\n",
      "    step: 50; loss: 7.208; l2dist: 2.239\n",
      "    step: 100; loss: 2.998; l2dist: 1.636\n",
      "    step: 150; loss: 2.174; l2dist: 1.403\n",
      "    step: 200; loss: 1.977; l2dist: 1.339\n",
      "    step: 250; loss: 1.899; l2dist: 1.314\n",
      "    step: 300; loss: 1.863; l2dist: 1.304\n",
      "    step: 350; loss: 1.855; l2dist: 1.290\n",
      "    step: 400; loss: 1.832; l2dist: 1.289\n",
      "    step: 450; loss: 1.899; l2dist: 1.295\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.287; l2dist: 0.000\n",
      "    step: 50; loss: 6.333; l2dist: 2.012\n",
      "    step: 100; loss: 2.850; l2dist: 1.544\n",
      "    step: 150; loss: 2.095; l2dist: 1.352\n",
      "    step: 200; loss: 1.918; l2dist: 1.306\n",
      "    step: 250; loss: 1.848; l2dist: 1.282\n",
      "    step: 300; loss: 1.816; l2dist: 1.276\n",
      "    step: 350; loss: 1.819; l2dist: 1.273\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.383; l2dist: 0.000\n",
      "    step: 50; loss: 5.827; l2dist: 1.880\n",
      "    step: 100; loss: 2.761; l2dist: 1.455\n",
      "    step: 150; loss: 2.064; l2dist: 1.300\n",
      "    step: 200; loss: 1.898; l2dist: 1.265\n",
      "    step: 250; loss: 1.839; l2dist: 1.252\n",
      "    step: 300; loss: 1.801; l2dist: 1.237\n",
      "    step: 350; loss: 1.807; l2dist: 1.240\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.185; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 5.832; l2dist: 1.872\n",
      "    step: 100; loss: 2.763; l2dist: 1.472\n",
      "    step: 150; loss: 2.047; l2dist: 1.311\n",
      "    step: 200; loss: 1.904; l2dist: 1.269\n",
      "    step: 250; loss: 1.822; l2dist: 1.253\n",
      "    step: 300; loss: 1.793; l2dist: 1.241\n",
      "    step: 350; loss: 1.778; l2dist: 1.242\n",
      "    step: 400; loss: 1.769; l2dist: 1.239\n",
      "    step: 450; loss: 1.767; l2dist: 1.236\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.685; l2dist: 0.000\n",
      "    step: 50; loss: 5.802; l2dist: 1.867\n",
      "    step: 100; loss: 2.769; l2dist: 1.466\n",
      "    step: 150; loss: 2.049; l2dist: 1.309\n",
      "    step: 200; loss: 1.898; l2dist: 1.274\n",
      "    step: 250; loss: 1.836; l2dist: 1.255\n",
      "    step: 300; loss: 1.810; l2dist: 1.247\n",
      "    step: 350; loss: 1.785; l2dist: 1.244\n",
      "    step: 400; loss: 1.773; l2dist: 1.241\n",
      "    step: 450; loss: 1.769; l2dist: 1.240\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.334; l2dist: 0.000\n",
      "    step: 50; loss: 5.791; l2dist: 1.857\n",
      "    step: 100; loss: 2.769; l2dist: 1.463\n",
      "    step: 150; loss: 2.047; l2dist: 1.305\n",
      "    step: 200; loss: 1.889; l2dist: 1.268\n",
      "    step: 250; loss: 1.839; l2dist: 1.253\n",
      "    step: 300; loss: 1.809; l2dist: 1.249\n",
      "    step: 350; loss: 1.794; l2dist: 1.245\n",
      "    step: 400; loss: 1.783; l2dist: 1.238\n",
      "    step: 450; loss: 1.781; l2dist: 1.244\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.527; l2dist: 0.000\n",
      "    step: 50; loss: 5.844; l2dist: 1.871\n",
      "    step: 100; loss: 2.778; l2dist: 1.475\n",
      "    step: 150; loss: 2.060; l2dist: 1.310\n",
      "    step: 200; loss: 1.906; l2dist: 1.276\n",
      "    step: 250; loss: 1.834; l2dist: 1.258\n",
      "    step: 300; loss: 1.806; l2dist: 1.253\n",
      "    step: 350; loss: 1.789; l2dist: 1.247\n",
      "    step: 400; loss: 1.789; l2dist: 1.254\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.398; l2dist: 0.000\n",
      "    step: 50; loss: 12.907; l2dist: 3.194\n",
      "    step: 100; loss: 5.063; l2dist: 2.149\n",
      "    step: 150; loss: 3.362; l2dist: 1.734\n",
      "    step: 200; loss: 2.712; l2dist: 1.535\n",
      "    step: 250; loss: 2.450; l2dist: 1.459\n",
      "    step: 300; loss: 2.305; l2dist: 1.414\n",
      "    step: 350; loss: 2.213; l2dist: 1.371\n",
      "    step: 400; loss: 2.189; l2dist: 1.368\n",
      "    step: 450; loss: 2.193; l2dist: 1.369\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 98.461; l2dist: 0.000\n",
      "    step: 50; loss: 10.811; l2dist: 2.900\n",
      "    step: 100; loss: 4.458; l2dist: 1.971\n",
      "    step: 150; loss: 3.104; l2dist: 1.633\n",
      "    step: 200; loss: 2.574; l2dist: 1.481\n",
      "    step: 250; loss: 2.332; l2dist: 1.412\n",
      "    step: 300; loss: 2.209; l2dist: 1.376\n",
      "    step: 350; loss: 2.118; l2dist: 1.345\n",
      "    step: 400; loss: 2.056; l2dist: 1.338\n",
      "    step: 450; loss: 2.039; l2dist: 1.332\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.521; l2dist: 0.000\n",
      "    step: 50; loss: 8.418; l2dist: 2.471\n",
      "    step: 100; loss: 3.452; l2dist: 1.728\n",
      "    step: 150; loss: 2.541; l2dist: 1.476\n",
      "    step: 200; loss: 2.227; l2dist: 1.382\n",
      "    step: 250; loss: 2.112; l2dist: 1.341\n",
      "    step: 300; loss: 2.023; l2dist: 1.323\n",
      "    step: 350; loss: 2.023; l2dist: 1.322\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.993; l2dist: 0.000\n",
      "    step: 50; loss: 6.905; l2dist: 2.140\n",
      "    step: 100; loss: 3.128; l2dist: 1.636\n",
      "    step: 150; loss: 2.333; l2dist: 1.411\n",
      "    step: 200; loss: 2.102; l2dist: 1.342\n",
      "    step: 250; loss: 2.020; l2dist: 1.320\n",
      "    step: 300; loss: 1.989; l2dist: 1.306\n",
      "    step: 350; loss: 2.003; l2dist: 1.299\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.247; l2dist: 0.000\n",
      "    step: 50; loss: 6.348; l2dist: 2.002\n",
      "    step: 100; loss: 3.013; l2dist: 1.556\n",
      "    step: 150; loss: 2.257; l2dist: 1.372\n",
      "    step: 200; loss: 2.066; l2dist: 1.314\n",
      "    step: 250; loss: 1.993; l2dist: 1.296\n",
      "    step: 300; loss: 1.953; l2dist: 1.285\n",
      "    step: 350; loss: 1.950; l2dist: 1.278\n",
      "    step: 400; loss: 1.939; l2dist: 1.272\n",
      "    step: 450; loss: 1.911; l2dist: 1.270\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.373; l2dist: 0.000\n",
      "    step: 50; loss: 6.051; l2dist: 1.909\n",
      "    step: 100; loss: 2.987; l2dist: 1.508\n",
      "    step: 150; loss: 2.241; l2dist: 1.338\n",
      "    step: 200; loss: 2.049; l2dist: 1.292\n",
      "    step: 250; loss: 1.988; l2dist: 1.275\n",
      "    step: 300; loss: 1.936; l2dist: 1.262\n",
      "    step: 350; loss: 1.928; l2dist: 1.255\n",
      "    step: 400; loss: 1.911; l2dist: 1.260\n",
      "    step: 450; loss: 1.906; l2dist: 1.249\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.435; l2dist: 0.000\n",
      "    step: 50; loss: 5.892; l2dist: 1.864\n",
      "    step: 100; loss: 2.957; l2dist: 1.488\n",
      "    step: 150; loss: 2.218; l2dist: 1.318\n",
      "    step: 200; loss: 2.037; l2dist: 1.269\n",
      "    step: 250; loss: 1.981; l2dist: 1.264\n",
      "    step: 300; loss: 1.938; l2dist: 1.248\n",
      "    step: 350; loss: 1.905; l2dist: 1.240\n",
      "    step: 400; loss: 1.904; l2dist: 1.236\n",
      "    step: 450; loss: 1.907; l2dist: 1.239\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.858; l2dist: 0.000\n",
      "    step: 50; loss: 5.910; l2dist: 1.870\n",
      "    step: 100; loss: 2.986; l2dist: 1.503\n",
      "    step: 150; loss: 2.240; l2dist: 1.341\n",
      "    step: 200; loss: 2.047; l2dist: 1.297\n",
      "    step: 250; loss: 1.972; l2dist: 1.274\n",
      "    step: 300; loss: 1.947; l2dist: 1.270\n",
      "    step: 350; loss: 1.930; l2dist: 1.268\n",
      "    step: 400; loss: 1.923; l2dist: 1.259\n",
      "    step: 450; loss: 1.899; l2dist: 1.254\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.623; l2dist: 0.000\n",
      "    step: 50; loss: 5.910; l2dist: 1.865\n",
      "    step: 100; loss: 2.997; l2dist: 1.509\n",
      "    step: 150; loss: 2.237; l2dist: 1.344\n",
      "    step: 200; loss: 2.052; l2dist: 1.298\n",
      "    step: 250; loss: 1.991; l2dist: 1.272\n",
      "    step: 300; loss: 1.951; l2dist: 1.268\n",
      "    step: 350; loss: 1.921; l2dist: 1.260\n",
      "    step: 400; loss: 1.917; l2dist: 1.257\n",
      "    step: 450; loss: 1.911; l2dist: 1.259\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.834; l2dist: 0.000\n",
      "    step: 50; loss: 5.943; l2dist: 1.873\n",
      "    step: 100; loss: 2.999; l2dist: 1.514\n",
      "    step: 150; loss: 2.238; l2dist: 1.343\n",
      "    step: 200; loss: 2.060; l2dist: 1.302\n",
      "    step: 250; loss: 1.977; l2dist: 1.281\n",
      "    step: 300; loss: 1.956; l2dist: 1.274\n",
      "    step: 350; loss: 1.932; l2dist: 1.266\n",
      "    step: 400; loss: 1.920; l2dist: 1.259\n",
      "    step: 450; loss: 1.918; l2dist: 1.263\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.249; l2dist: 0.000\n",
      "    step: 50; loss: 12.780; l2dist: 3.339\n",
      "    step: 100; loss: 5.762; l2dist: 2.309\n",
      "    step: 150; loss: 3.841; l2dist: 1.860\n",
      "    step: 200; loss: 3.080; l2dist: 1.656\n",
      "    step: 250; loss: 2.804; l2dist: 1.555\n",
      "    step: 300; loss: 2.645; l2dist: 1.517\n",
      "    step: 350; loss: 2.616; l2dist: 1.496\n",
      "    step: 400; loss: 2.486; l2dist: 1.467\n",
      "    step: 450; loss: 2.468; l2dist: 1.460\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 136.020; l2dist: 0.000\n",
      "    step: 50; loss: 12.227; l2dist: 3.218\n",
      "    step: 100; loss: 5.524; l2dist: 2.213\n",
      "    step: 150; loss: 3.778; l2dist: 1.812\n",
      "    step: 200; loss: 3.101; l2dist: 1.628\n",
      "    step: 250; loss: 2.756; l2dist: 1.543\n",
      "    step: 300; loss: 2.592; l2dist: 1.491\n",
      "    step: 350; loss: 2.518; l2dist: 1.475\n",
      "    step: 400; loss: 2.413; l2dist: 1.442\n",
      "    step: 450; loss: 2.422; l2dist: 1.450\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.063; l2dist: 0.000\n",
      "    step: 50; loss: 9.425; l2dist: 2.775\n",
      "    step: 100; loss: 4.219; l2dist: 1.915\n",
      "    step: 150; loss: 3.042; l2dist: 1.621\n",
      "    step: 200; loss: 2.652; l2dist: 1.513\n",
      "    step: 250; loss: 2.463; l2dist: 1.454\n",
      "    step: 300; loss: 2.379; l2dist: 1.430\n",
      "    step: 350; loss: 2.376; l2dist: 1.429\n",
      "    step: 400; loss: 2.296; l2dist: 1.403\n",
      "    step: 450; loss: 2.279; l2dist: 1.404\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.875; l2dist: 0.000\n",
      "    step: 50; loss: 7.828; l2dist: 2.423\n",
      "    step: 100; loss: 3.670; l2dist: 1.776\n",
      "    step: 150; loss: 2.731; l2dist: 1.534\n",
      "    step: 200; loss: 2.460; l2dist: 1.458\n",
      "    step: 250; loss: 2.336; l2dist: 1.412\n",
      "    step: 300; loss: 2.285; l2dist: 1.398\n",
      "    step: 350; loss: 2.254; l2dist: 1.386\n",
      "    step: 400; loss: 2.256; l2dist: 1.391\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.610; l2dist: 0.000\n",
      "    step: 50; loss: 6.963; l2dist: 2.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 3.503; l2dist: 1.682\n",
      "    step: 150; loss: 2.619; l2dist: 1.470\n",
      "    step: 200; loss: 2.359; l2dist: 1.412\n",
      "    step: 250; loss: 2.291; l2dist: 1.391\n",
      "    step: 300; loss: 2.241; l2dist: 1.381\n",
      "    step: 350; loss: 2.226; l2dist: 1.375\n",
      "    step: 400; loss: 2.217; l2dist: 1.368\n",
      "    step: 450; loss: 2.213; l2dist: 1.367\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.234; l2dist: 0.000\n",
      "    step: 50; loss: 6.495; l2dist: 2.071\n",
      "    step: 100; loss: 3.385; l2dist: 1.609\n",
      "    step: 150; loss: 2.542; l2dist: 1.427\n",
      "    step: 200; loss: 2.343; l2dist: 1.378\n",
      "    step: 250; loss: 2.272; l2dist: 1.358\n",
      "    step: 300; loss: 2.212; l2dist: 1.349\n",
      "    step: 350; loss: 2.195; l2dist: 1.346\n",
      "    step: 400; loss: 2.182; l2dist: 1.337\n",
      "    step: 450; loss: 2.171; l2dist: 1.338\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.604; l2dist: 0.000\n",
      "    step: 50; loss: 6.434; l2dist: 2.043\n",
      "    step: 100; loss: 3.338; l2dist: 1.593\n",
      "    step: 150; loss: 2.513; l2dist: 1.414\n",
      "    step: 200; loss: 2.314; l2dist: 1.363\n",
      "    step: 250; loss: 2.237; l2dist: 1.347\n",
      "    step: 300; loss: 2.214; l2dist: 1.337\n",
      "    step: 350; loss: 2.184; l2dist: 1.336\n",
      "    step: 400; loss: 2.159; l2dist: 1.328\n",
      "    step: 450; loss: 2.163; l2dist: 1.330\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.814; l2dist: 0.000\n",
      "    step: 50; loss: 6.385; l2dist: 2.032\n",
      "    step: 100; loss: 3.380; l2dist: 1.594\n",
      "    step: 150; loss: 2.543; l2dist: 1.422\n",
      "    step: 200; loss: 2.344; l2dist: 1.373\n",
      "    step: 250; loss: 2.281; l2dist: 1.353\n",
      "    step: 300; loss: 2.238; l2dist: 1.350\n",
      "    step: 350; loss: 2.216; l2dist: 1.341\n",
      "    step: 400; loss: 2.207; l2dist: 1.338\n",
      "    step: 450; loss: 2.197; l2dist: 1.338\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.081; l2dist: 0.000\n",
      "    step: 50; loss: 6.346; l2dist: 2.018\n",
      "    step: 100; loss: 3.370; l2dist: 1.588\n",
      "    step: 150; loss: 2.542; l2dist: 1.414\n",
      "    step: 200; loss: 2.341; l2dist: 1.362\n",
      "    step: 250; loss: 2.268; l2dist: 1.350\n",
      "    step: 300; loss: 2.242; l2dist: 1.346\n",
      "    step: 350; loss: 2.212; l2dist: 1.344\n",
      "    step: 400; loss: 2.208; l2dist: 1.339\n",
      "    step: 450; loss: 2.196; l2dist: 1.337\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.278; l2dist: 0.000\n",
      "    step: 50; loss: 6.403; l2dist: 2.025\n",
      "    step: 100; loss: 3.361; l2dist: 1.590\n",
      "    step: 150; loss: 2.529; l2dist: 1.417\n",
      "    step: 200; loss: 2.329; l2dist: 1.372\n",
      "    step: 250; loss: 2.259; l2dist: 1.356\n",
      "    step: 300; loss: 2.220; l2dist: 1.348\n",
      "    step: 350; loss: 2.217; l2dist: 1.344\n",
      "    step: 400; loss: 2.191; l2dist: 1.336\n",
      "    step: 450; loss: 2.195; l2dist: 1.337\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 124.505; l2dist: 0.000\n",
      "    step: 50; loss: 12.761; l2dist: 3.388\n",
      "    step: 100; loss: 5.874; l2dist: 2.337\n",
      "    step: 150; loss: 3.862; l2dist: 1.879\n",
      "    step: 200; loss: 3.121; l2dist: 1.683\n",
      "    step: 250; loss: 2.762; l2dist: 1.570\n",
      "    step: 300; loss: 2.607; l2dist: 1.527\n",
      "    step: 350; loss: 2.502; l2dist: 1.495\n",
      "    step: 400; loss: 2.494; l2dist: 1.473\n",
      "    step: 450; loss: 2.469; l2dist: 1.472\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 105.020; l2dist: 0.000\n",
      "    step: 50; loss: 10.875; l2dist: 3.094\n",
      "    step: 100; loss: 4.895; l2dist: 2.090\n",
      "    step: 150; loss: 3.398; l2dist: 1.726\n",
      "    step: 200; loss: 2.778; l2dist: 1.571\n",
      "    step: 250; loss: 2.544; l2dist: 1.500\n",
      "    step: 300; loss: 2.443; l2dist: 1.472\n",
      "    step: 350; loss: 2.393; l2dist: 1.453\n",
      "    step: 400; loss: 2.306; l2dist: 1.437\n",
      "    step: 450; loss: 2.292; l2dist: 1.429\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.680; l2dist: 0.000\n",
      "    step: 50; loss: 8.714; l2dist: 2.697\n",
      "    step: 100; loss: 3.891; l2dist: 1.861\n",
      "    step: 150; loss: 2.791; l2dist: 1.570\n",
      "    step: 200; loss: 2.489; l2dist: 1.486\n",
      "    step: 250; loss: 2.328; l2dist: 1.443\n",
      "    step: 300; loss: 2.279; l2dist: 1.419\n",
      "    step: 350; loss: 2.219; l2dist: 1.407\n",
      "    step: 400; loss: 2.214; l2dist: 1.406\n",
      "    step: 450; loss: 2.206; l2dist: 1.396\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.370; l2dist: 0.000\n",
      "    step: 50; loss: 7.623; l2dist: 2.428\n",
      "    step: 100; loss: 3.506; l2dist: 1.758\n",
      "    step: 150; loss: 2.609; l2dist: 1.511\n",
      "    step: 200; loss: 2.351; l2dist: 1.441\n",
      "    step: 250; loss: 2.268; l2dist: 1.415\n",
      "    step: 300; loss: 2.220; l2dist: 1.394\n",
      "    step: 350; loss: 2.200; l2dist: 1.398\n",
      "    step: 400; loss: 2.190; l2dist: 1.388\n",
      "    step: 450; loss: 2.178; l2dist: 1.386\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.315; l2dist: 0.000\n",
      "    step: 50; loss: 6.976; l2dist: 2.217\n",
      "    step: 100; loss: 3.358; l2dist: 1.680\n",
      "    step: 150; loss: 2.495; l2dist: 1.475\n",
      "    step: 200; loss: 2.275; l2dist: 1.406\n",
      "    step: 250; loss: 2.200; l2dist: 1.387\n",
      "    step: 300; loss: 2.169; l2dist: 1.374\n",
      "    step: 350; loss: 2.142; l2dist: 1.371\n",
      "    step: 400; loss: 2.160; l2dist: 1.363\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.210; l2dist: 0.000\n",
      "    step: 50; loss: 6.786; l2dist: 2.170\n",
      "    step: 100; loss: 3.297; l2dist: 1.642\n",
      "    step: 150; loss: 2.461; l2dist: 1.443\n",
      "    step: 200; loss: 2.264; l2dist: 1.393\n",
      "    step: 250; loss: 2.179; l2dist: 1.377\n",
      "    step: 300; loss: 2.156; l2dist: 1.364\n",
      "    step: 350; loss: 2.143; l2dist: 1.362\n",
      "    step: 400; loss: 2.125; l2dist: 1.360\n",
      "    step: 450; loss: 2.131; l2dist: 1.364\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.010; l2dist: 0.000\n",
      "    step: 50; loss: 6.627; l2dist: 2.112\n",
      "    step: 100; loss: 3.277; l2dist: 1.616\n",
      "    step: 150; loss: 2.485; l2dist: 1.417\n",
      "    step: 200; loss: 2.280; l2dist: 1.363\n",
      "    step: 250; loss: 2.207; l2dist: 1.350\n",
      "    step: 300; loss: 2.178; l2dist: 1.345\n",
      "    step: 350; loss: 2.166; l2dist: 1.346\n",
      "    step: 400; loss: 2.148; l2dist: 1.338\n",
      "    step: 450; loss: 2.144; l2dist: 1.336\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.306; l2dist: 0.000\n",
      "    step: 50; loss: 6.626; l2dist: 2.111\n",
      "    step: 100; loss: 3.294; l2dist: 1.625\n",
      "    step: 150; loss: 2.481; l2dist: 1.428\n",
      "    step: 200; loss: 2.297; l2dist: 1.395\n",
      "    step: 250; loss: 2.211; l2dist: 1.358\n",
      "    step: 300; loss: 2.185; l2dist: 1.357\n",
      "    step: 350; loss: 2.165; l2dist: 1.357\n",
      "    step: 400; loss: 2.162; l2dist: 1.349\n",
      "    step: 450; loss: 2.148; l2dist: 1.345\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.965; l2dist: 0.000\n",
      "    step: 50; loss: 6.608; l2dist: 2.106\n",
      "    step: 100; loss: 3.286; l2dist: 1.626\n",
      "    step: 150; loss: 2.472; l2dist: 1.425\n",
      "    step: 200; loss: 2.271; l2dist: 1.379\n",
      "    step: 250; loss: 2.209; l2dist: 1.366\n",
      "    step: 300; loss: 2.185; l2dist: 1.352\n",
      "    step: 350; loss: 2.166; l2dist: 1.348\n",
      "    step: 400; loss: 2.152; l2dist: 1.346\n",
      "    step: 450; loss: 2.158; l2dist: 1.346\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.184; l2dist: 0.000\n",
      "    step: 50; loss: 6.650; l2dist: 2.120\n",
      "    step: 100; loss: 3.297; l2dist: 1.631\n",
      "    step: 150; loss: 2.477; l2dist: 1.439\n",
      "    step: 200; loss: 2.280; l2dist: 1.385\n",
      "    step: 250; loss: 2.227; l2dist: 1.365\n",
      "    step: 300; loss: 2.220; l2dist: 1.375\n",
      "    step: 350; loss: 2.169; l2dist: 1.362\n",
      "    step: 400; loss: 2.162; l2dist: 1.358\n",
      "    step: 450; loss: 2.155; l2dist: 1.357\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 121.381; l2dist: 0.000\n",
      "    step: 50; loss: 15.151; l2dist: 3.412\n",
      "    step: 100; loss: 5.733; l2dist: 2.309\n",
      "    step: 150; loss: 3.749; l2dist: 1.848\n",
      "    step: 200; loss: 2.959; l2dist: 1.631\n",
      "    step: 250; loss: 2.615; l2dist: 1.531\n",
      "    step: 300; loss: 2.553; l2dist: 1.496\n",
      "    step: 350; loss: 2.454; l2dist: 1.466\n",
      "    step: 400; loss: 2.328; l2dist: 1.435\n",
      "    step: 450; loss: 2.279; l2dist: 1.419\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 100.091; l2dist: 0.000\n",
      "    step: 50; loss: 12.770; l2dist: 3.110\n",
      "    step: 100; loss: 4.929; l2dist: 2.090\n",
      "    step: 150; loss: 3.337; l2dist: 1.707\n",
      "    step: 200; loss: 2.776; l2dist: 1.554\n",
      "    step: 250; loss: 2.488; l2dist: 1.470\n",
      "    step: 300; loss: 2.385; l2dist: 1.434\n",
      "    step: 350; loss: 2.287; l2dist: 1.419\n",
      "    step: 400; loss: 2.266; l2dist: 1.406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 450; loss: 2.246; l2dist: 1.400\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.060; l2dist: 0.000\n",
      "    step: 50; loss: 9.625; l2dist: 2.661\n",
      "    step: 100; loss: 3.702; l2dist: 1.804\n",
      "    step: 150; loss: 2.718; l2dist: 1.538\n",
      "    step: 200; loss: 2.445; l2dist: 1.451\n",
      "    step: 250; loss: 2.288; l2dist: 1.414\n",
      "    step: 300; loss: 2.241; l2dist: 1.393\n",
      "    step: 350; loss: 2.171; l2dist: 1.369\n",
      "    step: 400; loss: 2.144; l2dist: 1.366\n",
      "    step: 450; loss: 2.150; l2dist: 1.370\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.751; l2dist: 0.000\n",
      "    step: 50; loss: 8.041; l2dist: 2.340\n",
      "    step: 100; loss: 3.309; l2dist: 1.683\n",
      "    step: 150; loss: 2.514; l2dist: 1.462\n",
      "    step: 200; loss: 2.294; l2dist: 1.411\n",
      "    step: 250; loss: 2.195; l2dist: 1.381\n",
      "    step: 300; loss: 2.163; l2dist: 1.370\n",
      "    step: 350; loss: 2.133; l2dist: 1.355\n",
      "    step: 400; loss: 2.100; l2dist: 1.355\n",
      "    step: 450; loss: 2.088; l2dist: 1.343\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.647; l2dist: 0.000\n",
      "    step: 50; loss: 7.079; l2dist: 2.093\n",
      "    step: 100; loss: 3.220; l2dist: 1.584\n",
      "    step: 150; loss: 2.457; l2dist: 1.425\n",
      "    step: 200; loss: 2.237; l2dist: 1.368\n",
      "    step: 250; loss: 2.161; l2dist: 1.352\n",
      "    step: 300; loss: 2.106; l2dist: 1.335\n",
      "    step: 350; loss: 2.086; l2dist: 1.322\n",
      "    step: 400; loss: 2.084; l2dist: 1.332\n",
      "    step: 450; loss: 2.088; l2dist: 1.318\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.177; l2dist: 0.000\n",
      "    step: 50; loss: 6.849; l2dist: 2.007\n",
      "    step: 100; loss: 3.154; l2dist: 1.544\n",
      "    step: 150; loss: 2.407; l2dist: 1.383\n",
      "    step: 200; loss: 2.213; l2dist: 1.339\n",
      "    step: 250; loss: 2.120; l2dist: 1.323\n",
      "    step: 300; loss: 2.082; l2dist: 1.315\n",
      "    step: 350; loss: 2.061; l2dist: 1.309\n",
      "    step: 400; loss: 2.065; l2dist: 1.305\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.466; l2dist: 0.000\n",
      "    step: 50; loss: 6.787; l2dist: 1.973\n",
      "    step: 100; loss: 3.179; l2dist: 1.552\n",
      "    step: 150; loss: 2.391; l2dist: 1.393\n",
      "    step: 200; loss: 2.199; l2dist: 1.353\n",
      "    step: 250; loss: 2.124; l2dist: 1.328\n",
      "    step: 300; loss: 2.089; l2dist: 1.315\n",
      "    step: 350; loss: 2.071; l2dist: 1.317\n",
      "    step: 400; loss: 2.056; l2dist: 1.312\n",
      "    step: 450; loss: 2.048; l2dist: 1.308\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.045; l2dist: 0.000\n",
      "    step: 50; loss: 6.715; l2dist: 1.952\n",
      "    step: 100; loss: 3.186; l2dist: 1.546\n",
      "    step: 150; loss: 2.398; l2dist: 1.386\n",
      "    step: 200; loss: 2.206; l2dist: 1.350\n",
      "    step: 250; loss: 2.126; l2dist: 1.327\n",
      "    step: 300; loss: 2.087; l2dist: 1.314\n",
      "    step: 350; loss: 2.068; l2dist: 1.310\n",
      "    step: 400; loss: 2.065; l2dist: 1.312\n",
      "    step: 450; loss: 2.050; l2dist: 1.304\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.546; l2dist: 0.000\n",
      "    step: 50; loss: 6.656; l2dist: 1.934\n",
      "    step: 100; loss: 3.177; l2dist: 1.539\n",
      "    step: 150; loss: 2.394; l2dist: 1.383\n",
      "    step: 200; loss: 2.184; l2dist: 1.336\n",
      "    step: 250; loss: 2.123; l2dist: 1.318\n",
      "    step: 300; loss: 2.084; l2dist: 1.313\n",
      "    step: 350; loss: 2.062; l2dist: 1.312\n",
      "    step: 400; loss: 2.056; l2dist: 1.302\n",
      "    step: 450; loss: 2.046; l2dist: 1.308\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.855; l2dist: 0.000\n",
      "    step: 50; loss: 6.718; l2dist: 1.950\n",
      "    step: 100; loss: 3.188; l2dist: 1.547\n",
      "    step: 150; loss: 2.396; l2dist: 1.388\n",
      "    step: 200; loss: 2.219; l2dist: 1.341\n",
      "    step: 250; loss: 2.124; l2dist: 1.327\n",
      "    step: 300; loss: 2.083; l2dist: 1.316\n",
      "    step: 350; loss: 2.065; l2dist: 1.313\n",
      "    step: 400; loss: 2.055; l2dist: 1.312\n",
      "    step: 450; loss: 2.051; l2dist: 1.312\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 126.057; l2dist: 0.000\n",
      "    step: 50; loss: 14.255; l2dist: 3.435\n",
      "    step: 100; loss: 5.966; l2dist: 2.368\n",
      "    step: 150; loss: 3.945; l2dist: 1.914\n",
      "    step: 200; loss: 3.113; l2dist: 1.695\n",
      "    step: 250; loss: 2.769; l2dist: 1.588\n",
      "    step: 300; loss: 2.647; l2dist: 1.532\n",
      "    step: 350; loss: 2.563; l2dist: 1.526\n",
      "    step: 400; loss: 2.553; l2dist: 1.519\n",
      "    step: 450; loss: 2.444; l2dist: 1.492\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 127.024; l2dist: 0.000\n",
      "    step: 50; loss: 12.966; l2dist: 3.251\n",
      "    step: 100; loss: 5.510; l2dist: 2.222\n",
      "    step: 150; loss: 3.772; l2dist: 1.826\n",
      "    step: 200; loss: 3.111; l2dist: 1.652\n",
      "    step: 250; loss: 2.791; l2dist: 1.568\n",
      "    step: 300; loss: 2.598; l2dist: 1.527\n",
      "    step: 350; loss: 2.553; l2dist: 1.503\n",
      "    step: 400; loss: 2.473; l2dist: 1.491\n",
      "    step: 450; loss: 2.423; l2dist: 1.474\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.186; l2dist: 0.000\n",
      "    step: 50; loss: 10.146; l2dist: 2.827\n",
      "    step: 100; loss: 4.248; l2dist: 1.946\n",
      "    step: 150; loss: 3.095; l2dist: 1.654\n",
      "    step: 200; loss: 2.681; l2dist: 1.543\n",
      "    step: 250; loss: 2.504; l2dist: 1.501\n",
      "    step: 300; loss: 2.423; l2dist: 1.468\n",
      "    step: 350; loss: 2.356; l2dist: 1.459\n",
      "    step: 400; loss: 2.361; l2dist: 1.448\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.548; l2dist: 0.000\n",
      "    step: 50; loss: 8.743; l2dist: 2.547\n",
      "    step: 100; loss: 3.772; l2dist: 1.828\n",
      "    step: 150; loss: 2.802; l2dist: 1.573\n",
      "    step: 200; loss: 2.502; l2dist: 1.494\n",
      "    step: 250; loss: 2.391; l2dist: 1.463\n",
      "    step: 300; loss: 2.344; l2dist: 1.449\n",
      "    step: 350; loss: 2.315; l2dist: 1.446\n",
      "    step: 400; loss: 2.278; l2dist: 1.436\n",
      "    step: 450; loss: 2.256; l2dist: 1.422\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.215; l2dist: 0.000\n",
      "    step: 50; loss: 8.201; l2dist: 2.393\n",
      "    step: 100; loss: 3.623; l2dist: 1.754\n",
      "    step: 150; loss: 2.719; l2dist: 1.538\n",
      "    step: 200; loss: 2.462; l2dist: 1.472\n",
      "    step: 250; loss: 2.344; l2dist: 1.444\n",
      "    step: 300; loss: 2.296; l2dist: 1.424\n",
      "    step: 350; loss: 2.263; l2dist: 1.422\n",
      "    step: 400; loss: 2.241; l2dist: 1.414\n",
      "    step: 450; loss: 2.230; l2dist: 1.411\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.471; l2dist: 0.000\n",
      "    step: 50; loss: 7.706; l2dist: 2.287\n",
      "    step: 100; loss: 3.472; l2dist: 1.683\n",
      "    step: 150; loss: 2.658; l2dist: 1.493\n",
      "    step: 200; loss: 2.401; l2dist: 1.429\n",
      "    step: 250; loss: 2.314; l2dist: 1.405\n",
      "    step: 300; loss: 2.281; l2dist: 1.399\n",
      "    step: 350; loss: 2.217; l2dist: 1.384\n",
      "    step: 400; loss: 2.231; l2dist: 1.386\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.158; l2dist: 0.000\n",
      "    step: 50; loss: 7.556; l2dist: 2.256\n",
      "    step: 100; loss: 3.433; l2dist: 1.664\n",
      "    step: 150; loss: 2.628; l2dist: 1.482\n",
      "    step: 200; loss: 2.391; l2dist: 1.425\n",
      "    step: 250; loss: 2.313; l2dist: 1.398\n",
      "    step: 300; loss: 2.289; l2dist: 1.400\n",
      "    step: 350; loss: 2.235; l2dist: 1.384\n",
      "    step: 400; loss: 2.225; l2dist: 1.387\n",
      "    step: 450; loss: 2.219; l2dist: 1.378\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.277; l2dist: 0.000\n",
      "    step: 50; loss: 7.482; l2dist: 2.241\n",
      "    step: 100; loss: 3.421; l2dist: 1.661\n",
      "    step: 150; loss: 2.619; l2dist: 1.481\n",
      "    step: 200; loss: 2.389; l2dist: 1.425\n",
      "    step: 250; loss: 2.286; l2dist: 1.395\n",
      "    step: 300; loss: 2.248; l2dist: 1.387\n",
      "    step: 350; loss: 2.216; l2dist: 1.379\n",
      "    step: 400; loss: 2.245; l2dist: 1.385\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.748; l2dist: 0.000\n",
      "    step: 50; loss: 7.453; l2dist: 2.236\n",
      "    step: 100; loss: 3.429; l2dist: 1.666\n",
      "    step: 150; loss: 2.614; l2dist: 1.478\n",
      "    step: 200; loss: 2.377; l2dist: 1.423\n",
      "    step: 250; loss: 2.297; l2dist: 1.398\n",
      "    step: 300; loss: 2.255; l2dist: 1.400\n",
      "    step: 350; loss: 2.228; l2dist: 1.387\n",
      "    step: 400; loss: 2.218; l2dist: 1.381\n",
      "    step: 450; loss: 2.207; l2dist: 1.382\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.898; l2dist: 0.000\n",
      "    step: 50; loss: 7.482; l2dist: 2.245\n",
      "    step: 100; loss: 3.434; l2dist: 1.674\n",
      "    step: 150; loss: 2.629; l2dist: 1.492\n",
      "    step: 200; loss: 2.394; l2dist: 1.428\n",
      "    step: 250; loss: 2.303; l2dist: 1.412\n",
      "    step: 300; loss: 2.250; l2dist: 1.399\n",
      "    step: 350; loss: 2.233; l2dist: 1.394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 2.225; l2dist: 1.389\n",
      "    step: 450; loss: 2.223; l2dist: 1.393\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.710; l2dist: 0.000\n",
      "    step: 50; loss: 12.560; l2dist: 3.247\n",
      "    step: 100; loss: 5.202; l2dist: 2.162\n",
      "    step: 150; loss: 3.443; l2dist: 1.734\n",
      "    step: 200; loss: 2.756; l2dist: 1.530\n",
      "    step: 250; loss: 2.489; l2dist: 1.443\n",
      "    step: 300; loss: 2.369; l2dist: 1.401\n",
      "    step: 350; loss: 2.356; l2dist: 1.390\n",
      "    step: 400; loss: 2.278; l2dist: 1.365\n",
      "    step: 450; loss: 2.304; l2dist: 1.372\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 78.766; l2dist: 0.000\n",
      "    step: 50; loss: 10.431; l2dist: 2.883\n",
      "    step: 100; loss: 4.025; l2dist: 1.859\n",
      "    step: 150; loss: 2.864; l2dist: 1.544\n",
      "    step: 200; loss: 2.471; l2dist: 1.420\n",
      "    step: 250; loss: 2.291; l2dist: 1.368\n",
      "    step: 300; loss: 2.241; l2dist: 1.348\n",
      "    step: 350; loss: 2.178; l2dist: 1.336\n",
      "    step: 400; loss: 2.194; l2dist: 1.332\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.949; l2dist: 0.000\n",
      "    step: 50; loss: 8.285; l2dist: 2.459\n",
      "    step: 100; loss: 3.298; l2dist: 1.670\n",
      "    step: 150; loss: 2.496; l2dist: 1.436\n",
      "    step: 200; loss: 2.272; l2dist: 1.362\n",
      "    step: 250; loss: 2.179; l2dist: 1.333\n",
      "    step: 300; loss: 2.143; l2dist: 1.318\n",
      "    step: 350; loss: 2.102; l2dist: 1.306\n",
      "    step: 400; loss: 2.124; l2dist: 1.308\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.797; l2dist: 0.000\n",
      "    step: 50; loss: 7.076; l2dist: 2.189\n",
      "    step: 100; loss: 3.082; l2dist: 1.601\n",
      "    step: 150; loss: 2.364; l2dist: 1.390\n",
      "    step: 200; loss: 2.184; l2dist: 1.331\n",
      "    step: 250; loss: 2.112; l2dist: 1.306\n",
      "    step: 300; loss: 2.076; l2dist: 1.295\n",
      "    step: 350; loss: 2.094; l2dist: 1.304\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.166; l2dist: 0.000\n",
      "    step: 50; loss: 6.234; l2dist: 1.973\n",
      "    step: 100; loss: 3.056; l2dist: 1.551\n",
      "    step: 150; loss: 2.315; l2dist: 1.359\n",
      "    step: 200; loss: 2.149; l2dist: 1.312\n",
      "    step: 250; loss: 2.086; l2dist: 1.296\n",
      "    step: 300; loss: 2.073; l2dist: 1.277\n",
      "    step: 350; loss: 2.041; l2dist: 1.278\n",
      "    step: 400; loss: 2.029; l2dist: 1.272\n",
      "    step: 450; loss: 2.027; l2dist: 1.276\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.070; l2dist: 0.000\n",
      "    step: 50; loss: 5.845; l2dist: 1.869\n",
      "    step: 100; loss: 3.003; l2dist: 1.482\n",
      "    step: 150; loss: 2.304; l2dist: 1.331\n",
      "    step: 200; loss: 2.138; l2dist: 1.290\n",
      "    step: 250; loss: 2.077; l2dist: 1.275\n",
      "    step: 300; loss: 2.047; l2dist: 1.263\n",
      "    step: 350; loss: 2.038; l2dist: 1.264\n",
      "    step: 400; loss: 2.024; l2dist: 1.261\n",
      "    step: 450; loss: 2.025; l2dist: 1.255\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.240; l2dist: 0.000\n",
      "    step: 50; loss: 5.808; l2dist: 1.863\n",
      "    step: 100; loss: 3.000; l2dist: 1.485\n",
      "    step: 150; loss: 2.296; l2dist: 1.321\n",
      "    step: 200; loss: 2.130; l2dist: 1.277\n",
      "    step: 250; loss: 2.076; l2dist: 1.262\n",
      "    step: 300; loss: 2.049; l2dist: 1.254\n",
      "    step: 350; loss: 2.032; l2dist: 1.251\n",
      "    step: 400; loss: 2.022; l2dist: 1.256\n",
      "    step: 450; loss: 2.018; l2dist: 1.248\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.612; l2dist: 0.000\n",
      "    step: 50; loss: 5.763; l2dist: 1.849\n",
      "    step: 100; loss: 3.009; l2dist: 1.474\n",
      "    step: 150; loss: 2.299; l2dist: 1.318\n",
      "    step: 200; loss: 2.152; l2dist: 1.281\n",
      "    step: 250; loss: 2.079; l2dist: 1.266\n",
      "    step: 300; loss: 2.054; l2dist: 1.260\n",
      "    step: 350; loss: 2.032; l2dist: 1.258\n",
      "    step: 400; loss: 2.032; l2dist: 1.257\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.341; l2dist: 0.000\n",
      "    step: 50; loss: 5.745; l2dist: 1.843\n",
      "    step: 100; loss: 3.010; l2dist: 1.470\n",
      "    step: 150; loss: 2.300; l2dist: 1.316\n",
      "    step: 200; loss: 2.133; l2dist: 1.277\n",
      "    step: 250; loss: 2.071; l2dist: 1.265\n",
      "    step: 300; loss: 2.046; l2dist: 1.263\n",
      "    step: 350; loss: 2.038; l2dist: 1.257\n",
      "    step: 400; loss: 2.019; l2dist: 1.250\n",
      "    step: 450; loss: 2.021; l2dist: 1.252\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.482; l2dist: 0.000\n",
      "    step: 50; loss: 5.779; l2dist: 1.854\n",
      "    step: 100; loss: 3.019; l2dist: 1.482\n",
      "    step: 150; loss: 2.308; l2dist: 1.325\n",
      "    step: 200; loss: 2.141; l2dist: 1.285\n",
      "    step: 250; loss: 2.077; l2dist: 1.270\n",
      "    step: 300; loss: 2.051; l2dist: 1.265\n",
      "    step: 350; loss: 2.049; l2dist: 1.262\n",
      "    step: 400; loss: 2.034; l2dist: 1.262\n",
      "    step: 450; loss: 2.018; l2dist: 1.257\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 114.668; l2dist: 0.000\n",
      "    step: 50; loss: 11.916; l2dist: 3.246\n",
      "    step: 100; loss: 5.048; l2dist: 2.169\n",
      "    step: 150; loss: 3.335; l2dist: 1.744\n",
      "    step: 200; loss: 2.679; l2dist: 1.550\n",
      "    step: 250; loss: 2.376; l2dist: 1.457\n",
      "    step: 300; loss: 2.263; l2dist: 1.409\n",
      "    step: 350; loss: 2.178; l2dist: 1.383\n",
      "    step: 400; loss: 2.135; l2dist: 1.373\n",
      "    step: 450; loss: 2.136; l2dist: 1.368\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 90.977; l2dist: 0.000\n",
      "    step: 50; loss: 10.040; l2dist: 2.924\n",
      "    step: 100; loss: 4.136; l2dist: 1.932\n",
      "    step: 150; loss: 2.870; l2dist: 1.590\n",
      "    step: 200; loss: 2.385; l2dist: 1.444\n",
      "    step: 250; loss: 2.156; l2dist: 1.380\n",
      "    step: 300; loss: 2.104; l2dist: 1.365\n",
      "    step: 350; loss: 2.016; l2dist: 1.331\n",
      "    step: 400; loss: 2.021; l2dist: 1.328\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.721; l2dist: 0.000\n",
      "    step: 50; loss: 8.107; l2dist: 2.536\n",
      "    step: 100; loss: 3.320; l2dist: 1.717\n",
      "    step: 150; loss: 2.443; l2dist: 1.470\n",
      "    step: 200; loss: 2.163; l2dist: 1.374\n",
      "    step: 250; loss: 2.047; l2dist: 1.341\n",
      "    step: 300; loss: 1.994; l2dist: 1.327\n",
      "    step: 350; loss: 1.961; l2dist: 1.314\n",
      "    step: 400; loss: 1.935; l2dist: 1.300\n",
      "    step: 450; loss: 1.946; l2dist: 1.301\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.128; l2dist: 0.000\n",
      "    step: 50; loss: 6.902; l2dist: 2.232\n",
      "    step: 100; loss: 3.051; l2dist: 1.631\n",
      "    step: 150; loss: 2.272; l2dist: 1.404\n",
      "    step: 200; loss: 2.066; l2dist: 1.342\n",
      "    step: 250; loss: 1.980; l2dist: 1.310\n",
      "    step: 300; loss: 1.928; l2dist: 1.297\n",
      "    step: 350; loss: 1.920; l2dist: 1.291\n",
      "    step: 400; loss: 1.907; l2dist: 1.282\n",
      "    step: 450; loss: 1.889; l2dist: 1.279\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.807; l2dist: 0.000\n",
      "    step: 50; loss: 6.162; l2dist: 2.034\n",
      "    step: 100; loss: 3.017; l2dist: 1.567\n",
      "    step: 150; loss: 2.236; l2dist: 1.376\n",
      "    step: 200; loss: 2.042; l2dist: 1.319\n",
      "    step: 250; loss: 1.969; l2dist: 1.300\n",
      "    step: 300; loss: 1.929; l2dist: 1.285\n",
      "    step: 350; loss: 1.915; l2dist: 1.275\n",
      "    step: 400; loss: 1.903; l2dist: 1.278\n",
      "    step: 450; loss: 1.897; l2dist: 1.273\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.067; l2dist: 0.000\n",
      "    step: 50; loss: 5.818; l2dist: 1.925\n",
      "    step: 100; loss: 2.928; l2dist: 1.510\n",
      "    step: 150; loss: 2.202; l2dist: 1.336\n",
      "    step: 200; loss: 2.005; l2dist: 1.289\n",
      "    step: 250; loss: 1.934; l2dist: 1.263\n",
      "    step: 300; loss: 1.894; l2dist: 1.260\n",
      "    step: 350; loss: 1.875; l2dist: 1.250\n",
      "    step: 400; loss: 1.864; l2dist: 1.250\n",
      "    step: 450; loss: 1.856; l2dist: 1.243\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.596; l2dist: 0.000\n",
      "    step: 50; loss: 5.697; l2dist: 1.886\n",
      "    step: 100; loss: 2.917; l2dist: 1.487\n",
      "    step: 150; loss: 2.174; l2dist: 1.318\n",
      "    step: 200; loss: 1.992; l2dist: 1.270\n",
      "    step: 250; loss: 1.923; l2dist: 1.254\n",
      "    step: 300; loss: 1.884; l2dist: 1.240\n",
      "    step: 350; loss: 1.862; l2dist: 1.238\n",
      "    step: 400; loss: 1.852; l2dist: 1.237\n",
      "    step: 450; loss: 1.850; l2dist: 1.235\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.783; l2dist: 0.000\n",
      "    step: 50; loss: 5.633; l2dist: 1.864\n",
      "    step: 100; loss: 2.922; l2dist: 1.481\n",
      "    step: 150; loss: 2.167; l2dist: 1.315\n",
      "    step: 200; loss: 1.984; l2dist: 1.277\n",
      "    step: 250; loss: 1.914; l2dist: 1.255\n",
      "    step: 300; loss: 1.875; l2dist: 1.244\n",
      "    step: 350; loss: 1.859; l2dist: 1.241\n",
      "    step: 400; loss: 1.858; l2dist: 1.240\n",
      "    step: 450; loss: 1.856; l2dist: 1.235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.494; l2dist: 0.000\n",
      "    step: 50; loss: 5.613; l2dist: 1.858\n",
      "    step: 100; loss: 2.918; l2dist: 1.477\n",
      "    step: 150; loss: 2.163; l2dist: 1.311\n",
      "    step: 200; loss: 1.980; l2dist: 1.270\n",
      "    step: 250; loss: 1.912; l2dist: 1.256\n",
      "    step: 300; loss: 1.876; l2dist: 1.241\n",
      "    step: 350; loss: 1.868; l2dist: 1.239\n",
      "    step: 400; loss: 1.852; l2dist: 1.233\n",
      "    step: 450; loss: 1.848; l2dist: 1.228\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.623; l2dist: 0.000\n",
      "    step: 50; loss: 5.644; l2dist: 1.869\n",
      "    step: 100; loss: 2.921; l2dist: 1.484\n",
      "    step: 150; loss: 2.163; l2dist: 1.321\n",
      "    step: 200; loss: 1.992; l2dist: 1.274\n",
      "    step: 250; loss: 1.915; l2dist: 1.252\n",
      "    step: 300; loss: 1.893; l2dist: 1.247\n",
      "    step: 350; loss: 1.867; l2dist: 1.242\n",
      "    step: 400; loss: 1.850; l2dist: 1.240\n",
      "    step: 450; loss: 1.846; l2dist: 1.235\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 121.583; l2dist: 0.000\n",
      "    step: 50; loss: 13.865; l2dist: 3.308\n",
      "    step: 100; loss: 5.719; l2dist: 2.306\n",
      "    step: 150; loss: 3.749; l2dist: 1.846\n",
      "    step: 200; loss: 3.021; l2dist: 1.653\n",
      "    step: 250; loss: 2.757; l2dist: 1.576\n",
      "    step: 300; loss: 2.539; l2dist: 1.509\n",
      "    step: 350; loss: 2.421; l2dist: 1.459\n",
      "    step: 400; loss: 2.364; l2dist: 1.450\n",
      "    step: 450; loss: 2.307; l2dist: 1.430\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 121.424; l2dist: 0.000\n",
      "    step: 50; loss: 12.970; l2dist: 3.132\n",
      "    step: 100; loss: 5.584; l2dist: 2.201\n",
      "    step: 150; loss: 3.751; l2dist: 1.794\n",
      "    step: 200; loss: 3.077; l2dist: 1.622\n",
      "    step: 250; loss: 2.718; l2dist: 1.529\n",
      "    step: 300; loss: 2.521; l2dist: 1.484\n",
      "    step: 350; loss: 2.418; l2dist: 1.452\n",
      "    step: 400; loss: 2.416; l2dist: 1.453\n",
      "    step: 450; loss: 2.354; l2dist: 1.434\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.611; l2dist: 0.000\n",
      "    step: 50; loss: 10.074; l2dist: 2.745\n",
      "    step: 100; loss: 4.177; l2dist: 1.912\n",
      "    step: 150; loss: 2.993; l2dist: 1.614\n",
      "    step: 200; loss: 2.589; l2dist: 1.505\n",
      "    step: 250; loss: 2.388; l2dist: 1.454\n",
      "    step: 300; loss: 2.299; l2dist: 1.421\n",
      "    step: 350; loss: 2.297; l2dist: 1.423\n",
      "    step: 400; loss: 2.208; l2dist: 1.389\n",
      "    step: 450; loss: 2.203; l2dist: 1.391\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.601; l2dist: 0.000\n",
      "    step: 50; loss: 8.264; l2dist: 2.446\n",
      "    step: 100; loss: 3.605; l2dist: 1.779\n",
      "    step: 150; loss: 2.670; l2dist: 1.531\n",
      "    step: 200; loss: 2.390; l2dist: 1.440\n",
      "    step: 250; loss: 2.250; l2dist: 1.411\n",
      "    step: 300; loss: 2.196; l2dist: 1.388\n",
      "    step: 350; loss: 2.186; l2dist: 1.385\n",
      "    step: 400; loss: 2.147; l2dist: 1.372\n",
      "    step: 450; loss: 2.136; l2dist: 1.369\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.001; l2dist: 0.000\n",
      "    step: 50; loss: 7.262; l2dist: 2.256\n",
      "    step: 100; loss: 3.374; l2dist: 1.677\n",
      "    step: 150; loss: 2.510; l2dist: 1.462\n",
      "    step: 200; loss: 2.293; l2dist: 1.403\n",
      "    step: 250; loss: 2.180; l2dist: 1.373\n",
      "    step: 300; loss: 2.136; l2dist: 1.362\n",
      "    step: 350; loss: 2.103; l2dist: 1.352\n",
      "    step: 400; loss: 2.108; l2dist: 1.343\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.099; l2dist: 0.000\n",
      "    step: 50; loss: 6.831; l2dist: 2.159\n",
      "    step: 100; loss: 3.241; l2dist: 1.614\n",
      "    step: 150; loss: 2.477; l2dist: 1.426\n",
      "    step: 200; loss: 2.238; l2dist: 1.368\n",
      "    step: 250; loss: 2.164; l2dist: 1.349\n",
      "    step: 300; loss: 2.107; l2dist: 1.328\n",
      "    step: 350; loss: 2.089; l2dist: 1.324\n",
      "    step: 400; loss: 2.069; l2dist: 1.318\n",
      "    step: 450; loss: 2.058; l2dist: 1.320\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.180; l2dist: 0.000\n",
      "    step: 50; loss: 6.751; l2dist: 2.132\n",
      "    step: 100; loss: 3.204; l2dist: 1.602\n",
      "    step: 150; loss: 2.461; l2dist: 1.416\n",
      "    step: 200; loss: 2.229; l2dist: 1.358\n",
      "    step: 250; loss: 2.136; l2dist: 1.336\n",
      "    step: 300; loss: 2.089; l2dist: 1.323\n",
      "    step: 350; loss: 2.083; l2dist: 1.317\n",
      "    step: 400; loss: 2.069; l2dist: 1.313\n",
      "    step: 450; loss: 2.051; l2dist: 1.316\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.384; l2dist: 0.000\n",
      "    step: 50; loss: 6.692; l2dist: 2.125\n",
      "    step: 100; loss: 3.201; l2dist: 1.608\n",
      "    step: 150; loss: 2.457; l2dist: 1.425\n",
      "    step: 200; loss: 2.224; l2dist: 1.368\n",
      "    step: 250; loss: 2.153; l2dist: 1.345\n",
      "    step: 300; loss: 2.095; l2dist: 1.331\n",
      "    step: 350; loss: 2.075; l2dist: 1.327\n",
      "    step: 400; loss: 2.076; l2dist: 1.331\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.678; l2dist: 0.000\n",
      "    step: 50; loss: 6.641; l2dist: 2.114\n",
      "    step: 100; loss: 3.195; l2dist: 1.598\n",
      "    step: 150; loss: 2.456; l2dist: 1.421\n",
      "    step: 200; loss: 2.232; l2dist: 1.369\n",
      "    step: 250; loss: 2.150; l2dist: 1.349\n",
      "    step: 300; loss: 2.102; l2dist: 1.335\n",
      "    step: 350; loss: 2.088; l2dist: 1.328\n",
      "    step: 400; loss: 2.074; l2dist: 1.325\n",
      "    step: 450; loss: 2.061; l2dist: 1.322\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.035; l2dist: 0.000\n",
      "    step: 50; loss: 6.679; l2dist: 2.124\n",
      "    step: 100; loss: 3.201; l2dist: 1.610\n",
      "    step: 150; loss: 2.459; l2dist: 1.424\n",
      "    step: 200; loss: 2.229; l2dist: 1.373\n",
      "    step: 250; loss: 2.152; l2dist: 1.347\n",
      "    step: 300; loss: 2.112; l2dist: 1.343\n",
      "    step: 350; loss: 2.083; l2dist: 1.333\n",
      "    step: 400; loss: 2.075; l2dist: 1.327\n",
      "    step: 450; loss: 2.063; l2dist: 1.323\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.065; l2dist: 0.000\n",
      "    step: 50; loss: 10.999; l2dist: 3.161\n",
      "    step: 100; loss: 4.832; l2dist: 2.104\n",
      "    step: 150; loss: 3.176; l2dist: 1.691\n",
      "    step: 200; loss: 2.602; l2dist: 1.518\n",
      "    step: 250; loss: 2.370; l2dist: 1.437\n",
      "    step: 300; loss: 2.260; l2dist: 1.408\n",
      "    step: 350; loss: 2.215; l2dist: 1.392\n",
      "    step: 400; loss: 2.167; l2dist: 1.369\n",
      "    step: 450; loss: 2.158; l2dist: 1.366\n",
      "binary step: 0; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.033; l2dist: 0.000\n",
      "    step: 50; loss: 8.147; l2dist: 2.634\n",
      "    step: 100; loss: 3.432; l2dist: 1.760\n",
      "    step: 150; loss: 2.479; l2dist: 1.483\n",
      "    step: 200; loss: 2.233; l2dist: 1.387\n",
      "    step: 250; loss: 2.102; l2dist: 1.351\n",
      "    step: 300; loss: 2.047; l2dist: 1.334\n",
      "    step: 350; loss: 2.018; l2dist: 1.324\n",
      "    step: 400; loss: 2.028; l2dist: 1.331\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.129; l2dist: 0.000\n",
      "    step: 50; loss: 6.822; l2dist: 2.345\n",
      "    step: 100; loss: 2.999; l2dist: 1.636\n",
      "    step: 150; loss: 2.246; l2dist: 1.401\n",
      "    step: 200; loss: 2.059; l2dist: 1.342\n",
      "    step: 250; loss: 1.981; l2dist: 1.314\n",
      "    step: 300; loss: 1.937; l2dist: 1.298\n",
      "    step: 350; loss: 1.931; l2dist: 1.288\n",
      "    step: 400; loss: 1.920; l2dist: 1.289\n",
      "    step: 450; loss: 1.913; l2dist: 1.291\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.942; l2dist: 0.000\n",
      "    step: 50; loss: 6.116; l2dist: 2.120\n",
      "    step: 100; loss: 2.910; l2dist: 1.594\n",
      "    step: 150; loss: 2.192; l2dist: 1.374\n",
      "    step: 200; loss: 2.004; l2dist: 1.316\n",
      "    step: 250; loss: 1.952; l2dist: 1.294\n",
      "    step: 300; loss: 1.917; l2dist: 1.279\n",
      "    step: 350; loss: 1.896; l2dist: 1.283\n",
      "    step: 400; loss: 1.895; l2dist: 1.277\n",
      "    step: 450; loss: 1.886; l2dist: 1.281\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 14.459; l2dist: 0.000\n",
      "    step: 50; loss: 5.512; l2dist: 1.887\n",
      "    step: 100; loss: 2.866; l2dist: 1.519\n",
      "    step: 150; loss: 2.145; l2dist: 1.338\n",
      "    step: 200; loss: 1.984; l2dist: 1.296\n",
      "    step: 250; loss: 1.915; l2dist: 1.274\n",
      "    step: 300; loss: 1.898; l2dist: 1.264\n",
      "    step: 350; loss: 1.875; l2dist: 1.263\n",
      "    step: 400; loss: 1.880; l2dist: 1.259\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 13.253; l2dist: 0.000\n",
      "    step: 50; loss: 5.326; l2dist: 1.792\n",
      "    step: 100; loss: 2.842; l2dist: 1.457\n",
      "    step: 150; loss: 2.136; l2dist: 1.294\n",
      "    step: 200; loss: 1.976; l2dist: 1.259\n",
      "    step: 250; loss: 1.911; l2dist: 1.248\n",
      "    step: 300; loss: 1.881; l2dist: 1.242\n",
      "    step: 350; loss: 1.871; l2dist: 1.243\n",
      "    step: 400; loss: 1.868; l2dist: 1.239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 450; loss: 1.853; l2dist: 1.235\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.691; l2dist: 0.000\n",
      "    step: 50; loss: 5.302; l2dist: 1.766\n",
      "    step: 100; loss: 2.844; l2dist: 1.452\n",
      "    step: 150; loss: 2.118; l2dist: 1.290\n",
      "    step: 200; loss: 1.964; l2dist: 1.254\n",
      "    step: 250; loss: 1.904; l2dist: 1.242\n",
      "    step: 300; loss: 1.876; l2dist: 1.228\n",
      "    step: 350; loss: 1.868; l2dist: 1.233\n",
      "    step: 400; loss: 1.857; l2dist: 1.227\n",
      "    step: 450; loss: 1.861; l2dist: 1.224\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.577; l2dist: 0.000\n",
      "    step: 50; loss: 5.285; l2dist: 1.754\n",
      "    step: 100; loss: 2.857; l2dist: 1.452\n",
      "    step: 150; loss: 2.125; l2dist: 1.289\n",
      "    step: 200; loss: 1.965; l2dist: 1.250\n",
      "    step: 250; loss: 1.919; l2dist: 1.232\n",
      "    step: 300; loss: 1.881; l2dist: 1.225\n",
      "    step: 350; loss: 1.875; l2dist: 1.228\n",
      "    step: 400; loss: 1.857; l2dist: 1.228\n",
      "    step: 450; loss: 1.863; l2dist: 1.220\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.563; l2dist: 0.000\n",
      "    step: 50; loss: 5.290; l2dist: 1.757\n",
      "    step: 100; loss: 2.862; l2dist: 1.454\n",
      "    step: 150; loss: 2.124; l2dist: 1.289\n",
      "    step: 200; loss: 1.971; l2dist: 1.248\n",
      "    step: 250; loss: 1.906; l2dist: 1.235\n",
      "    step: 300; loss: 1.886; l2dist: 1.233\n",
      "    step: 350; loss: 1.870; l2dist: 1.225\n",
      "    step: 400; loss: 1.858; l2dist: 1.221\n",
      "    step: 450; loss: 1.852; l2dist: 1.227\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.745; l2dist: 0.000\n",
      "    step: 50; loss: 5.329; l2dist: 1.772\n",
      "    step: 100; loss: 2.867; l2dist: 1.465\n",
      "    step: 150; loss: 2.128; l2dist: 1.296\n",
      "    step: 200; loss: 1.981; l2dist: 1.257\n",
      "    step: 250; loss: 1.918; l2dist: 1.239\n",
      "    step: 300; loss: 1.888; l2dist: 1.239\n",
      "    step: 350; loss: 1.874; l2dist: 1.232\n",
      "    step: 400; loss: 1.868; l2dist: 1.234\n",
      "    step: 450; loss: 1.862; l2dist: 1.232\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.754; l2dist: 0.000\n",
      "    step: 50; loss: 11.476; l2dist: 3.213\n",
      "    step: 100; loss: 4.934; l2dist: 2.133\n",
      "    step: 150; loss: 3.239; l2dist: 1.702\n",
      "    step: 200; loss: 2.621; l2dist: 1.525\n",
      "    step: 250; loss: 2.344; l2dist: 1.446\n",
      "    step: 300; loss: 2.244; l2dist: 1.400\n",
      "    step: 350; loss: 2.202; l2dist: 1.390\n",
      "    step: 400; loss: 2.104; l2dist: 1.361\n",
      "    step: 450; loss: 2.060; l2dist: 1.346\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 107.038; l2dist: 0.000\n",
      "    step: 50; loss: 10.254; l2dist: 2.961\n",
      "    step: 100; loss: 4.469; l2dist: 1.970\n",
      "    step: 150; loss: 2.995; l2dist: 1.605\n",
      "    step: 200; loss: 2.511; l2dist: 1.471\n",
      "    step: 250; loss: 2.256; l2dist: 1.397\n",
      "    step: 300; loss: 2.153; l2dist: 1.357\n",
      "    step: 350; loss: 2.048; l2dist: 1.336\n",
      "    step: 400; loss: 2.047; l2dist: 1.336\n",
      "    step: 450; loss: 2.007; l2dist: 1.323\n",
      "binary step: 1; number of successful adv: 99/100\n",
      "    step: 0; loss: 189.854; l2dist: 0.000\n",
      "    step: 50; loss: 11.655; l2dist: 3.166\n",
      "    step: 100; loss: 5.535; l2dist: 2.202\n",
      "    step: 150; loss: 3.592; l2dist: 1.750\n",
      "    step: 200; loss: 2.814; l2dist: 1.538\n",
      "    step: 250; loss: 2.479; l2dist: 1.445\n",
      "    step: 300; loss: 2.303; l2dist: 1.387\n",
      "    step: 350; loss: 2.196; l2dist: 1.357\n",
      "    step: 400; loss: 2.145; l2dist: 1.345\n",
      "    step: 450; loss: 2.094; l2dist: 1.330\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 108.801; l2dist: 0.000\n",
      "    step: 50; loss: 8.849; l2dist: 2.655\n",
      "    step: 100; loss: 3.969; l2dist: 1.849\n",
      "    step: 150; loss: 2.764; l2dist: 1.529\n",
      "    step: 200; loss: 2.376; l2dist: 1.410\n",
      "    step: 250; loss: 2.209; l2dist: 1.360\n",
      "    step: 300; loss: 2.129; l2dist: 1.334\n",
      "    step: 350; loss: 2.059; l2dist: 1.319\n",
      "    step: 400; loss: 2.032; l2dist: 1.312\n",
      "    step: 450; loss: 2.000; l2dist: 1.304\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.141; l2dist: 0.000\n",
      "    step: 50; loss: 7.002; l2dist: 2.273\n",
      "    step: 100; loss: 3.320; l2dist: 1.649\n",
      "    step: 150; loss: 2.452; l2dist: 1.420\n",
      "    step: 200; loss: 2.192; l2dist: 1.343\n",
      "    step: 250; loss: 2.066; l2dist: 1.312\n",
      "    step: 300; loss: 2.006; l2dist: 1.294\n",
      "    step: 350; loss: 1.967; l2dist: 1.286\n",
      "    step: 400; loss: 1.942; l2dist: 1.279\n",
      "    step: 450; loss: 1.914; l2dist: 1.277\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.834; l2dist: 0.000\n",
      "    step: 50; loss: 6.176; l2dist: 2.053\n",
      "    step: 100; loss: 3.021; l2dist: 1.525\n",
      "    step: 150; loss: 2.284; l2dist: 1.340\n",
      "    step: 200; loss: 2.058; l2dist: 1.290\n",
      "    step: 250; loss: 1.955; l2dist: 1.263\n",
      "    step: 300; loss: 1.908; l2dist: 1.250\n",
      "    step: 350; loss: 1.878; l2dist: 1.241\n",
      "    step: 400; loss: 1.851; l2dist: 1.239\n",
      "    step: 450; loss: 1.833; l2dist: 1.235\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.127; l2dist: 0.000\n",
      "    step: 50; loss: 5.934; l2dist: 1.976\n",
      "    step: 100; loss: 2.953; l2dist: 1.497\n",
      "    step: 150; loss: 2.231; l2dist: 1.321\n",
      "    step: 200; loss: 2.021; l2dist: 1.264\n",
      "    step: 250; loss: 1.933; l2dist: 1.242\n",
      "    step: 300; loss: 1.885; l2dist: 1.236\n",
      "    step: 350; loss: 1.862; l2dist: 1.232\n",
      "    step: 400; loss: 1.841; l2dist: 1.224\n",
      "    step: 450; loss: 1.843; l2dist: 1.225\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.616; l2dist: 0.000\n",
      "    step: 50; loss: 5.863; l2dist: 1.960\n",
      "    step: 100; loss: 2.932; l2dist: 1.491\n",
      "    step: 150; loss: 2.214; l2dist: 1.317\n",
      "    step: 200; loss: 2.013; l2dist: 1.269\n",
      "    step: 250; loss: 1.934; l2dist: 1.247\n",
      "    step: 300; loss: 1.901; l2dist: 1.240\n",
      "    step: 350; loss: 1.862; l2dist: 1.231\n",
      "    step: 400; loss: 1.852; l2dist: 1.235\n",
      "    step: 450; loss: 1.835; l2dist: 1.225\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.254; l2dist: 0.000\n",
      "    step: 50; loss: 5.791; l2dist: 1.939\n",
      "    step: 100; loss: 2.913; l2dist: 1.485\n",
      "    step: 150; loss: 2.201; l2dist: 1.315\n",
      "    step: 200; loss: 2.013; l2dist: 1.274\n",
      "    step: 250; loss: 1.923; l2dist: 1.249\n",
      "    step: 300; loss: 1.877; l2dist: 1.238\n",
      "    step: 350; loss: 1.867; l2dist: 1.237\n",
      "    step: 400; loss: 1.847; l2dist: 1.229\n",
      "    step: 450; loss: 1.830; l2dist: 1.228\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.539; l2dist: 0.000\n",
      "    step: 50; loss: 5.829; l2dist: 1.959\n",
      "    step: 100; loss: 2.925; l2dist: 1.495\n",
      "    step: 150; loss: 2.208; l2dist: 1.323\n",
      "    step: 200; loss: 2.018; l2dist: 1.273\n",
      "    step: 250; loss: 1.923; l2dist: 1.256\n",
      "    step: 300; loss: 1.891; l2dist: 1.243\n",
      "    step: 350; loss: 1.863; l2dist: 1.237\n",
      "    step: 400; loss: 1.845; l2dist: 1.236\n",
      "    step: 450; loss: 1.835; l2dist: 1.237\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 119.126; l2dist: 0.000\n",
      "    step: 50; loss: 12.950; l2dist: 3.267\n",
      "    step: 100; loss: 5.302; l2dist: 2.203\n",
      "    step: 150; loss: 3.508; l2dist: 1.776\n",
      "    step: 200; loss: 2.814; l2dist: 1.571\n",
      "    step: 250; loss: 2.520; l2dist: 1.491\n",
      "    step: 300; loss: 2.395; l2dist: 1.451\n",
      "    step: 350; loss: 2.370; l2dist: 1.436\n",
      "    step: 400; loss: 2.295; l2dist: 1.412\n",
      "    step: 450; loss: 2.256; l2dist: 1.407\n",
      "binary step: 0; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.563; l2dist: 0.000\n",
      "    step: 50; loss: 9.447; l2dist: 2.724\n",
      "    step: 100; loss: 3.721; l2dist: 1.831\n",
      "    step: 150; loss: 2.676; l2dist: 1.537\n",
      "    step: 200; loss: 2.357; l2dist: 1.436\n",
      "    step: 250; loss: 2.232; l2dist: 1.402\n",
      "    step: 300; loss: 2.153; l2dist: 1.376\n",
      "    step: 350; loss: 2.145; l2dist: 1.371\n",
      "    step: 400; loss: 2.141; l2dist: 1.360\n",
      "    step: 450; loss: 2.113; l2dist: 1.364\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.219; l2dist: 0.000\n",
      "    step: 50; loss: 8.084; l2dist: 2.466\n",
      "    step: 100; loss: 3.272; l2dist: 1.700\n",
      "    step: 150; loss: 2.436; l2dist: 1.462\n",
      "    step: 200; loss: 2.255; l2dist: 1.395\n",
      "    step: 250; loss: 2.140; l2dist: 1.369\n",
      "    step: 300; loss: 2.104; l2dist: 1.356\n",
      "    step: 350; loss: 2.091; l2dist: 1.352\n",
      "    step: 400; loss: 2.081; l2dist: 1.352\n",
      "    step: 450; loss: 2.057; l2dist: 1.339\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.824; l2dist: 0.000\n",
      "    step: 50; loss: 7.136; l2dist: 2.241\n",
      "    step: 100; loss: 3.065; l2dist: 1.638\n",
      "    step: 150; loss: 2.372; l2dist: 1.433\n",
      "    step: 200; loss: 2.164; l2dist: 1.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 250; loss: 2.105; l2dist: 1.353\n",
      "    step: 300; loss: 2.075; l2dist: 1.335\n",
      "    step: 350; loss: 2.046; l2dist: 1.335\n",
      "    step: 400; loss: 2.039; l2dist: 1.327\n",
      "    step: 450; loss: 2.023; l2dist: 1.322\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.349; l2dist: 0.000\n",
      "    step: 50; loss: 6.321; l2dist: 2.015\n",
      "    step: 100; loss: 3.054; l2dist: 1.603\n",
      "    step: 150; loss: 2.299; l2dist: 1.404\n",
      "    step: 200; loss: 2.117; l2dist: 1.348\n",
      "    step: 250; loss: 2.058; l2dist: 1.320\n",
      "    step: 300; loss: 2.025; l2dist: 1.311\n",
      "    step: 350; loss: 2.004; l2dist: 1.305\n",
      "    step: 400; loss: 1.999; l2dist: 1.310\n",
      "    step: 450; loss: 1.999; l2dist: 1.298\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.307; l2dist: 0.000\n",
      "    step: 50; loss: 6.178; l2dist: 1.936\n",
      "    step: 100; loss: 3.011; l2dist: 1.525\n",
      "    step: 150; loss: 2.289; l2dist: 1.358\n",
      "    step: 200; loss: 2.101; l2dist: 1.315\n",
      "    step: 250; loss: 2.041; l2dist: 1.297\n",
      "    step: 300; loss: 2.010; l2dist: 1.292\n",
      "    step: 350; loss: 1.996; l2dist: 1.282\n",
      "    step: 400; loss: 1.988; l2dist: 1.285\n",
      "    step: 450; loss: 1.981; l2dist: 1.285\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.763; l2dist: 0.000\n",
      "    step: 50; loss: 6.158; l2dist: 1.921\n",
      "    step: 100; loss: 3.007; l2dist: 1.529\n",
      "    step: 150; loss: 2.261; l2dist: 1.354\n",
      "    step: 200; loss: 2.098; l2dist: 1.316\n",
      "    step: 250; loss: 2.048; l2dist: 1.289\n",
      "    step: 300; loss: 2.013; l2dist: 1.288\n",
      "    step: 350; loss: 1.992; l2dist: 1.285\n",
      "    step: 400; loss: 1.981; l2dist: 1.284\n",
      "    step: 450; loss: 1.969; l2dist: 1.274\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.646; l2dist: 0.000\n",
      "    step: 50; loss: 6.194; l2dist: 1.923\n",
      "    step: 100; loss: 3.031; l2dist: 1.530\n",
      "    step: 150; loss: 2.279; l2dist: 1.362\n",
      "    step: 200; loss: 2.111; l2dist: 1.316\n",
      "    step: 250; loss: 2.050; l2dist: 1.299\n",
      "    step: 300; loss: 2.012; l2dist: 1.293\n",
      "    step: 350; loss: 1.995; l2dist: 1.291\n",
      "    step: 400; loss: 1.987; l2dist: 1.285\n",
      "    step: 450; loss: 1.982; l2dist: 1.287\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.598; l2dist: 0.000\n",
      "    step: 50; loss: 6.206; l2dist: 1.916\n",
      "    step: 100; loss: 3.026; l2dist: 1.526\n",
      "    step: 150; loss: 2.281; l2dist: 1.352\n",
      "    step: 200; loss: 2.120; l2dist: 1.326\n",
      "    step: 250; loss: 2.042; l2dist: 1.296\n",
      "    step: 300; loss: 2.014; l2dist: 1.292\n",
      "    step: 350; loss: 2.000; l2dist: 1.283\n",
      "    step: 400; loss: 1.993; l2dist: 1.283\n",
      "    step: 450; loss: 1.988; l2dist: 1.277\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.730; l2dist: 0.000\n",
      "    step: 50; loss: 6.240; l2dist: 1.924\n",
      "    step: 100; loss: 3.034; l2dist: 1.534\n",
      "    step: 150; loss: 2.296; l2dist: 1.363\n",
      "    step: 200; loss: 2.110; l2dist: 1.324\n",
      "    step: 250; loss: 2.059; l2dist: 1.303\n",
      "    step: 300; loss: 2.016; l2dist: 1.296\n",
      "    step: 350; loss: 2.009; l2dist: 1.294\n",
      "    step: 400; loss: 1.990; l2dist: 1.286\n",
      "    step: 450; loss: 1.994; l2dist: 1.284\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.362; l2dist: 0.000\n",
      "    step: 50; loss: 11.560; l2dist: 3.081\n",
      "    step: 100; loss: 4.912; l2dist: 2.113\n",
      "    step: 150; loss: 3.376; l2dist: 1.730\n",
      "    step: 200; loss: 2.680; l2dist: 1.525\n",
      "    step: 250; loss: 2.362; l2dist: 1.442\n",
      "    step: 300; loss: 2.223; l2dist: 1.389\n",
      "    step: 350; loss: 2.100; l2dist: 1.347\n",
      "    step: 400; loss: 2.069; l2dist: 1.346\n",
      "    step: 450; loss: 2.102; l2dist: 1.353\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 89.298; l2dist: 0.000\n",
      "    step: 50; loss: 9.843; l2dist: 2.787\n",
      "    step: 100; loss: 4.282; l2dist: 1.929\n",
      "    step: 150; loss: 2.999; l2dist: 1.607\n",
      "    step: 200; loss: 2.462; l2dist: 1.448\n",
      "    step: 250; loss: 2.229; l2dist: 1.381\n",
      "    step: 300; loss: 2.101; l2dist: 1.344\n",
      "    step: 350; loss: 2.038; l2dist: 1.325\n",
      "    step: 400; loss: 2.013; l2dist: 1.325\n",
      "    step: 450; loss: 2.007; l2dist: 1.317\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.287; l2dist: 0.000\n",
      "    step: 50; loss: 7.844; l2dist: 2.418\n",
      "    step: 100; loss: 3.422; l2dist: 1.723\n",
      "    step: 150; loss: 2.496; l2dist: 1.459\n",
      "    step: 200; loss: 2.188; l2dist: 1.371\n",
      "    step: 250; loss: 2.042; l2dist: 1.330\n",
      "    step: 300; loss: 1.998; l2dist: 1.304\n",
      "    step: 350; loss: 1.932; l2dist: 1.292\n",
      "    step: 400; loss: 1.920; l2dist: 1.288\n",
      "    step: 450; loss: 1.898; l2dist: 1.282\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.834; l2dist: 0.000\n",
      "    step: 50; loss: 7.068; l2dist: 2.235\n",
      "    step: 100; loss: 3.110; l2dist: 1.636\n",
      "    step: 150; loss: 2.306; l2dist: 1.410\n",
      "    step: 200; loss: 2.053; l2dist: 1.333\n",
      "    step: 250; loss: 1.971; l2dist: 1.295\n",
      "    step: 300; loss: 1.908; l2dist: 1.286\n",
      "    step: 350; loss: 1.893; l2dist: 1.281\n",
      "    step: 400; loss: 1.877; l2dist: 1.276\n",
      "    step: 450; loss: 1.872; l2dist: 1.273\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.022; l2dist: 0.000\n",
      "    step: 50; loss: 6.437; l2dist: 2.069\n",
      "    step: 100; loss: 2.997; l2dist: 1.580\n",
      "    step: 150; loss: 2.229; l2dist: 1.382\n",
      "    step: 200; loss: 2.013; l2dist: 1.312\n",
      "    step: 250; loss: 1.925; l2dist: 1.290\n",
      "    step: 300; loss: 1.895; l2dist: 1.271\n",
      "    step: 350; loss: 1.877; l2dist: 1.265\n",
      "    step: 400; loss: 1.856; l2dist: 1.257\n",
      "    step: 450; loss: 1.849; l2dist: 1.257\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.705; l2dist: 0.000\n",
      "    step: 50; loss: 6.004; l2dist: 1.938\n",
      "    step: 100; loss: 2.871; l2dist: 1.487\n",
      "    step: 150; loss: 2.176; l2dist: 1.329\n",
      "    step: 200; loss: 1.976; l2dist: 1.277\n",
      "    step: 250; loss: 1.899; l2dist: 1.257\n",
      "    step: 300; loss: 1.858; l2dist: 1.239\n",
      "    step: 350; loss: 1.834; l2dist: 1.237\n",
      "    step: 400; loss: 1.829; l2dist: 1.233\n",
      "    step: 450; loss: 1.828; l2dist: 1.234\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.069; l2dist: 0.000\n",
      "    step: 50; loss: 5.878; l2dist: 1.899\n",
      "    step: 100; loss: 2.849; l2dist: 1.466\n",
      "    step: 150; loss: 2.169; l2dist: 1.312\n",
      "    step: 200; loss: 1.967; l2dist: 1.261\n",
      "    step: 250; loss: 1.898; l2dist: 1.240\n",
      "    step: 300; loss: 1.868; l2dist: 1.230\n",
      "    step: 350; loss: 1.842; l2dist: 1.224\n",
      "    step: 400; loss: 1.834; l2dist: 1.227\n",
      "    step: 450; loss: 1.827; l2dist: 1.223\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.645; l2dist: 0.000\n",
      "    step: 50; loss: 5.827; l2dist: 1.884\n",
      "    step: 100; loss: 2.846; l2dist: 1.461\n",
      "    step: 150; loss: 2.159; l2dist: 1.300\n",
      "    step: 200; loss: 1.965; l2dist: 1.250\n",
      "    step: 250; loss: 1.894; l2dist: 1.235\n",
      "    step: 300; loss: 1.854; l2dist: 1.223\n",
      "    step: 350; loss: 1.838; l2dist: 1.222\n",
      "    step: 400; loss: 1.824; l2dist: 1.219\n",
      "    step: 450; loss: 1.824; l2dist: 1.217\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.234; l2dist: 0.000\n",
      "    step: 50; loss: 5.793; l2dist: 1.871\n",
      "    step: 100; loss: 2.843; l2dist: 1.463\n",
      "    step: 150; loss: 2.158; l2dist: 1.300\n",
      "    step: 200; loss: 1.961; l2dist: 1.253\n",
      "    step: 250; loss: 1.896; l2dist: 1.235\n",
      "    step: 300; loss: 1.860; l2dist: 1.231\n",
      "    step: 350; loss: 1.839; l2dist: 1.220\n",
      "    step: 400; loss: 1.829; l2dist: 1.218\n",
      "    step: 450; loss: 1.822; l2dist: 1.219\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.474; l2dist: 0.000\n",
      "    step: 50; loss: 5.832; l2dist: 1.885\n",
      "    step: 100; loss: 2.844; l2dist: 1.468\n",
      "    step: 150; loss: 2.166; l2dist: 1.315\n",
      "    step: 200; loss: 1.963; l2dist: 1.260\n",
      "    step: 250; loss: 1.894; l2dist: 1.242\n",
      "    step: 300; loss: 1.861; l2dist: 1.235\n",
      "    step: 350; loss: 1.845; l2dist: 1.231\n",
      "    step: 400; loss: 1.835; l2dist: 1.222\n",
      "    step: 450; loss: 1.835; l2dist: 1.225\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 117.666; l2dist: 0.000\n",
      "    step: 50; loss: 13.063; l2dist: 3.289\n",
      "    step: 100; loss: 5.493; l2dist: 2.249\n",
      "    step: 150; loss: 3.599; l2dist: 1.802\n",
      "    step: 200; loss: 2.930; l2dist: 1.611\n",
      "    step: 250; loss: 2.588; l2dist: 1.518\n",
      "    step: 300; loss: 2.458; l2dist: 1.462\n",
      "    step: 350; loss: 2.367; l2dist: 1.430\n",
      "    step: 400; loss: 2.284; l2dist: 1.419\n",
      "    step: 450; loss: 2.274; l2dist: 1.411\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 107.436; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 11.448; l2dist: 3.046\n",
      "    step: 100; loss: 4.760; l2dist: 2.047\n",
      "    step: 150; loss: 3.297; l2dist: 1.683\n",
      "    step: 200; loss: 2.727; l2dist: 1.531\n",
      "    step: 250; loss: 2.474; l2dist: 1.457\n",
      "    step: 300; loss: 2.329; l2dist: 1.421\n",
      "    step: 350; loss: 2.279; l2dist: 1.409\n",
      "    step: 400; loss: 2.207; l2dist: 1.383\n",
      "    step: 450; loss: 2.172; l2dist: 1.376\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.307; l2dist: 0.000\n",
      "    step: 50; loss: 8.909; l2dist: 2.616\n",
      "    step: 100; loss: 3.730; l2dist: 1.803\n",
      "    step: 150; loss: 2.728; l2dist: 1.536\n",
      "    step: 200; loss: 2.410; l2dist: 1.452\n",
      "    step: 250; loss: 2.257; l2dist: 1.398\n",
      "    step: 300; loss: 2.158; l2dist: 1.369\n",
      "    step: 350; loss: 2.115; l2dist: 1.359\n",
      "    step: 400; loss: 2.102; l2dist: 1.361\n",
      "    step: 450; loss: 2.101; l2dist: 1.357\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.309; l2dist: 0.000\n",
      "    step: 50; loss: 7.398; l2dist: 2.319\n",
      "    step: 100; loss: 3.382; l2dist: 1.715\n",
      "    step: 150; loss: 2.507; l2dist: 1.471\n",
      "    step: 200; loss: 2.264; l2dist: 1.397\n",
      "    step: 250; loss: 2.153; l2dist: 1.369\n",
      "    step: 300; loss: 2.099; l2dist: 1.353\n",
      "    step: 350; loss: 2.071; l2dist: 1.341\n",
      "    step: 400; loss: 2.069; l2dist: 1.341\n",
      "    step: 450; loss: 2.048; l2dist: 1.336\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.609; l2dist: 0.000\n",
      "    step: 50; loss: 6.322; l2dist: 2.041\n",
      "    step: 100; loss: 3.257; l2dist: 1.595\n",
      "    step: 150; loss: 2.435; l2dist: 1.420\n",
      "    step: 200; loss: 2.187; l2dist: 1.356\n",
      "    step: 250; loss: 2.104; l2dist: 1.334\n",
      "    step: 300; loss: 2.055; l2dist: 1.317\n",
      "    step: 350; loss: 2.042; l2dist: 1.315\n",
      "    step: 400; loss: 2.037; l2dist: 1.309\n",
      "    step: 450; loss: 2.022; l2dist: 1.308\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.961; l2dist: 0.000\n",
      "    step: 50; loss: 6.146; l2dist: 1.978\n",
      "    step: 100; loss: 3.147; l2dist: 1.539\n",
      "    step: 150; loss: 2.359; l2dist: 1.367\n",
      "    step: 200; loss: 2.136; l2dist: 1.312\n",
      "    step: 250; loss: 2.058; l2dist: 1.293\n",
      "    step: 300; loss: 2.017; l2dist: 1.281\n",
      "    step: 350; loss: 2.006; l2dist: 1.277\n",
      "    step: 400; loss: 2.002; l2dist: 1.277\n",
      "    step: 450; loss: 1.987; l2dist: 1.273\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.883; l2dist: 0.000\n",
      "    step: 50; loss: 6.124; l2dist: 1.964\n",
      "    step: 100; loss: 3.164; l2dist: 1.545\n",
      "    step: 150; loss: 2.370; l2dist: 1.384\n",
      "    step: 200; loss: 2.155; l2dist: 1.337\n",
      "    step: 250; loss: 2.080; l2dist: 1.311\n",
      "    step: 300; loss: 2.042; l2dist: 1.307\n",
      "    step: 350; loss: 2.027; l2dist: 1.293\n",
      "    step: 400; loss: 2.008; l2dist: 1.294\n",
      "    step: 450; loss: 1.995; l2dist: 1.291\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.737; l2dist: 0.000\n",
      "    step: 50; loss: 6.054; l2dist: 1.942\n",
      "    step: 100; loss: 3.143; l2dist: 1.538\n",
      "    step: 150; loss: 2.354; l2dist: 1.372\n",
      "    step: 200; loss: 2.144; l2dist: 1.327\n",
      "    step: 250; loss: 2.068; l2dist: 1.308\n",
      "    step: 300; loss: 2.029; l2dist: 1.297\n",
      "    step: 350; loss: 2.012; l2dist: 1.294\n",
      "    step: 400; loss: 2.009; l2dist: 1.294\n",
      "    step: 450; loss: 2.005; l2dist: 1.291\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.323; l2dist: 0.000\n",
      "    step: 50; loss: 6.043; l2dist: 1.937\n",
      "    step: 100; loss: 3.145; l2dist: 1.540\n",
      "    step: 150; loss: 2.350; l2dist: 1.375\n",
      "    step: 200; loss: 2.141; l2dist: 1.329\n",
      "    step: 250; loss: 2.066; l2dist: 1.309\n",
      "    step: 300; loss: 2.038; l2dist: 1.295\n",
      "    step: 350; loss: 2.017; l2dist: 1.301\n",
      "    step: 400; loss: 2.007; l2dist: 1.299\n",
      "    step: 450; loss: 2.001; l2dist: 1.291\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.610; l2dist: 0.000\n",
      "    step: 50; loss: 6.073; l2dist: 1.953\n",
      "    step: 100; loss: 3.159; l2dist: 1.549\n",
      "    step: 150; loss: 2.354; l2dist: 1.387\n",
      "    step: 200; loss: 2.146; l2dist: 1.334\n",
      "    step: 250; loss: 2.084; l2dist: 1.319\n",
      "    step: 300; loss: 2.038; l2dist: 1.307\n",
      "    step: 350; loss: 2.028; l2dist: 1.302\n",
      "    step: 400; loss: 2.007; l2dist: 1.298\n",
      "    step: 450; loss: 2.005; l2dist: 1.300\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.306; l2dist: 0.000\n",
      "    step: 50; loss: 11.871; l2dist: 3.244\n",
      "    step: 100; loss: 5.147; l2dist: 2.195\n",
      "    step: 150; loss: 3.338; l2dist: 1.756\n",
      "    step: 200; loss: 2.686; l2dist: 1.565\n",
      "    step: 250; loss: 2.425; l2dist: 1.473\n",
      "    step: 300; loss: 2.305; l2dist: 1.438\n",
      "    step: 350; loss: 2.225; l2dist: 1.406\n",
      "    step: 400; loss: 2.199; l2dist: 1.385\n",
      "    step: 450; loss: 2.123; l2dist: 1.383\n",
      "binary step: 0; number of successful adv: 90/100\n",
      "    step: 0; loss: 167.281; l2dist: 0.000\n",
      "    step: 50; loss: 12.880; l2dist: 3.354\n",
      "    step: 100; loss: 6.169; l2dist: 2.340\n",
      "    step: 150; loss: 4.124; l2dist: 1.893\n",
      "    step: 200; loss: 3.235; l2dist: 1.678\n",
      "    step: 250; loss: 2.768; l2dist: 1.555\n",
      "    step: 300; loss: 2.520; l2dist: 1.491\n",
      "    step: 350; loss: 2.328; l2dist: 1.439\n",
      "    step: 400; loss: 2.265; l2dist: 1.419\n",
      "    step: 450; loss: 2.241; l2dist: 1.415\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 92.844; l2dist: 0.000\n",
      "    step: 50; loss: 9.826; l2dist: 2.882\n",
      "    step: 100; loss: 4.418; l2dist: 1.977\n",
      "    step: 150; loss: 3.054; l2dist: 1.637\n",
      "    step: 200; loss: 2.550; l2dist: 1.501\n",
      "    step: 250; loss: 2.313; l2dist: 1.434\n",
      "    step: 300; loss: 2.192; l2dist: 1.392\n",
      "    step: 350; loss: 2.135; l2dist: 1.372\n",
      "    step: 400; loss: 2.098; l2dist: 1.366\n",
      "    step: 450; loss: 2.041; l2dist: 1.347\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 57.361; l2dist: 0.000\n",
      "    step: 50; loss: 8.066; l2dist: 2.540\n",
      "    step: 100; loss: 3.581; l2dist: 1.777\n",
      "    step: 150; loss: 2.559; l2dist: 1.508\n",
      "    step: 200; loss: 2.231; l2dist: 1.412\n",
      "    step: 250; loss: 2.113; l2dist: 1.370\n",
      "    step: 300; loss: 2.024; l2dist: 1.342\n",
      "    step: 350; loss: 2.007; l2dist: 1.337\n",
      "    step: 400; loss: 1.973; l2dist: 1.327\n",
      "    step: 450; loss: 1.964; l2dist: 1.327\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.265; l2dist: 0.000\n",
      "    step: 50; loss: 6.948; l2dist: 2.239\n",
      "    step: 100; loss: 3.267; l2dist: 1.647\n",
      "    step: 150; loss: 2.391; l2dist: 1.432\n",
      "    step: 200; loss: 2.130; l2dist: 1.359\n",
      "    step: 250; loss: 2.030; l2dist: 1.329\n",
      "    step: 300; loss: 1.983; l2dist: 1.312\n",
      "    step: 350; loss: 2.018; l2dist: 1.315\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.568; l2dist: 0.000\n",
      "    step: 50; loss: 6.421; l2dist: 2.132\n",
      "    step: 100; loss: 3.124; l2dist: 1.578\n",
      "    step: 150; loss: 2.305; l2dist: 1.385\n",
      "    step: 200; loss: 2.083; l2dist: 1.324\n",
      "    step: 250; loss: 1.995; l2dist: 1.300\n",
      "    step: 300; loss: 1.958; l2dist: 1.292\n",
      "    step: 350; loss: 1.961; l2dist: 1.287\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.510; l2dist: 0.000\n",
      "    step: 50; loss: 6.295; l2dist: 2.084\n",
      "    step: 100; loss: 3.067; l2dist: 1.553\n",
      "    step: 150; loss: 2.256; l2dist: 1.365\n",
      "    step: 200; loss: 2.047; l2dist: 1.307\n",
      "    step: 250; loss: 1.975; l2dist: 1.285\n",
      "    step: 300; loss: 1.939; l2dist: 1.279\n",
      "    step: 350; loss: 1.923; l2dist: 1.273\n",
      "    step: 400; loss: 1.912; l2dist: 1.270\n",
      "    step: 450; loss: 1.898; l2dist: 1.270\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.719; l2dist: 0.000\n",
      "    step: 50; loss: 6.217; l2dist: 2.072\n",
      "    step: 100; loss: 3.073; l2dist: 1.560\n",
      "    step: 150; loss: 2.256; l2dist: 1.362\n",
      "    step: 200; loss: 2.060; l2dist: 1.307\n",
      "    step: 250; loss: 1.973; l2dist: 1.289\n",
      "    step: 300; loss: 1.941; l2dist: 1.281\n",
      "    step: 350; loss: 1.923; l2dist: 1.275\n",
      "    step: 400; loss: 1.913; l2dist: 1.276\n",
      "    step: 450; loss: 1.918; l2dist: 1.278\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.730; l2dist: 0.000\n",
      "    step: 50; loss: 6.144; l2dist: 2.051\n",
      "    step: 100; loss: 3.055; l2dist: 1.548\n",
      "    step: 150; loss: 2.247; l2dist: 1.365\n",
      "    step: 200; loss: 2.043; l2dist: 1.303\n",
      "    step: 250; loss: 1.974; l2dist: 1.287\n",
      "    step: 300; loss: 1.948; l2dist: 1.277\n",
      "    step: 350; loss: 1.923; l2dist: 1.271\n",
      "    step: 400; loss: 1.895; l2dist: 1.264\n",
      "    step: 450; loss: 1.902; l2dist: 1.265\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.885; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 6.174; l2dist: 2.060\n",
      "    step: 100; loss: 3.069; l2dist: 1.554\n",
      "    step: 150; loss: 2.239; l2dist: 1.358\n",
      "    step: 200; loss: 2.052; l2dist: 1.305\n",
      "    step: 250; loss: 1.961; l2dist: 1.289\n",
      "    step: 300; loss: 1.937; l2dist: 1.277\n",
      "    step: 350; loss: 1.906; l2dist: 1.270\n",
      "    step: 400; loss: 1.906; l2dist: 1.270\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 117.452; l2dist: 0.000\n",
      "    step: 50; loss: 12.977; l2dist: 3.298\n",
      "    step: 100; loss: 5.412; l2dist: 2.241\n",
      "    step: 150; loss: 3.495; l2dist: 1.776\n",
      "    step: 200; loss: 2.787; l2dist: 1.568\n",
      "    step: 250; loss: 2.454; l2dist: 1.468\n",
      "    step: 300; loss: 2.291; l2dist: 1.413\n",
      "    step: 350; loss: 2.247; l2dist: 1.384\n",
      "    step: 400; loss: 2.218; l2dist: 1.372\n",
      "    step: 450; loss: 2.183; l2dist: 1.363\n",
      "binary step: 0; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.726; l2dist: 0.000\n",
      "    step: 50; loss: 9.684; l2dist: 2.765\n",
      "    step: 100; loss: 3.628; l2dist: 1.812\n",
      "    step: 150; loss: 2.589; l2dist: 1.506\n",
      "    step: 200; loss: 2.267; l2dist: 1.412\n",
      "    step: 250; loss: 2.128; l2dist: 1.358\n",
      "    step: 300; loss: 2.106; l2dist: 1.339\n",
      "    step: 350; loss: 2.069; l2dist: 1.328\n",
      "    step: 400; loss: 2.025; l2dist: 1.313\n",
      "    step: 450; loss: 2.007; l2dist: 1.318\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.663; l2dist: 0.000\n",
      "    step: 50; loss: 7.845; l2dist: 2.432\n",
      "    step: 100; loss: 3.086; l2dist: 1.655\n",
      "    step: 150; loss: 2.330; l2dist: 1.429\n",
      "    step: 200; loss: 2.107; l2dist: 1.354\n",
      "    step: 250; loss: 2.024; l2dist: 1.326\n",
      "    step: 300; loss: 1.983; l2dist: 1.304\n",
      "    step: 350; loss: 1.979; l2dist: 1.302\n",
      "    step: 400; loss: 1.958; l2dist: 1.302\n",
      "    step: 450; loss: 1.949; l2dist: 1.298\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.665; l2dist: 0.000\n",
      "    step: 50; loss: 6.961; l2dist: 2.207\n",
      "    step: 100; loss: 3.013; l2dist: 1.608\n",
      "    step: 150; loss: 2.270; l2dist: 1.399\n",
      "    step: 200; loss: 2.067; l2dist: 1.329\n",
      "    step: 250; loss: 2.001; l2dist: 1.302\n",
      "    step: 300; loss: 1.955; l2dist: 1.296\n",
      "    step: 350; loss: 1.938; l2dist: 1.286\n",
      "    step: 400; loss: 1.925; l2dist: 1.288\n",
      "    step: 450; loss: 1.926; l2dist: 1.283\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.364; l2dist: 0.000\n",
      "    step: 50; loss: 6.237; l2dist: 1.986\n",
      "    step: 100; loss: 2.985; l2dist: 1.548\n",
      "    step: 150; loss: 2.243; l2dist: 1.373\n",
      "    step: 200; loss: 2.049; l2dist: 1.311\n",
      "    step: 250; loss: 1.968; l2dist: 1.295\n",
      "    step: 300; loss: 1.941; l2dist: 1.280\n",
      "    step: 350; loss: 1.921; l2dist: 1.274\n",
      "    step: 400; loss: 1.916; l2dist: 1.276\n",
      "    step: 450; loss: 1.914; l2dist: 1.272\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.038; l2dist: 0.000\n",
      "    step: 50; loss: 6.024; l2dist: 1.903\n",
      "    step: 100; loss: 2.950; l2dist: 1.493\n",
      "    step: 150; loss: 2.227; l2dist: 1.338\n",
      "    step: 200; loss: 2.032; l2dist: 1.294\n",
      "    step: 250; loss: 1.957; l2dist: 1.273\n",
      "    step: 300; loss: 1.925; l2dist: 1.269\n",
      "    step: 350; loss: 1.910; l2dist: 1.259\n",
      "    step: 400; loss: 1.898; l2dist: 1.259\n",
      "    step: 450; loss: 1.896; l2dist: 1.255\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.453; l2dist: 0.000\n",
      "    step: 50; loss: 5.925; l2dist: 1.862\n",
      "    step: 100; loss: 2.943; l2dist: 1.478\n",
      "    step: 150; loss: 2.217; l2dist: 1.321\n",
      "    step: 200; loss: 2.023; l2dist: 1.273\n",
      "    step: 250; loss: 1.953; l2dist: 1.258\n",
      "    step: 300; loss: 1.919; l2dist: 1.250\n",
      "    step: 350; loss: 1.912; l2dist: 1.244\n",
      "    step: 400; loss: 1.896; l2dist: 1.247\n",
      "    step: 450; loss: 1.893; l2dist: 1.240\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.310; l2dist: 0.000\n",
      "    step: 50; loss: 5.894; l2dist: 1.851\n",
      "    step: 100; loss: 2.975; l2dist: 1.471\n",
      "    step: 150; loss: 2.242; l2dist: 1.329\n",
      "    step: 200; loss: 2.047; l2dist: 1.284\n",
      "    step: 250; loss: 1.976; l2dist: 1.269\n",
      "    step: 300; loss: 1.941; l2dist: 1.263\n",
      "    step: 350; loss: 1.930; l2dist: 1.254\n",
      "    step: 400; loss: 1.914; l2dist: 1.252\n",
      "    step: 450; loss: 1.908; l2dist: 1.254\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.175; l2dist: 0.000\n",
      "    step: 50; loss: 5.865; l2dist: 1.842\n",
      "    step: 100; loss: 2.971; l2dist: 1.471\n",
      "    step: 150; loss: 2.244; l2dist: 1.335\n",
      "    step: 200; loss: 2.041; l2dist: 1.283\n",
      "    step: 250; loss: 1.978; l2dist: 1.270\n",
      "    step: 300; loss: 1.940; l2dist: 1.261\n",
      "    step: 350; loss: 1.926; l2dist: 1.255\n",
      "    step: 400; loss: 1.918; l2dist: 1.250\n",
      "    step: 450; loss: 1.910; l2dist: 1.251\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.347; l2dist: 0.000\n",
      "    step: 50; loss: 5.910; l2dist: 1.853\n",
      "    step: 100; loss: 2.968; l2dist: 1.482\n",
      "    step: 150; loss: 2.231; l2dist: 1.335\n",
      "    step: 200; loss: 2.033; l2dist: 1.286\n",
      "    step: 250; loss: 1.971; l2dist: 1.267\n",
      "    step: 300; loss: 1.932; l2dist: 1.267\n",
      "    step: 350; loss: 1.922; l2dist: 1.258\n",
      "    step: 400; loss: 1.911; l2dist: 1.252\n",
      "    step: 450; loss: 1.902; l2dist: 1.257\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.477; l2dist: 0.000\n",
      "    step: 50; loss: 10.442; l2dist: 3.070\n",
      "    step: 100; loss: 4.705; l2dist: 2.077\n",
      "    step: 150; loss: 3.088; l2dist: 1.668\n",
      "    step: 200; loss: 2.575; l2dist: 1.515\n",
      "    step: 250; loss: 2.271; l2dist: 1.407\n",
      "    step: 300; loss: 2.130; l2dist: 1.362\n",
      "    step: 350; loss: 2.024; l2dist: 1.327\n",
      "    step: 400; loss: 2.020; l2dist: 1.334\n",
      "    step: 450; loss: 2.013; l2dist: 1.320\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 119.536; l2dist: 0.000\n",
      "    step: 50; loss: 10.269; l2dist: 2.977\n",
      "    step: 100; loss: 4.791; l2dist: 2.041\n",
      "    step: 150; loss: 3.214; l2dist: 1.649\n",
      "    step: 200; loss: 2.604; l2dist: 1.485\n",
      "    step: 250; loss: 2.288; l2dist: 1.394\n",
      "    step: 300; loss: 2.159; l2dist: 1.361\n",
      "    step: 350; loss: 2.037; l2dist: 1.326\n",
      "    step: 400; loss: 2.025; l2dist: 1.314\n",
      "    step: 450; loss: 2.012; l2dist: 1.311\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.867; l2dist: 0.000\n",
      "    step: 50; loss: 7.919; l2dist: 2.538\n",
      "    step: 100; loss: 3.491; l2dist: 1.742\n",
      "    step: 150; loss: 2.480; l2dist: 1.462\n",
      "    step: 200; loss: 2.144; l2dist: 1.365\n",
      "    step: 250; loss: 1.992; l2dist: 1.313\n",
      "    step: 300; loss: 1.921; l2dist: 1.293\n",
      "    step: 350; loss: 1.892; l2dist: 1.281\n",
      "    step: 400; loss: 1.865; l2dist: 1.274\n",
      "    step: 450; loss: 1.881; l2dist: 1.275\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.548; l2dist: 0.000\n",
      "    step: 50; loss: 6.685; l2dist: 2.225\n",
      "    step: 100; loss: 3.046; l2dist: 1.616\n",
      "    step: 150; loss: 2.255; l2dist: 1.393\n",
      "    step: 200; loss: 2.000; l2dist: 1.314\n",
      "    step: 250; loss: 1.907; l2dist: 1.283\n",
      "    step: 300; loss: 1.862; l2dist: 1.266\n",
      "    step: 350; loss: 1.823; l2dist: 1.258\n",
      "    step: 400; loss: 1.818; l2dist: 1.251\n",
      "    step: 450; loss: 1.803; l2dist: 1.252\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.511; l2dist: 0.000\n",
      "    step: 50; loss: 6.017; l2dist: 2.053\n",
      "    step: 100; loss: 2.876; l2dist: 1.540\n",
      "    step: 150; loss: 2.148; l2dist: 1.351\n",
      "    step: 200; loss: 1.938; l2dist: 1.290\n",
      "    step: 250; loss: 1.858; l2dist: 1.260\n",
      "    step: 300; loss: 1.832; l2dist: 1.246\n",
      "    step: 350; loss: 1.809; l2dist: 1.239\n",
      "    step: 400; loss: 1.791; l2dist: 1.240\n",
      "    step: 450; loss: 1.774; l2dist: 1.229\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.648; l2dist: 0.000\n",
      "    step: 50; loss: 5.643; l2dist: 1.935\n",
      "    step: 100; loss: 2.757; l2dist: 1.476\n",
      "    step: 150; loss: 2.082; l2dist: 1.297\n",
      "    step: 200; loss: 1.904; l2dist: 1.245\n",
      "    step: 250; loss: 1.827; l2dist: 1.221\n",
      "    step: 300; loss: 1.786; l2dist: 1.216\n",
      "    step: 350; loss: 1.770; l2dist: 1.209\n",
      "    step: 400; loss: 1.763; l2dist: 1.205\n",
      "    step: 450; loss: 1.757; l2dist: 1.209\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.872; l2dist: 0.000\n",
      "    step: 50; loss: 5.474; l2dist: 1.876\n",
      "    step: 100; loss: 2.748; l2dist: 1.456\n",
      "    step: 150; loss: 2.069; l2dist: 1.276\n",
      "    step: 200; loss: 1.903; l2dist: 1.238\n",
      "    step: 250; loss: 1.835; l2dist: 1.214\n",
      "    step: 300; loss: 1.805; l2dist: 1.211\n",
      "    step: 350; loss: 1.768; l2dist: 1.201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 1.776; l2dist: 1.204\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.796; l2dist: 0.000\n",
      "    step: 50; loss: 5.441; l2dist: 1.869\n",
      "    step: 100; loss: 2.751; l2dist: 1.454\n",
      "    step: 150; loss: 2.059; l2dist: 1.280\n",
      "    step: 200; loss: 1.889; l2dist: 1.230\n",
      "    step: 250; loss: 1.825; l2dist: 1.220\n",
      "    step: 300; loss: 1.792; l2dist: 1.204\n",
      "    step: 350; loss: 1.763; l2dist: 1.206\n",
      "    step: 400; loss: 1.755; l2dist: 1.203\n",
      "    step: 450; loss: 1.741; l2dist: 1.199\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.366; l2dist: 0.000\n",
      "    step: 50; loss: 5.421; l2dist: 1.863\n",
      "    step: 100; loss: 2.754; l2dist: 1.456\n",
      "    step: 150; loss: 2.064; l2dist: 1.275\n",
      "    step: 200; loss: 1.883; l2dist: 1.237\n",
      "    step: 250; loss: 1.824; l2dist: 1.221\n",
      "    step: 300; loss: 1.783; l2dist: 1.212\n",
      "    step: 350; loss: 1.765; l2dist: 1.205\n",
      "    step: 400; loss: 1.752; l2dist: 1.202\n",
      "    step: 450; loss: 1.754; l2dist: 1.204\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.510; l2dist: 0.000\n",
      "    step: 50; loss: 5.450; l2dist: 1.875\n",
      "    step: 100; loss: 2.757; l2dist: 1.460\n",
      "    step: 150; loss: 2.062; l2dist: 1.288\n",
      "    step: 200; loss: 1.896; l2dist: 1.239\n",
      "    step: 250; loss: 1.828; l2dist: 1.225\n",
      "    step: 300; loss: 1.794; l2dist: 1.227\n",
      "    step: 350; loss: 1.769; l2dist: 1.215\n",
      "    step: 400; loss: 1.757; l2dist: 1.211\n",
      "    step: 450; loss: 1.757; l2dist: 1.209\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.821; l2dist: 0.000\n",
      "    step: 50; loss: 12.324; l2dist: 3.190\n",
      "    step: 100; loss: 5.096; l2dist: 2.135\n",
      "    step: 150; loss: 3.385; l2dist: 1.723\n",
      "    step: 200; loss: 2.737; l2dist: 1.539\n",
      "    step: 250; loss: 2.463; l2dist: 1.453\n",
      "    step: 300; loss: 2.293; l2dist: 1.416\n",
      "    step: 350; loss: 2.257; l2dist: 1.382\n",
      "    step: 400; loss: 2.226; l2dist: 1.387\n",
      "    step: 450; loss: 2.206; l2dist: 1.372\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 76.534; l2dist: 0.000\n",
      "    step: 50; loss: 10.092; l2dist: 2.800\n",
      "    step: 100; loss: 4.052; l2dist: 1.874\n",
      "    step: 150; loss: 2.830; l2dist: 1.560\n",
      "    step: 200; loss: 2.448; l2dist: 1.449\n",
      "    step: 250; loss: 2.238; l2dist: 1.389\n",
      "    step: 300; loss: 2.163; l2dist: 1.366\n",
      "    step: 350; loss: 2.094; l2dist: 1.343\n",
      "    step: 400; loss: 2.120; l2dist: 1.334\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.712; l2dist: 0.000\n",
      "    step: 50; loss: 8.274; l2dist: 2.446\n",
      "    step: 100; loss: 3.426; l2dist: 1.710\n",
      "    step: 150; loss: 2.533; l2dist: 1.474\n",
      "    step: 200; loss: 2.264; l2dist: 1.381\n",
      "    step: 250; loss: 2.116; l2dist: 1.347\n",
      "    step: 300; loss: 2.067; l2dist: 1.332\n",
      "    step: 350; loss: 2.068; l2dist: 1.323\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.361; l2dist: 0.000\n",
      "    step: 50; loss: 7.353; l2dist: 2.234\n",
      "    step: 100; loss: 3.163; l2dist: 1.645\n",
      "    step: 150; loss: 2.360; l2dist: 1.421\n",
      "    step: 200; loss: 2.167; l2dist: 1.361\n",
      "    step: 250; loss: 2.080; l2dist: 1.325\n",
      "    step: 300; loss: 2.039; l2dist: 1.318\n",
      "    step: 350; loss: 2.023; l2dist: 1.309\n",
      "    step: 400; loss: 2.002; l2dist: 1.305\n",
      "    step: 450; loss: 1.993; l2dist: 1.303\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.657; l2dist: 0.000\n",
      "    step: 50; loss: 6.582; l2dist: 2.052\n",
      "    step: 100; loss: 3.112; l2dist: 1.591\n",
      "    step: 150; loss: 2.332; l2dist: 1.401\n",
      "    step: 200; loss: 2.135; l2dist: 1.337\n",
      "    step: 250; loss: 2.063; l2dist: 1.311\n",
      "    step: 300; loss: 2.024; l2dist: 1.307\n",
      "    step: 350; loss: 1.998; l2dist: 1.294\n",
      "    step: 400; loss: 1.976; l2dist: 1.290\n",
      "    step: 450; loss: 1.980; l2dist: 1.288\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.398; l2dist: 0.000\n",
      "    step: 50; loss: 6.342; l2dist: 1.949\n",
      "    step: 100; loss: 3.037; l2dist: 1.518\n",
      "    step: 150; loss: 2.281; l2dist: 1.349\n",
      "    step: 200; loss: 2.088; l2dist: 1.299\n",
      "    step: 250; loss: 2.028; l2dist: 1.283\n",
      "    step: 300; loss: 1.992; l2dist: 1.271\n",
      "    step: 350; loss: 1.965; l2dist: 1.265\n",
      "    step: 400; loss: 1.971; l2dist: 1.266\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.799; l2dist: 0.000\n",
      "    step: 50; loss: 6.280; l2dist: 1.933\n",
      "    step: 100; loss: 3.029; l2dist: 1.515\n",
      "    step: 150; loss: 2.265; l2dist: 1.334\n",
      "    step: 200; loss: 2.089; l2dist: 1.288\n",
      "    step: 250; loss: 2.017; l2dist: 1.266\n",
      "    step: 300; loss: 1.999; l2dist: 1.267\n",
      "    step: 350; loss: 1.979; l2dist: 1.254\n",
      "    step: 400; loss: 1.955; l2dist: 1.256\n",
      "    step: 450; loss: 1.951; l2dist: 1.249\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.206; l2dist: 0.000\n",
      "    step: 50; loss: 6.223; l2dist: 1.915\n",
      "    step: 100; loss: 3.034; l2dist: 1.505\n",
      "    step: 150; loss: 2.264; l2dist: 1.326\n",
      "    step: 200; loss: 2.091; l2dist: 1.275\n",
      "    step: 250; loss: 2.009; l2dist: 1.258\n",
      "    step: 300; loss: 1.980; l2dist: 1.256\n",
      "    step: 350; loss: 1.964; l2dist: 1.250\n",
      "    step: 400; loss: 1.958; l2dist: 1.247\n",
      "    step: 450; loss: 1.950; l2dist: 1.246\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.003; l2dist: 0.000\n",
      "    step: 50; loss: 6.214; l2dist: 1.914\n",
      "    step: 100; loss: 3.035; l2dist: 1.504\n",
      "    step: 150; loss: 2.266; l2dist: 1.327\n",
      "    step: 200; loss: 2.100; l2dist: 1.284\n",
      "    step: 250; loss: 2.025; l2dist: 1.267\n",
      "    step: 300; loss: 1.987; l2dist: 1.259\n",
      "    step: 350; loss: 1.965; l2dist: 1.255\n",
      "    step: 400; loss: 1.964; l2dist: 1.248\n",
      "    step: 450; loss: 1.966; l2dist: 1.252\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.183; l2dist: 0.000\n",
      "    step: 50; loss: 6.259; l2dist: 1.925\n",
      "    step: 100; loss: 3.036; l2dist: 1.512\n",
      "    step: 150; loss: 2.275; l2dist: 1.336\n",
      "    step: 200; loss: 2.101; l2dist: 1.295\n",
      "    step: 250; loss: 2.018; l2dist: 1.276\n",
      "    step: 300; loss: 1.988; l2dist: 1.269\n",
      "    step: 350; loss: 1.974; l2dist: 1.262\n",
      "    step: 400; loss: 1.964; l2dist: 1.262\n",
      "    step: 450; loss: 1.959; l2dist: 1.258\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 112.916; l2dist: 0.000\n",
      "    step: 50; loss: 12.580; l2dist: 3.221\n",
      "    step: 100; loss: 5.263; l2dist: 2.186\n",
      "    step: 150; loss: 3.544; l2dist: 1.783\n",
      "    step: 200; loss: 2.932; l2dist: 1.600\n",
      "    step: 250; loss: 2.653; l2dist: 1.530\n",
      "    step: 300; loss: 2.513; l2dist: 1.475\n",
      "    step: 350; loss: 2.397; l2dist: 1.452\n",
      "    step: 400; loss: 2.384; l2dist: 1.440\n",
      "    step: 450; loss: 2.302; l2dist: 1.414\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 88.914; l2dist: 0.000\n",
      "    step: 50; loss: 10.641; l2dist: 2.883\n",
      "    step: 100; loss: 4.429; l2dist: 1.969\n",
      "    step: 150; loss: 3.124; l2dist: 1.641\n",
      "    step: 200; loss: 2.647; l2dist: 1.514\n",
      "    step: 250; loss: 2.445; l2dist: 1.455\n",
      "    step: 300; loss: 2.370; l2dist: 1.420\n",
      "    step: 350; loss: 2.272; l2dist: 1.408\n",
      "    step: 400; loss: 2.226; l2dist: 1.391\n",
      "    step: 450; loss: 2.213; l2dist: 1.383\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.455; l2dist: 0.000\n",
      "    step: 50; loss: 8.012; l2dist: 2.454\n",
      "    step: 100; loss: 3.476; l2dist: 1.748\n",
      "    step: 150; loss: 2.588; l2dist: 1.503\n",
      "    step: 200; loss: 2.341; l2dist: 1.424\n",
      "    step: 250; loss: 2.202; l2dist: 1.382\n",
      "    step: 300; loss: 2.169; l2dist: 1.374\n",
      "    step: 350; loss: 2.143; l2dist: 1.371\n",
      "    step: 400; loss: 2.131; l2dist: 1.361\n",
      "    step: 450; loss: 2.086; l2dist: 1.353\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.153; l2dist: 0.000\n",
      "    step: 50; loss: 6.915; l2dist: 2.218\n",
      "    step: 100; loss: 3.267; l2dist: 1.673\n",
      "    step: 150; loss: 2.447; l2dist: 1.454\n",
      "    step: 200; loss: 2.250; l2dist: 1.398\n",
      "    step: 250; loss: 2.145; l2dist: 1.358\n",
      "    step: 300; loss: 2.122; l2dist: 1.348\n",
      "    step: 350; loss: 2.082; l2dist: 1.350\n",
      "    step: 400; loss: 2.062; l2dist: 1.333\n",
      "    step: 450; loss: 2.067; l2dist: 1.338\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.230; l2dist: 0.000\n",
      "    step: 50; loss: 6.195; l2dist: 2.027\n",
      "    step: 100; loss: 3.180; l2dist: 1.582\n",
      "    step: 150; loss: 2.395; l2dist: 1.405\n",
      "    step: 200; loss: 2.196; l2dist: 1.360\n",
      "    step: 250; loss: 2.105; l2dist: 1.335\n",
      "    step: 300; loss: 2.073; l2dist: 1.321\n",
      "    step: 350; loss: 2.037; l2dist: 1.318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 2.051; l2dist: 1.316\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.509; l2dist: 0.000\n",
      "    step: 50; loss: 5.817; l2dist: 1.902\n",
      "    step: 100; loss: 3.051; l2dist: 1.507\n",
      "    step: 150; loss: 2.329; l2dist: 1.349\n",
      "    step: 200; loss: 2.142; l2dist: 1.310\n",
      "    step: 250; loss: 2.066; l2dist: 1.285\n",
      "    step: 300; loss: 2.033; l2dist: 1.280\n",
      "    step: 350; loss: 2.006; l2dist: 1.282\n",
      "    step: 400; loss: 1.993; l2dist: 1.276\n",
      "    step: 450; loss: 1.996; l2dist: 1.270\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.142; l2dist: 0.000\n",
      "    step: 50; loss: 5.892; l2dist: 1.898\n",
      "    step: 100; loss: 3.102; l2dist: 1.506\n",
      "    step: 150; loss: 2.404; l2dist: 1.366\n",
      "    step: 200; loss: 2.188; l2dist: 1.312\n",
      "    step: 250; loss: 2.105; l2dist: 1.301\n",
      "    step: 300; loss: 2.063; l2dist: 1.293\n",
      "    step: 350; loss: 2.050; l2dist: 1.290\n",
      "    step: 400; loss: 2.034; l2dist: 1.284\n",
      "    step: 450; loss: 2.025; l2dist: 1.285\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.651; l2dist: 0.000\n",
      "    step: 50; loss: 5.899; l2dist: 1.899\n",
      "    step: 100; loss: 3.113; l2dist: 1.515\n",
      "    step: 150; loss: 2.377; l2dist: 1.358\n",
      "    step: 200; loss: 2.183; l2dist: 1.326\n",
      "    step: 250; loss: 2.108; l2dist: 1.303\n",
      "    step: 300; loss: 2.077; l2dist: 1.292\n",
      "    step: 350; loss: 2.051; l2dist: 1.287\n",
      "    step: 400; loss: 2.039; l2dist: 1.287\n",
      "    step: 450; loss: 2.035; l2dist: 1.288\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.373; l2dist: 0.000\n",
      "    step: 50; loss: 5.886; l2dist: 1.891\n",
      "    step: 100; loss: 3.117; l2dist: 1.515\n",
      "    step: 150; loss: 2.367; l2dist: 1.364\n",
      "    step: 200; loss: 2.189; l2dist: 1.316\n",
      "    step: 250; loss: 2.112; l2dist: 1.299\n",
      "    step: 300; loss: 2.074; l2dist: 1.291\n",
      "    step: 350; loss: 2.049; l2dist: 1.286\n",
      "    step: 400; loss: 2.052; l2dist: 1.286\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.618; l2dist: 0.000\n",
      "    step: 50; loss: 5.924; l2dist: 1.904\n",
      "    step: 100; loss: 3.125; l2dist: 1.522\n",
      "    step: 150; loss: 2.371; l2dist: 1.364\n",
      "    step: 200; loss: 2.188; l2dist: 1.321\n",
      "    step: 250; loss: 2.113; l2dist: 1.307\n",
      "    step: 300; loss: 2.087; l2dist: 1.305\n",
      "    step: 350; loss: 2.052; l2dist: 1.292\n",
      "    step: 400; loss: 2.036; l2dist: 1.293\n",
      "    step: 450; loss: 2.031; l2dist: 1.289\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 126.167; l2dist: 0.000\n",
      "    step: 50; loss: 14.163; l2dist: 3.394\n",
      "    step: 100; loss: 5.709; l2dist: 2.321\n",
      "    step: 150; loss: 3.784; l2dist: 1.862\n",
      "    step: 200; loss: 3.027; l2dist: 1.667\n",
      "    step: 250; loss: 2.701; l2dist: 1.567\n",
      "    step: 300; loss: 2.563; l2dist: 1.522\n",
      "    step: 350; loss: 2.474; l2dist: 1.488\n",
      "    step: 400; loss: 2.376; l2dist: 1.466\n",
      "    step: 450; loss: 2.351; l2dist: 1.457\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 156.369; l2dist: 0.000\n",
      "    step: 50; loss: 13.599; l2dist: 3.295\n",
      "    step: 100; loss: 5.948; l2dist: 2.301\n",
      "    step: 150; loss: 4.065; l2dist: 1.888\n",
      "    step: 200; loss: 3.254; l2dist: 1.688\n",
      "    step: 250; loss: 2.836; l2dist: 1.580\n",
      "    step: 300; loss: 2.636; l2dist: 1.528\n",
      "    step: 350; loss: 2.480; l2dist: 1.495\n",
      "    step: 400; loss: 2.383; l2dist: 1.462\n",
      "    step: 450; loss: 2.392; l2dist: 1.463\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 87.075; l2dist: 0.000\n",
      "    step: 50; loss: 10.537; l2dist: 2.840\n",
      "    step: 100; loss: 4.427; l2dist: 1.976\n",
      "    step: 150; loss: 3.152; l2dist: 1.672\n",
      "    step: 200; loss: 2.683; l2dist: 1.540\n",
      "    step: 250; loss: 2.460; l2dist: 1.481\n",
      "    step: 300; loss: 2.362; l2dist: 1.458\n",
      "    step: 350; loss: 2.309; l2dist: 1.435\n",
      "    step: 400; loss: 2.271; l2dist: 1.424\n",
      "    step: 450; loss: 2.229; l2dist: 1.415\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.437; l2dist: 0.000\n",
      "    step: 50; loss: 8.380; l2dist: 2.431\n",
      "    step: 100; loss: 3.740; l2dist: 1.802\n",
      "    step: 150; loss: 2.729; l2dist: 1.555\n",
      "    step: 200; loss: 2.425; l2dist: 1.461\n",
      "    step: 250; loss: 2.290; l2dist: 1.433\n",
      "    step: 300; loss: 2.217; l2dist: 1.407\n",
      "    step: 350; loss: 2.203; l2dist: 1.402\n",
      "    step: 400; loss: 2.204; l2dist: 1.400\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.511; l2dist: 0.000\n",
      "    step: 50; loss: 7.424; l2dist: 2.230\n",
      "    step: 100; loss: 3.449; l2dist: 1.686\n",
      "    step: 150; loss: 2.552; l2dist: 1.483\n",
      "    step: 200; loss: 2.311; l2dist: 1.416\n",
      "    step: 250; loss: 2.191; l2dist: 1.382\n",
      "    step: 300; loss: 2.164; l2dist: 1.370\n",
      "    step: 350; loss: 2.132; l2dist: 1.362\n",
      "    step: 400; loss: 2.111; l2dist: 1.357\n",
      "    step: 450; loss: 2.103; l2dist: 1.355\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.276; l2dist: 0.000\n",
      "    step: 50; loss: 6.734; l2dist: 2.070\n",
      "    step: 100; loss: 3.240; l2dist: 1.574\n",
      "    step: 150; loss: 2.456; l2dist: 1.411\n",
      "    step: 200; loss: 2.253; l2dist: 1.361\n",
      "    step: 250; loss: 2.155; l2dist: 1.340\n",
      "    step: 300; loss: 2.136; l2dist: 1.335\n",
      "    step: 350; loss: 2.104; l2dist: 1.328\n",
      "    step: 400; loss: 2.091; l2dist: 1.321\n",
      "    step: 450; loss: 2.081; l2dist: 1.320\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.350; l2dist: 0.000\n",
      "    step: 50; loss: 6.579; l2dist: 2.045\n",
      "    step: 100; loss: 3.202; l2dist: 1.576\n",
      "    step: 150; loss: 2.437; l2dist: 1.404\n",
      "    step: 200; loss: 2.241; l2dist: 1.360\n",
      "    step: 250; loss: 2.157; l2dist: 1.346\n",
      "    step: 300; loss: 2.110; l2dist: 1.329\n",
      "    step: 350; loss: 2.088; l2dist: 1.327\n",
      "    step: 400; loss: 2.080; l2dist: 1.315\n",
      "    step: 450; loss: 2.076; l2dist: 1.326\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.863; l2dist: 0.000\n",
      "    step: 50; loss: 6.491; l2dist: 2.025\n",
      "    step: 100; loss: 3.185; l2dist: 1.561\n",
      "    step: 150; loss: 2.422; l2dist: 1.399\n",
      "    step: 200; loss: 2.235; l2dist: 1.358\n",
      "    step: 250; loss: 2.153; l2dist: 1.331\n",
      "    step: 300; loss: 2.110; l2dist: 1.322\n",
      "    step: 350; loss: 2.095; l2dist: 1.319\n",
      "    step: 400; loss: 2.085; l2dist: 1.319\n",
      "    step: 450; loss: 2.086; l2dist: 1.319\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.146; l2dist: 0.000\n",
      "    step: 50; loss: 6.450; l2dist: 2.019\n",
      "    step: 100; loss: 3.184; l2dist: 1.554\n",
      "    step: 150; loss: 2.418; l2dist: 1.391\n",
      "    step: 200; loss: 2.229; l2dist: 1.352\n",
      "    step: 250; loss: 2.154; l2dist: 1.338\n",
      "    step: 300; loss: 2.104; l2dist: 1.323\n",
      "    step: 350; loss: 2.085; l2dist: 1.325\n",
      "    step: 400; loss: 2.077; l2dist: 1.322\n",
      "    step: 450; loss: 2.071; l2dist: 1.321\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.378; l2dist: 0.000\n",
      "    step: 50; loss: 6.500; l2dist: 2.033\n",
      "    step: 100; loss: 3.200; l2dist: 1.565\n",
      "    step: 150; loss: 2.432; l2dist: 1.406\n",
      "    step: 200; loss: 2.238; l2dist: 1.363\n",
      "    step: 250; loss: 2.143; l2dist: 1.345\n",
      "    step: 300; loss: 2.135; l2dist: 1.339\n",
      "    step: 350; loss: 2.097; l2dist: 1.325\n",
      "    step: 400; loss: 2.076; l2dist: 1.328\n",
      "    step: 450; loss: 2.090; l2dist: 1.330\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 116.222; l2dist: 0.000\n",
      "    step: 50; loss: 12.290; l2dist: 3.268\n",
      "    step: 100; loss: 5.358; l2dist: 2.222\n",
      "    step: 150; loss: 3.520; l2dist: 1.779\n",
      "    step: 200; loss: 2.830; l2dist: 1.575\n",
      "    step: 250; loss: 2.572; l2dist: 1.498\n",
      "    step: 300; loss: 2.416; l2dist: 1.445\n",
      "    step: 350; loss: 2.318; l2dist: 1.425\n",
      "    step: 400; loss: 2.243; l2dist: 1.388\n",
      "    step: 450; loss: 2.260; l2dist: 1.400\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 118.277; l2dist: 0.000\n",
      "    step: 50; loss: 11.571; l2dist: 3.135\n",
      "    step: 100; loss: 5.122; l2dist: 2.117\n",
      "    step: 150; loss: 3.503; l2dist: 1.729\n",
      "    step: 200; loss: 2.838; l2dist: 1.554\n",
      "    step: 250; loss: 2.574; l2dist: 1.484\n",
      "    step: 300; loss: 2.440; l2dist: 1.444\n",
      "    step: 350; loss: 2.349; l2dist: 1.418\n",
      "    step: 400; loss: 2.313; l2dist: 1.407\n",
      "    step: 450; loss: 2.318; l2dist: 1.402\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.139; l2dist: 0.000\n",
      "    step: 50; loss: 8.853; l2dist: 2.657\n",
      "    step: 100; loss: 3.839; l2dist: 1.816\n",
      "    step: 150; loss: 2.776; l2dist: 1.539\n",
      "    step: 200; loss: 2.455; l2dist: 1.450\n",
      "    step: 250; loss: 2.307; l2dist: 1.413\n",
      "    step: 300; loss: 2.229; l2dist: 1.382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 2.218; l2dist: 1.376\n",
      "    step: 400; loss: 2.191; l2dist: 1.364\n",
      "    step: 450; loss: 2.161; l2dist: 1.363\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.538; l2dist: 0.000\n",
      "    step: 50; loss: 7.441; l2dist: 2.335\n",
      "    step: 100; loss: 3.401; l2dist: 1.707\n",
      "    step: 150; loss: 2.565; l2dist: 1.483\n",
      "    step: 200; loss: 2.295; l2dist: 1.403\n",
      "    step: 250; loss: 2.205; l2dist: 1.375\n",
      "    step: 300; loss: 2.159; l2dist: 1.362\n",
      "    step: 350; loss: 2.146; l2dist: 1.354\n",
      "    step: 400; loss: 2.124; l2dist: 1.349\n",
      "    step: 450; loss: 2.111; l2dist: 1.342\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.191; l2dist: 0.000\n",
      "    step: 50; loss: 6.868; l2dist: 2.198\n",
      "    step: 100; loss: 3.244; l2dist: 1.639\n",
      "    step: 150; loss: 2.452; l2dist: 1.433\n",
      "    step: 200; loss: 2.262; l2dist: 1.381\n",
      "    step: 250; loss: 2.183; l2dist: 1.357\n",
      "    step: 300; loss: 2.136; l2dist: 1.347\n",
      "    step: 350; loss: 2.125; l2dist: 1.335\n",
      "    step: 400; loss: 2.111; l2dist: 1.329\n",
      "    step: 450; loss: 2.108; l2dist: 1.339\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.613; l2dist: 0.000\n",
      "    step: 50; loss: 6.564; l2dist: 2.102\n",
      "    step: 100; loss: 3.226; l2dist: 1.600\n",
      "    step: 150; loss: 2.452; l2dist: 1.410\n",
      "    step: 200; loss: 2.254; l2dist: 1.358\n",
      "    step: 250; loss: 2.172; l2dist: 1.340\n",
      "    step: 300; loss: 2.146; l2dist: 1.333\n",
      "    step: 350; loss: 2.118; l2dist: 1.328\n",
      "    step: 400; loss: 2.113; l2dist: 1.320\n",
      "    step: 450; loss: 2.100; l2dist: 1.322\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.126; l2dist: 0.000\n",
      "    step: 50; loss: 6.404; l2dist: 2.062\n",
      "    step: 100; loss: 3.213; l2dist: 1.589\n",
      "    step: 150; loss: 2.429; l2dist: 1.407\n",
      "    step: 200; loss: 2.246; l2dist: 1.361\n",
      "    step: 250; loss: 2.175; l2dist: 1.341\n",
      "    step: 300; loss: 2.139; l2dist: 1.342\n",
      "    step: 350; loss: 2.110; l2dist: 1.329\n",
      "    step: 400; loss: 2.093; l2dist: 1.324\n",
      "    step: 450; loss: 2.085; l2dist: 1.322\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.076; l2dist: 0.000\n",
      "    step: 50; loss: 6.357; l2dist: 2.054\n",
      "    step: 100; loss: 3.189; l2dist: 1.575\n",
      "    step: 150; loss: 2.414; l2dist: 1.404\n",
      "    step: 200; loss: 2.227; l2dist: 1.357\n",
      "    step: 250; loss: 2.154; l2dist: 1.337\n",
      "    step: 300; loss: 2.120; l2dist: 1.326\n",
      "    step: 350; loss: 2.104; l2dist: 1.321\n",
      "    step: 400; loss: 2.089; l2dist: 1.314\n",
      "    step: 450; loss: 2.092; l2dist: 1.326\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.429; l2dist: 0.000\n",
      "    step: 50; loss: 6.326; l2dist: 2.043\n",
      "    step: 100; loss: 3.172; l2dist: 1.568\n",
      "    step: 150; loss: 2.411; l2dist: 1.399\n",
      "    step: 200; loss: 2.224; l2dist: 1.357\n",
      "    step: 250; loss: 2.160; l2dist: 1.333\n",
      "    step: 300; loss: 2.121; l2dist: 1.327\n",
      "    step: 350; loss: 2.111; l2dist: 1.326\n",
      "    step: 400; loss: 2.094; l2dist: 1.314\n",
      "    step: 450; loss: 2.082; l2dist: 1.320\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.690; l2dist: 0.000\n",
      "    step: 50; loss: 6.359; l2dist: 2.054\n",
      "    step: 100; loss: 3.190; l2dist: 1.576\n",
      "    step: 150; loss: 2.419; l2dist: 1.406\n",
      "    step: 200; loss: 2.233; l2dist: 1.361\n",
      "    step: 250; loss: 2.164; l2dist: 1.338\n",
      "    step: 300; loss: 2.129; l2dist: 1.335\n",
      "    step: 350; loss: 2.107; l2dist: 1.325\n",
      "    step: 400; loss: 2.099; l2dist: 1.324\n",
      "    step: 450; loss: 2.085; l2dist: 1.319\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 109.350; l2dist: 0.000\n",
      "    step: 50; loss: 10.684; l2dist: 3.063\n",
      "    step: 100; loss: 4.674; l2dist: 2.069\n",
      "    step: 150; loss: 3.092; l2dist: 1.660\n",
      "    step: 200; loss: 2.385; l2dist: 1.454\n",
      "    step: 250; loss: 2.162; l2dist: 1.377\n",
      "    step: 300; loss: 2.042; l2dist: 1.338\n",
      "    step: 350; loss: 2.005; l2dist: 1.301\n",
      "    step: 400; loss: 1.965; l2dist: 1.299\n",
      "    step: 450; loss: 1.930; l2dist: 1.293\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 123.934; l2dist: 0.000\n",
      "    step: 50; loss: 10.283; l2dist: 2.938\n",
      "    step: 100; loss: 4.768; l2dist: 2.018\n",
      "    step: 150; loss: 3.229; l2dist: 1.644\n",
      "    step: 200; loss: 2.537; l2dist: 1.466\n",
      "    step: 250; loss: 2.208; l2dist: 1.374\n",
      "    step: 300; loss: 2.052; l2dist: 1.328\n",
      "    step: 350; loss: 1.968; l2dist: 1.304\n",
      "    step: 400; loss: 1.930; l2dist: 1.281\n",
      "    step: 450; loss: 1.903; l2dist: 1.268\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.826; l2dist: 0.000\n",
      "    step: 50; loss: 7.943; l2dist: 2.496\n",
      "    step: 100; loss: 3.508; l2dist: 1.727\n",
      "    step: 150; loss: 2.471; l2dist: 1.443\n",
      "    step: 200; loss: 2.092; l2dist: 1.335\n",
      "    step: 250; loss: 1.986; l2dist: 1.303\n",
      "    step: 300; loss: 1.875; l2dist: 1.264\n",
      "    step: 350; loss: 1.822; l2dist: 1.247\n",
      "    step: 400; loss: 1.781; l2dist: 1.237\n",
      "    step: 450; loss: 1.764; l2dist: 1.228\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.283; l2dist: 0.000\n",
      "    step: 50; loss: 6.739; l2dist: 2.278\n",
      "    step: 100; loss: 3.047; l2dist: 1.620\n",
      "    step: 150; loss: 2.201; l2dist: 1.375\n",
      "    step: 200; loss: 1.952; l2dist: 1.296\n",
      "    step: 250; loss: 1.850; l2dist: 1.264\n",
      "    step: 300; loss: 1.789; l2dist: 1.239\n",
      "    step: 350; loss: 1.759; l2dist: 1.234\n",
      "    step: 400; loss: 1.746; l2dist: 1.228\n",
      "    step: 450; loss: 1.768; l2dist: 1.239\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.448; l2dist: 0.000\n",
      "    step: 50; loss: 5.898; l2dist: 2.066\n",
      "    step: 100; loss: 2.859; l2dist: 1.547\n",
      "    step: 150; loss: 2.062; l2dist: 1.330\n",
      "    step: 200; loss: 1.870; l2dist: 1.267\n",
      "    step: 250; loss: 1.788; l2dist: 1.235\n",
      "    step: 300; loss: 1.745; l2dist: 1.222\n",
      "    step: 350; loss: 1.719; l2dist: 1.215\n",
      "    step: 400; loss: 1.716; l2dist: 1.210\n",
      "    step: 450; loss: 1.700; l2dist: 1.203\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.967; l2dist: 0.000\n",
      "    step: 50; loss: 5.619; l2dist: 1.976\n",
      "    step: 100; loss: 2.745; l2dist: 1.483\n",
      "    step: 150; loss: 2.026; l2dist: 1.289\n",
      "    step: 200; loss: 1.838; l2dist: 1.237\n",
      "    step: 250; loss: 1.769; l2dist: 1.211\n",
      "    step: 300; loss: 1.725; l2dist: 1.197\n",
      "    step: 350; loss: 1.714; l2dist: 1.196\n",
      "    step: 400; loss: 1.703; l2dist: 1.193\n",
      "    step: 450; loss: 1.691; l2dist: 1.186\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.203; l2dist: 0.000\n",
      "    step: 50; loss: 5.482; l2dist: 1.935\n",
      "    step: 100; loss: 2.723; l2dist: 1.455\n",
      "    step: 150; loss: 2.002; l2dist: 1.274\n",
      "    step: 200; loss: 1.830; l2dist: 1.228\n",
      "    step: 250; loss: 1.760; l2dist: 1.205\n",
      "    step: 300; loss: 1.727; l2dist: 1.194\n",
      "    step: 350; loss: 1.705; l2dist: 1.189\n",
      "    step: 400; loss: 1.698; l2dist: 1.185\n",
      "    step: 450; loss: 1.706; l2dist: 1.188\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.114; l2dist: 0.000\n",
      "    step: 50; loss: 5.466; l2dist: 1.924\n",
      "    step: 100; loss: 2.705; l2dist: 1.451\n",
      "    step: 150; loss: 1.993; l2dist: 1.269\n",
      "    step: 200; loss: 1.830; l2dist: 1.222\n",
      "    step: 250; loss: 1.747; l2dist: 1.199\n",
      "    step: 300; loss: 1.721; l2dist: 1.193\n",
      "    step: 350; loss: 1.710; l2dist: 1.195\n",
      "    step: 400; loss: 1.695; l2dist: 1.184\n",
      "    step: 450; loss: 1.687; l2dist: 1.180\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.497; l2dist: 0.000\n",
      "    step: 50; loss: 5.448; l2dist: 1.919\n",
      "    step: 100; loss: 2.711; l2dist: 1.454\n",
      "    step: 150; loss: 1.988; l2dist: 1.272\n",
      "    step: 200; loss: 1.818; l2dist: 1.225\n",
      "    step: 250; loss: 1.764; l2dist: 1.200\n",
      "    step: 300; loss: 1.720; l2dist: 1.198\n",
      "    step: 350; loss: 1.715; l2dist: 1.189\n",
      "    step: 400; loss: 1.697; l2dist: 1.187\n",
      "    step: 450; loss: 1.704; l2dist: 1.189\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.621; l2dist: 0.000\n",
      "    step: 50; loss: 5.470; l2dist: 1.926\n",
      "    step: 100; loss: 2.716; l2dist: 1.458\n",
      "    step: 150; loss: 1.988; l2dist: 1.283\n",
      "    step: 200; loss: 1.824; l2dist: 1.231\n",
      "    step: 250; loss: 1.760; l2dist: 1.214\n",
      "    step: 300; loss: 1.730; l2dist: 1.197\n",
      "    step: 350; loss: 1.708; l2dist: 1.194\n",
      "    step: 400; loss: 1.692; l2dist: 1.185\n",
      "    step: 450; loss: 1.709; l2dist: 1.187\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 121.201; l2dist: 0.000\n",
      "    step: 50; loss: 12.626; l2dist: 3.378\n",
      "    step: 100; loss: 5.654; l2dist: 2.279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 3.752; l2dist: 1.843\n",
      "    step: 200; loss: 3.044; l2dist: 1.650\n",
      "    step: 250; loss: 2.721; l2dist: 1.550\n",
      "    step: 300; loss: 2.530; l2dist: 1.493\n",
      "    step: 350; loss: 2.466; l2dist: 1.476\n",
      "    step: 400; loss: 2.449; l2dist: 1.472\n",
      "    step: 450; loss: 2.399; l2dist: 1.452\n",
      "binary step: 0; number of successful adv: 92/100\n",
      "    step: 0; loss: 165.769; l2dist: 0.000\n",
      "    step: 50; loss: 13.428; l2dist: 3.426\n",
      "    step: 100; loss: 6.690; l2dist: 2.406\n",
      "    step: 150; loss: 4.475; l2dist: 1.942\n",
      "    step: 200; loss: 3.485; l2dist: 1.716\n",
      "    step: 250; loss: 2.960; l2dist: 1.595\n",
      "    step: 300; loss: 2.730; l2dist: 1.528\n",
      "    step: 350; loss: 2.574; l2dist: 1.498\n",
      "    step: 400; loss: 2.511; l2dist: 1.478\n",
      "    step: 450; loss: 2.430; l2dist: 1.453\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 90.306; l2dist: 0.000\n",
      "    step: 50; loss: 10.282; l2dist: 2.941\n",
      "    step: 100; loss: 4.683; l2dist: 2.006\n",
      "    step: 150; loss: 3.237; l2dist: 1.667\n",
      "    step: 200; loss: 2.719; l2dist: 1.534\n",
      "    step: 250; loss: 2.507; l2dist: 1.479\n",
      "    step: 300; loss: 2.381; l2dist: 1.438\n",
      "    step: 350; loss: 2.334; l2dist: 1.428\n",
      "    step: 400; loss: 2.291; l2dist: 1.421\n",
      "    step: 450; loss: 2.248; l2dist: 1.405\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.293; l2dist: 0.000\n",
      "    step: 50; loss: 8.526; l2dist: 2.549\n",
      "    step: 100; loss: 3.817; l2dist: 1.808\n",
      "    step: 150; loss: 2.788; l2dist: 1.555\n",
      "    step: 200; loss: 2.503; l2dist: 1.469\n",
      "    step: 250; loss: 2.329; l2dist: 1.424\n",
      "    step: 300; loss: 2.261; l2dist: 1.411\n",
      "    step: 350; loss: 2.325; l2dist: 1.410\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.693; l2dist: 0.000\n",
      "    step: 50; loss: 7.809; l2dist: 2.332\n",
      "    step: 100; loss: 3.604; l2dist: 1.688\n",
      "    step: 150; loss: 2.725; l2dist: 1.492\n",
      "    step: 200; loss: 2.434; l2dist: 1.417\n",
      "    step: 250; loss: 2.333; l2dist: 1.395\n",
      "    step: 300; loss: 2.280; l2dist: 1.380\n",
      "    step: 350; loss: 2.241; l2dist: 1.376\n",
      "    step: 400; loss: 2.230; l2dist: 1.370\n",
      "    step: 450; loss: 2.218; l2dist: 1.366\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.453; l2dist: 0.000\n",
      "    step: 50; loss: 7.197; l2dist: 2.171\n",
      "    step: 100; loss: 3.399; l2dist: 1.565\n",
      "    step: 150; loss: 2.621; l2dist: 1.407\n",
      "    step: 200; loss: 2.376; l2dist: 1.361\n",
      "    step: 250; loss: 2.284; l2dist: 1.331\n",
      "    step: 300; loss: 2.229; l2dist: 1.336\n",
      "    step: 350; loss: 2.204; l2dist: 1.322\n",
      "    step: 400; loss: 2.183; l2dist: 1.322\n",
      "    step: 450; loss: 2.193; l2dist: 1.324\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.838; l2dist: 0.000\n",
      "    step: 50; loss: 7.014; l2dist: 2.129\n",
      "    step: 100; loss: 3.330; l2dist: 1.556\n",
      "    step: 150; loss: 2.567; l2dist: 1.400\n",
      "    step: 200; loss: 2.345; l2dist: 1.353\n",
      "    step: 250; loss: 2.245; l2dist: 1.328\n",
      "    step: 300; loss: 2.197; l2dist: 1.324\n",
      "    step: 350; loss: 2.168; l2dist: 1.319\n",
      "    step: 400; loss: 2.158; l2dist: 1.314\n",
      "    step: 450; loss: 2.146; l2dist: 1.319\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.494; l2dist: 0.000\n",
      "    step: 50; loss: 6.931; l2dist: 2.113\n",
      "    step: 100; loss: 3.341; l2dist: 1.544\n",
      "    step: 150; loss: 2.568; l2dist: 1.392\n",
      "    step: 200; loss: 2.341; l2dist: 1.346\n",
      "    step: 250; loss: 2.244; l2dist: 1.328\n",
      "    step: 300; loss: 2.204; l2dist: 1.322\n",
      "    step: 350; loss: 2.174; l2dist: 1.315\n",
      "    step: 400; loss: 2.200; l2dist: 1.323\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.848; l2dist: 0.000\n",
      "    step: 50; loss: 6.905; l2dist: 2.103\n",
      "    step: 100; loss: 3.328; l2dist: 1.550\n",
      "    step: 150; loss: 2.549; l2dist: 1.393\n",
      "    step: 200; loss: 2.336; l2dist: 1.348\n",
      "    step: 250; loss: 2.238; l2dist: 1.334\n",
      "    step: 300; loss: 2.186; l2dist: 1.322\n",
      "    step: 350; loss: 2.165; l2dist: 1.320\n",
      "    step: 400; loss: 2.163; l2dist: 1.314\n",
      "    step: 450; loss: 2.151; l2dist: 1.317\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.034; l2dist: 0.000\n",
      "    step: 50; loss: 6.947; l2dist: 2.115\n",
      "    step: 100; loss: 3.342; l2dist: 1.561\n",
      "    step: 150; loss: 2.566; l2dist: 1.404\n",
      "    step: 200; loss: 2.345; l2dist: 1.360\n",
      "    step: 250; loss: 2.245; l2dist: 1.339\n",
      "    step: 300; loss: 2.217; l2dist: 1.340\n",
      "    step: 350; loss: 2.197; l2dist: 1.333\n",
      "    step: 400; loss: 2.172; l2dist: 1.329\n",
      "    step: 450; loss: 2.168; l2dist: 1.324\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 118.574; l2dist: 0.000\n",
      "    step: 50; loss: 12.529; l2dist: 3.294\n",
      "    step: 100; loss: 5.528; l2dist: 2.271\n",
      "    step: 150; loss: 3.717; l2dist: 1.848\n",
      "    step: 200; loss: 3.017; l2dist: 1.647\n",
      "    step: 250; loss: 2.740; l2dist: 1.550\n",
      "    step: 300; loss: 2.469; l2dist: 1.480\n",
      "    step: 350; loss: 2.400; l2dist: 1.459\n",
      "    step: 400; loss: 2.385; l2dist: 1.456\n",
      "    step: 450; loss: 2.333; l2dist: 1.442\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 73.822; l2dist: 0.000\n",
      "    step: 50; loss: 9.937; l2dist: 2.876\n",
      "    step: 100; loss: 4.252; l2dist: 1.952\n",
      "    step: 150; loss: 3.056; l2dist: 1.639\n",
      "    step: 200; loss: 2.677; l2dist: 1.522\n",
      "    step: 250; loss: 2.430; l2dist: 1.457\n",
      "    step: 300; loss: 2.344; l2dist: 1.431\n",
      "    step: 350; loss: 2.304; l2dist: 1.424\n",
      "    step: 400; loss: 2.265; l2dist: 1.409\n",
      "    step: 450; loss: 2.269; l2dist: 1.419\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.433; l2dist: 0.000\n",
      "    step: 50; loss: 7.951; l2dist: 2.500\n",
      "    step: 100; loss: 3.546; l2dist: 1.764\n",
      "    step: 150; loss: 2.645; l2dist: 1.527\n",
      "    step: 200; loss: 2.374; l2dist: 1.454\n",
      "    step: 250; loss: 2.290; l2dist: 1.413\n",
      "    step: 300; loss: 2.224; l2dist: 1.400\n",
      "    step: 350; loss: 2.193; l2dist: 1.394\n",
      "    step: 400; loss: 2.183; l2dist: 1.390\n",
      "    step: 450; loss: 2.158; l2dist: 1.377\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.947; l2dist: 0.000\n",
      "    step: 50; loss: 6.726; l2dist: 2.178\n",
      "    step: 100; loss: 3.326; l2dist: 1.709\n",
      "    step: 150; loss: 2.472; l2dist: 1.475\n",
      "    step: 200; loss: 2.275; l2dist: 1.416\n",
      "    step: 250; loss: 2.192; l2dist: 1.392\n",
      "    step: 300; loss: 2.183; l2dist: 1.385\n",
      "    step: 350; loss: 2.134; l2dist: 1.373\n",
      "    step: 400; loss: 2.123; l2dist: 1.373\n",
      "    step: 450; loss: 2.114; l2dist: 1.368\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.952; l2dist: 0.000\n",
      "    step: 50; loss: 5.950; l2dist: 1.970\n",
      "    step: 100; loss: 3.287; l2dist: 1.622\n",
      "    step: 150; loss: 2.439; l2dist: 1.437\n",
      "    step: 200; loss: 2.232; l2dist: 1.389\n",
      "    step: 250; loss: 2.164; l2dist: 1.366\n",
      "    step: 300; loss: 2.131; l2dist: 1.353\n",
      "    step: 350; loss: 2.098; l2dist: 1.353\n",
      "    step: 400; loss: 2.096; l2dist: 1.350\n",
      "    step: 450; loss: 2.091; l2dist: 1.344\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.056; l2dist: 0.000\n",
      "    step: 50; loss: 5.493; l2dist: 1.824\n",
      "    step: 100; loss: 3.174; l2dist: 1.520\n",
      "    step: 150; loss: 2.398; l2dist: 1.369\n",
      "    step: 200; loss: 2.188; l2dist: 1.331\n",
      "    step: 250; loss: 2.119; l2dist: 1.323\n",
      "    step: 300; loss: 2.082; l2dist: 1.312\n",
      "    step: 350; loss: 2.066; l2dist: 1.305\n",
      "    step: 400; loss: 2.054; l2dist: 1.305\n",
      "    step: 450; loss: 2.044; l2dist: 1.307\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.068; l2dist: 0.000\n",
      "    step: 50; loss: 5.514; l2dist: 1.813\n",
      "    step: 100; loss: 3.216; l2dist: 1.527\n",
      "    step: 150; loss: 2.410; l2dist: 1.382\n",
      "    step: 200; loss: 2.204; l2dist: 1.337\n",
      "    step: 250; loss: 2.128; l2dist: 1.322\n",
      "    step: 300; loss: 2.094; l2dist: 1.318\n",
      "    step: 350; loss: 2.077; l2dist: 1.314\n",
      "    step: 400; loss: 2.060; l2dist: 1.311\n",
      "    step: 450; loss: 2.059; l2dist: 1.314\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.132; l2dist: 0.000\n",
      "    step: 50; loss: 5.536; l2dist: 1.815\n",
      "    step: 100; loss: 3.244; l2dist: 1.540\n",
      "    step: 150; loss: 2.427; l2dist: 1.390\n",
      "    step: 200; loss: 2.203; l2dist: 1.350\n",
      "    step: 250; loss: 2.131; l2dist: 1.327\n",
      "    step: 300; loss: 2.098; l2dist: 1.328\n",
      "    step: 350; loss: 2.085; l2dist: 1.321\n",
      "    step: 400; loss: 2.066; l2dist: 1.317\n",
      "    step: 450; loss: 2.059; l2dist: 1.319\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.870; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 5.519; l2dist: 1.811\n",
      "    step: 100; loss: 3.256; l2dist: 1.547\n",
      "    step: 150; loss: 2.422; l2dist: 1.394\n",
      "    step: 200; loss: 2.214; l2dist: 1.352\n",
      "    step: 250; loss: 2.135; l2dist: 1.338\n",
      "    step: 300; loss: 2.109; l2dist: 1.324\n",
      "    step: 350; loss: 2.079; l2dist: 1.323\n",
      "    step: 400; loss: 2.074; l2dist: 1.323\n",
      "    step: 450; loss: 2.064; l2dist: 1.319\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.171; l2dist: 0.000\n",
      "    step: 50; loss: 5.581; l2dist: 1.830\n",
      "    step: 100; loss: 3.260; l2dist: 1.556\n",
      "    step: 150; loss: 2.426; l2dist: 1.401\n",
      "    step: 200; loss: 2.217; l2dist: 1.357\n",
      "    step: 250; loss: 2.139; l2dist: 1.346\n",
      "    step: 300; loss: 2.111; l2dist: 1.331\n",
      "    step: 350; loss: 2.086; l2dist: 1.335\n",
      "    step: 400; loss: 2.080; l2dist: 1.324\n",
      "    step: 450; loss: 2.071; l2dist: 1.332\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 117.455; l2dist: 0.000\n",
      "    step: 50; loss: 13.962; l2dist: 3.367\n",
      "    step: 100; loss: 5.787; l2dist: 2.289\n",
      "    step: 150; loss: 3.841; l2dist: 1.829\n",
      "    step: 200; loss: 3.106; l2dist: 1.626\n",
      "    step: 250; loss: 2.795; l2dist: 1.533\n",
      "    step: 300; loss: 2.675; l2dist: 1.493\n",
      "    step: 350; loss: 2.601; l2dist: 1.479\n",
      "    step: 400; loss: 2.531; l2dist: 1.454\n",
      "    step: 450; loss: 2.486; l2dist: 1.449\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 92.669; l2dist: 0.000\n",
      "    step: 50; loss: 11.969; l2dist: 3.054\n",
      "    step: 100; loss: 4.762; l2dist: 2.014\n",
      "    step: 150; loss: 3.337; l2dist: 1.666\n",
      "    step: 200; loss: 2.846; l2dist: 1.540\n",
      "    step: 250; loss: 2.628; l2dist: 1.480\n",
      "    step: 300; loss: 2.480; l2dist: 1.438\n",
      "    step: 350; loss: 2.452; l2dist: 1.418\n",
      "    step: 400; loss: 2.397; l2dist: 1.404\n",
      "    step: 450; loss: 2.374; l2dist: 1.405\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.931; l2dist: 0.000\n",
      "    step: 50; loss: 9.598; l2dist: 2.647\n",
      "    step: 100; loss: 3.930; l2dist: 1.818\n",
      "    step: 150; loss: 2.881; l2dist: 1.552\n",
      "    step: 200; loss: 2.566; l2dist: 1.460\n",
      "    step: 250; loss: 2.452; l2dist: 1.424\n",
      "    step: 300; loss: 2.420; l2dist: 1.411\n",
      "    step: 350; loss: 2.354; l2dist: 1.392\n",
      "    step: 400; loss: 2.361; l2dist: 1.392\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.161; l2dist: 0.000\n",
      "    step: 50; loss: 8.101; l2dist: 2.373\n",
      "    step: 100; loss: 3.594; l2dist: 1.719\n",
      "    step: 150; loss: 2.708; l2dist: 1.486\n",
      "    step: 200; loss: 2.485; l2dist: 1.419\n",
      "    step: 250; loss: 2.387; l2dist: 1.395\n",
      "    step: 300; loss: 2.362; l2dist: 1.381\n",
      "    step: 350; loss: 2.320; l2dist: 1.372\n",
      "    step: 400; loss: 2.307; l2dist: 1.371\n",
      "    step: 450; loss: 2.286; l2dist: 1.367\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.434; l2dist: 0.000\n",
      "    step: 50; loss: 7.631; l2dist: 2.232\n",
      "    step: 100; loss: 3.511; l2dist: 1.664\n",
      "    step: 150; loss: 2.653; l2dist: 1.470\n",
      "    step: 200; loss: 2.441; l2dist: 1.408\n",
      "    step: 250; loss: 2.371; l2dist: 1.378\n",
      "    step: 300; loss: 2.317; l2dist: 1.369\n",
      "    step: 350; loss: 2.302; l2dist: 1.362\n",
      "    step: 400; loss: 2.270; l2dist: 1.356\n",
      "    step: 450; loss: 2.261; l2dist: 1.358\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.680; l2dist: 0.000\n",
      "    step: 50; loss: 7.374; l2dist: 2.161\n",
      "    step: 100; loss: 3.459; l2dist: 1.623\n",
      "    step: 150; loss: 2.642; l2dist: 1.439\n",
      "    step: 200; loss: 2.431; l2dist: 1.394\n",
      "    step: 250; loss: 2.350; l2dist: 1.370\n",
      "    step: 300; loss: 2.317; l2dist: 1.364\n",
      "    step: 350; loss: 2.294; l2dist: 1.357\n",
      "    step: 400; loss: 2.287; l2dist: 1.352\n",
      "    step: 450; loss: 2.277; l2dist: 1.350\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.156; l2dist: 0.000\n",
      "    step: 50; loss: 7.211; l2dist: 2.132\n",
      "    step: 100; loss: 3.418; l2dist: 1.604\n",
      "    step: 150; loss: 2.611; l2dist: 1.426\n",
      "    step: 200; loss: 2.413; l2dist: 1.375\n",
      "    step: 250; loss: 2.327; l2dist: 1.357\n",
      "    step: 300; loss: 2.287; l2dist: 1.351\n",
      "    step: 350; loss: 2.269; l2dist: 1.340\n",
      "    step: 400; loss: 2.260; l2dist: 1.341\n",
      "    step: 450; loss: 2.258; l2dist: 1.342\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.916; l2dist: 0.000\n",
      "    step: 50; loss: 7.226; l2dist: 2.132\n",
      "    step: 100; loss: 3.442; l2dist: 1.611\n",
      "    step: 150; loss: 2.624; l2dist: 1.431\n",
      "    step: 200; loss: 2.425; l2dist: 1.381\n",
      "    step: 250; loss: 2.354; l2dist: 1.362\n",
      "    step: 300; loss: 2.312; l2dist: 1.349\n",
      "    step: 350; loss: 2.308; l2dist: 1.350\n",
      "    step: 400; loss: 2.283; l2dist: 1.344\n",
      "    step: 450; loss: 2.277; l2dist: 1.342\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.731; l2dist: 0.000\n",
      "    step: 50; loss: 7.219; l2dist: 2.129\n",
      "    step: 100; loss: 3.443; l2dist: 1.607\n",
      "    step: 150; loss: 2.626; l2dist: 1.429\n",
      "    step: 200; loss: 2.437; l2dist: 1.383\n",
      "    step: 250; loss: 2.350; l2dist: 1.365\n",
      "    step: 300; loss: 2.317; l2dist: 1.352\n",
      "    step: 350; loss: 2.315; l2dist: 1.353\n",
      "    step: 400; loss: 2.288; l2dist: 1.353\n",
      "    step: 450; loss: 2.281; l2dist: 1.346\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.870; l2dist: 0.000\n",
      "    step: 50; loss: 7.227; l2dist: 2.136\n",
      "    step: 100; loss: 3.446; l2dist: 1.611\n",
      "    step: 150; loss: 2.640; l2dist: 1.429\n",
      "    step: 200; loss: 2.418; l2dist: 1.386\n",
      "    step: 250; loss: 2.349; l2dist: 1.366\n",
      "    step: 300; loss: 2.312; l2dist: 1.358\n",
      "    step: 350; loss: 2.296; l2dist: 1.354\n",
      "    step: 400; loss: 2.279; l2dist: 1.351\n",
      "    step: 450; loss: 2.269; l2dist: 1.349\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 116.372; l2dist: 0.000\n",
      "    step: 50; loss: 11.788; l2dist: 3.173\n",
      "    step: 100; loss: 4.957; l2dist: 2.143\n",
      "    step: 150; loss: 3.260; l2dist: 1.729\n",
      "    step: 200; loss: 2.621; l2dist: 1.545\n",
      "    step: 250; loss: 2.326; l2dist: 1.443\n",
      "    step: 300; loss: 2.219; l2dist: 1.419\n",
      "    step: 350; loss: 2.123; l2dist: 1.378\n",
      "    step: 400; loss: 2.091; l2dist: 1.370\n",
      "    step: 450; loss: 2.079; l2dist: 1.364\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 86.364; l2dist: 0.000\n",
      "    step: 50; loss: 9.459; l2dist: 2.787\n",
      "    step: 100; loss: 3.893; l2dist: 1.860\n",
      "    step: 150; loss: 2.764; l2dist: 1.556\n",
      "    step: 200; loss: 2.353; l2dist: 1.435\n",
      "    step: 250; loss: 2.200; l2dist: 1.387\n",
      "    step: 300; loss: 2.152; l2dist: 1.379\n",
      "    step: 350; loss: 2.049; l2dist: 1.345\n",
      "    step: 400; loss: 2.009; l2dist: 1.334\n",
      "    step: 450; loss: 2.000; l2dist: 1.334\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.508; l2dist: 0.000\n",
      "    step: 50; loss: 7.484; l2dist: 2.418\n",
      "    step: 100; loss: 3.224; l2dist: 1.689\n",
      "    step: 150; loss: 2.398; l2dist: 1.452\n",
      "    step: 200; loss: 2.160; l2dist: 1.386\n",
      "    step: 250; loss: 2.046; l2dist: 1.345\n",
      "    step: 300; loss: 1.999; l2dist: 1.333\n",
      "    step: 350; loss: 1.968; l2dist: 1.323\n",
      "    step: 400; loss: 1.957; l2dist: 1.318\n",
      "    step: 450; loss: 1.905; l2dist: 1.304\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.635; l2dist: 0.000\n",
      "    step: 50; loss: 6.360; l2dist: 2.160\n",
      "    step: 100; loss: 2.985; l2dist: 1.618\n",
      "    step: 150; loss: 2.224; l2dist: 1.401\n",
      "    step: 200; loss: 2.020; l2dist: 1.337\n",
      "    step: 250; loss: 1.947; l2dist: 1.315\n",
      "    step: 300; loss: 1.918; l2dist: 1.303\n",
      "    step: 350; loss: 1.881; l2dist: 1.294\n",
      "    step: 400; loss: 1.870; l2dist: 1.288\n",
      "    step: 450; loss: 1.890; l2dist: 1.291\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.634; l2dist: 0.000\n",
      "    step: 50; loss: 5.636; l2dist: 1.932\n",
      "    step: 100; loss: 2.905; l2dist: 1.510\n",
      "    step: 150; loss: 2.154; l2dist: 1.346\n",
      "    step: 200; loss: 1.977; l2dist: 1.299\n",
      "    step: 250; loss: 1.901; l2dist: 1.278\n",
      "    step: 300; loss: 1.856; l2dist: 1.265\n",
      "    step: 350; loss: 1.833; l2dist: 1.267\n",
      "    step: 400; loss: 1.837; l2dist: 1.266\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.878; l2dist: 0.000\n",
      "    step: 50; loss: 5.273; l2dist: 1.808\n",
      "    step: 100; loss: 2.788; l2dist: 1.438\n",
      "    step: 150; loss: 2.097; l2dist: 1.291\n",
      "    step: 200; loss: 1.916; l2dist: 1.251\n",
      "    step: 250; loss: 1.847; l2dist: 1.236\n",
      "    step: 300; loss: 1.808; l2dist: 1.224\n",
      "    step: 350; loss: 1.788; l2dist: 1.224\n",
      "    step: 400; loss: 1.795; l2dist: 1.225\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.781; l2dist: 0.000\n",
      "    step: 50; loss: 5.238; l2dist: 1.794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 2.772; l2dist: 1.428\n",
      "    step: 150; loss: 2.069; l2dist: 1.281\n",
      "    step: 200; loss: 1.897; l2dist: 1.238\n",
      "    step: 250; loss: 1.849; l2dist: 1.224\n",
      "    step: 300; loss: 1.799; l2dist: 1.212\n",
      "    step: 350; loss: 1.786; l2dist: 1.213\n",
      "    step: 400; loss: 1.772; l2dist: 1.209\n",
      "    step: 450; loss: 1.776; l2dist: 1.208\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.735; l2dist: 0.000\n",
      "    step: 50; loss: 5.251; l2dist: 1.797\n",
      "    step: 100; loss: 2.784; l2dist: 1.432\n",
      "    step: 150; loss: 2.080; l2dist: 1.288\n",
      "    step: 200; loss: 1.905; l2dist: 1.247\n",
      "    step: 250; loss: 1.859; l2dist: 1.232\n",
      "    step: 300; loss: 1.825; l2dist: 1.230\n",
      "    step: 350; loss: 1.795; l2dist: 1.215\n",
      "    step: 400; loss: 1.797; l2dist: 1.221\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.383; l2dist: 0.000\n",
      "    step: 50; loss: 5.221; l2dist: 1.790\n",
      "    step: 100; loss: 2.779; l2dist: 1.430\n",
      "    step: 150; loss: 2.080; l2dist: 1.287\n",
      "    step: 200; loss: 1.903; l2dist: 1.244\n",
      "    step: 250; loss: 1.839; l2dist: 1.230\n",
      "    step: 300; loss: 1.815; l2dist: 1.218\n",
      "    step: 350; loss: 1.801; l2dist: 1.221\n",
      "    step: 400; loss: 1.790; l2dist: 1.212\n",
      "    step: 450; loss: 1.793; l2dist: 1.215\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.565; l2dist: 0.000\n",
      "    step: 50; loss: 5.254; l2dist: 1.804\n",
      "    step: 100; loss: 2.786; l2dist: 1.445\n",
      "    step: 150; loss: 2.085; l2dist: 1.292\n",
      "    step: 200; loss: 1.907; l2dist: 1.255\n",
      "    step: 250; loss: 1.839; l2dist: 1.237\n",
      "    step: 300; loss: 1.815; l2dist: 1.236\n",
      "    step: 350; loss: 1.795; l2dist: 1.225\n",
      "    step: 400; loss: 1.786; l2dist: 1.221\n",
      "    step: 450; loss: 1.780; l2dist: 1.224\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 107.708; l2dist: 0.000\n",
      "    step: 50; loss: 12.421; l2dist: 3.141\n",
      "    step: 100; loss: 5.014; l2dist: 2.139\n",
      "    step: 150; loss: 3.310; l2dist: 1.713\n",
      "    step: 200; loss: 2.705; l2dist: 1.531\n",
      "    step: 250; loss: 2.359; l2dist: 1.432\n",
      "    step: 300; loss: 2.212; l2dist: 1.381\n",
      "    step: 350; loss: 2.185; l2dist: 1.373\n",
      "    step: 400; loss: 2.120; l2dist: 1.334\n",
      "    step: 450; loss: 2.088; l2dist: 1.334\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 97.347; l2dist: 0.000\n",
      "    step: 50; loss: 11.237; l2dist: 2.873\n",
      "    step: 100; loss: 4.570; l2dist: 1.971\n",
      "    step: 150; loss: 3.196; l2dist: 1.621\n",
      "    step: 200; loss: 2.617; l2dist: 1.472\n",
      "    step: 250; loss: 2.376; l2dist: 1.397\n",
      "    step: 300; loss: 2.230; l2dist: 1.363\n",
      "    step: 350; loss: 2.167; l2dist: 1.346\n",
      "    step: 400; loss: 2.091; l2dist: 1.328\n",
      "    step: 450; loss: 2.076; l2dist: 1.320\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.644; l2dist: 0.000\n",
      "    step: 50; loss: 8.688; l2dist: 2.481\n",
      "    step: 100; loss: 3.566; l2dist: 1.727\n",
      "    step: 150; loss: 2.581; l2dist: 1.471\n",
      "    step: 200; loss: 2.251; l2dist: 1.372\n",
      "    step: 250; loss: 2.091; l2dist: 1.325\n",
      "    step: 300; loss: 2.023; l2dist: 1.297\n",
      "    step: 350; loss: 1.981; l2dist: 1.289\n",
      "    step: 400; loss: 1.972; l2dist: 1.286\n",
      "    step: 450; loss: 1.936; l2dist: 1.276\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.333; l2dist: 0.000\n",
      "    step: 50; loss: 7.242; l2dist: 2.215\n",
      "    step: 100; loss: 3.164; l2dist: 1.616\n",
      "    step: 150; loss: 2.357; l2dist: 1.398\n",
      "    step: 200; loss: 2.091; l2dist: 1.325\n",
      "    step: 250; loss: 2.026; l2dist: 1.303\n",
      "    step: 300; loss: 1.939; l2dist: 1.279\n",
      "    step: 350; loss: 1.913; l2dist: 1.266\n",
      "    step: 400; loss: 1.914; l2dist: 1.258\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.131; l2dist: 0.000\n",
      "    step: 50; loss: 6.667; l2dist: 2.097\n",
      "    step: 100; loss: 3.036; l2dist: 1.557\n",
      "    step: 150; loss: 2.262; l2dist: 1.358\n",
      "    step: 200; loss: 2.045; l2dist: 1.296\n",
      "    step: 250; loss: 1.953; l2dist: 1.273\n",
      "    step: 300; loss: 1.898; l2dist: 1.258\n",
      "    step: 350; loss: 1.884; l2dist: 1.253\n",
      "    step: 400; loss: 1.862; l2dist: 1.242\n",
      "    step: 450; loss: 1.855; l2dist: 1.251\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.045; l2dist: 0.000\n",
      "    step: 50; loss: 6.481; l2dist: 2.031\n",
      "    step: 100; loss: 2.936; l2dist: 1.498\n",
      "    step: 150; loss: 2.227; l2dist: 1.334\n",
      "    step: 200; loss: 2.016; l2dist: 1.280\n",
      "    step: 250; loss: 1.927; l2dist: 1.254\n",
      "    step: 300; loss: 1.867; l2dist: 1.239\n",
      "    step: 350; loss: 1.854; l2dist: 1.229\n",
      "    step: 400; loss: 1.849; l2dist: 1.227\n",
      "    step: 450; loss: 1.825; l2dist: 1.225\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.320; l2dist: 0.000\n",
      "    step: 50; loss: 6.435; l2dist: 2.014\n",
      "    step: 100; loss: 2.975; l2dist: 1.505\n",
      "    step: 150; loss: 2.248; l2dist: 1.342\n",
      "    step: 200; loss: 2.036; l2dist: 1.283\n",
      "    step: 250; loss: 1.957; l2dist: 1.261\n",
      "    step: 300; loss: 1.902; l2dist: 1.246\n",
      "    step: 350; loss: 1.888; l2dist: 1.239\n",
      "    step: 400; loss: 1.866; l2dist: 1.234\n",
      "    step: 450; loss: 1.878; l2dist: 1.231\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.406; l2dist: 0.000\n",
      "    step: 50; loss: 6.352; l2dist: 1.996\n",
      "    step: 100; loss: 2.942; l2dist: 1.480\n",
      "    step: 150; loss: 2.233; l2dist: 1.323\n",
      "    step: 200; loss: 2.021; l2dist: 1.269\n",
      "    step: 250; loss: 1.936; l2dist: 1.247\n",
      "    step: 300; loss: 1.890; l2dist: 1.236\n",
      "    step: 350; loss: 1.874; l2dist: 1.233\n",
      "    step: 400; loss: 1.855; l2dist: 1.227\n",
      "    step: 450; loss: 1.855; l2dist: 1.222\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.970; l2dist: 0.000\n",
      "    step: 50; loss: 6.332; l2dist: 1.994\n",
      "    step: 100; loss: 2.944; l2dist: 1.480\n",
      "    step: 150; loss: 2.230; l2dist: 1.325\n",
      "    step: 200; loss: 2.026; l2dist: 1.264\n",
      "    step: 250; loss: 1.936; l2dist: 1.241\n",
      "    step: 300; loss: 1.897; l2dist: 1.234\n",
      "    step: 350; loss: 1.867; l2dist: 1.229\n",
      "    step: 400; loss: 1.863; l2dist: 1.223\n",
      "    step: 450; loss: 1.885; l2dist: 1.232\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.098; l2dist: 0.000\n",
      "    step: 50; loss: 6.352; l2dist: 1.998\n",
      "    step: 100; loss: 2.953; l2dist: 1.486\n",
      "    step: 150; loss: 2.234; l2dist: 1.327\n",
      "    step: 200; loss: 2.030; l2dist: 1.277\n",
      "    step: 250; loss: 1.943; l2dist: 1.249\n",
      "    step: 300; loss: 1.893; l2dist: 1.237\n",
      "    step: 350; loss: 1.877; l2dist: 1.231\n",
      "    step: 400; loss: 1.868; l2dist: 1.229\n",
      "    step: 450; loss: 1.850; l2dist: 1.226\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 127.263; l2dist: 0.000\n",
      "    step: 50; loss: 15.075; l2dist: 3.569\n",
      "    step: 100; loss: 6.444; l2dist: 2.454\n",
      "    step: 150; loss: 4.264; l2dist: 1.973\n",
      "    step: 200; loss: 3.317; l2dist: 1.724\n",
      "    step: 250; loss: 2.940; l2dist: 1.621\n",
      "    step: 300; loss: 2.749; l2dist: 1.568\n",
      "    step: 350; loss: 2.641; l2dist: 1.521\n",
      "    step: 400; loss: 2.606; l2dist: 1.509\n",
      "    step: 450; loss: 2.558; l2dist: 1.492\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 89.852; l2dist: 0.000\n",
      "    step: 50; loss: 12.278; l2dist: 3.160\n",
      "    step: 100; loss: 4.990; l2dist: 2.109\n",
      "    step: 150; loss: 3.406; l2dist: 1.729\n",
      "    step: 200; loss: 2.901; l2dist: 1.593\n",
      "    step: 250; loss: 2.642; l2dist: 1.514\n",
      "    step: 300; loss: 2.542; l2dist: 1.484\n",
      "    step: 350; loss: 2.492; l2dist: 1.470\n",
      "    step: 400; loss: 2.486; l2dist: 1.472\n",
      "    step: 450; loss: 2.430; l2dist: 1.463\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.959; l2dist: 0.000\n",
      "    step: 50; loss: 9.876; l2dist: 2.723\n",
      "    step: 100; loss: 3.906; l2dist: 1.861\n",
      "    step: 150; loss: 2.880; l2dist: 1.593\n",
      "    step: 200; loss: 2.574; l2dist: 1.498\n",
      "    step: 250; loss: 2.463; l2dist: 1.467\n",
      "    step: 300; loss: 2.417; l2dist: 1.444\n",
      "    step: 350; loss: 2.385; l2dist: 1.441\n",
      "    step: 400; loss: 2.363; l2dist: 1.432\n",
      "    step: 450; loss: 2.356; l2dist: 1.433\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.787; l2dist: 0.000\n",
      "    step: 50; loss: 8.546; l2dist: 2.450\n",
      "    step: 100; loss: 3.624; l2dist: 1.774\n",
      "    step: 150; loss: 2.741; l2dist: 1.544\n",
      "    step: 200; loss: 2.513; l2dist: 1.478\n",
      "    step: 250; loss: 2.449; l2dist: 1.451\n",
      "    step: 300; loss: 2.393; l2dist: 1.440\n",
      "    step: 350; loss: 2.355; l2dist: 1.426\n",
      "    step: 400; loss: 2.342; l2dist: 1.422\n",
      "    step: 450; loss: 2.309; l2dist: 1.421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.656; l2dist: 0.000\n",
      "    step: 50; loss: 7.630; l2dist: 2.216\n",
      "    step: 100; loss: 3.577; l2dist: 1.715\n",
      "    step: 150; loss: 2.696; l2dist: 1.516\n",
      "    step: 200; loss: 2.476; l2dist: 1.449\n",
      "    step: 250; loss: 2.400; l2dist: 1.428\n",
      "    step: 300; loss: 2.355; l2dist: 1.415\n",
      "    step: 350; loss: 2.321; l2dist: 1.411\n",
      "    step: 400; loss: 2.294; l2dist: 1.407\n",
      "    step: 450; loss: 2.281; l2dist: 1.403\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.096; l2dist: 0.000\n",
      "    step: 50; loss: 7.393; l2dist: 2.162\n",
      "    step: 100; loss: 3.517; l2dist: 1.673\n",
      "    step: 150; loss: 2.670; l2dist: 1.482\n",
      "    step: 200; loss: 2.457; l2dist: 1.431\n",
      "    step: 250; loss: 2.381; l2dist: 1.412\n",
      "    step: 300; loss: 2.327; l2dist: 1.397\n",
      "    step: 350; loss: 2.307; l2dist: 1.395\n",
      "    step: 400; loss: 2.296; l2dist: 1.391\n",
      "    step: 450; loss: 2.275; l2dist: 1.387\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.339; l2dist: 0.000\n",
      "    step: 50; loss: 7.313; l2dist: 2.117\n",
      "    step: 100; loss: 3.529; l2dist: 1.664\n",
      "    step: 150; loss: 2.669; l2dist: 1.467\n",
      "    step: 200; loss: 2.448; l2dist: 1.424\n",
      "    step: 250; loss: 2.361; l2dist: 1.404\n",
      "    step: 300; loss: 2.317; l2dist: 1.391\n",
      "    step: 350; loss: 2.298; l2dist: 1.389\n",
      "    step: 400; loss: 2.283; l2dist: 1.385\n",
      "    step: 450; loss: 2.278; l2dist: 1.381\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.044; l2dist: 0.000\n",
      "    step: 50; loss: 7.260; l2dist: 2.101\n",
      "    step: 100; loss: 3.518; l2dist: 1.648\n",
      "    step: 150; loss: 2.665; l2dist: 1.464\n",
      "    step: 200; loss: 2.451; l2dist: 1.414\n",
      "    step: 250; loss: 2.368; l2dist: 1.395\n",
      "    step: 300; loss: 2.323; l2dist: 1.386\n",
      "    step: 350; loss: 2.309; l2dist: 1.377\n",
      "    step: 400; loss: 2.287; l2dist: 1.376\n",
      "    step: 450; loss: 2.268; l2dist: 1.373\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.915; l2dist: 0.000\n",
      "    step: 50; loss: 7.248; l2dist: 2.091\n",
      "    step: 100; loss: 3.521; l2dist: 1.643\n",
      "    step: 150; loss: 2.659; l2dist: 1.463\n",
      "    step: 200; loss: 2.435; l2dist: 1.411\n",
      "    step: 250; loss: 2.360; l2dist: 1.390\n",
      "    step: 300; loss: 2.304; l2dist: 1.383\n",
      "    step: 350; loss: 2.303; l2dist: 1.381\n",
      "    step: 400; loss: 2.284; l2dist: 1.372\n",
      "    step: 450; loss: 2.262; l2dist: 1.371\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.181; l2dist: 0.000\n",
      "    step: 50; loss: 7.298; l2dist: 2.106\n",
      "    step: 100; loss: 3.525; l2dist: 1.653\n",
      "    step: 150; loss: 2.662; l2dist: 1.473\n",
      "    step: 200; loss: 2.444; l2dist: 1.416\n",
      "    step: 250; loss: 2.367; l2dist: 1.397\n",
      "    step: 300; loss: 2.314; l2dist: 1.390\n",
      "    step: 350; loss: 2.286; l2dist: 1.384\n",
      "    step: 400; loss: 2.276; l2dist: 1.377\n",
      "    step: 450; loss: 2.266; l2dist: 1.376\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 124.539; l2dist: 0.000\n",
      "    step: 50; loss: 12.232; l2dist: 3.315\n",
      "    step: 100; loss: 5.570; l2dist: 2.282\n",
      "    step: 150; loss: 3.644; l2dist: 1.835\n",
      "    step: 200; loss: 2.912; l2dist: 1.629\n",
      "    step: 250; loss: 2.565; l2dist: 1.532\n",
      "    step: 300; loss: 2.410; l2dist: 1.479\n",
      "    step: 350; loss: 2.341; l2dist: 1.456\n",
      "    step: 400; loss: 2.279; l2dist: 1.437\n",
      "    step: 450; loss: 2.262; l2dist: 1.427\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 92.292; l2dist: 0.000\n",
      "    step: 50; loss: 9.990; l2dist: 2.968\n",
      "    step: 100; loss: 4.356; l2dist: 1.989\n",
      "    step: 150; loss: 3.042; l2dist: 1.652\n",
      "    step: 200; loss: 2.571; l2dist: 1.517\n",
      "    step: 250; loss: 2.371; l2dist: 1.450\n",
      "    step: 300; loss: 2.252; l2dist: 1.419\n",
      "    step: 350; loss: 2.205; l2dist: 1.412\n",
      "    step: 400; loss: 2.154; l2dist: 1.395\n",
      "    step: 450; loss: 2.134; l2dist: 1.387\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.510; l2dist: 0.000\n",
      "    step: 50; loss: 8.018; l2dist: 2.573\n",
      "    step: 100; loss: 3.474; l2dist: 1.774\n",
      "    step: 150; loss: 2.577; l2dist: 1.519\n",
      "    step: 200; loss: 2.300; l2dist: 1.435\n",
      "    step: 250; loss: 2.178; l2dist: 1.396\n",
      "    step: 300; loss: 2.125; l2dist: 1.379\n",
      "    step: 350; loss: 2.106; l2dist: 1.375\n",
      "    step: 400; loss: 2.105; l2dist: 1.367\n",
      "    step: 450; loss: 2.075; l2dist: 1.362\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.205; l2dist: 0.000\n",
      "    step: 50; loss: 6.829; l2dist: 2.295\n",
      "    step: 100; loss: 3.210; l2dist: 1.697\n",
      "    step: 150; loss: 2.399; l2dist: 1.472\n",
      "    step: 200; loss: 2.177; l2dist: 1.397\n",
      "    step: 250; loss: 2.105; l2dist: 1.370\n",
      "    step: 300; loss: 2.061; l2dist: 1.359\n",
      "    step: 350; loss: 2.035; l2dist: 1.347\n",
      "    step: 400; loss: 2.032; l2dist: 1.353\n",
      "    step: 450; loss: 2.030; l2dist: 1.345\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.941; l2dist: 0.000\n",
      "    step: 50; loss: 6.192; l2dist: 2.092\n",
      "    step: 100; loss: 3.156; l2dist: 1.620\n",
      "    step: 150; loss: 2.345; l2dist: 1.431\n",
      "    step: 200; loss: 2.153; l2dist: 1.381\n",
      "    step: 250; loss: 2.084; l2dist: 1.354\n",
      "    step: 300; loss: 2.045; l2dist: 1.343\n",
      "    step: 350; loss: 2.045; l2dist: 1.339\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.240; l2dist: 0.000\n",
      "    step: 50; loss: 5.865; l2dist: 1.990\n",
      "    step: 100; loss: 3.130; l2dist: 1.571\n",
      "    step: 150; loss: 2.340; l2dist: 1.396\n",
      "    step: 200; loss: 2.153; l2dist: 1.346\n",
      "    step: 250; loss: 2.089; l2dist: 1.337\n",
      "    step: 300; loss: 2.048; l2dist: 1.327\n",
      "    step: 350; loss: 2.023; l2dist: 1.320\n",
      "    step: 400; loss: 2.020; l2dist: 1.313\n",
      "    step: 450; loss: 2.010; l2dist: 1.316\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.858; l2dist: 0.000\n",
      "    step: 50; loss: 5.780; l2dist: 1.969\n",
      "    step: 100; loss: 3.122; l2dist: 1.564\n",
      "    step: 150; loss: 2.310; l2dist: 1.388\n",
      "    step: 200; loss: 2.130; l2dist: 1.346\n",
      "    step: 250; loss: 2.058; l2dist: 1.320\n",
      "    step: 300; loss: 2.026; l2dist: 1.320\n",
      "    step: 350; loss: 2.020; l2dist: 1.317\n",
      "    step: 400; loss: 2.004; l2dist: 1.312\n",
      "    step: 450; loss: 1.990; l2dist: 1.308\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.235; l2dist: 0.000\n",
      "    step: 50; loss: 5.780; l2dist: 1.971\n",
      "    step: 100; loss: 3.133; l2dist: 1.569\n",
      "    step: 150; loss: 2.314; l2dist: 1.396\n",
      "    step: 200; loss: 2.138; l2dist: 1.353\n",
      "    step: 250; loss: 2.078; l2dist: 1.333\n",
      "    step: 300; loss: 2.026; l2dist: 1.329\n",
      "    step: 350; loss: 2.016; l2dist: 1.320\n",
      "    step: 400; loss: 2.007; l2dist: 1.322\n",
      "    step: 450; loss: 2.001; l2dist: 1.317\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.898; l2dist: 0.000\n",
      "    step: 50; loss: 5.762; l2dist: 1.965\n",
      "    step: 100; loss: 3.137; l2dist: 1.565\n",
      "    step: 150; loss: 2.324; l2dist: 1.399\n",
      "    step: 200; loss: 2.146; l2dist: 1.347\n",
      "    step: 250; loss: 2.065; l2dist: 1.331\n",
      "    step: 300; loss: 2.044; l2dist: 1.328\n",
      "    step: 350; loss: 2.017; l2dist: 1.319\n",
      "    step: 400; loss: 2.003; l2dist: 1.318\n",
      "    step: 450; loss: 2.008; l2dist: 1.316\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.192; l2dist: 0.000\n",
      "    step: 50; loss: 5.803; l2dist: 1.981\n",
      "    step: 100; loss: 3.140; l2dist: 1.571\n",
      "    step: 150; loss: 2.330; l2dist: 1.399\n",
      "    step: 200; loss: 2.143; l2dist: 1.355\n",
      "    step: 250; loss: 2.076; l2dist: 1.334\n",
      "    step: 300; loss: 2.044; l2dist: 1.331\n",
      "    step: 350; loss: 2.026; l2dist: 1.332\n",
      "    step: 400; loss: 2.020; l2dist: 1.323\n",
      "    step: 450; loss: 2.002; l2dist: 1.323\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.156; l2dist: 0.000\n",
      "    step: 50; loss: 11.613; l2dist: 3.165\n",
      "    step: 100; loss: 4.936; l2dist: 2.134\n",
      "    step: 150; loss: 3.292; l2dist: 1.724\n",
      "    step: 200; loss: 2.641; l2dist: 1.533\n",
      "    step: 250; loss: 2.388; l2dist: 1.455\n",
      "    step: 300; loss: 2.288; l2dist: 1.426\n",
      "    step: 350; loss: 2.201; l2dist: 1.392\n",
      "    step: 400; loss: 2.215; l2dist: 1.398\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 82.744; l2dist: 0.000\n",
      "    step: 50; loss: 9.608; l2dist: 2.790\n",
      "    step: 100; loss: 4.047; l2dist: 1.878\n",
      "    step: 150; loss: 2.883; l2dist: 1.577\n",
      "    step: 200; loss: 2.451; l2dist: 1.456\n",
      "    step: 250; loss: 2.256; l2dist: 1.392\n",
      "    step: 300; loss: 2.202; l2dist: 1.379\n",
      "    step: 350; loss: 2.150; l2dist: 1.362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 2.124; l2dist: 1.349\n",
      "    step: 450; loss: 2.084; l2dist: 1.341\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.535; l2dist: 0.000\n",
      "    step: 50; loss: 7.565; l2dist: 2.391\n",
      "    step: 100; loss: 3.301; l2dist: 1.689\n",
      "    step: 150; loss: 2.475; l2dist: 1.454\n",
      "    step: 200; loss: 2.230; l2dist: 1.387\n",
      "    step: 250; loss: 2.112; l2dist: 1.357\n",
      "    step: 300; loss: 2.079; l2dist: 1.333\n",
      "    step: 350; loss: 2.030; l2dist: 1.324\n",
      "    step: 400; loss: 2.011; l2dist: 1.321\n",
      "    step: 450; loss: 1.995; l2dist: 1.317\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.521; l2dist: 0.000\n",
      "    step: 50; loss: 6.198; l2dist: 2.054\n",
      "    step: 100; loss: 3.072; l2dist: 1.616\n",
      "    step: 150; loss: 2.317; l2dist: 1.409\n",
      "    step: 200; loss: 2.127; l2dist: 1.353\n",
      "    step: 250; loss: 2.055; l2dist: 1.325\n",
      "    step: 300; loss: 2.017; l2dist: 1.325\n",
      "    step: 350; loss: 1.974; l2dist: 1.306\n",
      "    step: 400; loss: 1.968; l2dist: 1.297\n",
      "    step: 450; loss: 1.939; l2dist: 1.293\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.471; l2dist: 0.000\n",
      "    step: 50; loss: 5.347; l2dist: 1.801\n",
      "    step: 100; loss: 3.037; l2dist: 1.536\n",
      "    step: 150; loss: 2.260; l2dist: 1.373\n",
      "    step: 200; loss: 2.048; l2dist: 1.320\n",
      "    step: 250; loss: 1.982; l2dist: 1.300\n",
      "    step: 300; loss: 1.933; l2dist: 1.288\n",
      "    step: 350; loss: 1.916; l2dist: 1.279\n",
      "    step: 400; loss: 1.901; l2dist: 1.275\n",
      "    step: 450; loss: 1.919; l2dist: 1.276\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.014; l2dist: 0.000\n",
      "    step: 50; loss: 4.946; l2dist: 1.685\n",
      "    step: 100; loss: 3.041; l2dist: 1.465\n",
      "    step: 150; loss: 2.283; l2dist: 1.334\n",
      "    step: 200; loss: 2.085; l2dist: 1.297\n",
      "    step: 250; loss: 1.983; l2dist: 1.274\n",
      "    step: 300; loss: 1.945; l2dist: 1.267\n",
      "    step: 350; loss: 1.914; l2dist: 1.264\n",
      "    step: 400; loss: 1.915; l2dist: 1.261\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.156; l2dist: 0.000\n",
      "    step: 50; loss: 4.991; l2dist: 1.693\n",
      "    step: 100; loss: 3.061; l2dist: 1.483\n",
      "    step: 150; loss: 2.278; l2dist: 1.349\n",
      "    step: 200; loss: 2.079; l2dist: 1.309\n",
      "    step: 250; loss: 1.981; l2dist: 1.285\n",
      "    step: 300; loss: 1.940; l2dist: 1.279\n",
      "    step: 350; loss: 1.923; l2dist: 1.270\n",
      "    step: 400; loss: 1.931; l2dist: 1.269\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.639; l2dist: 0.000\n",
      "    step: 50; loss: 4.954; l2dist: 1.678\n",
      "    step: 100; loss: 3.067; l2dist: 1.482\n",
      "    step: 150; loss: 2.280; l2dist: 1.343\n",
      "    step: 200; loss: 2.077; l2dist: 1.306\n",
      "    step: 250; loss: 1.983; l2dist: 1.283\n",
      "    step: 300; loss: 1.950; l2dist: 1.273\n",
      "    step: 350; loss: 1.922; l2dist: 1.271\n",
      "    step: 400; loss: 1.906; l2dist: 1.269\n",
      "    step: 450; loss: 1.903; l2dist: 1.264\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.565; l2dist: 0.000\n",
      "    step: 50; loss: 4.931; l2dist: 1.675\n",
      "    step: 100; loss: 3.072; l2dist: 1.472\n",
      "    step: 150; loss: 2.286; l2dist: 1.343\n",
      "    step: 200; loss: 2.079; l2dist: 1.304\n",
      "    step: 250; loss: 1.982; l2dist: 1.279\n",
      "    step: 300; loss: 1.949; l2dist: 1.283\n",
      "    step: 350; loss: 1.924; l2dist: 1.268\n",
      "    step: 400; loss: 1.906; l2dist: 1.266\n",
      "    step: 450; loss: 1.912; l2dist: 1.262\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.778; l2dist: 0.000\n",
      "    step: 50; loss: 4.985; l2dist: 1.695\n",
      "    step: 100; loss: 3.080; l2dist: 1.489\n",
      "    step: 150; loss: 2.285; l2dist: 1.353\n",
      "    step: 200; loss: 2.082; l2dist: 1.310\n",
      "    step: 250; loss: 1.984; l2dist: 1.285\n",
      "    step: 300; loss: 1.950; l2dist: 1.280\n",
      "    step: 350; loss: 1.925; l2dist: 1.275\n",
      "    step: 400; loss: 1.916; l2dist: 1.266\n",
      "    step: 450; loss: 1.898; l2dist: 1.270\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 113.986; l2dist: 0.000\n",
      "    step: 50; loss: 11.223; l2dist: 3.190\n",
      "    step: 100; loss: 5.034; l2dist: 2.164\n",
      "    step: 150; loss: 3.342; l2dist: 1.748\n",
      "    step: 200; loss: 2.694; l2dist: 1.547\n",
      "    step: 250; loss: 2.475; l2dist: 1.482\n",
      "    step: 300; loss: 2.359; l2dist: 1.434\n",
      "    step: 350; loss: 2.256; l2dist: 1.402\n",
      "    step: 400; loss: 2.235; l2dist: 1.410\n",
      "    step: 450; loss: 2.210; l2dist: 1.395\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 104.853; l2dist: 0.000\n",
      "    step: 50; loss: 9.879; l2dist: 2.925\n",
      "    step: 100; loss: 4.347; l2dist: 1.964\n",
      "    step: 150; loss: 3.015; l2dist: 1.616\n",
      "    step: 200; loss: 2.536; l2dist: 1.478\n",
      "    step: 250; loss: 2.348; l2dist: 1.428\n",
      "    step: 300; loss: 2.267; l2dist: 1.403\n",
      "    step: 350; loss: 2.204; l2dist: 1.386\n",
      "    step: 400; loss: 2.130; l2dist: 1.364\n",
      "    step: 450; loss: 2.131; l2dist: 1.368\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.360; l2dist: 0.000\n",
      "    step: 50; loss: 7.461; l2dist: 2.443\n",
      "    step: 100; loss: 3.326; l2dist: 1.714\n",
      "    step: 150; loss: 2.498; l2dist: 1.476\n",
      "    step: 200; loss: 2.257; l2dist: 1.402\n",
      "    step: 250; loss: 2.161; l2dist: 1.369\n",
      "    step: 300; loss: 2.092; l2dist: 1.347\n",
      "    step: 350; loss: 2.069; l2dist: 1.343\n",
      "    step: 400; loss: 2.043; l2dist: 1.338\n",
      "    step: 450; loss: 2.036; l2dist: 1.331\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.217; l2dist: 0.000\n",
      "    step: 50; loss: 6.211; l2dist: 2.160\n",
      "    step: 100; loss: 3.057; l2dist: 1.618\n",
      "    step: 150; loss: 2.338; l2dist: 1.423\n",
      "    step: 200; loss: 2.152; l2dist: 1.366\n",
      "    step: 250; loss: 2.066; l2dist: 1.344\n",
      "    step: 300; loss: 2.028; l2dist: 1.324\n",
      "    step: 350; loss: 2.007; l2dist: 1.325\n",
      "    step: 400; loss: 1.993; l2dist: 1.320\n",
      "    step: 450; loss: 1.977; l2dist: 1.313\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.841; l2dist: 0.000\n",
      "    step: 50; loss: 5.631; l2dist: 1.981\n",
      "    step: 100; loss: 2.987; l2dist: 1.541\n",
      "    step: 150; loss: 2.293; l2dist: 1.383\n",
      "    step: 200; loss: 2.114; l2dist: 1.331\n",
      "    step: 250; loss: 2.038; l2dist: 1.313\n",
      "    step: 300; loss: 1.999; l2dist: 1.304\n",
      "    step: 350; loss: 1.996; l2dist: 1.301\n",
      "    step: 400; loss: 1.969; l2dist: 1.297\n",
      "    step: 450; loss: 1.961; l2dist: 1.288\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.385; l2dist: 0.000\n",
      "    step: 50; loss: 5.343; l2dist: 1.869\n",
      "    step: 100; loss: 2.976; l2dist: 1.505\n",
      "    step: 150; loss: 2.286; l2dist: 1.357\n",
      "    step: 200; loss: 2.117; l2dist: 1.319\n",
      "    step: 250; loss: 2.041; l2dist: 1.301\n",
      "    step: 300; loss: 2.006; l2dist: 1.295\n",
      "    step: 350; loss: 1.994; l2dist: 1.284\n",
      "    step: 400; loss: 1.972; l2dist: 1.284\n",
      "    step: 450; loss: 1.969; l2dist: 1.279\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.049; l2dist: 0.000\n",
      "    step: 50; loss: 5.180; l2dist: 1.806\n",
      "    step: 100; loss: 2.964; l2dist: 1.479\n",
      "    step: 150; loss: 2.265; l2dist: 1.338\n",
      "    step: 200; loss: 2.095; l2dist: 1.304\n",
      "    step: 250; loss: 2.029; l2dist: 1.284\n",
      "    step: 300; loss: 1.998; l2dist: 1.280\n",
      "    step: 350; loss: 1.972; l2dist: 1.276\n",
      "    step: 400; loss: 1.963; l2dist: 1.274\n",
      "    step: 450; loss: 1.954; l2dist: 1.274\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.499; l2dist: 0.000\n",
      "    step: 50; loss: 5.140; l2dist: 1.796\n",
      "    step: 100; loss: 2.976; l2dist: 1.471\n",
      "    step: 150; loss: 2.279; l2dist: 1.339\n",
      "    step: 200; loss: 2.114; l2dist: 1.297\n",
      "    step: 250; loss: 2.044; l2dist: 1.290\n",
      "    step: 300; loss: 2.009; l2dist: 1.281\n",
      "    step: 350; loss: 1.997; l2dist: 1.281\n",
      "    step: 400; loss: 1.977; l2dist: 1.278\n",
      "    step: 450; loss: 1.964; l2dist: 1.278\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.095; l2dist: 0.000\n",
      "    step: 50; loss: 5.128; l2dist: 1.790\n",
      "    step: 100; loss: 2.962; l2dist: 1.471\n",
      "    step: 150; loss: 2.274; l2dist: 1.333\n",
      "    step: 200; loss: 2.100; l2dist: 1.301\n",
      "    step: 250; loss: 2.029; l2dist: 1.283\n",
      "    step: 300; loss: 2.000; l2dist: 1.284\n",
      "    step: 350; loss: 1.970; l2dist: 1.270\n",
      "    step: 400; loss: 1.970; l2dist: 1.273\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.291; l2dist: 0.000\n",
      "    step: 50; loss: 5.172; l2dist: 1.806\n",
      "    step: 100; loss: 2.966; l2dist: 1.485\n",
      "    step: 150; loss: 2.267; l2dist: 1.342\n",
      "    step: 200; loss: 2.104; l2dist: 1.306\n",
      "    step: 250; loss: 2.028; l2dist: 1.290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 1.993; l2dist: 1.284\n",
      "    step: 350; loss: 1.975; l2dist: 1.285\n",
      "    step: 400; loss: 1.964; l2dist: 1.281\n",
      "    step: 450; loss: 1.962; l2dist: 1.276\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.943; l2dist: 0.000\n",
      "    step: 50; loss: 13.248; l2dist: 3.346\n",
      "    step: 100; loss: 5.697; l2dist: 2.289\n",
      "    step: 150; loss: 3.773; l2dist: 1.846\n",
      "    step: 200; loss: 2.979; l2dist: 1.631\n",
      "    step: 250; loss: 2.624; l2dist: 1.527\n",
      "    step: 300; loss: 2.472; l2dist: 1.479\n",
      "    step: 350; loss: 2.406; l2dist: 1.447\n",
      "    step: 400; loss: 2.346; l2dist: 1.443\n",
      "    step: 450; loss: 2.335; l2dist: 1.419\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 72.482; l2dist: 0.000\n",
      "    step: 50; loss: 10.147; l2dist: 2.863\n",
      "    step: 100; loss: 4.136; l2dist: 1.919\n",
      "    step: 150; loss: 2.910; l2dist: 1.604\n",
      "    step: 200; loss: 2.500; l2dist: 1.478\n",
      "    step: 250; loss: 2.335; l2dist: 1.425\n",
      "    step: 300; loss: 2.246; l2dist: 1.406\n",
      "    step: 350; loss: 2.232; l2dist: 1.396\n",
      "    step: 400; loss: 2.185; l2dist: 1.379\n",
      "    step: 450; loss: 2.159; l2dist: 1.372\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.560; l2dist: 0.000\n",
      "    step: 50; loss: 7.910; l2dist: 2.455\n",
      "    step: 100; loss: 3.398; l2dist: 1.735\n",
      "    step: 150; loss: 2.525; l2dist: 1.491\n",
      "    step: 200; loss: 2.280; l2dist: 1.413\n",
      "    step: 250; loss: 2.199; l2dist: 1.382\n",
      "    step: 300; loss: 2.141; l2dist: 1.374\n",
      "    step: 350; loss: 2.128; l2dist: 1.358\n",
      "    step: 400; loss: 2.104; l2dist: 1.360\n",
      "    step: 450; loss: 2.120; l2dist: 1.349\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.158; l2dist: 0.000\n",
      "    step: 50; loss: 6.903; l2dist: 2.221\n",
      "    step: 100; loss: 3.197; l2dist: 1.668\n",
      "    step: 150; loss: 2.410; l2dist: 1.450\n",
      "    step: 200; loss: 2.216; l2dist: 1.390\n",
      "    step: 250; loss: 2.129; l2dist: 1.357\n",
      "    step: 300; loss: 2.093; l2dist: 1.346\n",
      "    step: 350; loss: 2.060; l2dist: 1.339\n",
      "    step: 400; loss: 2.058; l2dist: 1.343\n",
      "    step: 450; loss: 2.049; l2dist: 1.336\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.931; l2dist: 0.000\n",
      "    step: 50; loss: 6.349; l2dist: 2.063\n",
      "    step: 100; loss: 3.142; l2dist: 1.605\n",
      "    step: 150; loss: 2.377; l2dist: 1.419\n",
      "    step: 200; loss: 2.173; l2dist: 1.358\n",
      "    step: 250; loss: 2.098; l2dist: 1.344\n",
      "    step: 300; loss: 2.061; l2dist: 1.327\n",
      "    step: 350; loss: 2.034; l2dist: 1.325\n",
      "    step: 400; loss: 2.032; l2dist: 1.316\n",
      "    step: 450; loss: 2.019; l2dist: 1.314\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.035; l2dist: 0.000\n",
      "    step: 50; loss: 6.092; l2dist: 1.980\n",
      "    step: 100; loss: 3.108; l2dist: 1.580\n",
      "    step: 150; loss: 2.343; l2dist: 1.393\n",
      "    step: 200; loss: 2.152; l2dist: 1.343\n",
      "    step: 250; loss: 2.084; l2dist: 1.327\n",
      "    step: 300; loss: 2.055; l2dist: 1.310\n",
      "    step: 350; loss: 2.024; l2dist: 1.308\n",
      "    step: 400; loss: 2.028; l2dist: 1.305\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.423; l2dist: 0.000\n",
      "    step: 50; loss: 6.057; l2dist: 1.968\n",
      "    step: 100; loss: 3.095; l2dist: 1.570\n",
      "    step: 150; loss: 2.341; l2dist: 1.388\n",
      "    step: 200; loss: 2.151; l2dist: 1.340\n",
      "    step: 250; loss: 2.086; l2dist: 1.321\n",
      "    step: 300; loss: 2.053; l2dist: 1.306\n",
      "    step: 350; loss: 2.042; l2dist: 1.309\n",
      "    step: 400; loss: 2.033; l2dist: 1.305\n",
      "    step: 450; loss: 2.021; l2dist: 1.298\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.078; l2dist: 0.000\n",
      "    step: 50; loss: 6.046; l2dist: 1.963\n",
      "    step: 100; loss: 3.097; l2dist: 1.566\n",
      "    step: 150; loss: 2.336; l2dist: 1.385\n",
      "    step: 200; loss: 2.155; l2dist: 1.338\n",
      "    step: 250; loss: 2.076; l2dist: 1.319\n",
      "    step: 300; loss: 2.063; l2dist: 1.311\n",
      "    step: 350; loss: 2.028; l2dist: 1.304\n",
      "    step: 400; loss: 2.015; l2dist: 1.304\n",
      "    step: 450; loss: 2.017; l2dist: 1.305\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.845; l2dist: 0.000\n",
      "    step: 50; loss: 6.012; l2dist: 1.957\n",
      "    step: 100; loss: 3.102; l2dist: 1.573\n",
      "    step: 150; loss: 2.332; l2dist: 1.383\n",
      "    step: 200; loss: 2.157; l2dist: 1.334\n",
      "    step: 250; loss: 2.078; l2dist: 1.321\n",
      "    step: 300; loss: 2.048; l2dist: 1.313\n",
      "    step: 350; loss: 2.043; l2dist: 1.301\n",
      "    step: 400; loss: 2.022; l2dist: 1.305\n",
      "    step: 450; loss: 2.014; l2dist: 1.300\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.970; l2dist: 0.000\n",
      "    step: 50; loss: 6.038; l2dist: 1.965\n",
      "    step: 100; loss: 3.103; l2dist: 1.575\n",
      "    step: 150; loss: 2.340; l2dist: 1.391\n",
      "    step: 200; loss: 2.153; l2dist: 1.342\n",
      "    step: 250; loss: 2.083; l2dist: 1.329\n",
      "    step: 300; loss: 2.060; l2dist: 1.314\n",
      "    step: 350; loss: 2.030; l2dist: 1.312\n",
      "    step: 400; loss: 2.028; l2dist: 1.314\n",
      "    step: 450; loss: 2.025; l2dist: 1.302\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 116.750; l2dist: 0.000\n",
      "    step: 50; loss: 12.413; l2dist: 3.270\n",
      "    step: 100; loss: 5.516; l2dist: 2.240\n",
      "    step: 150; loss: 3.679; l2dist: 1.811\n",
      "    step: 200; loss: 2.951; l2dist: 1.617\n",
      "    step: 250; loss: 2.683; l2dist: 1.529\n",
      "    step: 300; loss: 2.515; l2dist: 1.480\n",
      "    step: 350; loss: 2.435; l2dist: 1.457\n",
      "    step: 400; loss: 2.337; l2dist: 1.420\n",
      "    step: 450; loss: 2.325; l2dist: 1.414\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 94.212; l2dist: 0.000\n",
      "    step: 50; loss: 10.423; l2dist: 2.937\n",
      "    step: 100; loss: 4.564; l2dist: 1.992\n",
      "    step: 150; loss: 3.137; l2dist: 1.643\n",
      "    step: 200; loss: 2.617; l2dist: 1.494\n",
      "    step: 250; loss: 2.406; l2dist: 1.442\n",
      "    step: 300; loss: 2.274; l2dist: 1.399\n",
      "    step: 350; loss: 2.207; l2dist: 1.377\n",
      "    step: 400; loss: 2.164; l2dist: 1.376\n",
      "    step: 450; loss: 2.164; l2dist: 1.372\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.099; l2dist: 0.000\n",
      "    step: 50; loss: 8.424; l2dist: 2.568\n",
      "    step: 100; loss: 3.629; l2dist: 1.779\n",
      "    step: 150; loss: 2.645; l2dist: 1.507\n",
      "    step: 200; loss: 2.332; l2dist: 1.424\n",
      "    step: 250; loss: 2.209; l2dist: 1.379\n",
      "    step: 300; loss: 2.113; l2dist: 1.359\n",
      "    step: 350; loss: 2.092; l2dist: 1.352\n",
      "    step: 400; loss: 2.062; l2dist: 1.344\n",
      "    step: 450; loss: 2.064; l2dist: 1.334\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.302; l2dist: 0.000\n",
      "    step: 50; loss: 7.329; l2dist: 2.314\n",
      "    step: 100; loss: 3.287; l2dist: 1.691\n",
      "    step: 150; loss: 2.452; l2dist: 1.459\n",
      "    step: 200; loss: 2.195; l2dist: 1.386\n",
      "    step: 250; loss: 2.108; l2dist: 1.350\n",
      "    step: 300; loss: 2.069; l2dist: 1.335\n",
      "    step: 350; loss: 2.035; l2dist: 1.335\n",
      "    step: 400; loss: 2.013; l2dist: 1.323\n",
      "    step: 450; loss: 2.011; l2dist: 1.322\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.324; l2dist: 0.000\n",
      "    step: 50; loss: 6.500; l2dist: 2.107\n",
      "    step: 100; loss: 3.167; l2dist: 1.611\n",
      "    step: 150; loss: 2.365; l2dist: 1.413\n",
      "    step: 200; loss: 2.143; l2dist: 1.356\n",
      "    step: 250; loss: 2.050; l2dist: 1.326\n",
      "    step: 300; loss: 2.018; l2dist: 1.313\n",
      "    step: 350; loss: 1.992; l2dist: 1.313\n",
      "    step: 400; loss: 1.984; l2dist: 1.306\n",
      "    step: 450; loss: 1.974; l2dist: 1.300\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.188; l2dist: 0.000\n",
      "    step: 50; loss: 6.174; l2dist: 1.995\n",
      "    step: 100; loss: 3.055; l2dist: 1.531\n",
      "    step: 150; loss: 2.307; l2dist: 1.365\n",
      "    step: 200; loss: 2.112; l2dist: 1.309\n",
      "    step: 250; loss: 2.044; l2dist: 1.304\n",
      "    step: 300; loss: 2.002; l2dist: 1.289\n",
      "    step: 350; loss: 1.973; l2dist: 1.289\n",
      "    step: 400; loss: 1.961; l2dist: 1.282\n",
      "    step: 450; loss: 1.952; l2dist: 1.277\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.765; l2dist: 0.000\n",
      "    step: 50; loss: 6.090; l2dist: 1.966\n",
      "    step: 100; loss: 3.029; l2dist: 1.520\n",
      "    step: 150; loss: 2.279; l2dist: 1.346\n",
      "    step: 200; loss: 2.092; l2dist: 1.298\n",
      "    step: 250; loss: 2.020; l2dist: 1.282\n",
      "    step: 300; loss: 1.980; l2dist: 1.272\n",
      "    step: 350; loss: 1.959; l2dist: 1.268\n",
      "    step: 400; loss: 1.958; l2dist: 1.261\n",
      "    step: 450; loss: 1.964; l2dist: 1.272\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.863; l2dist: 0.000\n",
      "    step: 50; loss: 6.019; l2dist: 1.943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 3.015; l2dist: 1.509\n",
      "    step: 150; loss: 2.271; l2dist: 1.336\n",
      "    step: 200; loss: 2.081; l2dist: 1.289\n",
      "    step: 250; loss: 2.014; l2dist: 1.274\n",
      "    step: 300; loss: 1.985; l2dist: 1.266\n",
      "    step: 350; loss: 1.960; l2dist: 1.264\n",
      "    step: 400; loss: 1.949; l2dist: 1.257\n",
      "    step: 450; loss: 1.948; l2dist: 1.258\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.571; l2dist: 0.000\n",
      "    step: 50; loss: 6.010; l2dist: 1.938\n",
      "    step: 100; loss: 3.016; l2dist: 1.511\n",
      "    step: 150; loss: 2.271; l2dist: 1.335\n",
      "    step: 200; loss: 2.081; l2dist: 1.291\n",
      "    step: 250; loss: 2.008; l2dist: 1.274\n",
      "    step: 300; loss: 1.972; l2dist: 1.270\n",
      "    step: 350; loss: 1.957; l2dist: 1.260\n",
      "    step: 400; loss: 1.953; l2dist: 1.260\n",
      "    step: 450; loss: 1.946; l2dist: 1.262\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.735; l2dist: 0.000\n",
      "    step: 50; loss: 6.050; l2dist: 1.952\n",
      "    step: 100; loss: 3.040; l2dist: 1.520\n",
      "    step: 150; loss: 2.286; l2dist: 1.344\n",
      "    step: 200; loss: 2.096; l2dist: 1.303\n",
      "    step: 250; loss: 2.027; l2dist: 1.286\n",
      "    step: 300; loss: 1.993; l2dist: 1.273\n",
      "    step: 350; loss: 1.981; l2dist: 1.269\n",
      "    step: 400; loss: 1.979; l2dist: 1.266\n",
      "    step: 450; loss: 1.955; l2dist: 1.269\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.575; l2dist: 0.000\n",
      "    step: 50; loss: 13.838; l2dist: 3.467\n",
      "    step: 100; loss: 5.820; l2dist: 2.322\n",
      "    step: 150; loss: 3.891; l2dist: 1.876\n",
      "    step: 200; loss: 3.138; l2dist: 1.677\n",
      "    step: 250; loss: 2.764; l2dist: 1.565\n",
      "    step: 300; loss: 2.674; l2dist: 1.533\n",
      "    step: 350; loss: 2.550; l2dist: 1.498\n",
      "    step: 400; loss: 2.502; l2dist: 1.485\n",
      "    step: 450; loss: 2.500; l2dist: 1.465\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 127.143; l2dist: 0.000\n",
      "    step: 50; loss: 13.463; l2dist: 3.316\n",
      "    step: 100; loss: 5.731; l2dist: 2.248\n",
      "    step: 150; loss: 3.966; l2dist: 1.851\n",
      "    step: 200; loss: 3.251; l2dist: 1.671\n",
      "    step: 250; loss: 2.897; l2dist: 1.576\n",
      "    step: 300; loss: 2.686; l2dist: 1.524\n",
      "    step: 350; loss: 2.615; l2dist: 1.493\n",
      "    step: 400; loss: 2.518; l2dist: 1.473\n",
      "    step: 450; loss: 2.488; l2dist: 1.469\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.669; l2dist: 0.000\n",
      "    step: 50; loss: 10.282; l2dist: 2.826\n",
      "    step: 100; loss: 4.331; l2dist: 1.931\n",
      "    step: 150; loss: 3.141; l2dist: 1.640\n",
      "    step: 200; loss: 2.707; l2dist: 1.528\n",
      "    step: 250; loss: 2.535; l2dist: 1.477\n",
      "    step: 300; loss: 2.402; l2dist: 1.446\n",
      "    step: 350; loss: 2.354; l2dist: 1.431\n",
      "    step: 400; loss: 2.313; l2dist: 1.415\n",
      "    step: 450; loss: 2.319; l2dist: 1.418\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.331; l2dist: 0.000\n",
      "    step: 50; loss: 8.582; l2dist: 2.546\n",
      "    step: 100; loss: 3.785; l2dist: 1.810\n",
      "    step: 150; loss: 2.797; l2dist: 1.557\n",
      "    step: 200; loss: 2.544; l2dist: 1.481\n",
      "    step: 250; loss: 2.399; l2dist: 1.437\n",
      "    step: 300; loss: 2.333; l2dist: 1.419\n",
      "    step: 350; loss: 2.300; l2dist: 1.410\n",
      "    step: 400; loss: 2.275; l2dist: 1.404\n",
      "    step: 450; loss: 2.254; l2dist: 1.398\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.311; l2dist: 0.000\n",
      "    step: 50; loss: 7.677; l2dist: 2.318\n",
      "    step: 100; loss: 3.685; l2dist: 1.707\n",
      "    step: 150; loss: 2.736; l2dist: 1.508\n",
      "    step: 200; loss: 2.502; l2dist: 1.445\n",
      "    step: 250; loss: 2.408; l2dist: 1.425\n",
      "    step: 300; loss: 2.353; l2dist: 1.410\n",
      "    step: 350; loss: 2.337; l2dist: 1.400\n",
      "    step: 400; loss: 2.292; l2dist: 1.392\n",
      "    step: 450; loss: 2.273; l2dist: 1.384\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.813; l2dist: 0.000\n",
      "    step: 50; loss: 7.289; l2dist: 2.206\n",
      "    step: 100; loss: 3.558; l2dist: 1.656\n",
      "    step: 150; loss: 2.719; l2dist: 1.469\n",
      "    step: 200; loss: 2.496; l2dist: 1.422\n",
      "    step: 250; loss: 2.402; l2dist: 1.398\n",
      "    step: 300; loss: 2.346; l2dist: 1.385\n",
      "    step: 350; loss: 2.332; l2dist: 1.376\n",
      "    step: 400; loss: 2.301; l2dist: 1.378\n",
      "    step: 450; loss: 2.292; l2dist: 1.368\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.684; l2dist: 0.000\n",
      "    step: 50; loss: 7.166; l2dist: 2.155\n",
      "    step: 100; loss: 3.515; l2dist: 1.642\n",
      "    step: 150; loss: 2.664; l2dist: 1.457\n",
      "    step: 200; loss: 2.452; l2dist: 1.409\n",
      "    step: 250; loss: 2.368; l2dist: 1.390\n",
      "    step: 300; loss: 2.326; l2dist: 1.378\n",
      "    step: 350; loss: 2.297; l2dist: 1.365\n",
      "    step: 400; loss: 2.287; l2dist: 1.365\n",
      "    step: 450; loss: 2.265; l2dist: 1.362\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.110; l2dist: 0.000\n",
      "    step: 50; loss: 7.133; l2dist: 2.145\n",
      "    step: 100; loss: 3.550; l2dist: 1.653\n",
      "    step: 150; loss: 2.686; l2dist: 1.462\n",
      "    step: 200; loss: 2.469; l2dist: 1.417\n",
      "    step: 250; loss: 2.381; l2dist: 1.396\n",
      "    step: 300; loss: 2.344; l2dist: 1.384\n",
      "    step: 350; loss: 2.324; l2dist: 1.377\n",
      "    step: 400; loss: 2.298; l2dist: 1.375\n",
      "    step: 450; loss: 2.299; l2dist: 1.375\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.585; l2dist: 0.000\n",
      "    step: 50; loss: 7.095; l2dist: 2.133\n",
      "    step: 100; loss: 3.532; l2dist: 1.651\n",
      "    step: 150; loss: 2.669; l2dist: 1.463\n",
      "    step: 200; loss: 2.460; l2dist: 1.420\n",
      "    step: 250; loss: 2.377; l2dist: 1.397\n",
      "    step: 300; loss: 2.346; l2dist: 1.384\n",
      "    step: 350; loss: 2.320; l2dist: 1.385\n",
      "    step: 400; loss: 2.314; l2dist: 1.381\n",
      "    step: 450; loss: 2.289; l2dist: 1.378\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.716; l2dist: 0.000\n",
      "    step: 50; loss: 7.118; l2dist: 2.141\n",
      "    step: 100; loss: 3.535; l2dist: 1.655\n",
      "    step: 150; loss: 2.670; l2dist: 1.471\n",
      "    step: 200; loss: 2.469; l2dist: 1.423\n",
      "    step: 250; loss: 2.378; l2dist: 1.399\n",
      "    step: 300; loss: 2.336; l2dist: 1.390\n",
      "    step: 350; loss: 2.319; l2dist: 1.382\n",
      "    step: 400; loss: 2.311; l2dist: 1.384\n",
      "    step: 450; loss: 2.312; l2dist: 1.383\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 105.994; l2dist: 0.000\n",
      "    step: 50; loss: 10.703; l2dist: 3.011\n",
      "    step: 100; loss: 4.562; l2dist: 2.052\n",
      "    step: 150; loss: 2.994; l2dist: 1.640\n",
      "    step: 200; loss: 2.463; l2dist: 1.480\n",
      "    step: 250; loss: 2.230; l2dist: 1.394\n",
      "    step: 300; loss: 2.121; l2dist: 1.361\n",
      "    step: 350; loss: 2.056; l2dist: 1.334\n",
      "    step: 400; loss: 2.038; l2dist: 1.323\n",
      "    step: 450; loss: 2.039; l2dist: 1.331\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 86.769; l2dist: 0.000\n",
      "    step: 50; loss: 9.280; l2dist: 2.743\n",
      "    step: 100; loss: 3.946; l2dist: 1.857\n",
      "    step: 150; loss: 2.760; l2dist: 1.537\n",
      "    step: 200; loss: 2.309; l2dist: 1.400\n",
      "    step: 250; loss: 2.152; l2dist: 1.343\n",
      "    step: 300; loss: 2.054; l2dist: 1.322\n",
      "    step: 350; loss: 1.989; l2dist: 1.298\n",
      "    step: 400; loss: 1.957; l2dist: 1.290\n",
      "    step: 450; loss: 1.928; l2dist: 1.283\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.438; l2dist: 0.000\n",
      "    step: 50; loss: 7.333; l2dist: 2.390\n",
      "    step: 100; loss: 3.228; l2dist: 1.667\n",
      "    step: 150; loss: 2.340; l2dist: 1.416\n",
      "    step: 200; loss: 2.096; l2dist: 1.334\n",
      "    step: 250; loss: 1.987; l2dist: 1.304\n",
      "    step: 300; loss: 1.933; l2dist: 1.279\n",
      "    step: 350; loss: 1.919; l2dist: 1.274\n",
      "    step: 400; loss: 1.906; l2dist: 1.274\n",
      "    step: 450; loss: 1.906; l2dist: 1.269\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.392; l2dist: 0.000\n",
      "    step: 50; loss: 6.325; l2dist: 2.180\n",
      "    step: 100; loss: 2.928; l2dist: 1.582\n",
      "    step: 150; loss: 2.175; l2dist: 1.366\n",
      "    step: 200; loss: 2.012; l2dist: 1.312\n",
      "    step: 250; loss: 1.908; l2dist: 1.274\n",
      "    step: 300; loss: 1.891; l2dist: 1.256\n",
      "    step: 350; loss: 1.865; l2dist: 1.262\n",
      "    step: 400; loss: 1.863; l2dist: 1.264\n",
      "    step: 450; loss: 1.838; l2dist: 1.252\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.051; l2dist: 0.000\n",
      "    step: 50; loss: 5.719; l2dist: 2.018\n",
      "    step: 100; loss: 2.842; l2dist: 1.523\n",
      "    step: 150; loss: 2.142; l2dist: 1.337\n",
      "    step: 200; loss: 1.956; l2dist: 1.286\n",
      "    step: 250; loss: 1.889; l2dist: 1.259\n",
      "    step: 300; loss: 1.858; l2dist: 1.249\n",
      "    step: 350; loss: 1.839; l2dist: 1.239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 1.828; l2dist: 1.232\n",
      "    step: 450; loss: 1.823; l2dist: 1.236\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.722; l2dist: 0.000\n",
      "    step: 50; loss: 5.336; l2dist: 1.895\n",
      "    step: 100; loss: 2.792; l2dist: 1.460\n",
      "    step: 150; loss: 2.097; l2dist: 1.295\n",
      "    step: 200; loss: 1.948; l2dist: 1.251\n",
      "    step: 250; loss: 1.873; l2dist: 1.230\n",
      "    step: 300; loss: 1.835; l2dist: 1.228\n",
      "    step: 350; loss: 1.828; l2dist: 1.218\n",
      "    step: 400; loss: 1.808; l2dist: 1.219\n",
      "    step: 450; loss: 1.803; l2dist: 1.214\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.958; l2dist: 0.000\n",
      "    step: 50; loss: 5.302; l2dist: 1.880\n",
      "    step: 100; loss: 2.764; l2dist: 1.444\n",
      "    step: 150; loss: 2.078; l2dist: 1.274\n",
      "    step: 200; loss: 1.922; l2dist: 1.232\n",
      "    step: 250; loss: 1.861; l2dist: 1.212\n",
      "    step: 300; loss: 1.824; l2dist: 1.212\n",
      "    step: 350; loss: 1.816; l2dist: 1.207\n",
      "    step: 400; loss: 1.803; l2dist: 1.203\n",
      "    step: 450; loss: 1.800; l2dist: 1.203\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.187; l2dist: 0.000\n",
      "    step: 50; loss: 5.281; l2dist: 1.869\n",
      "    step: 100; loss: 2.773; l2dist: 1.445\n",
      "    step: 150; loss: 2.079; l2dist: 1.277\n",
      "    step: 200; loss: 1.922; l2dist: 1.235\n",
      "    step: 250; loss: 1.874; l2dist: 1.221\n",
      "    step: 300; loss: 1.825; l2dist: 1.212\n",
      "    step: 350; loss: 1.817; l2dist: 1.206\n",
      "    step: 400; loss: 1.804; l2dist: 1.206\n",
      "    step: 450; loss: 1.803; l2dist: 1.205\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.165; l2dist: 0.000\n",
      "    step: 50; loss: 5.274; l2dist: 1.867\n",
      "    step: 100; loss: 2.779; l2dist: 1.449\n",
      "    step: 150; loss: 2.092; l2dist: 1.279\n",
      "    step: 200; loss: 1.916; l2dist: 1.234\n",
      "    step: 250; loss: 1.860; l2dist: 1.215\n",
      "    step: 300; loss: 1.827; l2dist: 1.212\n",
      "    step: 350; loss: 1.819; l2dist: 1.211\n",
      "    step: 400; loss: 1.799; l2dist: 1.206\n",
      "    step: 450; loss: 1.796; l2dist: 1.201\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.396; l2dist: 0.000\n",
      "    step: 50; loss: 5.305; l2dist: 1.875\n",
      "    step: 100; loss: 2.775; l2dist: 1.447\n",
      "    step: 150; loss: 2.078; l2dist: 1.277\n",
      "    step: 200; loss: 1.920; l2dist: 1.236\n",
      "    step: 250; loss: 1.868; l2dist: 1.224\n",
      "    step: 300; loss: 1.836; l2dist: 1.208\n",
      "    step: 350; loss: 1.814; l2dist: 1.211\n",
      "    step: 400; loss: 1.800; l2dist: 1.208\n",
      "    step: 450; loss: 1.799; l2dist: 1.202\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.698; l2dist: 0.000\n",
      "    step: 50; loss: 14.194; l2dist: 3.409\n",
      "    step: 100; loss: 5.856; l2dist: 2.332\n",
      "    step: 150; loss: 3.966; l2dist: 1.897\n",
      "    step: 200; loss: 3.159; l2dist: 1.696\n",
      "    step: 250; loss: 2.806; l2dist: 1.594\n",
      "    step: 300; loss: 2.677; l2dist: 1.542\n",
      "    step: 350; loss: 2.595; l2dist: 1.507\n",
      "    step: 400; loss: 2.535; l2dist: 1.501\n",
      "    step: 450; loss: 2.522; l2dist: 1.510\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 97.818; l2dist: 0.000\n",
      "    step: 50; loss: 11.811; l2dist: 3.041\n",
      "    step: 100; loss: 4.964; l2dist: 2.090\n",
      "    step: 150; loss: 3.401; l2dist: 1.730\n",
      "    step: 200; loss: 2.856; l2dist: 1.582\n",
      "    step: 250; loss: 2.651; l2dist: 1.524\n",
      "    step: 300; loss: 2.567; l2dist: 1.507\n",
      "    step: 350; loss: 2.473; l2dist: 1.471\n",
      "    step: 400; loss: 2.447; l2dist: 1.466\n",
      "    step: 450; loss: 2.428; l2dist: 1.459\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.397; l2dist: 0.000\n",
      "    step: 50; loss: 9.109; l2dist: 2.612\n",
      "    step: 100; loss: 3.833; l2dist: 1.842\n",
      "    step: 150; loss: 2.872; l2dist: 1.588\n",
      "    step: 200; loss: 2.583; l2dist: 1.508\n",
      "    step: 250; loss: 2.434; l2dist: 1.463\n",
      "    step: 300; loss: 2.379; l2dist: 1.444\n",
      "    step: 350; loss: 2.333; l2dist: 1.435\n",
      "    step: 400; loss: 2.310; l2dist: 1.426\n",
      "    step: 450; loss: 2.287; l2dist: 1.429\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.935; l2dist: 0.000\n",
      "    step: 50; loss: 7.856; l2dist: 2.348\n",
      "    step: 100; loss: 3.542; l2dist: 1.765\n",
      "    step: 150; loss: 2.701; l2dist: 1.533\n",
      "    step: 200; loss: 2.475; l2dist: 1.469\n",
      "    step: 250; loss: 2.365; l2dist: 1.438\n",
      "    step: 300; loss: 2.317; l2dist: 1.432\n",
      "    step: 350; loss: 2.288; l2dist: 1.419\n",
      "    step: 400; loss: 2.287; l2dist: 1.425\n",
      "    step: 450; loss: 2.263; l2dist: 1.411\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.968; l2dist: 0.000\n",
      "    step: 50; loss: 6.942; l2dist: 2.128\n",
      "    step: 100; loss: 3.425; l2dist: 1.670\n",
      "    step: 150; loss: 2.599; l2dist: 1.489\n",
      "    step: 200; loss: 2.405; l2dist: 1.438\n",
      "    step: 250; loss: 2.315; l2dist: 1.417\n",
      "    step: 300; loss: 2.265; l2dist: 1.403\n",
      "    step: 350; loss: 2.269; l2dist: 1.400\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.162; l2dist: 0.000\n",
      "    step: 50; loss: 6.571; l2dist: 2.035\n",
      "    step: 100; loss: 3.367; l2dist: 1.610\n",
      "    step: 150; loss: 2.568; l2dist: 1.445\n",
      "    step: 200; loss: 2.374; l2dist: 1.402\n",
      "    step: 250; loss: 2.306; l2dist: 1.380\n",
      "    step: 300; loss: 2.258; l2dist: 1.380\n",
      "    step: 350; loss: 2.234; l2dist: 1.370\n",
      "    step: 400; loss: 2.232; l2dist: 1.372\n",
      "    step: 450; loss: 2.217; l2dist: 1.366\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.978; l2dist: 0.000\n",
      "    step: 50; loss: 6.395; l2dist: 1.975\n",
      "    step: 100; loss: 3.352; l2dist: 1.592\n",
      "    step: 150; loss: 2.561; l2dist: 1.440\n",
      "    step: 200; loss: 2.363; l2dist: 1.396\n",
      "    step: 250; loss: 2.279; l2dist: 1.384\n",
      "    step: 300; loss: 2.268; l2dist: 1.375\n",
      "    step: 350; loss: 2.237; l2dist: 1.368\n",
      "    step: 400; loss: 2.218; l2dist: 1.365\n",
      "    step: 450; loss: 2.230; l2dist: 1.365\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.807; l2dist: 0.000\n",
      "    step: 50; loss: 6.437; l2dist: 1.983\n",
      "    step: 100; loss: 3.361; l2dist: 1.604\n",
      "    step: 150; loss: 2.554; l2dist: 1.444\n",
      "    step: 200; loss: 2.361; l2dist: 1.400\n",
      "    step: 250; loss: 2.282; l2dist: 1.386\n",
      "    step: 300; loss: 2.253; l2dist: 1.376\n",
      "    step: 350; loss: 2.229; l2dist: 1.374\n",
      "    step: 400; loss: 2.223; l2dist: 1.369\n",
      "    step: 450; loss: 2.216; l2dist: 1.368\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.451; l2dist: 0.000\n",
      "    step: 50; loss: 6.414; l2dist: 1.980\n",
      "    step: 100; loss: 3.347; l2dist: 1.599\n",
      "    step: 150; loss: 2.546; l2dist: 1.440\n",
      "    step: 200; loss: 2.350; l2dist: 1.393\n",
      "    step: 250; loss: 2.274; l2dist: 1.380\n",
      "    step: 300; loss: 2.253; l2dist: 1.371\n",
      "    step: 350; loss: 2.243; l2dist: 1.367\n",
      "    step: 400; loss: 2.222; l2dist: 1.370\n",
      "    step: 450; loss: 2.211; l2dist: 1.367\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.751; l2dist: 0.000\n",
      "    step: 50; loss: 6.473; l2dist: 1.995\n",
      "    step: 100; loss: 3.355; l2dist: 1.614\n",
      "    step: 150; loss: 2.552; l2dist: 1.446\n",
      "    step: 200; loss: 2.359; l2dist: 1.404\n",
      "    step: 250; loss: 2.286; l2dist: 1.386\n",
      "    step: 300; loss: 2.255; l2dist: 1.377\n",
      "    step: 350; loss: 2.243; l2dist: 1.373\n",
      "    step: 400; loss: 2.228; l2dist: 1.379\n",
      "    step: 450; loss: 2.222; l2dist: 1.374\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 118.228; l2dist: 0.000\n",
      "    step: 50; loss: 12.572; l2dist: 3.292\n",
      "    step: 100; loss: 5.401; l2dist: 2.236\n",
      "    step: 150; loss: 3.643; l2dist: 1.812\n",
      "    step: 200; loss: 2.850; l2dist: 1.600\n",
      "    step: 250; loss: 2.540; l2dist: 1.513\n",
      "    step: 300; loss: 2.429; l2dist: 1.471\n",
      "    step: 350; loss: 2.341; l2dist: 1.445\n",
      "    step: 400; loss: 2.332; l2dist: 1.438\n",
      "    step: 450; loss: 2.259; l2dist: 1.414\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 146.433; l2dist: 0.000\n",
      "    step: 50; loss: 12.534; l2dist: 3.233\n",
      "    step: 100; loss: 5.819; l2dist: 2.255\n",
      "    step: 150; loss: 3.943; l2dist: 1.834\n",
      "    step: 200; loss: 3.126; l2dist: 1.638\n",
      "    step: 250; loss: 2.775; l2dist: 1.546\n",
      "    step: 300; loss: 2.528; l2dist: 1.482\n",
      "    step: 350; loss: 2.421; l2dist: 1.454\n",
      "    step: 400; loss: 2.361; l2dist: 1.444\n",
      "    step: 450; loss: 2.285; l2dist: 1.413\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.718; l2dist: 0.000\n",
      "    step: 50; loss: 9.511; l2dist: 2.746\n",
      "    step: 100; loss: 4.214; l2dist: 1.911\n",
      "    step: 150; loss: 2.963; l2dist: 1.607\n",
      "    step: 200; loss: 2.513; l2dist: 1.481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 250; loss: 2.330; l2dist: 1.431\n",
      "    step: 300; loss: 2.221; l2dist: 1.398\n",
      "    step: 350; loss: 2.131; l2dist: 1.368\n",
      "    step: 400; loss: 2.127; l2dist: 1.376\n",
      "    step: 450; loss: 2.102; l2dist: 1.368\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.329; l2dist: 0.000\n",
      "    step: 50; loss: 7.693; l2dist: 2.356\n",
      "    step: 100; loss: 3.500; l2dist: 1.737\n",
      "    step: 150; loss: 2.508; l2dist: 1.480\n",
      "    step: 200; loss: 2.218; l2dist: 1.396\n",
      "    step: 250; loss: 2.127; l2dist: 1.370\n",
      "    step: 300; loss: 2.062; l2dist: 1.350\n",
      "    step: 350; loss: 2.036; l2dist: 1.336\n",
      "    step: 400; loss: 2.069; l2dist: 1.347\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.059; l2dist: 0.000\n",
      "    step: 50; loss: 6.849; l2dist: 2.167\n",
      "    step: 100; loss: 3.257; l2dist: 1.648\n",
      "    step: 150; loss: 2.363; l2dist: 1.426\n",
      "    step: 200; loss: 2.153; l2dist: 1.369\n",
      "    step: 250; loss: 2.073; l2dist: 1.342\n",
      "    step: 300; loss: 2.027; l2dist: 1.327\n",
      "    step: 350; loss: 2.022; l2dist: 1.324\n",
      "    step: 400; loss: 2.002; l2dist: 1.316\n",
      "    step: 450; loss: 2.002; l2dist: 1.316\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.477; l2dist: 0.000\n",
      "    step: 50; loss: 6.530; l2dist: 2.083\n",
      "    step: 100; loss: 3.131; l2dist: 1.567\n",
      "    step: 150; loss: 2.339; l2dist: 1.389\n",
      "    step: 200; loss: 2.145; l2dist: 1.344\n",
      "    step: 250; loss: 2.070; l2dist: 1.323\n",
      "    step: 300; loss: 2.030; l2dist: 1.311\n",
      "    step: 350; loss: 2.012; l2dist: 1.309\n",
      "    step: 400; loss: 2.003; l2dist: 1.308\n",
      "    step: 450; loss: 2.020; l2dist: 1.303\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.048; l2dist: 0.000\n",
      "    step: 50; loss: 6.455; l2dist: 2.063\n",
      "    step: 100; loss: 3.108; l2dist: 1.550\n",
      "    step: 150; loss: 2.322; l2dist: 1.373\n",
      "    step: 200; loss: 2.129; l2dist: 1.331\n",
      "    step: 250; loss: 2.058; l2dist: 1.310\n",
      "    step: 300; loss: 2.044; l2dist: 1.310\n",
      "    step: 350; loss: 2.011; l2dist: 1.300\n",
      "    step: 400; loss: 2.005; l2dist: 1.294\n",
      "    step: 450; loss: 1.987; l2dist: 1.291\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.033; l2dist: 0.000\n",
      "    step: 50; loss: 6.420; l2dist: 2.057\n",
      "    step: 100; loss: 3.109; l2dist: 1.559\n",
      "    step: 150; loss: 2.337; l2dist: 1.386\n",
      "    step: 200; loss: 2.143; l2dist: 1.335\n",
      "    step: 250; loss: 2.067; l2dist: 1.319\n",
      "    step: 300; loss: 2.029; l2dist: 1.310\n",
      "    step: 350; loss: 2.017; l2dist: 1.305\n",
      "    step: 400; loss: 2.040; l2dist: 1.308\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.307; l2dist: 0.000\n",
      "    step: 50; loss: 6.367; l2dist: 2.047\n",
      "    step: 100; loss: 3.093; l2dist: 1.551\n",
      "    step: 150; loss: 2.321; l2dist: 1.378\n",
      "    step: 200; loss: 2.131; l2dist: 1.332\n",
      "    step: 250; loss: 2.055; l2dist: 1.315\n",
      "    step: 300; loss: 2.031; l2dist: 1.307\n",
      "    step: 350; loss: 2.009; l2dist: 1.302\n",
      "    step: 400; loss: 1.995; l2dist: 1.304\n",
      "    step: 450; loss: 1.997; l2dist: 1.302\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.534; l2dist: 0.000\n",
      "    step: 50; loss: 6.399; l2dist: 2.055\n",
      "    step: 100; loss: 3.102; l2dist: 1.562\n",
      "    step: 150; loss: 2.320; l2dist: 1.383\n",
      "    step: 200; loss: 2.143; l2dist: 1.340\n",
      "    step: 250; loss: 2.074; l2dist: 1.330\n",
      "    step: 300; loss: 2.036; l2dist: 1.314\n",
      "    step: 350; loss: 2.013; l2dist: 1.308\n",
      "    step: 400; loss: 2.009; l2dist: 1.304\n",
      "    step: 450; loss: 1.992; l2dist: 1.306\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.775; l2dist: 0.000\n",
      "    step: 50; loss: 13.413; l2dist: 3.352\n",
      "    step: 100; loss: 5.694; l2dist: 2.304\n",
      "    step: 150; loss: 3.826; l2dist: 1.872\n",
      "    step: 200; loss: 3.104; l2dist: 1.679\n",
      "    step: 250; loss: 2.764; l2dist: 1.567\n",
      "    step: 300; loss: 2.584; l2dist: 1.519\n",
      "    step: 350; loss: 2.503; l2dist: 1.481\n",
      "    step: 400; loss: 2.488; l2dist: 1.475\n",
      "    step: 450; loss: 2.455; l2dist: 1.469\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 69.665; l2dist: 0.000\n",
      "    step: 50; loss: 10.618; l2dist: 2.926\n",
      "    step: 100; loss: 4.293; l2dist: 1.981\n",
      "    step: 150; loss: 3.032; l2dist: 1.647\n",
      "    step: 200; loss: 2.586; l2dist: 1.519\n",
      "    step: 250; loss: 2.429; l2dist: 1.473\n",
      "    step: 300; loss: 2.330; l2dist: 1.448\n",
      "    step: 350; loss: 2.267; l2dist: 1.425\n",
      "    step: 400; loss: 2.285; l2dist: 1.422\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.809; l2dist: 0.000\n",
      "    step: 50; loss: 8.754; l2dist: 2.602\n",
      "    step: 100; loss: 3.615; l2dist: 1.801\n",
      "    step: 150; loss: 2.687; l2dist: 1.552\n",
      "    step: 200; loss: 2.417; l2dist: 1.467\n",
      "    step: 250; loss: 2.299; l2dist: 1.435\n",
      "    step: 300; loss: 2.266; l2dist: 1.420\n",
      "    step: 350; loss: 2.235; l2dist: 1.413\n",
      "    step: 400; loss: 2.214; l2dist: 1.410\n",
      "    step: 450; loss: 2.215; l2dist: 1.392\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.190; l2dist: 0.000\n",
      "    step: 50; loss: 7.879; l2dist: 2.411\n",
      "    step: 100; loss: 3.431; l2dist: 1.740\n",
      "    step: 150; loss: 2.576; l2dist: 1.522\n",
      "    step: 200; loss: 2.363; l2dist: 1.449\n",
      "    step: 250; loss: 2.248; l2dist: 1.415\n",
      "    step: 300; loss: 2.203; l2dist: 1.397\n",
      "    step: 350; loss: 2.167; l2dist: 1.382\n",
      "    step: 400; loss: 2.166; l2dist: 1.385\n",
      "    step: 450; loss: 2.138; l2dist: 1.380\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.283; l2dist: 0.000\n",
      "    step: 50; loss: 7.227; l2dist: 2.254\n",
      "    step: 100; loss: 3.308; l2dist: 1.642\n",
      "    step: 150; loss: 2.481; l2dist: 1.459\n",
      "    step: 200; loss: 2.286; l2dist: 1.403\n",
      "    step: 250; loss: 2.209; l2dist: 1.374\n",
      "    step: 300; loss: 2.152; l2dist: 1.362\n",
      "    step: 350; loss: 2.130; l2dist: 1.357\n",
      "    step: 400; loss: 2.117; l2dist: 1.358\n",
      "    step: 450; loss: 2.108; l2dist: 1.361\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.448; l2dist: 0.000\n",
      "    step: 50; loss: 6.928; l2dist: 2.175\n",
      "    step: 100; loss: 3.224; l2dist: 1.594\n",
      "    step: 150; loss: 2.486; l2dist: 1.422\n",
      "    step: 200; loss: 2.261; l2dist: 1.365\n",
      "    step: 250; loss: 2.180; l2dist: 1.344\n",
      "    step: 300; loss: 2.142; l2dist: 1.342\n",
      "    step: 350; loss: 2.120; l2dist: 1.331\n",
      "    step: 400; loss: 2.107; l2dist: 1.332\n",
      "    step: 450; loss: 2.102; l2dist: 1.325\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.569; l2dist: 0.000\n",
      "    step: 50; loss: 6.927; l2dist: 2.159\n",
      "    step: 100; loss: 3.263; l2dist: 1.593\n",
      "    step: 150; loss: 2.511; l2dist: 1.428\n",
      "    step: 200; loss: 2.279; l2dist: 1.377\n",
      "    step: 250; loss: 2.192; l2dist: 1.352\n",
      "    step: 300; loss: 2.169; l2dist: 1.339\n",
      "    step: 350; loss: 2.171; l2dist: 1.331\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.254; l2dist: 0.000\n",
      "    step: 50; loss: 6.895; l2dist: 2.142\n",
      "    step: 100; loss: 3.240; l2dist: 1.590\n",
      "    step: 150; loss: 2.496; l2dist: 1.420\n",
      "    step: 200; loss: 2.280; l2dist: 1.364\n",
      "    step: 250; loss: 2.206; l2dist: 1.354\n",
      "    step: 300; loss: 2.168; l2dist: 1.347\n",
      "    step: 350; loss: 2.130; l2dist: 1.328\n",
      "    step: 400; loss: 2.133; l2dist: 1.335\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.050; l2dist: 0.000\n",
      "    step: 50; loss: 6.879; l2dist: 2.138\n",
      "    step: 100; loss: 3.258; l2dist: 1.593\n",
      "    step: 150; loss: 2.501; l2dist: 1.431\n",
      "    step: 200; loss: 2.273; l2dist: 1.369\n",
      "    step: 250; loss: 2.205; l2dist: 1.354\n",
      "    step: 300; loss: 2.165; l2dist: 1.344\n",
      "    step: 350; loss: 2.139; l2dist: 1.337\n",
      "    step: 400; loss: 2.122; l2dist: 1.333\n",
      "    step: 450; loss: 2.123; l2dist: 1.337\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.192; l2dist: 0.000\n",
      "    step: 50; loss: 6.906; l2dist: 2.147\n",
      "    step: 100; loss: 3.262; l2dist: 1.607\n",
      "    step: 150; loss: 2.502; l2dist: 1.433\n",
      "    step: 200; loss: 2.285; l2dist: 1.378\n",
      "    step: 250; loss: 2.206; l2dist: 1.361\n",
      "    step: 300; loss: 2.171; l2dist: 1.351\n",
      "    step: 350; loss: 2.156; l2dist: 1.347\n",
      "    step: 400; loss: 2.144; l2dist: 1.342\n",
      "    step: 450; loss: 2.140; l2dist: 1.343\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 109.244; l2dist: 0.000\n",
      "    step: 50; loss: 12.247; l2dist: 3.218\n",
      "    step: 100; loss: 5.017; l2dist: 2.144\n",
      "    step: 150; loss: 3.282; l2dist: 1.724\n",
      "    step: 200; loss: 2.680; l2dist: 1.537\n",
      "    step: 250; loss: 2.458; l2dist: 1.453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 2.285; l2dist: 1.417\n",
      "    step: 350; loss: 2.225; l2dist: 1.390\n",
      "    step: 400; loss: 2.202; l2dist: 1.387\n",
      "    step: 450; loss: 2.186; l2dist: 1.384\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 79.316; l2dist: 0.000\n",
      "    step: 50; loss: 9.887; l2dist: 2.815\n",
      "    step: 100; loss: 4.072; l2dist: 1.890\n",
      "    step: 150; loss: 2.861; l2dist: 1.563\n",
      "    step: 200; loss: 2.448; l2dist: 1.441\n",
      "    step: 250; loss: 2.284; l2dist: 1.387\n",
      "    step: 300; loss: 2.223; l2dist: 1.368\n",
      "    step: 350; loss: 2.135; l2dist: 1.351\n",
      "    step: 400; loss: 2.105; l2dist: 1.348\n",
      "    step: 450; loss: 2.101; l2dist: 1.343\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.344; l2dist: 0.000\n",
      "    step: 50; loss: 8.046; l2dist: 2.435\n",
      "    step: 100; loss: 3.321; l2dist: 1.696\n",
      "    step: 150; loss: 2.476; l2dist: 1.459\n",
      "    step: 200; loss: 2.233; l2dist: 1.386\n",
      "    step: 250; loss: 2.142; l2dist: 1.351\n",
      "    step: 300; loss: 2.052; l2dist: 1.326\n",
      "    step: 350; loss: 2.022; l2dist: 1.318\n",
      "    step: 400; loss: 2.010; l2dist: 1.315\n",
      "    step: 450; loss: 1.992; l2dist: 1.313\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.419; l2dist: 0.000\n",
      "    step: 50; loss: 6.861; l2dist: 2.163\n",
      "    step: 100; loss: 3.080; l2dist: 1.628\n",
      "    step: 150; loss: 2.311; l2dist: 1.410\n",
      "    step: 200; loss: 2.122; l2dist: 1.349\n",
      "    step: 250; loss: 2.040; l2dist: 1.321\n",
      "    step: 300; loss: 2.006; l2dist: 1.313\n",
      "    step: 350; loss: 1.969; l2dist: 1.303\n",
      "    step: 400; loss: 1.959; l2dist: 1.304\n",
      "    step: 450; loss: 1.954; l2dist: 1.300\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.783; l2dist: 0.000\n",
      "    step: 50; loss: 6.284; l2dist: 1.984\n",
      "    step: 100; loss: 3.017; l2dist: 1.549\n",
      "    step: 150; loss: 2.271; l2dist: 1.382\n",
      "    step: 200; loss: 2.080; l2dist: 1.330\n",
      "    step: 250; loss: 2.011; l2dist: 1.310\n",
      "    step: 300; loss: 1.968; l2dist: 1.289\n",
      "    step: 350; loss: 1.952; l2dist: 1.286\n",
      "    step: 400; loss: 1.935; l2dist: 1.283\n",
      "    step: 450; loss: 1.927; l2dist: 1.283\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.679; l2dist: 0.000\n",
      "    step: 50; loss: 5.952; l2dist: 1.882\n",
      "    step: 100; loss: 2.924; l2dist: 1.475\n",
      "    step: 150; loss: 2.216; l2dist: 1.321\n",
      "    step: 200; loss: 2.042; l2dist: 1.282\n",
      "    step: 250; loss: 1.975; l2dist: 1.267\n",
      "    step: 300; loss: 1.936; l2dist: 1.260\n",
      "    step: 350; loss: 1.919; l2dist: 1.254\n",
      "    step: 400; loss: 1.918; l2dist: 1.256\n",
      "    step: 450; loss: 1.911; l2dist: 1.249\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.778; l2dist: 0.000\n",
      "    step: 50; loss: 5.898; l2dist: 1.863\n",
      "    step: 100; loss: 2.926; l2dist: 1.478\n",
      "    step: 150; loss: 2.217; l2dist: 1.322\n",
      "    step: 200; loss: 2.043; l2dist: 1.284\n",
      "    step: 250; loss: 1.971; l2dist: 1.265\n",
      "    step: 300; loss: 1.942; l2dist: 1.257\n",
      "    step: 350; loss: 1.915; l2dist: 1.248\n",
      "    step: 400; loss: 1.908; l2dist: 1.245\n",
      "    step: 450; loss: 1.918; l2dist: 1.249\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.441; l2dist: 0.000\n",
      "    step: 50; loss: 5.917; l2dist: 1.863\n",
      "    step: 100; loss: 2.924; l2dist: 1.478\n",
      "    step: 150; loss: 2.228; l2dist: 1.329\n",
      "    step: 200; loss: 2.049; l2dist: 1.285\n",
      "    step: 250; loss: 1.977; l2dist: 1.263\n",
      "    step: 300; loss: 1.945; l2dist: 1.262\n",
      "    step: 350; loss: 1.927; l2dist: 1.256\n",
      "    step: 400; loss: 1.920; l2dist: 1.256\n",
      "    step: 450; loss: 1.917; l2dist: 1.258\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.135; l2dist: 0.000\n",
      "    step: 50; loss: 5.887; l2dist: 1.855\n",
      "    step: 100; loss: 2.924; l2dist: 1.475\n",
      "    step: 150; loss: 2.218; l2dist: 1.327\n",
      "    step: 200; loss: 2.046; l2dist: 1.289\n",
      "    step: 250; loss: 1.982; l2dist: 1.273\n",
      "    step: 300; loss: 1.952; l2dist: 1.262\n",
      "    step: 350; loss: 1.943; l2dist: 1.256\n",
      "    step: 400; loss: 1.933; l2dist: 1.258\n",
      "    step: 450; loss: 1.926; l2dist: 1.250\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.299; l2dist: 0.000\n",
      "    step: 50; loss: 5.927; l2dist: 1.868\n",
      "    step: 100; loss: 2.933; l2dist: 1.488\n",
      "    step: 150; loss: 2.225; l2dist: 1.335\n",
      "    step: 200; loss: 2.056; l2dist: 1.293\n",
      "    step: 250; loss: 1.986; l2dist: 1.273\n",
      "    step: 300; loss: 1.949; l2dist: 1.269\n",
      "    step: 350; loss: 1.933; l2dist: 1.273\n",
      "    step: 400; loss: 1.926; l2dist: 1.261\n",
      "    step: 450; loss: 1.916; l2dist: 1.259\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.666; l2dist: 0.000\n",
      "    step: 50; loss: 13.269; l2dist: 3.248\n",
      "    step: 100; loss: 5.320; l2dist: 2.206\n",
      "    step: 150; loss: 3.572; l2dist: 1.774\n",
      "    step: 200; loss: 2.873; l2dist: 1.605\n",
      "    step: 250; loss: 2.538; l2dist: 1.502\n",
      "    step: 300; loss: 2.413; l2dist: 1.451\n",
      "    step: 350; loss: 2.293; l2dist: 1.425\n",
      "    step: 400; loss: 2.255; l2dist: 1.391\n",
      "    step: 450; loss: 2.282; l2dist: 1.408\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 103.862; l2dist: 0.000\n",
      "    step: 50; loss: 11.309; l2dist: 2.930\n",
      "    step: 100; loss: 4.717; l2dist: 2.014\n",
      "    step: 150; loss: 3.266; l2dist: 1.670\n",
      "    step: 200; loss: 2.793; l2dist: 1.531\n",
      "    step: 250; loss: 2.480; l2dist: 1.457\n",
      "    step: 300; loss: 2.352; l2dist: 1.422\n",
      "    step: 350; loss: 2.275; l2dist: 1.402\n",
      "    step: 400; loss: 2.248; l2dist: 1.398\n",
      "    step: 450; loss: 2.213; l2dist: 1.373\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.925; l2dist: 0.000\n",
      "    step: 50; loss: 8.602; l2dist: 2.504\n",
      "    step: 100; loss: 3.664; l2dist: 1.777\n",
      "    step: 150; loss: 2.677; l2dist: 1.514\n",
      "    step: 200; loss: 2.372; l2dist: 1.430\n",
      "    step: 250; loss: 2.207; l2dist: 1.387\n",
      "    step: 300; loss: 2.137; l2dist: 1.364\n",
      "    step: 350; loss: 2.105; l2dist: 1.354\n",
      "    step: 400; loss: 2.098; l2dist: 1.348\n",
      "    step: 450; loss: 2.090; l2dist: 1.344\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.019; l2dist: 0.000\n",
      "    step: 50; loss: 7.489; l2dist: 2.249\n",
      "    step: 100; loss: 3.396; l2dist: 1.696\n",
      "    step: 150; loss: 2.481; l2dist: 1.465\n",
      "    step: 200; loss: 2.244; l2dist: 1.392\n",
      "    step: 250; loss: 2.140; l2dist: 1.361\n",
      "    step: 300; loss: 2.105; l2dist: 1.347\n",
      "    step: 350; loss: 2.081; l2dist: 1.334\n",
      "    step: 400; loss: 2.051; l2dist: 1.329\n",
      "    step: 450; loss: 2.056; l2dist: 1.332\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.473; l2dist: 0.000\n",
      "    step: 50; loss: 6.630; l2dist: 2.046\n",
      "    step: 100; loss: 3.276; l2dist: 1.593\n",
      "    step: 150; loss: 2.406; l2dist: 1.414\n",
      "    step: 200; loss: 2.195; l2dist: 1.364\n",
      "    step: 250; loss: 2.120; l2dist: 1.339\n",
      "    step: 300; loss: 2.064; l2dist: 1.325\n",
      "    step: 350; loss: 2.072; l2dist: 1.328\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.185; l2dist: 0.000\n",
      "    step: 50; loss: 6.317; l2dist: 1.965\n",
      "    step: 100; loss: 3.159; l2dist: 1.540\n",
      "    step: 150; loss: 2.350; l2dist: 1.371\n",
      "    step: 200; loss: 2.167; l2dist: 1.328\n",
      "    step: 250; loss: 2.090; l2dist: 1.314\n",
      "    step: 300; loss: 2.058; l2dist: 1.302\n",
      "    step: 350; loss: 2.030; l2dist: 1.292\n",
      "    step: 400; loss: 2.015; l2dist: 1.300\n",
      "    step: 450; loss: 2.024; l2dist: 1.291\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.650; l2dist: 0.000\n",
      "    step: 50; loss: 6.200; l2dist: 1.943\n",
      "    step: 100; loss: 3.104; l2dist: 1.519\n",
      "    step: 150; loss: 2.312; l2dist: 1.353\n",
      "    step: 200; loss: 2.134; l2dist: 1.308\n",
      "    step: 250; loss: 2.080; l2dist: 1.293\n",
      "    step: 300; loss: 2.037; l2dist: 1.284\n",
      "    step: 350; loss: 2.020; l2dist: 1.282\n",
      "    step: 400; loss: 2.007; l2dist: 1.281\n",
      "    step: 450; loss: 2.002; l2dist: 1.276\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.102; l2dist: 0.000\n",
      "    step: 50; loss: 6.183; l2dist: 1.933\n",
      "    step: 100; loss: 3.106; l2dist: 1.515\n",
      "    step: 150; loss: 2.317; l2dist: 1.348\n",
      "    step: 200; loss: 2.143; l2dist: 1.308\n",
      "    step: 250; loss: 2.073; l2dist: 1.294\n",
      "    step: 300; loss: 2.040; l2dist: 1.286\n",
      "    step: 350; loss: 2.027; l2dist: 1.283\n",
      "    step: 400; loss: 2.015; l2dist: 1.279\n",
      "    step: 450; loss: 2.018; l2dist: 1.279\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.646; l2dist: 0.000\n",
      "    step: 50; loss: 6.146; l2dist: 1.925\n",
      "    step: 100; loss: 3.109; l2dist: 1.515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 2.316; l2dist: 1.347\n",
      "    step: 200; loss: 2.136; l2dist: 1.306\n",
      "    step: 250; loss: 2.082; l2dist: 1.300\n",
      "    step: 300; loss: 2.040; l2dist: 1.285\n",
      "    step: 350; loss: 2.028; l2dist: 1.285\n",
      "    step: 400; loss: 2.013; l2dist: 1.282\n",
      "    step: 450; loss: 2.008; l2dist: 1.278\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.792; l2dist: 0.000\n",
      "    step: 50; loss: 6.168; l2dist: 1.934\n",
      "    step: 100; loss: 3.120; l2dist: 1.522\n",
      "    step: 150; loss: 2.328; l2dist: 1.356\n",
      "    step: 200; loss: 2.163; l2dist: 1.316\n",
      "    step: 250; loss: 2.093; l2dist: 1.297\n",
      "    step: 300; loss: 2.044; l2dist: 1.291\n",
      "    step: 350; loss: 2.025; l2dist: 1.288\n",
      "    step: 400; loss: 2.017; l2dist: 1.286\n",
      "    step: 450; loss: 2.013; l2dist: 1.280\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 110.726; l2dist: 0.000\n",
      "    step: 50; loss: 10.885; l2dist: 3.137\n",
      "    step: 100; loss: 4.882; l2dist: 2.108\n",
      "    step: 150; loss: 3.176; l2dist: 1.680\n",
      "    step: 200; loss: 2.588; l2dist: 1.505\n",
      "    step: 250; loss: 2.301; l2dist: 1.422\n",
      "    step: 300; loss: 2.178; l2dist: 1.372\n",
      "    step: 350; loss: 2.092; l2dist: 1.351\n",
      "    step: 400; loss: 2.098; l2dist: 1.346\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 100.649; l2dist: 0.000\n",
      "    step: 50; loss: 9.965; l2dist: 2.924\n",
      "    step: 100; loss: 4.392; l2dist: 1.943\n",
      "    step: 150; loss: 2.934; l2dist: 1.578\n",
      "    step: 200; loss: 2.443; l2dist: 1.444\n",
      "    step: 250; loss: 2.179; l2dist: 1.361\n",
      "    step: 300; loss: 2.102; l2dist: 1.340\n",
      "    step: 350; loss: 2.019; l2dist: 1.309\n",
      "    step: 400; loss: 1.998; l2dist: 1.305\n",
      "    step: 450; loss: 1.980; l2dist: 1.296\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.093; l2dist: 0.000\n",
      "    step: 50; loss: 7.709; l2dist: 2.459\n",
      "    step: 100; loss: 3.336; l2dist: 1.697\n",
      "    step: 150; loss: 2.393; l2dist: 1.434\n",
      "    step: 200; loss: 2.109; l2dist: 1.344\n",
      "    step: 250; loss: 2.002; l2dist: 1.303\n",
      "    step: 300; loss: 1.940; l2dist: 1.289\n",
      "    step: 350; loss: 1.933; l2dist: 1.283\n",
      "    step: 400; loss: 1.925; l2dist: 1.281\n",
      "    step: 450; loss: 1.883; l2dist: 1.265\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.039; l2dist: 0.000\n",
      "    step: 50; loss: 6.304; l2dist: 2.152\n",
      "    step: 100; loss: 2.961; l2dist: 1.583\n",
      "    step: 150; loss: 2.180; l2dist: 1.366\n",
      "    step: 200; loss: 1.974; l2dist: 1.298\n",
      "    step: 250; loss: 1.888; l2dist: 1.263\n",
      "    step: 300; loss: 1.874; l2dist: 1.271\n",
      "    step: 350; loss: 1.833; l2dist: 1.242\n",
      "    step: 400; loss: 1.826; l2dist: 1.248\n",
      "    step: 450; loss: 1.807; l2dist: 1.240\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.290; l2dist: 0.000\n",
      "    step: 50; loss: 5.706; l2dist: 1.987\n",
      "    step: 100; loss: 2.877; l2dist: 1.525\n",
      "    step: 150; loss: 2.151; l2dist: 1.336\n",
      "    step: 200; loss: 1.966; l2dist: 1.284\n",
      "    step: 250; loss: 1.879; l2dist: 1.259\n",
      "    step: 300; loss: 1.853; l2dist: 1.247\n",
      "    step: 350; loss: 1.822; l2dist: 1.243\n",
      "    step: 400; loss: 1.820; l2dist: 1.236\n",
      "    step: 450; loss: 1.829; l2dist: 1.238\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.891; l2dist: 0.000\n",
      "    step: 50; loss: 5.409; l2dist: 1.868\n",
      "    step: 100; loss: 2.830; l2dist: 1.458\n",
      "    step: 150; loss: 2.121; l2dist: 1.293\n",
      "    step: 200; loss: 1.955; l2dist: 1.253\n",
      "    step: 250; loss: 1.883; l2dist: 1.237\n",
      "    step: 300; loss: 1.855; l2dist: 1.220\n",
      "    step: 350; loss: 1.824; l2dist: 1.222\n",
      "    step: 400; loss: 1.814; l2dist: 1.225\n",
      "    step: 450; loss: 1.811; l2dist: 1.215\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.986; l2dist: 0.000\n",
      "    step: 50; loss: 5.309; l2dist: 1.831\n",
      "    step: 100; loss: 2.820; l2dist: 1.454\n",
      "    step: 150; loss: 2.112; l2dist: 1.290\n",
      "    step: 200; loss: 1.924; l2dist: 1.244\n",
      "    step: 250; loss: 1.864; l2dist: 1.227\n",
      "    step: 300; loss: 1.844; l2dist: 1.219\n",
      "    step: 350; loss: 1.815; l2dist: 1.214\n",
      "    step: 400; loss: 1.802; l2dist: 1.211\n",
      "    step: 450; loss: 1.797; l2dist: 1.213\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 16.192; l2dist: 0.000\n",
      "    step: 50; loss: 5.294; l2dist: 1.820\n",
      "    step: 100; loss: 2.829; l2dist: 1.458\n",
      "    step: 150; loss: 2.109; l2dist: 1.294\n",
      "    step: 200; loss: 1.936; l2dist: 1.246\n",
      "    step: 250; loss: 1.871; l2dist: 1.232\n",
      "    step: 300; loss: 1.831; l2dist: 1.227\n",
      "    step: 350; loss: 1.822; l2dist: 1.218\n",
      "    step: 400; loss: 1.829; l2dist: 1.223\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.819; l2dist: 0.000\n",
      "    step: 50; loss: 5.284; l2dist: 1.817\n",
      "    step: 100; loss: 2.835; l2dist: 1.462\n",
      "    step: 150; loss: 2.108; l2dist: 1.294\n",
      "    step: 200; loss: 1.935; l2dist: 1.249\n",
      "    step: 250; loss: 1.870; l2dist: 1.234\n",
      "    step: 300; loss: 1.842; l2dist: 1.232\n",
      "    step: 350; loss: 1.816; l2dist: 1.224\n",
      "    step: 400; loss: 1.807; l2dist: 1.219\n",
      "    step: 450; loss: 1.804; l2dist: 1.220\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.964; l2dist: 0.000\n",
      "    step: 50; loss: 5.312; l2dist: 1.828\n",
      "    step: 100; loss: 2.841; l2dist: 1.470\n",
      "    step: 150; loss: 2.116; l2dist: 1.296\n",
      "    step: 200; loss: 1.951; l2dist: 1.254\n",
      "    step: 250; loss: 1.871; l2dist: 1.240\n",
      "    step: 300; loss: 1.839; l2dist: 1.234\n",
      "    step: 350; loss: 1.817; l2dist: 1.229\n",
      "    step: 400; loss: 1.807; l2dist: 1.225\n",
      "    step: 450; loss: 1.805; l2dist: 1.228\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 119.458; l2dist: 0.000\n",
      "    step: 50; loss: 13.644; l2dist: 3.262\n",
      "    step: 100; loss: 5.533; l2dist: 2.267\n",
      "    step: 150; loss: 3.695; l2dist: 1.830\n",
      "    step: 200; loss: 3.018; l2dist: 1.630\n",
      "    step: 250; loss: 2.696; l2dist: 1.543\n",
      "    step: 300; loss: 2.513; l2dist: 1.485\n",
      "    step: 350; loss: 2.490; l2dist: 1.470\n",
      "    step: 400; loss: 2.392; l2dist: 1.437\n",
      "    step: 450; loss: 2.328; l2dist: 1.422\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 94.836; l2dist: 0.000\n",
      "    step: 50; loss: 11.025; l2dist: 2.932\n",
      "    step: 100; loss: 4.476; l2dist: 2.006\n",
      "    step: 150; loss: 3.162; l2dist: 1.662\n",
      "    step: 200; loss: 2.670; l2dist: 1.524\n",
      "    step: 250; loss: 2.456; l2dist: 1.456\n",
      "    step: 300; loss: 2.344; l2dist: 1.427\n",
      "    step: 350; loss: 2.281; l2dist: 1.412\n",
      "    step: 400; loss: 2.223; l2dist: 1.387\n",
      "    step: 450; loss: 2.215; l2dist: 1.389\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.745; l2dist: 0.000\n",
      "    step: 50; loss: 8.586; l2dist: 2.501\n",
      "    step: 100; loss: 3.590; l2dist: 1.783\n",
      "    step: 150; loss: 2.702; l2dist: 1.534\n",
      "    step: 200; loss: 2.391; l2dist: 1.442\n",
      "    step: 250; loss: 2.247; l2dist: 1.400\n",
      "    step: 300; loss: 2.195; l2dist: 1.385\n",
      "    step: 350; loss: 2.169; l2dist: 1.378\n",
      "    step: 400; loss: 2.152; l2dist: 1.356\n",
      "    step: 450; loss: 2.165; l2dist: 1.353\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.160; l2dist: 0.000\n",
      "    step: 50; loss: 7.447; l2dist: 2.254\n",
      "    step: 100; loss: 3.385; l2dist: 1.701\n",
      "    step: 150; loss: 2.518; l2dist: 1.474\n",
      "    step: 200; loss: 2.263; l2dist: 1.395\n",
      "    step: 250; loss: 2.153; l2dist: 1.360\n",
      "    step: 300; loss: 2.097; l2dist: 1.348\n",
      "    step: 350; loss: 2.084; l2dist: 1.333\n",
      "    step: 400; loss: 2.058; l2dist: 1.334\n",
      "    step: 450; loss: 2.046; l2dist: 1.330\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.862; l2dist: 0.000\n",
      "    step: 50; loss: 6.862; l2dist: 2.091\n",
      "    step: 100; loss: 3.326; l2dist: 1.618\n",
      "    step: 150; loss: 2.505; l2dist: 1.430\n",
      "    step: 200; loss: 2.269; l2dist: 1.369\n",
      "    step: 250; loss: 2.166; l2dist: 1.337\n",
      "    step: 300; loss: 2.125; l2dist: 1.325\n",
      "    step: 350; loss: 2.107; l2dist: 1.323\n",
      "    step: 400; loss: 2.103; l2dist: 1.317\n",
      "    step: 450; loss: 2.093; l2dist: 1.312\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.579; l2dist: 0.000\n",
      "    step: 50; loss: 6.543; l2dist: 1.970\n",
      "    step: 100; loss: 3.222; l2dist: 1.558\n",
      "    step: 150; loss: 2.453; l2dist: 1.388\n",
      "    step: 200; loss: 2.259; l2dist: 1.334\n",
      "    step: 250; loss: 2.142; l2dist: 1.314\n",
      "    step: 300; loss: 2.094; l2dist: 1.305\n",
      "    step: 350; loss: 2.071; l2dist: 1.295\n",
      "    step: 400; loss: 2.083; l2dist: 1.299\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.828; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 6.436; l2dist: 1.942\n",
      "    step: 100; loss: 3.238; l2dist: 1.543\n",
      "    step: 150; loss: 2.444; l2dist: 1.387\n",
      "    step: 200; loss: 2.236; l2dist: 1.335\n",
      "    step: 250; loss: 2.143; l2dist: 1.317\n",
      "    step: 300; loss: 2.095; l2dist: 1.307\n",
      "    step: 350; loss: 2.081; l2dist: 1.300\n",
      "    step: 400; loss: 2.047; l2dist: 1.296\n",
      "    step: 450; loss: 2.050; l2dist: 1.296\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.952; l2dist: 0.000\n",
      "    step: 50; loss: 6.391; l2dist: 1.926\n",
      "    step: 100; loss: 3.228; l2dist: 1.543\n",
      "    step: 150; loss: 2.435; l2dist: 1.383\n",
      "    step: 200; loss: 2.223; l2dist: 1.333\n",
      "    step: 250; loss: 2.139; l2dist: 1.314\n",
      "    step: 300; loss: 2.092; l2dist: 1.303\n",
      "    step: 350; loss: 2.063; l2dist: 1.299\n",
      "    step: 400; loss: 2.048; l2dist: 1.292\n",
      "    step: 450; loss: 2.053; l2dist: 1.297\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.520; l2dist: 0.000\n",
      "    step: 50; loss: 6.351; l2dist: 1.915\n",
      "    step: 100; loss: 3.224; l2dist: 1.539\n",
      "    step: 150; loss: 2.436; l2dist: 1.386\n",
      "    step: 200; loss: 2.222; l2dist: 1.333\n",
      "    step: 250; loss: 2.127; l2dist: 1.315\n",
      "    step: 300; loss: 2.093; l2dist: 1.303\n",
      "    step: 350; loss: 2.059; l2dist: 1.295\n",
      "    step: 400; loss: 2.054; l2dist: 1.294\n",
      "    step: 450; loss: 2.046; l2dist: 1.292\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.620; l2dist: 0.000\n",
      "    step: 50; loss: 6.378; l2dist: 1.924\n",
      "    step: 100; loss: 3.231; l2dist: 1.545\n",
      "    step: 150; loss: 2.439; l2dist: 1.386\n",
      "    step: 200; loss: 2.221; l2dist: 1.336\n",
      "    step: 250; loss: 2.132; l2dist: 1.325\n",
      "    step: 300; loss: 2.094; l2dist: 1.312\n",
      "    step: 350; loss: 2.080; l2dist: 1.305\n",
      "    step: 400; loss: 2.049; l2dist: 1.297\n",
      "    step: 450; loss: 2.053; l2dist: 1.298\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.533; l2dist: 0.000\n",
      "    step: 50; loss: 12.457; l2dist: 3.267\n",
      "    step: 100; loss: 5.332; l2dist: 2.206\n",
      "    step: 150; loss: 3.538; l2dist: 1.763\n",
      "    step: 200; loss: 2.868; l2dist: 1.579\n",
      "    step: 250; loss: 2.538; l2dist: 1.485\n",
      "    step: 300; loss: 2.363; l2dist: 1.423\n",
      "    step: 350; loss: 2.322; l2dist: 1.408\n",
      "    step: 400; loss: 2.289; l2dist: 1.406\n",
      "    step: 450; loss: 2.289; l2dist: 1.407\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 70.290; l2dist: 0.000\n",
      "    step: 50; loss: 9.975; l2dist: 2.862\n",
      "    step: 100; loss: 4.030; l2dist: 1.883\n",
      "    step: 150; loss: 2.827; l2dist: 1.568\n",
      "    step: 200; loss: 2.431; l2dist: 1.444\n",
      "    step: 250; loss: 2.286; l2dist: 1.400\n",
      "    step: 300; loss: 2.206; l2dist: 1.379\n",
      "    step: 350; loss: 2.159; l2dist: 1.355\n",
      "    step: 400; loss: 2.111; l2dist: 1.351\n",
      "    step: 450; loss: 2.140; l2dist: 1.356\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.837; l2dist: 0.000\n",
      "    step: 50; loss: 8.020; l2dist: 2.469\n",
      "    step: 100; loss: 3.295; l2dist: 1.699\n",
      "    step: 150; loss: 2.457; l2dist: 1.461\n",
      "    step: 200; loss: 2.237; l2dist: 1.382\n",
      "    step: 250; loss: 2.144; l2dist: 1.359\n",
      "    step: 300; loss: 2.102; l2dist: 1.341\n",
      "    step: 350; loss: 2.070; l2dist: 1.333\n",
      "    step: 400; loss: 2.064; l2dist: 1.323\n",
      "    step: 450; loss: 2.045; l2dist: 1.318\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.154; l2dist: 0.000\n",
      "    step: 50; loss: 6.817; l2dist: 2.174\n",
      "    step: 100; loss: 3.132; l2dist: 1.632\n",
      "    step: 150; loss: 2.357; l2dist: 1.417\n",
      "    step: 200; loss: 2.169; l2dist: 1.358\n",
      "    step: 250; loss: 2.081; l2dist: 1.333\n",
      "    step: 300; loss: 2.047; l2dist: 1.324\n",
      "    step: 350; loss: 2.031; l2dist: 1.317\n",
      "    step: 400; loss: 2.009; l2dist: 1.314\n",
      "    step: 450; loss: 2.012; l2dist: 1.313\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 15.727; l2dist: 0.000\n",
      "    step: 50; loss: 6.220; l2dist: 1.938\n",
      "    step: 100; loss: 3.170; l2dist: 1.562\n",
      "    step: 150; loss: 2.354; l2dist: 1.382\n",
      "    step: 200; loss: 2.159; l2dist: 1.333\n",
      "    step: 250; loss: 2.080; l2dist: 1.318\n",
      "    step: 300; loss: 2.044; l2dist: 1.303\n",
      "    step: 350; loss: 2.026; l2dist: 1.297\n",
      "    step: 400; loss: 2.016; l2dist: 1.296\n",
      "    step: 450; loss: 2.008; l2dist: 1.296\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 13.767; l2dist: 0.000\n",
      "    step: 50; loss: 5.892; l2dist: 1.810\n",
      "    step: 100; loss: 3.102; l2dist: 1.478\n",
      "    step: 150; loss: 2.333; l2dist: 1.340\n",
      "    step: 200; loss: 2.150; l2dist: 1.308\n",
      "    step: 250; loss: 2.073; l2dist: 1.297\n",
      "    step: 300; loss: 2.042; l2dist: 1.283\n",
      "    step: 350; loss: 2.014; l2dist: 1.281\n",
      "    step: 400; loss: 2.006; l2dist: 1.277\n",
      "    step: 450; loss: 1.999; l2dist: 1.284\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 13.091; l2dist: 0.000\n",
      "    step: 50; loss: 5.796; l2dist: 1.785\n",
      "    step: 100; loss: 3.085; l2dist: 1.478\n",
      "    step: 150; loss: 2.320; l2dist: 1.330\n",
      "    step: 200; loss: 2.136; l2dist: 1.293\n",
      "    step: 250; loss: 2.059; l2dist: 1.277\n",
      "    step: 300; loss: 2.033; l2dist: 1.271\n",
      "    step: 350; loss: 2.011; l2dist: 1.263\n",
      "    step: 400; loss: 1.996; l2dist: 1.267\n",
      "    step: 450; loss: 1.989; l2dist: 1.263\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.832; l2dist: 0.000\n",
      "    step: 50; loss: 5.776; l2dist: 1.773\n",
      "    step: 100; loss: 3.084; l2dist: 1.476\n",
      "    step: 150; loss: 2.319; l2dist: 1.332\n",
      "    step: 200; loss: 2.136; l2dist: 1.291\n",
      "    step: 250; loss: 2.062; l2dist: 1.282\n",
      "    step: 300; loss: 2.035; l2dist: 1.269\n",
      "    step: 350; loss: 2.008; l2dist: 1.269\n",
      "    step: 400; loss: 2.001; l2dist: 1.269\n",
      "    step: 450; loss: 1.993; l2dist: 1.268\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.727; l2dist: 0.000\n",
      "    step: 50; loss: 5.771; l2dist: 1.772\n",
      "    step: 100; loss: 3.089; l2dist: 1.479\n",
      "    step: 150; loss: 2.314; l2dist: 1.331\n",
      "    step: 200; loss: 2.132; l2dist: 1.302\n",
      "    step: 250; loss: 2.062; l2dist: 1.279\n",
      "    step: 300; loss: 2.031; l2dist: 1.279\n",
      "    step: 350; loss: 2.011; l2dist: 1.273\n",
      "    step: 400; loss: 2.001; l2dist: 1.268\n",
      "    step: 450; loss: 1.988; l2dist: 1.265\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 12.921; l2dist: 0.000\n",
      "    step: 50; loss: 5.825; l2dist: 1.789\n",
      "    step: 100; loss: 3.098; l2dist: 1.490\n",
      "    step: 150; loss: 2.319; l2dist: 1.342\n",
      "    step: 200; loss: 2.135; l2dist: 1.304\n",
      "    step: 250; loss: 2.072; l2dist: 1.295\n",
      "    step: 300; loss: 2.039; l2dist: 1.279\n",
      "    step: 350; loss: 2.016; l2dist: 1.276\n",
      "    step: 400; loss: 2.003; l2dist: 1.275\n",
      "    step: 450; loss: 1.997; l2dist: 1.271\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 124.295; l2dist: 0.000\n",
      "    step: 50; loss: 13.726; l2dist: 3.414\n",
      "    step: 100; loss: 5.970; l2dist: 2.365\n",
      "    step: 150; loss: 3.985; l2dist: 1.919\n",
      "    step: 200; loss: 3.219; l2dist: 1.706\n",
      "    step: 250; loss: 2.826; l2dist: 1.600\n",
      "    step: 300; loss: 2.628; l2dist: 1.545\n",
      "    step: 350; loss: 2.514; l2dist: 1.509\n",
      "    step: 400; loss: 2.485; l2dist: 1.501\n",
      "    step: 450; loss: 2.436; l2dist: 1.471\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 104.628; l2dist: 0.000\n",
      "    step: 50; loss: 12.033; l2dist: 3.130\n",
      "    step: 100; loss: 5.178; l2dist: 2.142\n",
      "    step: 150; loss: 3.564; l2dist: 1.765\n",
      "    step: 200; loss: 3.002; l2dist: 1.615\n",
      "    step: 250; loss: 2.692; l2dist: 1.533\n",
      "    step: 300; loss: 2.559; l2dist: 1.492\n",
      "    step: 350; loss: 2.482; l2dist: 1.478\n",
      "    step: 400; loss: 2.425; l2dist: 1.459\n",
      "    step: 450; loss: 2.433; l2dist: 1.465\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.839; l2dist: 0.000\n",
      "    step: 50; loss: 9.414; l2dist: 2.681\n",
      "    step: 100; loss: 4.072; l2dist: 1.893\n",
      "    step: 150; loss: 2.997; l2dist: 1.622\n",
      "    step: 200; loss: 2.634; l2dist: 1.524\n",
      "    step: 250; loss: 2.456; l2dist: 1.480\n",
      "    step: 300; loss: 2.366; l2dist: 1.437\n",
      "    step: 350; loss: 2.351; l2dist: 1.438\n",
      "    step: 400; loss: 2.294; l2dist: 1.425\n",
      "    step: 450; loss: 2.327; l2dist: 1.439\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.741; l2dist: 0.000\n",
      "    step: 50; loss: 8.233; l2dist: 2.451\n",
      "    step: 100; loss: 3.826; l2dist: 1.816\n",
      "    step: 150; loss: 2.850; l2dist: 1.568\n",
      "    step: 200; loss: 2.535; l2dist: 1.483\n",
      "    step: 250; loss: 2.428; l2dist: 1.453\n",
      "    step: 300; loss: 2.332; l2dist: 1.430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 2.318; l2dist: 1.436\n",
      "    step: 400; loss: 2.282; l2dist: 1.415\n",
      "    step: 450; loss: 2.254; l2dist: 1.412\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.662; l2dist: 0.000\n",
      "    step: 50; loss: 7.392; l2dist: 2.284\n",
      "    step: 100; loss: 3.659; l2dist: 1.725\n",
      "    step: 150; loss: 2.752; l2dist: 1.523\n",
      "    step: 200; loss: 2.465; l2dist: 1.451\n",
      "    step: 250; loss: 2.380; l2dist: 1.427\n",
      "    step: 300; loss: 2.296; l2dist: 1.408\n",
      "    step: 350; loss: 2.253; l2dist: 1.398\n",
      "    step: 400; loss: 2.248; l2dist: 1.392\n",
      "    step: 450; loss: 2.218; l2dist: 1.391\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.982; l2dist: 0.000\n",
      "    step: 50; loss: 6.967; l2dist: 2.187\n",
      "    step: 100; loss: 3.561; l2dist: 1.665\n",
      "    step: 150; loss: 2.699; l2dist: 1.466\n",
      "    step: 200; loss: 2.436; l2dist: 1.415\n",
      "    step: 250; loss: 2.345; l2dist: 1.388\n",
      "    step: 300; loss: 2.306; l2dist: 1.388\n",
      "    step: 350; loss: 2.357; l2dist: 1.389\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.080; l2dist: 0.000\n",
      "    step: 50; loss: 6.884; l2dist: 2.173\n",
      "    step: 100; loss: 3.535; l2dist: 1.655\n",
      "    step: 150; loss: 2.676; l2dist: 1.468\n",
      "    step: 200; loss: 2.432; l2dist: 1.416\n",
      "    step: 250; loss: 2.327; l2dist: 1.389\n",
      "    step: 300; loss: 2.268; l2dist: 1.385\n",
      "    step: 350; loss: 2.253; l2dist: 1.370\n",
      "    step: 400; loss: 2.213; l2dist: 1.366\n",
      "    step: 450; loss: 2.201; l2dist: 1.363\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.575; l2dist: 0.000\n",
      "    step: 50; loss: 6.891; l2dist: 2.171\n",
      "    step: 100; loss: 3.544; l2dist: 1.656\n",
      "    step: 150; loss: 2.691; l2dist: 1.472\n",
      "    step: 200; loss: 2.430; l2dist: 1.418\n",
      "    step: 250; loss: 2.341; l2dist: 1.397\n",
      "    step: 300; loss: 2.282; l2dist: 1.389\n",
      "    step: 350; loss: 2.243; l2dist: 1.390\n",
      "    step: 400; loss: 2.208; l2dist: 1.378\n",
      "    step: 450; loss: 2.197; l2dist: 1.371\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.340; l2dist: 0.000\n",
      "    step: 50; loss: 6.885; l2dist: 2.169\n",
      "    step: 100; loss: 3.537; l2dist: 1.659\n",
      "    step: 150; loss: 2.686; l2dist: 1.477\n",
      "    step: 200; loss: 2.444; l2dist: 1.425\n",
      "    step: 250; loss: 2.335; l2dist: 1.400\n",
      "    step: 300; loss: 2.265; l2dist: 1.388\n",
      "    step: 350; loss: 2.223; l2dist: 1.377\n",
      "    step: 400; loss: 2.211; l2dist: 1.379\n",
      "    step: 450; loss: 2.191; l2dist: 1.374\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.474; l2dist: 0.000\n",
      "    step: 50; loss: 6.916; l2dist: 2.176\n",
      "    step: 100; loss: 3.550; l2dist: 1.665\n",
      "    step: 150; loss: 2.695; l2dist: 1.481\n",
      "    step: 200; loss: 2.443; l2dist: 1.432\n",
      "    step: 250; loss: 2.325; l2dist: 1.403\n",
      "    step: 300; loss: 2.268; l2dist: 1.392\n",
      "    step: 350; loss: 2.233; l2dist: 1.380\n",
      "    step: 400; loss: 2.210; l2dist: 1.385\n",
      "    step: 450; loss: 2.202; l2dist: 1.377\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 127.153; l2dist: 0.000\n",
      "    step: 50; loss: 13.390; l2dist: 3.388\n",
      "    step: 100; loss: 5.996; l2dist: 2.369\n",
      "    step: 150; loss: 3.959; l2dist: 1.908\n",
      "    step: 200; loss: 3.067; l2dist: 1.664\n",
      "    step: 250; loss: 2.691; l2dist: 1.551\n",
      "    step: 300; loss: 2.526; l2dist: 1.500\n",
      "    step: 350; loss: 2.411; l2dist: 1.466\n",
      "    step: 400; loss: 2.334; l2dist: 1.446\n",
      "    step: 450; loss: 2.296; l2dist: 1.431\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 115.364; l2dist: 0.000\n",
      "    step: 50; loss: 12.027; l2dist: 3.181\n",
      "    step: 100; loss: 5.197; l2dist: 2.165\n",
      "    step: 150; loss: 3.536; l2dist: 1.762\n",
      "    step: 200; loss: 2.863; l2dist: 1.585\n",
      "    step: 250; loss: 2.556; l2dist: 1.504\n",
      "    step: 300; loss: 2.410; l2dist: 1.455\n",
      "    step: 350; loss: 2.340; l2dist: 1.432\n",
      "    step: 400; loss: 2.297; l2dist: 1.427\n",
      "    step: 450; loss: 2.248; l2dist: 1.403\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.063; l2dist: 0.000\n",
      "    step: 50; loss: 9.324; l2dist: 2.712\n",
      "    step: 100; loss: 3.871; l2dist: 1.861\n",
      "    step: 150; loss: 2.776; l2dist: 1.566\n",
      "    step: 200; loss: 2.422; l2dist: 1.464\n",
      "    step: 250; loss: 2.268; l2dist: 1.421\n",
      "    step: 300; loss: 2.201; l2dist: 1.400\n",
      "    step: 350; loss: 2.175; l2dist: 1.388\n",
      "    step: 400; loss: 2.154; l2dist: 1.383\n",
      "    step: 450; loss: 2.149; l2dist: 1.383\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.039; l2dist: 0.000\n",
      "    step: 50; loss: 8.025; l2dist: 2.444\n",
      "    step: 100; loss: 3.490; l2dist: 1.757\n",
      "    step: 150; loss: 2.548; l2dist: 1.506\n",
      "    step: 200; loss: 2.309; l2dist: 1.421\n",
      "    step: 250; loss: 2.198; l2dist: 1.402\n",
      "    step: 300; loss: 2.146; l2dist: 1.377\n",
      "    step: 350; loss: 2.138; l2dist: 1.370\n",
      "    step: 400; loss: 2.112; l2dist: 1.368\n",
      "    step: 450; loss: 2.094; l2dist: 1.365\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.336; l2dist: 0.000\n",
      "    step: 50; loss: 7.227; l2dist: 2.276\n",
      "    step: 100; loss: 3.361; l2dist: 1.679\n",
      "    step: 150; loss: 2.494; l2dist: 1.476\n",
      "    step: 200; loss: 2.284; l2dist: 1.412\n",
      "    step: 250; loss: 2.182; l2dist: 1.381\n",
      "    step: 300; loss: 2.148; l2dist: 1.368\n",
      "    step: 350; loss: 2.122; l2dist: 1.364\n",
      "    step: 400; loss: 2.120; l2dist: 1.358\n",
      "    step: 450; loss: 2.115; l2dist: 1.362\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.965; l2dist: 0.000\n",
      "    step: 50; loss: 6.987; l2dist: 2.215\n",
      "    step: 100; loss: 3.283; l2dist: 1.644\n",
      "    step: 150; loss: 2.450; l2dist: 1.439\n",
      "    step: 200; loss: 2.246; l2dist: 1.383\n",
      "    step: 250; loss: 2.181; l2dist: 1.370\n",
      "    step: 300; loss: 2.143; l2dist: 1.353\n",
      "    step: 350; loss: 2.113; l2dist: 1.350\n",
      "    step: 400; loss: 2.106; l2dist: 1.347\n",
      "    step: 450; loss: 2.103; l2dist: 1.347\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.547; l2dist: 0.000\n",
      "    step: 50; loss: 6.869; l2dist: 2.188\n",
      "    step: 100; loss: 3.231; l2dist: 1.631\n",
      "    step: 150; loss: 2.442; l2dist: 1.446\n",
      "    step: 200; loss: 2.244; l2dist: 1.382\n",
      "    step: 250; loss: 2.160; l2dist: 1.367\n",
      "    step: 300; loss: 2.141; l2dist: 1.364\n",
      "    step: 350; loss: 2.109; l2dist: 1.349\n",
      "    step: 400; loss: 2.111; l2dist: 1.341\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.356; l2dist: 0.000\n",
      "    step: 50; loss: 6.845; l2dist: 2.175\n",
      "    step: 100; loss: 3.229; l2dist: 1.616\n",
      "    step: 150; loss: 2.427; l2dist: 1.423\n",
      "    step: 200; loss: 2.240; l2dist: 1.374\n",
      "    step: 250; loss: 2.166; l2dist: 1.362\n",
      "    step: 300; loss: 2.133; l2dist: 1.349\n",
      "    step: 350; loss: 2.123; l2dist: 1.338\n",
      "    step: 400; loss: 2.108; l2dist: 1.345\n",
      "    step: 450; loss: 2.094; l2dist: 1.338\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.213; l2dist: 0.000\n",
      "    step: 50; loss: 6.830; l2dist: 2.174\n",
      "    step: 100; loss: 3.226; l2dist: 1.618\n",
      "    step: 150; loss: 2.430; l2dist: 1.424\n",
      "    step: 200; loss: 2.260; l2dist: 1.375\n",
      "    step: 250; loss: 2.162; l2dist: 1.357\n",
      "    step: 300; loss: 2.142; l2dist: 1.349\n",
      "    step: 350; loss: 2.118; l2dist: 1.348\n",
      "    step: 400; loss: 2.102; l2dist: 1.343\n",
      "    step: 450; loss: 2.104; l2dist: 1.335\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.393; l2dist: 0.000\n",
      "    step: 50; loss: 6.864; l2dist: 2.185\n",
      "    step: 100; loss: 3.233; l2dist: 1.622\n",
      "    step: 150; loss: 2.446; l2dist: 1.441\n",
      "    step: 200; loss: 2.241; l2dist: 1.379\n",
      "    step: 250; loss: 2.176; l2dist: 1.365\n",
      "    step: 300; loss: 2.137; l2dist: 1.350\n",
      "    step: 350; loss: 2.110; l2dist: 1.347\n",
      "    step: 400; loss: 2.113; l2dist: 1.348\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 111.549; l2dist: 0.000\n",
      "    step: 50; loss: 13.175; l2dist: 3.204\n",
      "    step: 100; loss: 5.215; l2dist: 2.179\n",
      "    step: 150; loss: 3.515; l2dist: 1.761\n",
      "    step: 200; loss: 2.847; l2dist: 1.569\n",
      "    step: 250; loss: 2.540; l2dist: 1.473\n",
      "    step: 300; loss: 2.359; l2dist: 1.421\n",
      "    step: 350; loss: 2.252; l2dist: 1.400\n",
      "    step: 400; loss: 2.262; l2dist: 1.392\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 123.623; l2dist: 0.000\n",
      "    step: 50; loss: 13.157; l2dist: 3.165\n",
      "    step: 100; loss: 5.440; l2dist: 2.145\n",
      "    step: 150; loss: 3.616; l2dist: 1.729\n",
      "    step: 200; loss: 2.894; l2dist: 1.544\n",
      "    step: 250; loss: 2.555; l2dist: 1.456\n",
      "    step: 300; loss: 2.458; l2dist: 1.430\n",
      "    step: 350; loss: 2.283; l2dist: 1.385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 2.221; l2dist: 1.372\n",
      "    step: 450; loss: 2.175; l2dist: 1.360\n",
      "binary step: 1; number of successful adv: 99/100\n",
      "    step: 0; loss: 215.070; l2dist: 0.000\n",
      "    step: 50; loss: 17.873; l2dist: 3.688\n",
      "    step: 100; loss: 8.082; l2dist: 2.658\n",
      "    step: 150; loss: 5.299; l2dist: 2.127\n",
      "    step: 200; loss: 3.905; l2dist: 1.814\n",
      "    step: 250; loss: 3.144; l2dist: 1.632\n",
      "    step: 300; loss: 2.728; l2dist: 1.518\n",
      "    step: 350; loss: 2.583; l2dist: 1.474\n",
      "    step: 400; loss: 2.423; l2dist: 1.438\n",
      "    step: 450; loss: 2.356; l2dist: 1.418\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 119.180; l2dist: 0.000\n",
      "    step: 50; loss: 12.968; l2dist: 3.087\n",
      "    step: 100; loss: 5.390; l2dist: 2.147\n",
      "    step: 150; loss: 3.513; l2dist: 1.716\n",
      "    step: 200; loss: 2.784; l2dist: 1.530\n",
      "    step: 250; loss: 2.422; l2dist: 1.432\n",
      "    step: 300; loss: 2.317; l2dist: 1.399\n",
      "    step: 350; loss: 2.187; l2dist: 1.361\n",
      "    step: 400; loss: 2.155; l2dist: 1.357\n",
      "    step: 450; loss: 2.110; l2dist: 1.339\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.691; l2dist: 0.000\n",
      "    step: 50; loss: 10.359; l2dist: 2.748\n",
      "    step: 100; loss: 4.104; l2dist: 1.830\n",
      "    step: 150; loss: 2.798; l2dist: 1.517\n",
      "    step: 200; loss: 2.388; l2dist: 1.404\n",
      "    step: 250; loss: 2.232; l2dist: 1.360\n",
      "    step: 300; loss: 2.119; l2dist: 1.324\n",
      "    step: 350; loss: 2.092; l2dist: 1.320\n",
      "    step: 400; loss: 2.063; l2dist: 1.311\n",
      "    step: 450; loss: 2.077; l2dist: 1.309\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.235; l2dist: 0.000\n",
      "    step: 50; loss: 8.994; l2dist: 2.535\n",
      "    step: 100; loss: 3.449; l2dist: 1.638\n",
      "    step: 150; loss: 2.474; l2dist: 1.387\n",
      "    step: 200; loss: 2.184; l2dist: 1.305\n",
      "    step: 250; loss: 2.070; l2dist: 1.271\n",
      "    step: 300; loss: 2.003; l2dist: 1.254\n",
      "    step: 350; loss: 1.970; l2dist: 1.250\n",
      "    step: 400; loss: 1.975; l2dist: 1.249\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.576; l2dist: 0.000\n",
      "    step: 50; loss: 8.362; l2dist: 2.431\n",
      "    step: 100; loss: 3.248; l2dist: 1.575\n",
      "    step: 150; loss: 2.409; l2dist: 1.369\n",
      "    step: 200; loss: 2.168; l2dist: 1.298\n",
      "    step: 250; loss: 2.121; l2dist: 1.280\n",
      "    step: 300; loss: 2.021; l2dist: 1.261\n",
      "    step: 350; loss: 1.995; l2dist: 1.257\n",
      "    step: 400; loss: 1.976; l2dist: 1.251\n",
      "    step: 450; loss: 1.958; l2dist: 1.252\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.163; l2dist: 0.000\n",
      "    step: 50; loss: 8.082; l2dist: 2.383\n",
      "    step: 100; loss: 3.140; l2dist: 1.535\n",
      "    step: 150; loss: 2.345; l2dist: 1.339\n",
      "    step: 200; loss: 2.140; l2dist: 1.283\n",
      "    step: 250; loss: 2.057; l2dist: 1.263\n",
      "    step: 300; loss: 1.992; l2dist: 1.251\n",
      "    step: 350; loss: 1.980; l2dist: 1.240\n",
      "    step: 400; loss: 1.957; l2dist: 1.242\n",
      "    step: 450; loss: 1.933; l2dist: 1.235\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.440; l2dist: 0.000\n",
      "    step: 50; loss: 7.944; l2dist: 2.347\n",
      "    step: 100; loss: 3.110; l2dist: 1.520\n",
      "    step: 150; loss: 2.331; l2dist: 1.333\n",
      "    step: 200; loss: 2.122; l2dist: 1.277\n",
      "    step: 250; loss: 2.101; l2dist: 1.266\n",
      "    step: 300; loss: 2.004; l2dist: 1.250\n",
      "    step: 350; loss: 1.981; l2dist: 1.245\n",
      "    step: 400; loss: 1.967; l2dist: 1.245\n",
      "    step: 450; loss: 1.958; l2dist: 1.248\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.631; l2dist: 0.000\n",
      "    step: 50; loss: 7.965; l2dist: 2.350\n",
      "    step: 100; loss: 3.128; l2dist: 1.530\n",
      "    step: 150; loss: 2.339; l2dist: 1.345\n",
      "    step: 200; loss: 2.134; l2dist: 1.289\n",
      "    step: 250; loss: 2.042; l2dist: 1.268\n",
      "    step: 300; loss: 2.003; l2dist: 1.255\n",
      "    step: 350; loss: 1.976; l2dist: 1.250\n",
      "    step: 400; loss: 1.968; l2dist: 1.249\n",
      "    step: 450; loss: 1.943; l2dist: 1.243\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 124.077; l2dist: 0.000\n",
      "    step: 50; loss: 13.976; l2dist: 3.454\n",
      "    step: 100; loss: 5.923; l2dist: 2.336\n",
      "    step: 150; loss: 3.915; l2dist: 1.879\n",
      "    step: 200; loss: 3.195; l2dist: 1.667\n",
      "    step: 250; loss: 2.782; l2dist: 1.560\n",
      "    step: 300; loss: 2.679; l2dist: 1.518\n",
      "    step: 350; loss: 2.621; l2dist: 1.496\n",
      "    step: 400; loss: 2.563; l2dist: 1.484\n",
      "    step: 450; loss: 2.499; l2dist: 1.466\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 97.366; l2dist: 0.000\n",
      "    step: 50; loss: 12.026; l2dist: 3.175\n",
      "    step: 100; loss: 4.948; l2dist: 2.087\n",
      "    step: 150; loss: 3.445; l2dist: 1.715\n",
      "    step: 200; loss: 2.962; l2dist: 1.576\n",
      "    step: 250; loss: 2.656; l2dist: 1.489\n",
      "    step: 300; loss: 2.571; l2dist: 1.479\n",
      "    step: 350; loss: 2.479; l2dist: 1.444\n",
      "    step: 400; loss: 2.440; l2dist: 1.434\n",
      "    step: 450; loss: 2.468; l2dist: 1.445\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.493; l2dist: 0.000\n",
      "    step: 50; loss: 9.412; l2dist: 2.734\n",
      "    step: 100; loss: 3.861; l2dist: 1.824\n",
      "    step: 150; loss: 2.883; l2dist: 1.557\n",
      "    step: 200; loss: 2.600; l2dist: 1.476\n",
      "    step: 250; loss: 2.465; l2dist: 1.440\n",
      "    step: 300; loss: 2.420; l2dist: 1.420\n",
      "    step: 350; loss: 2.355; l2dist: 1.409\n",
      "    step: 400; loss: 2.340; l2dist: 1.414\n",
      "    step: 450; loss: 2.310; l2dist: 1.400\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.191; l2dist: 0.000\n",
      "    step: 50; loss: 8.276; l2dist: 2.449\n",
      "    step: 100; loss: 3.549; l2dist: 1.736\n",
      "    step: 150; loss: 2.688; l2dist: 1.501\n",
      "    step: 200; loss: 2.478; l2dist: 1.444\n",
      "    step: 250; loss: 2.387; l2dist: 1.403\n",
      "    step: 300; loss: 2.333; l2dist: 1.399\n",
      "    step: 350; loss: 2.303; l2dist: 1.399\n",
      "    step: 400; loss: 2.297; l2dist: 1.387\n",
      "    step: 450; loss: 2.274; l2dist: 1.383\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.507; l2dist: 0.000\n",
      "    step: 50; loss: 7.329; l2dist: 2.207\n",
      "    step: 100; loss: 3.472; l2dist: 1.671\n",
      "    step: 150; loss: 2.632; l2dist: 1.477\n",
      "    step: 200; loss: 2.428; l2dist: 1.416\n",
      "    step: 250; loss: 2.343; l2dist: 1.396\n",
      "    step: 300; loss: 2.299; l2dist: 1.387\n",
      "    step: 350; loss: 2.281; l2dist: 1.377\n",
      "    step: 400; loss: 2.266; l2dist: 1.376\n",
      "    step: 450; loss: 2.262; l2dist: 1.370\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.467; l2dist: 0.000\n",
      "    step: 50; loss: 6.809; l2dist: 2.052\n",
      "    step: 100; loss: 3.387; l2dist: 1.566\n",
      "    step: 150; loss: 2.591; l2dist: 1.414\n",
      "    step: 200; loss: 2.385; l2dist: 1.378\n",
      "    step: 250; loss: 2.308; l2dist: 1.354\n",
      "    step: 300; loss: 2.273; l2dist: 1.347\n",
      "    step: 350; loss: 2.248; l2dist: 1.339\n",
      "    step: 400; loss: 2.234; l2dist: 1.341\n",
      "    step: 450; loss: 2.239; l2dist: 1.337\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.066; l2dist: 0.000\n",
      "    step: 50; loss: 6.957; l2dist: 2.069\n",
      "    step: 100; loss: 3.397; l2dist: 1.571\n",
      "    step: 150; loss: 2.596; l2dist: 1.411\n",
      "    step: 200; loss: 2.404; l2dist: 1.365\n",
      "    step: 250; loss: 2.326; l2dist: 1.354\n",
      "    step: 300; loss: 2.285; l2dist: 1.351\n",
      "    step: 350; loss: 2.260; l2dist: 1.338\n",
      "    step: 400; loss: 2.251; l2dist: 1.338\n",
      "    step: 450; loss: 2.240; l2dist: 1.335\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.598; l2dist: 0.000\n",
      "    step: 50; loss: 6.939; l2dist: 2.066\n",
      "    step: 100; loss: 3.417; l2dist: 1.584\n",
      "    step: 150; loss: 2.593; l2dist: 1.421\n",
      "    step: 200; loss: 2.401; l2dist: 1.376\n",
      "    step: 250; loss: 2.330; l2dist: 1.360\n",
      "    step: 300; loss: 2.291; l2dist: 1.347\n",
      "    step: 350; loss: 2.256; l2dist: 1.350\n",
      "    step: 400; loss: 2.260; l2dist: 1.346\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.419; l2dist: 0.000\n",
      "    step: 50; loss: 6.931; l2dist: 2.066\n",
      "    step: 100; loss: 3.412; l2dist: 1.580\n",
      "    step: 150; loss: 2.598; l2dist: 1.420\n",
      "    step: 200; loss: 2.406; l2dist: 1.382\n",
      "    step: 250; loss: 2.328; l2dist: 1.366\n",
      "    step: 300; loss: 2.279; l2dist: 1.353\n",
      "    step: 350; loss: 2.259; l2dist: 1.345\n",
      "    step: 400; loss: 2.259; l2dist: 1.350\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.601; l2dist: 0.000\n",
      "    step: 50; loss: 6.968; l2dist: 2.075\n",
      "    step: 100; loss: 3.418; l2dist: 1.591\n",
      "    step: 150; loss: 2.609; l2dist: 1.424\n",
      "    step: 200; loss: 2.406; l2dist: 1.387\n",
      "    step: 250; loss: 2.325; l2dist: 1.369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 2.285; l2dist: 1.359\n",
      "    step: 350; loss: 2.270; l2dist: 1.357\n",
      "    step: 400; loss: 2.254; l2dist: 1.356\n",
      "    step: 450; loss: 2.245; l2dist: 1.345\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 151.598; l2dist: 0.000\n",
      "    step: 50; loss: 16.331; l2dist: 3.816\n",
      "    step: 100; loss: 7.644; l2dist: 2.711\n",
      "    step: 150; loss: 5.117; l2dist: 2.202\n",
      "    step: 200; loss: 4.082; l2dist: 1.956\n",
      "    step: 250; loss: 3.647; l2dist: 1.841\n",
      "    step: 300; loss: 3.481; l2dist: 1.795\n",
      "    step: 350; loss: 3.411; l2dist: 1.772\n",
      "    step: 400; loss: 3.243; l2dist: 1.731\n",
      "    step: 450; loss: 3.222; l2dist: 1.724\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 179.844; l2dist: 0.000\n",
      "    step: 50; loss: 15.547; l2dist: 3.708\n",
      "    step: 100; loss: 7.336; l2dist: 2.613\n",
      "    step: 150; loss: 5.048; l2dist: 2.143\n",
      "    step: 200; loss: 4.191; l2dist: 1.942\n",
      "    step: 250; loss: 3.711; l2dist: 1.840\n",
      "    step: 300; loss: 3.475; l2dist: 1.783\n",
      "    step: 350; loss: 3.331; l2dist: 1.745\n",
      "    step: 400; loss: 3.211; l2dist: 1.723\n",
      "    step: 450; loss: 3.152; l2dist: 1.705\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 107.238; l2dist: 0.000\n",
      "    step: 50; loss: 12.553; l2dist: 3.299\n",
      "    step: 100; loss: 5.665; l2dist: 2.279\n",
      "    step: 150; loss: 4.158; l2dist: 1.944\n",
      "    step: 200; loss: 3.591; l2dist: 1.808\n",
      "    step: 250; loss: 3.290; l2dist: 1.746\n",
      "    step: 300; loss: 3.151; l2dist: 1.702\n",
      "    step: 350; loss: 3.056; l2dist: 1.678\n",
      "    step: 400; loss: 3.043; l2dist: 1.676\n",
      "    step: 450; loss: 2.998; l2dist: 1.660\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.333; l2dist: 0.000\n",
      "    step: 50; loss: 11.025; l2dist: 3.015\n",
      "    step: 100; loss: 5.102; l2dist: 2.155\n",
      "    step: 150; loss: 3.798; l2dist: 1.856\n",
      "    step: 200; loss: 3.339; l2dist: 1.748\n",
      "    step: 250; loss: 3.128; l2dist: 1.694\n",
      "    step: 300; loss: 3.036; l2dist: 1.674\n",
      "    step: 350; loss: 3.017; l2dist: 1.658\n",
      "    step: 400; loss: 2.963; l2dist: 1.647\n",
      "    step: 450; loss: 2.950; l2dist: 1.646\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.178; l2dist: 0.000\n",
      "    step: 50; loss: 10.109; l2dist: 2.852\n",
      "    step: 100; loss: 4.797; l2dist: 2.069\n",
      "    step: 150; loss: 3.639; l2dist: 1.814\n",
      "    step: 200; loss: 3.236; l2dist: 1.720\n",
      "    step: 250; loss: 3.088; l2dist: 1.672\n",
      "    step: 300; loss: 2.990; l2dist: 1.658\n",
      "    step: 350; loss: 2.945; l2dist: 1.644\n",
      "    step: 400; loss: 2.921; l2dist: 1.638\n",
      "    step: 450; loss: 2.920; l2dist: 1.641\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.706; l2dist: 0.000\n",
      "    step: 50; loss: 9.689; l2dist: 2.796\n",
      "    step: 100; loss: 4.585; l2dist: 2.032\n",
      "    step: 150; loss: 3.502; l2dist: 1.779\n",
      "    step: 200; loss: 3.163; l2dist: 1.694\n",
      "    step: 250; loss: 3.076; l2dist: 1.664\n",
      "    step: 300; loss: 2.991; l2dist: 1.642\n",
      "    step: 350; loss: 2.942; l2dist: 1.632\n",
      "    step: 400; loss: 2.915; l2dist: 1.629\n",
      "    step: 450; loss: 2.907; l2dist: 1.625\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.770; l2dist: 0.000\n",
      "    step: 50; loss: 9.536; l2dist: 2.764\n",
      "    step: 100; loss: 4.531; l2dist: 2.016\n",
      "    step: 150; loss: 3.490; l2dist: 1.776\n",
      "    step: 200; loss: 3.153; l2dist: 1.689\n",
      "    step: 250; loss: 3.021; l2dist: 1.662\n",
      "    step: 300; loss: 2.954; l2dist: 1.642\n",
      "    step: 350; loss: 2.920; l2dist: 1.630\n",
      "    step: 400; loss: 2.900; l2dist: 1.626\n",
      "    step: 450; loss: 2.889; l2dist: 1.623\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.567; l2dist: 0.000\n",
      "    step: 50; loss: 9.488; l2dist: 2.762\n",
      "    step: 100; loss: 4.523; l2dist: 2.013\n",
      "    step: 150; loss: 3.460; l2dist: 1.772\n",
      "    step: 200; loss: 3.124; l2dist: 1.690\n",
      "    step: 250; loss: 3.016; l2dist: 1.661\n",
      "    step: 300; loss: 2.957; l2dist: 1.647\n",
      "    step: 350; loss: 2.912; l2dist: 1.631\n",
      "    step: 400; loss: 2.905; l2dist: 1.628\n",
      "    step: 450; loss: 2.894; l2dist: 1.625\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.472; l2dist: 0.000\n",
      "    step: 50; loss: 9.428; l2dist: 2.747\n",
      "    step: 100; loss: 4.496; l2dist: 2.005\n",
      "    step: 150; loss: 3.477; l2dist: 1.771\n",
      "    step: 200; loss: 3.141; l2dist: 1.693\n",
      "    step: 250; loss: 3.022; l2dist: 1.654\n",
      "    step: 300; loss: 2.938; l2dist: 1.642\n",
      "    step: 350; loss: 2.916; l2dist: 1.639\n",
      "    step: 400; loss: 2.906; l2dist: 1.630\n",
      "    step: 450; loss: 2.900; l2dist: 1.629\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.616; l2dist: 0.000\n",
      "    step: 50; loss: 9.449; l2dist: 2.753\n",
      "    step: 100; loss: 4.511; l2dist: 2.008\n",
      "    step: 150; loss: 3.444; l2dist: 1.776\n",
      "    step: 200; loss: 3.142; l2dist: 1.691\n",
      "    step: 250; loss: 3.027; l2dist: 1.660\n",
      "    step: 300; loss: 2.956; l2dist: 1.649\n",
      "    step: 350; loss: 2.937; l2dist: 1.630\n",
      "    step: 400; loss: 2.907; l2dist: 1.629\n",
      "    step: 450; loss: 2.918; l2dist: 1.627\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 142.969; l2dist: 0.000\n",
      "    step: 50; loss: 16.328; l2dist: 3.742\n",
      "    step: 100; loss: 6.977; l2dist: 2.579\n",
      "    step: 150; loss: 4.699; l2dist: 2.088\n",
      "    step: 200; loss: 3.840; l2dist: 1.866\n",
      "    step: 250; loss: 3.386; l2dist: 1.752\n",
      "    step: 300; loss: 3.161; l2dist: 1.686\n",
      "    step: 350; loss: 3.065; l2dist: 1.652\n",
      "    step: 400; loss: 3.013; l2dist: 1.642\n",
      "    step: 450; loss: 3.022; l2dist: 1.622\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 169.547; l2dist: 0.000\n",
      "    step: 50; loss: 16.445; l2dist: 3.682\n",
      "    step: 100; loss: 7.282; l2dist: 2.560\n",
      "    step: 150; loss: 5.038; l2dist: 2.099\n",
      "    step: 200; loss: 4.021; l2dist: 1.875\n",
      "    step: 250; loss: 3.636; l2dist: 1.775\n",
      "    step: 300; loss: 3.321; l2dist: 1.708\n",
      "    step: 350; loss: 3.257; l2dist: 1.688\n",
      "    step: 400; loss: 3.058; l2dist: 1.641\n",
      "    step: 450; loss: 3.014; l2dist: 1.629\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 93.507; l2dist: 0.000\n",
      "    step: 50; loss: 12.312; l2dist: 3.134\n",
      "    step: 100; loss: 5.326; l2dist: 2.171\n",
      "    step: 150; loss: 3.907; l2dist: 1.851\n",
      "    step: 200; loss: 3.349; l2dist: 1.716\n",
      "    step: 250; loss: 3.103; l2dist: 1.652\n",
      "    step: 300; loss: 2.994; l2dist: 1.633\n",
      "    step: 350; loss: 2.882; l2dist: 1.595\n",
      "    step: 400; loss: 2.843; l2dist: 1.589\n",
      "    step: 450; loss: 2.844; l2dist: 1.585\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.471; l2dist: 0.000\n",
      "    step: 50; loss: 10.093; l2dist: 2.734\n",
      "    step: 100; loss: 4.609; l2dist: 2.005\n",
      "    step: 150; loss: 3.486; l2dist: 1.748\n",
      "    step: 200; loss: 3.087; l2dist: 1.647\n",
      "    step: 250; loss: 2.946; l2dist: 1.607\n",
      "    step: 300; loss: 2.845; l2dist: 1.587\n",
      "    step: 350; loss: 2.805; l2dist: 1.576\n",
      "    step: 400; loss: 2.805; l2dist: 1.577\n",
      "    step: 450; loss: 2.779; l2dist: 1.574\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.106; l2dist: 0.000\n",
      "    step: 50; loss: 9.048; l2dist: 2.541\n",
      "    step: 100; loss: 4.357; l2dist: 1.913\n",
      "    step: 150; loss: 3.313; l2dist: 1.679\n",
      "    step: 200; loss: 3.002; l2dist: 1.605\n",
      "    step: 250; loss: 2.862; l2dist: 1.578\n",
      "    step: 300; loss: 2.795; l2dist: 1.563\n",
      "    step: 350; loss: 2.769; l2dist: 1.550\n",
      "    step: 400; loss: 2.790; l2dist: 1.565\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.275; l2dist: 0.000\n",
      "    step: 50; loss: 8.576; l2dist: 2.442\n",
      "    step: 100; loss: 4.244; l2dist: 1.877\n",
      "    step: 150; loss: 3.199; l2dist: 1.653\n",
      "    step: 200; loss: 2.927; l2dist: 1.585\n",
      "    step: 250; loss: 2.797; l2dist: 1.557\n",
      "    step: 300; loss: 2.794; l2dist: 1.553\n",
      "    step: 350; loss: 2.733; l2dist: 1.537\n",
      "    step: 400; loss: 2.722; l2dist: 1.540\n",
      "    step: 450; loss: 2.719; l2dist: 1.534\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.677; l2dist: 0.000\n",
      "    step: 50; loss: 8.385; l2dist: 2.407\n",
      "    step: 100; loss: 4.153; l2dist: 1.856\n",
      "    step: 150; loss: 3.154; l2dist: 1.640\n",
      "    step: 200; loss: 2.887; l2dist: 1.570\n",
      "    step: 250; loss: 2.781; l2dist: 1.542\n",
      "    step: 300; loss: 2.737; l2dist: 1.532\n",
      "    step: 350; loss: 2.699; l2dist: 1.522\n",
      "    step: 400; loss: 2.676; l2dist: 1.517\n",
      "    step: 450; loss: 2.698; l2dist: 1.515\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.549; l2dist: 0.000\n",
      "    step: 50; loss: 8.375; l2dist: 2.404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 4.163; l2dist: 1.860\n",
      "    step: 150; loss: 3.172; l2dist: 1.640\n",
      "    step: 200; loss: 2.909; l2dist: 1.575\n",
      "    step: 250; loss: 2.791; l2dist: 1.550\n",
      "    step: 300; loss: 2.758; l2dist: 1.539\n",
      "    step: 350; loss: 2.728; l2dist: 1.534\n",
      "    step: 400; loss: 2.692; l2dist: 1.530\n",
      "    step: 450; loss: 2.683; l2dist: 1.525\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.896; l2dist: 0.000\n",
      "    step: 50; loss: 8.369; l2dist: 2.394\n",
      "    step: 100; loss: 4.145; l2dist: 1.857\n",
      "    step: 150; loss: 3.169; l2dist: 1.639\n",
      "    step: 200; loss: 2.916; l2dist: 1.577\n",
      "    step: 250; loss: 2.805; l2dist: 1.559\n",
      "    step: 300; loss: 2.754; l2dist: 1.544\n",
      "    step: 350; loss: 2.731; l2dist: 1.534\n",
      "    step: 400; loss: 2.705; l2dist: 1.530\n",
      "    step: 450; loss: 2.691; l2dist: 1.526\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.112; l2dist: 0.000\n",
      "    step: 50; loss: 8.411; l2dist: 2.405\n",
      "    step: 100; loss: 4.134; l2dist: 1.856\n",
      "    step: 150; loss: 3.146; l2dist: 1.643\n",
      "    step: 200; loss: 2.896; l2dist: 1.583\n",
      "    step: 250; loss: 2.807; l2dist: 1.567\n",
      "    step: 300; loss: 2.752; l2dist: 1.544\n",
      "    step: 350; loss: 2.731; l2dist: 1.542\n",
      "    step: 400; loss: 2.712; l2dist: 1.540\n",
      "    step: 450; loss: 2.699; l2dist: 1.529\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 138.363; l2dist: 0.000\n",
      "    step: 50; loss: 16.318; l2dist: 3.668\n",
      "    step: 100; loss: 6.683; l2dist: 2.495\n",
      "    step: 150; loss: 4.466; l2dist: 2.021\n",
      "    step: 200; loss: 3.582; l2dist: 1.796\n",
      "    step: 250; loss: 3.225; l2dist: 1.691\n",
      "    step: 300; loss: 2.970; l2dist: 1.624\n",
      "    step: 350; loss: 2.871; l2dist: 1.600\n",
      "    step: 400; loss: 2.787; l2dist: 1.571\n",
      "    step: 450; loss: 2.787; l2dist: 1.576\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 93.593; l2dist: 0.000\n",
      "    step: 50; loss: 13.073; l2dist: 3.235\n",
      "    step: 100; loss: 5.192; l2dist: 2.165\n",
      "    step: 150; loss: 3.666; l2dist: 1.801\n",
      "    step: 200; loss: 3.151; l2dist: 1.670\n",
      "    step: 250; loss: 2.879; l2dist: 1.594\n",
      "    step: 300; loss: 2.772; l2dist: 1.563\n",
      "    step: 350; loss: 2.733; l2dist: 1.546\n",
      "    step: 400; loss: 2.707; l2dist: 1.547\n",
      "    step: 450; loss: 2.674; l2dist: 1.534\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.037; l2dist: 0.000\n",
      "    step: 50; loss: 10.281; l2dist: 2.780\n",
      "    step: 100; loss: 4.193; l2dist: 1.932\n",
      "    step: 150; loss: 3.109; l2dist: 1.653\n",
      "    step: 200; loss: 2.857; l2dist: 1.581\n",
      "    step: 250; loss: 2.713; l2dist: 1.543\n",
      "    step: 300; loss: 2.647; l2dist: 1.530\n",
      "    step: 350; loss: 2.637; l2dist: 1.526\n",
      "    step: 400; loss: 2.628; l2dist: 1.518\n",
      "    step: 450; loss: 2.600; l2dist: 1.514\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.874; l2dist: 0.000\n",
      "    step: 50; loss: 9.061; l2dist: 2.514\n",
      "    step: 100; loss: 3.939; l2dist: 1.856\n",
      "    step: 150; loss: 2.970; l2dist: 1.621\n",
      "    step: 200; loss: 2.724; l2dist: 1.545\n",
      "    step: 250; loss: 2.606; l2dist: 1.514\n",
      "    step: 300; loss: 2.583; l2dist: 1.498\n",
      "    step: 350; loss: 2.545; l2dist: 1.495\n",
      "    step: 400; loss: 2.541; l2dist: 1.488\n",
      "    step: 450; loss: 2.531; l2dist: 1.488\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.626; l2dist: 0.000\n",
      "    step: 50; loss: 8.250; l2dist: 2.318\n",
      "    step: 100; loss: 3.890; l2dist: 1.778\n",
      "    step: 150; loss: 2.925; l2dist: 1.584\n",
      "    step: 200; loss: 2.700; l2dist: 1.523\n",
      "    step: 250; loss: 2.611; l2dist: 1.504\n",
      "    step: 300; loss: 2.564; l2dist: 1.488\n",
      "    step: 350; loss: 2.535; l2dist: 1.485\n",
      "    step: 400; loss: 2.528; l2dist: 1.479\n",
      "    step: 450; loss: 2.521; l2dist: 1.473\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.362; l2dist: 0.000\n",
      "    step: 50; loss: 7.939; l2dist: 2.232\n",
      "    step: 100; loss: 3.738; l2dist: 1.722\n",
      "    step: 150; loss: 2.868; l2dist: 1.539\n",
      "    step: 200; loss: 2.666; l2dist: 1.489\n",
      "    step: 250; loss: 2.573; l2dist: 1.469\n",
      "    step: 300; loss: 2.530; l2dist: 1.464\n",
      "    step: 350; loss: 2.507; l2dist: 1.460\n",
      "    step: 400; loss: 2.502; l2dist: 1.462\n",
      "    step: 450; loss: 2.511; l2dist: 1.457\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.442; l2dist: 0.000\n",
      "    step: 50; loss: 7.916; l2dist: 2.214\n",
      "    step: 100; loss: 3.758; l2dist: 1.728\n",
      "    step: 150; loss: 2.874; l2dist: 1.554\n",
      "    step: 200; loss: 2.648; l2dist: 1.500\n",
      "    step: 250; loss: 2.571; l2dist: 1.478\n",
      "    step: 300; loss: 2.530; l2dist: 1.471\n",
      "    step: 350; loss: 2.519; l2dist: 1.477\n",
      "    step: 400; loss: 2.505; l2dist: 1.472\n",
      "    step: 450; loss: 2.488; l2dist: 1.461\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.799; l2dist: 0.000\n",
      "    step: 50; loss: 7.852; l2dist: 2.200\n",
      "    step: 100; loss: 3.779; l2dist: 1.728\n",
      "    step: 150; loss: 2.873; l2dist: 1.556\n",
      "    step: 200; loss: 2.658; l2dist: 1.511\n",
      "    step: 250; loss: 2.573; l2dist: 1.489\n",
      "    step: 300; loss: 2.546; l2dist: 1.470\n",
      "    step: 350; loss: 2.522; l2dist: 1.470\n",
      "    step: 400; loss: 2.505; l2dist: 1.468\n",
      "    step: 450; loss: 2.502; l2dist: 1.470\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.446; l2dist: 0.000\n",
      "    step: 50; loss: 7.835; l2dist: 2.185\n",
      "    step: 100; loss: 3.758; l2dist: 1.721\n",
      "    step: 150; loss: 2.866; l2dist: 1.550\n",
      "    step: 200; loss: 2.644; l2dist: 1.501\n",
      "    step: 250; loss: 2.573; l2dist: 1.478\n",
      "    step: 300; loss: 2.533; l2dist: 1.476\n",
      "    step: 350; loss: 2.506; l2dist: 1.468\n",
      "    step: 400; loss: 2.506; l2dist: 1.467\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.672; l2dist: 0.000\n",
      "    step: 50; loss: 7.878; l2dist: 2.197\n",
      "    step: 100; loss: 3.759; l2dist: 1.728\n",
      "    step: 150; loss: 2.877; l2dist: 1.554\n",
      "    step: 200; loss: 2.647; l2dist: 1.504\n",
      "    step: 250; loss: 2.568; l2dist: 1.487\n",
      "    step: 300; loss: 2.533; l2dist: 1.483\n",
      "    step: 350; loss: 2.516; l2dist: 1.469\n",
      "    step: 400; loss: 2.500; l2dist: 1.470\n",
      "    step: 450; loss: 2.495; l2dist: 1.467\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 159.874; l2dist: 0.000\n",
      "    step: 50; loss: 16.258; l2dist: 3.900\n",
      "    step: 100; loss: 8.104; l2dist: 2.809\n",
      "    step: 150; loss: 5.427; l2dist: 2.294\n",
      "    step: 200; loss: 4.378; l2dist: 2.047\n",
      "    step: 250; loss: 3.876; l2dist: 1.918\n",
      "    step: 300; loss: 3.610; l2dist: 1.851\n",
      "    step: 350; loss: 3.517; l2dist: 1.830\n",
      "    step: 400; loss: 3.392; l2dist: 1.797\n",
      "    step: 450; loss: 3.442; l2dist: 1.790\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 183.371; l2dist: 0.000\n",
      "    step: 50; loss: 15.998; l2dist: 3.778\n",
      "    step: 100; loss: 7.775; l2dist: 2.691\n",
      "    step: 150; loss: 5.382; l2dist: 2.223\n",
      "    step: 200; loss: 4.444; l2dist: 2.003\n",
      "    step: 250; loss: 3.894; l2dist: 1.901\n",
      "    step: 300; loss: 3.641; l2dist: 1.835\n",
      "    step: 350; loss: 3.473; l2dist: 1.800\n",
      "    step: 400; loss: 3.426; l2dist: 1.790\n",
      "    step: 450; loss: 3.400; l2dist: 1.788\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 100.596; l2dist: 0.000\n",
      "    step: 50; loss: 12.567; l2dist: 3.313\n",
      "    step: 100; loss: 5.716; l2dist: 2.301\n",
      "    step: 150; loss: 4.255; l2dist: 1.974\n",
      "    step: 200; loss: 3.694; l2dist: 1.851\n",
      "    step: 250; loss: 3.447; l2dist: 1.786\n",
      "    step: 300; loss: 3.351; l2dist: 1.769\n",
      "    step: 350; loss: 3.318; l2dist: 1.751\n",
      "    step: 400; loss: 3.253; l2dist: 1.743\n",
      "    step: 450; loss: 3.221; l2dist: 1.728\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.421; l2dist: 0.000\n",
      "    step: 50; loss: 10.503; l2dist: 2.905\n",
      "    step: 100; loss: 4.922; l2dist: 2.132\n",
      "    step: 150; loss: 3.704; l2dist: 1.857\n",
      "    step: 200; loss: 3.376; l2dist: 1.776\n",
      "    step: 250; loss: 3.239; l2dist: 1.740\n",
      "    step: 300; loss: 3.161; l2dist: 1.718\n",
      "    step: 350; loss: 3.160; l2dist: 1.714\n",
      "    step: 400; loss: 3.098; l2dist: 1.701\n",
      "    step: 450; loss: 3.134; l2dist: 1.711\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.669; l2dist: 0.000\n",
      "    step: 50; loss: 9.128; l2dist: 2.630\n",
      "    step: 100; loss: 4.540; l2dist: 1.974\n",
      "    step: 150; loss: 3.538; l2dist: 1.786\n",
      "    step: 200; loss: 3.269; l2dist: 1.721\n",
      "    step: 250; loss: 3.145; l2dist: 1.703\n",
      "    step: 300; loss: 3.079; l2dist: 1.684\n",
      "    step: 350; loss: 3.056; l2dist: 1.678\n",
      "    step: 400; loss: 3.063; l2dist: 1.679\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.349; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 8.835; l2dist: 2.569\n",
      "    step: 100; loss: 4.438; l2dist: 1.956\n",
      "    step: 150; loss: 3.471; l2dist: 1.767\n",
      "    step: 200; loss: 3.220; l2dist: 1.706\n",
      "    step: 250; loss: 3.132; l2dist: 1.684\n",
      "    step: 300; loss: 3.127; l2dist: 1.681\n",
      "    step: 350; loss: 3.078; l2dist: 1.679\n",
      "    step: 400; loss: 3.040; l2dist: 1.669\n",
      "    step: 450; loss: 3.039; l2dist: 1.674\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.431; l2dist: 0.000\n",
      "    step: 50; loss: 8.655; l2dist: 2.541\n",
      "    step: 100; loss: 4.425; l2dist: 1.947\n",
      "    step: 150; loss: 3.443; l2dist: 1.763\n",
      "    step: 200; loss: 3.206; l2dist: 1.716\n",
      "    step: 250; loss: 3.108; l2dist: 1.690\n",
      "    step: 300; loss: 3.090; l2dist: 1.692\n",
      "    step: 350; loss: 3.046; l2dist: 1.679\n",
      "    step: 400; loss: 3.044; l2dist: 1.668\n",
      "    step: 450; loss: 3.034; l2dist: 1.667\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.547; l2dist: 0.000\n",
      "    step: 50; loss: 8.593; l2dist: 2.527\n",
      "    step: 100; loss: 4.424; l2dist: 1.941\n",
      "    step: 150; loss: 3.440; l2dist: 1.761\n",
      "    step: 200; loss: 3.201; l2dist: 1.711\n",
      "    step: 250; loss: 3.111; l2dist: 1.685\n",
      "    step: 300; loss: 3.063; l2dist: 1.672\n",
      "    step: 350; loss: 3.050; l2dist: 1.670\n",
      "    step: 400; loss: 3.037; l2dist: 1.673\n",
      "    step: 450; loss: 3.016; l2dist: 1.664\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.698; l2dist: 0.000\n",
      "    step: 50; loss: 8.538; l2dist: 2.518\n",
      "    step: 100; loss: 4.420; l2dist: 1.935\n",
      "    step: 150; loss: 3.442; l2dist: 1.762\n",
      "    step: 200; loss: 3.200; l2dist: 1.703\n",
      "    step: 250; loss: 3.101; l2dist: 1.690\n",
      "    step: 300; loss: 3.068; l2dist: 1.680\n",
      "    step: 350; loss: 3.038; l2dist: 1.670\n",
      "    step: 400; loss: 3.043; l2dist: 1.676\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.881; l2dist: 0.000\n",
      "    step: 50; loss: 8.574; l2dist: 2.528\n",
      "    step: 100; loss: 4.424; l2dist: 1.944\n",
      "    step: 150; loss: 3.430; l2dist: 1.765\n",
      "    step: 200; loss: 3.200; l2dist: 1.713\n",
      "    step: 250; loss: 3.102; l2dist: 1.692\n",
      "    step: 300; loss: 3.071; l2dist: 1.682\n",
      "    step: 350; loss: 3.057; l2dist: 1.684\n",
      "    step: 400; loss: 3.026; l2dist: 1.675\n",
      "    step: 450; loss: 3.018; l2dist: 1.672\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 153.649; l2dist: 0.000\n",
      "    step: 50; loss: 16.917; l2dist: 3.873\n",
      "    step: 100; loss: 7.497; l2dist: 2.684\n",
      "    step: 150; loss: 5.107; l2dist: 2.194\n",
      "    step: 200; loss: 4.121; l2dist: 1.959\n",
      "    step: 250; loss: 3.706; l2dist: 1.852\n",
      "    step: 300; loss: 3.478; l2dist: 1.788\n",
      "    step: 350; loss: 3.365; l2dist: 1.751\n",
      "    step: 400; loss: 3.275; l2dist: 1.726\n",
      "    step: 450; loss: 3.266; l2dist: 1.725\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 90.283; l2dist: 0.000\n",
      "    step: 50; loss: 13.002; l2dist: 3.332\n",
      "    step: 100; loss: 5.504; l2dist: 2.266\n",
      "    step: 150; loss: 4.014; l2dist: 1.926\n",
      "    step: 200; loss: 3.533; l2dist: 1.790\n",
      "    step: 250; loss: 3.315; l2dist: 1.737\n",
      "    step: 300; loss: 3.218; l2dist: 1.704\n",
      "    step: 350; loss: 3.130; l2dist: 1.689\n",
      "    step: 400; loss: 3.119; l2dist: 1.679\n",
      "    step: 450; loss: 3.112; l2dist: 1.682\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.119; l2dist: 0.000\n",
      "    step: 50; loss: 10.524; l2dist: 2.904\n",
      "    step: 100; loss: 4.612; l2dist: 2.060\n",
      "    step: 150; loss: 3.564; l2dist: 1.806\n",
      "    step: 200; loss: 3.245; l2dist: 1.719\n",
      "    step: 250; loss: 3.117; l2dist: 1.676\n",
      "    step: 300; loss: 3.073; l2dist: 1.670\n",
      "    step: 350; loss: 3.044; l2dist: 1.669\n",
      "    step: 400; loss: 3.026; l2dist: 1.665\n",
      "    step: 450; loss: 3.032; l2dist: 1.656\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.717; l2dist: 0.000\n",
      "    step: 50; loss: 9.457; l2dist: 2.678\n",
      "    step: 100; loss: 4.456; l2dist: 2.008\n",
      "    step: 150; loss: 3.476; l2dist: 1.775\n",
      "    step: 200; loss: 3.189; l2dist: 1.701\n",
      "    step: 250; loss: 3.093; l2dist: 1.673\n",
      "    step: 300; loss: 3.052; l2dist: 1.657\n",
      "    step: 350; loss: 3.021; l2dist: 1.657\n",
      "    step: 400; loss: 3.009; l2dist: 1.642\n",
      "    step: 450; loss: 2.981; l2dist: 1.639\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.402; l2dist: 0.000\n",
      "    step: 50; loss: 8.732; l2dist: 2.532\n",
      "    step: 100; loss: 4.390; l2dist: 1.926\n",
      "    step: 150; loss: 3.429; l2dist: 1.734\n",
      "    step: 200; loss: 3.158; l2dist: 1.664\n",
      "    step: 250; loss: 3.051; l2dist: 1.641\n",
      "    step: 300; loss: 3.013; l2dist: 1.630\n",
      "    step: 350; loss: 2.988; l2dist: 1.627\n",
      "    step: 400; loss: 2.955; l2dist: 1.622\n",
      "    step: 450; loss: 2.947; l2dist: 1.623\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.067; l2dist: 0.000\n",
      "    step: 50; loss: 8.472; l2dist: 2.467\n",
      "    step: 100; loss: 4.321; l2dist: 1.898\n",
      "    step: 150; loss: 3.395; l2dist: 1.703\n",
      "    step: 200; loss: 3.133; l2dist: 1.642\n",
      "    step: 250; loss: 3.026; l2dist: 1.625\n",
      "    step: 300; loss: 2.985; l2dist: 1.611\n",
      "    step: 350; loss: 2.968; l2dist: 1.609\n",
      "    step: 400; loss: 2.961; l2dist: 1.610\n",
      "    step: 450; loss: 2.943; l2dist: 1.609\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.764; l2dist: 0.000\n",
      "    step: 50; loss: 8.422; l2dist: 2.445\n",
      "    step: 100; loss: 4.341; l2dist: 1.894\n",
      "    step: 150; loss: 3.394; l2dist: 1.704\n",
      "    step: 200; loss: 3.126; l2dist: 1.648\n",
      "    step: 250; loss: 3.033; l2dist: 1.633\n",
      "    step: 300; loss: 2.999; l2dist: 1.619\n",
      "    step: 350; loss: 2.972; l2dist: 1.618\n",
      "    step: 400; loss: 2.946; l2dist: 1.613\n",
      "    step: 450; loss: 2.947; l2dist: 1.618\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.062; l2dist: 0.000\n",
      "    step: 50; loss: 8.369; l2dist: 2.430\n",
      "    step: 100; loss: 4.349; l2dist: 1.898\n",
      "    step: 150; loss: 3.405; l2dist: 1.712\n",
      "    step: 200; loss: 3.124; l2dist: 1.653\n",
      "    step: 250; loss: 3.033; l2dist: 1.634\n",
      "    step: 300; loss: 2.982; l2dist: 1.621\n",
      "    step: 350; loss: 2.969; l2dist: 1.624\n",
      "    step: 400; loss: 2.959; l2dist: 1.617\n",
      "    step: 450; loss: 2.949; l2dist: 1.614\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.917; l2dist: 0.000\n",
      "    step: 50; loss: 8.379; l2dist: 2.432\n",
      "    step: 100; loss: 4.354; l2dist: 1.901\n",
      "    step: 150; loss: 3.396; l2dist: 1.719\n",
      "    step: 200; loss: 3.118; l2dist: 1.649\n",
      "    step: 250; loss: 3.035; l2dist: 1.631\n",
      "    step: 300; loss: 3.011; l2dist: 1.626\n",
      "    step: 350; loss: 2.965; l2dist: 1.627\n",
      "    step: 400; loss: 2.948; l2dist: 1.616\n",
      "    step: 450; loss: 2.937; l2dist: 1.619\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.091; l2dist: 0.000\n",
      "    step: 50; loss: 8.412; l2dist: 2.441\n",
      "    step: 100; loss: 4.357; l2dist: 1.905\n",
      "    step: 150; loss: 3.404; l2dist: 1.720\n",
      "    step: 200; loss: 3.138; l2dist: 1.659\n",
      "    step: 250; loss: 3.021; l2dist: 1.635\n",
      "    step: 300; loss: 2.988; l2dist: 1.625\n",
      "    step: 350; loss: 2.961; l2dist: 1.623\n",
      "    step: 400; loss: 2.960; l2dist: 1.619\n",
      "    step: 450; loss: 2.946; l2dist: 1.623\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 137.989; l2dist: 0.000\n",
      "    step: 50; loss: 17.420; l2dist: 3.640\n",
      "    step: 100; loss: 6.885; l2dist: 2.554\n",
      "    step: 150; loss: 4.662; l2dist: 2.088\n",
      "    step: 200; loss: 3.813; l2dist: 1.872\n",
      "    step: 250; loss: 3.396; l2dist: 1.768\n",
      "    step: 300; loss: 3.187; l2dist: 1.711\n",
      "    step: 350; loss: 3.105; l2dist: 1.675\n",
      "    step: 400; loss: 3.090; l2dist: 1.683\n",
      "    step: 450; loss: 3.013; l2dist: 1.656\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 127.663; l2dist: 0.000\n",
      "    step: 50; loss: 15.004; l2dist: 3.361\n",
      "    step: 100; loss: 6.071; l2dist: 2.339\n",
      "    step: 150; loss: 4.261; l2dist: 1.951\n",
      "    step: 200; loss: 3.590; l2dist: 1.788\n",
      "    step: 250; loss: 3.251; l2dist: 1.701\n",
      "    step: 300; loss: 3.092; l2dist: 1.668\n",
      "    step: 350; loss: 2.966; l2dist: 1.646\n",
      "    step: 400; loss: 2.918; l2dist: 1.625\n",
      "    step: 450; loss: 2.831; l2dist: 1.606\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.739; l2dist: 0.000\n",
      "    step: 50; loss: 11.707; l2dist: 2.962\n",
      "    step: 100; loss: 4.782; l2dist: 2.076\n",
      "    step: 150; loss: 3.544; l2dist: 1.782\n",
      "    step: 200; loss: 3.103; l2dist: 1.671\n",
      "    step: 250; loss: 2.926; l2dist: 1.627\n",
      "    step: 300; loss: 2.884; l2dist: 1.611\n",
      "    step: 350; loss: 2.839; l2dist: 1.605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 400; loss: 2.812; l2dist: 1.575\n",
      "    step: 450; loss: 2.755; l2dist: 1.578\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.332; l2dist: 0.000\n",
      "    step: 50; loss: 10.042; l2dist: 2.665\n",
      "    step: 100; loss: 4.396; l2dist: 1.967\n",
      "    step: 150; loss: 3.280; l2dist: 1.712\n",
      "    step: 200; loss: 2.937; l2dist: 1.625\n",
      "    step: 250; loss: 2.832; l2dist: 1.589\n",
      "    step: 300; loss: 2.766; l2dist: 1.568\n",
      "    step: 350; loss: 2.718; l2dist: 1.563\n",
      "    step: 400; loss: 2.672; l2dist: 1.554\n",
      "    step: 450; loss: 2.669; l2dist: 1.553\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.372; l2dist: 0.000\n",
      "    step: 50; loss: 9.250; l2dist: 2.509\n",
      "    step: 100; loss: 4.161; l2dist: 1.891\n",
      "    step: 150; loss: 3.149; l2dist: 1.660\n",
      "    step: 200; loss: 2.886; l2dist: 1.598\n",
      "    step: 250; loss: 2.776; l2dist: 1.570\n",
      "    step: 300; loss: 2.739; l2dist: 1.563\n",
      "    step: 350; loss: 2.683; l2dist: 1.552\n",
      "    step: 400; loss: 2.690; l2dist: 1.549\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.022; l2dist: 0.000\n",
      "    step: 50; loss: 9.018; l2dist: 2.456\n",
      "    step: 100; loss: 4.098; l2dist: 1.859\n",
      "    step: 150; loss: 3.106; l2dist: 1.644\n",
      "    step: 200; loss: 2.861; l2dist: 1.587\n",
      "    step: 250; loss: 2.766; l2dist: 1.559\n",
      "    step: 300; loss: 2.716; l2dist: 1.552\n",
      "    step: 350; loss: 2.691; l2dist: 1.549\n",
      "    step: 400; loss: 2.688; l2dist: 1.540\n",
      "    step: 450; loss: 2.662; l2dist: 1.540\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.754; l2dist: 0.000\n",
      "    step: 50; loss: 8.870; l2dist: 2.439\n",
      "    step: 100; loss: 4.047; l2dist: 1.853\n",
      "    step: 150; loss: 3.088; l2dist: 1.641\n",
      "    step: 200; loss: 2.861; l2dist: 1.587\n",
      "    step: 250; loss: 2.762; l2dist: 1.566\n",
      "    step: 300; loss: 2.709; l2dist: 1.547\n",
      "    step: 350; loss: 2.677; l2dist: 1.540\n",
      "    step: 400; loss: 2.664; l2dist: 1.541\n",
      "    step: 450; loss: 2.668; l2dist: 1.537\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.117; l2dist: 0.000\n",
      "    step: 50; loss: 8.830; l2dist: 2.431\n",
      "    step: 100; loss: 4.059; l2dist: 1.860\n",
      "    step: 150; loss: 3.104; l2dist: 1.657\n",
      "    step: 200; loss: 2.845; l2dist: 1.592\n",
      "    step: 250; loss: 2.754; l2dist: 1.567\n",
      "    step: 300; loss: 2.719; l2dist: 1.559\n",
      "    step: 350; loss: 2.700; l2dist: 1.547\n",
      "    step: 400; loss: 2.691; l2dist: 1.557\n",
      "    step: 450; loss: 2.678; l2dist: 1.551\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.438; l2dist: 0.000\n",
      "    step: 50; loss: 8.780; l2dist: 2.420\n",
      "    step: 100; loss: 4.043; l2dist: 1.858\n",
      "    step: 150; loss: 3.078; l2dist: 1.649\n",
      "    step: 200; loss: 2.841; l2dist: 1.591\n",
      "    step: 250; loss: 2.742; l2dist: 1.566\n",
      "    step: 300; loss: 2.706; l2dist: 1.553\n",
      "    step: 350; loss: 2.691; l2dist: 1.547\n",
      "    step: 400; loss: 2.665; l2dist: 1.543\n",
      "    step: 450; loss: 2.673; l2dist: 1.545\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.629; l2dist: 0.000\n",
      "    step: 50; loss: 8.806; l2dist: 2.428\n",
      "    step: 100; loss: 4.052; l2dist: 1.862\n",
      "    step: 150; loss: 3.086; l2dist: 1.654\n",
      "    step: 200; loss: 2.829; l2dist: 1.586\n",
      "    step: 250; loss: 2.750; l2dist: 1.565\n",
      "    step: 300; loss: 2.710; l2dist: 1.555\n",
      "    step: 350; loss: 2.679; l2dist: 1.549\n",
      "    step: 400; loss: 2.691; l2dist: 1.549\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 125.501; l2dist: 0.000\n",
      "    step: 50; loss: 14.720; l2dist: 3.439\n",
      "    step: 100; loss: 6.200; l2dist: 2.397\n",
      "    step: 150; loss: 4.097; l2dist: 1.925\n",
      "    step: 200; loss: 3.262; l2dist: 1.691\n",
      "    step: 250; loss: 2.906; l2dist: 1.592\n",
      "    step: 300; loss: 2.712; l2dist: 1.537\n",
      "    step: 350; loss: 2.639; l2dist: 1.512\n",
      "    step: 400; loss: 2.590; l2dist: 1.493\n",
      "    step: 450; loss: 2.568; l2dist: 1.489\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 88.230; l2dist: 0.000\n",
      "    step: 50; loss: 11.680; l2dist: 3.038\n",
      "    step: 100; loss: 4.751; l2dist: 2.046\n",
      "    step: 150; loss: 3.301; l2dist: 1.676\n",
      "    step: 200; loss: 2.821; l2dist: 1.552\n",
      "    step: 250; loss: 2.632; l2dist: 1.497\n",
      "    step: 300; loss: 2.503; l2dist: 1.464\n",
      "    step: 350; loss: 2.411; l2dist: 1.435\n",
      "    step: 400; loss: 2.364; l2dist: 1.423\n",
      "    step: 450; loss: 2.378; l2dist: 1.420\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.862; l2dist: 0.000\n",
      "    step: 50; loss: 9.346; l2dist: 2.648\n",
      "    step: 100; loss: 3.763; l2dist: 1.808\n",
      "    step: 150; loss: 2.831; l2dist: 1.548\n",
      "    step: 200; loss: 2.559; l2dist: 1.465\n",
      "    step: 250; loss: 2.426; l2dist: 1.440\n",
      "    step: 300; loss: 2.379; l2dist: 1.424\n",
      "    step: 350; loss: 2.342; l2dist: 1.419\n",
      "    step: 400; loss: 2.339; l2dist: 1.415\n",
      "    step: 450; loss: 2.301; l2dist: 1.407\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.197; l2dist: 0.000\n",
      "    step: 50; loss: 7.918; l2dist: 2.371\n",
      "    step: 100; loss: 3.421; l2dist: 1.719\n",
      "    step: 150; loss: 2.646; l2dist: 1.493\n",
      "    step: 200; loss: 2.430; l2dist: 1.445\n",
      "    step: 250; loss: 2.351; l2dist: 1.420\n",
      "    step: 300; loss: 2.291; l2dist: 1.399\n",
      "    step: 350; loss: 2.287; l2dist: 1.392\n",
      "    step: 400; loss: 2.268; l2dist: 1.389\n",
      "    step: 450; loss: 2.246; l2dist: 1.385\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.581; l2dist: 0.000\n",
      "    step: 50; loss: 7.514; l2dist: 2.267\n",
      "    step: 100; loss: 3.380; l2dist: 1.670\n",
      "    step: 150; loss: 2.616; l2dist: 1.484\n",
      "    step: 200; loss: 2.411; l2dist: 1.430\n",
      "    step: 250; loss: 2.356; l2dist: 1.407\n",
      "    step: 300; loss: 2.310; l2dist: 1.396\n",
      "    step: 350; loss: 2.278; l2dist: 1.387\n",
      "    step: 400; loss: 2.266; l2dist: 1.390\n",
      "    step: 450; loss: 2.283; l2dist: 1.395\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.479; l2dist: 0.000\n",
      "    step: 50; loss: 7.189; l2dist: 2.127\n",
      "    step: 100; loss: 3.307; l2dist: 1.607\n",
      "    step: 150; loss: 2.573; l2dist: 1.444\n",
      "    step: 200; loss: 2.381; l2dist: 1.393\n",
      "    step: 250; loss: 2.312; l2dist: 1.377\n",
      "    step: 300; loss: 2.272; l2dist: 1.368\n",
      "    step: 350; loss: 2.256; l2dist: 1.360\n",
      "    step: 400; loss: 2.242; l2dist: 1.358\n",
      "    step: 450; loss: 2.228; l2dist: 1.357\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.696; l2dist: 0.000\n",
      "    step: 50; loss: 7.173; l2dist: 2.132\n",
      "    step: 100; loss: 3.314; l2dist: 1.620\n",
      "    step: 150; loss: 2.546; l2dist: 1.445\n",
      "    step: 200; loss: 2.374; l2dist: 1.405\n",
      "    step: 250; loss: 2.295; l2dist: 1.379\n",
      "    step: 300; loss: 2.265; l2dist: 1.371\n",
      "    step: 350; loss: 2.244; l2dist: 1.361\n",
      "    step: 400; loss: 2.244; l2dist: 1.364\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.013; l2dist: 0.000\n",
      "    step: 50; loss: 7.119; l2dist: 2.117\n",
      "    step: 100; loss: 3.302; l2dist: 1.617\n",
      "    step: 150; loss: 2.548; l2dist: 1.438\n",
      "    step: 200; loss: 2.371; l2dist: 1.390\n",
      "    step: 250; loss: 2.296; l2dist: 1.376\n",
      "    step: 300; loss: 2.272; l2dist: 1.367\n",
      "    step: 350; loss: 2.247; l2dist: 1.366\n",
      "    step: 400; loss: 2.247; l2dist: 1.364\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.653; l2dist: 0.000\n",
      "    step: 50; loss: 7.083; l2dist: 2.106\n",
      "    step: 100; loss: 3.299; l2dist: 1.613\n",
      "    step: 150; loss: 2.539; l2dist: 1.434\n",
      "    step: 200; loss: 2.366; l2dist: 1.387\n",
      "    step: 250; loss: 2.295; l2dist: 1.379\n",
      "    step: 300; loss: 2.273; l2dist: 1.372\n",
      "    step: 350; loss: 2.234; l2dist: 1.361\n",
      "    step: 400; loss: 2.239; l2dist: 1.358\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.833; l2dist: 0.000\n",
      "    step: 50; loss: 7.118; l2dist: 2.119\n",
      "    step: 100; loss: 3.298; l2dist: 1.618\n",
      "    step: 150; loss: 2.541; l2dist: 1.443\n",
      "    step: 200; loss: 2.369; l2dist: 1.406\n",
      "    step: 250; loss: 2.299; l2dist: 1.378\n",
      "    step: 300; loss: 2.261; l2dist: 1.374\n",
      "    step: 350; loss: 2.236; l2dist: 1.365\n",
      "    step: 400; loss: 2.248; l2dist: 1.363\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 136.738; l2dist: 0.000\n",
      "    step: 50; loss: 16.602; l2dist: 3.550\n",
      "    step: 100; loss: 6.531; l2dist: 2.464\n",
      "    step: 150; loss: 4.411; l2dist: 2.008\n",
      "    step: 200; loss: 3.543; l2dist: 1.786\n",
      "    step: 250; loss: 3.141; l2dist: 1.678\n",
      "    step: 300; loss: 2.908; l2dist: 1.607\n",
      "    step: 350; loss: 2.879; l2dist: 1.595\n",
      "    step: 400; loss: 2.753; l2dist: 1.557\n",
      "    step: 450; loss: 2.775; l2dist: 1.565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 121.991; l2dist: 0.000\n",
      "    step: 50; loss: 14.683; l2dist: 3.265\n",
      "    step: 100; loss: 5.679; l2dist: 2.231\n",
      "    step: 150; loss: 3.988; l2dist: 1.847\n",
      "    step: 200; loss: 3.302; l2dist: 1.687\n",
      "    step: 250; loss: 2.998; l2dist: 1.615\n",
      "    step: 300; loss: 2.854; l2dist: 1.577\n",
      "    step: 350; loss: 2.750; l2dist: 1.545\n",
      "    step: 400; loss: 2.668; l2dist: 1.534\n",
      "    step: 450; loss: 2.684; l2dist: 1.526\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 75.156; l2dist: 0.000\n",
      "    step: 50; loss: 11.680; l2dist: 2.872\n",
      "    step: 100; loss: 4.588; l2dist: 1.996\n",
      "    step: 150; loss: 3.344; l2dist: 1.709\n",
      "    step: 200; loss: 2.917; l2dist: 1.601\n",
      "    step: 250; loss: 2.764; l2dist: 1.554\n",
      "    step: 300; loss: 2.679; l2dist: 1.536\n",
      "    step: 350; loss: 2.609; l2dist: 1.514\n",
      "    step: 400; loss: 2.602; l2dist: 1.513\n",
      "    step: 450; loss: 2.598; l2dist: 1.513\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.747; l2dist: 0.000\n",
      "    step: 50; loss: 9.783; l2dist: 2.575\n",
      "    step: 100; loss: 4.066; l2dist: 1.873\n",
      "    step: 150; loss: 3.049; l2dist: 1.634\n",
      "    step: 200; loss: 2.762; l2dist: 1.555\n",
      "    step: 250; loss: 2.661; l2dist: 1.523\n",
      "    step: 300; loss: 2.604; l2dist: 1.511\n",
      "    step: 350; loss: 2.558; l2dist: 1.495\n",
      "    step: 400; loss: 2.549; l2dist: 1.494\n",
      "    step: 450; loss: 2.526; l2dist: 1.495\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.801; l2dist: 0.000\n",
      "    step: 50; loss: 8.847; l2dist: 2.409\n",
      "    step: 100; loss: 3.921; l2dist: 1.798\n",
      "    step: 150; loss: 2.969; l2dist: 1.590\n",
      "    step: 200; loss: 2.706; l2dist: 1.526\n",
      "    step: 250; loss: 2.612; l2dist: 1.502\n",
      "    step: 300; loss: 2.542; l2dist: 1.483\n",
      "    step: 350; loss: 2.550; l2dist: 1.483\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.020; l2dist: 0.000\n",
      "    step: 50; loss: 8.586; l2dist: 2.325\n",
      "    step: 100; loss: 3.826; l2dist: 1.765\n",
      "    step: 150; loss: 2.945; l2dist: 1.571\n",
      "    step: 200; loss: 2.710; l2dist: 1.513\n",
      "    step: 250; loss: 2.614; l2dist: 1.483\n",
      "    step: 300; loss: 2.554; l2dist: 1.471\n",
      "    step: 350; loss: 2.520; l2dist: 1.462\n",
      "    step: 400; loss: 2.507; l2dist: 1.466\n",
      "    step: 450; loss: 2.505; l2dist: 1.465\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.974; l2dist: 0.000\n",
      "    step: 50; loss: 8.453; l2dist: 2.315\n",
      "    step: 100; loss: 3.795; l2dist: 1.764\n",
      "    step: 150; loss: 2.905; l2dist: 1.567\n",
      "    step: 200; loss: 2.660; l2dist: 1.507\n",
      "    step: 250; loss: 2.573; l2dist: 1.480\n",
      "    step: 300; loss: 2.536; l2dist: 1.469\n",
      "    step: 350; loss: 2.524; l2dist: 1.469\n",
      "    step: 400; loss: 2.502; l2dist: 1.461\n",
      "    step: 450; loss: 2.486; l2dist: 1.454\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.801; l2dist: 0.000\n",
      "    step: 50; loss: 8.360; l2dist: 2.301\n",
      "    step: 100; loss: 3.771; l2dist: 1.762\n",
      "    step: 150; loss: 2.908; l2dist: 1.558\n",
      "    step: 200; loss: 2.680; l2dist: 1.504\n",
      "    step: 250; loss: 2.591; l2dist: 1.484\n",
      "    step: 300; loss: 2.547; l2dist: 1.472\n",
      "    step: 350; loss: 2.520; l2dist: 1.463\n",
      "    step: 400; loss: 2.505; l2dist: 1.458\n",
      "    step: 450; loss: 2.495; l2dist: 1.462\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.061; l2dist: 0.000\n",
      "    step: 50; loss: 8.284; l2dist: 2.289\n",
      "    step: 100; loss: 3.771; l2dist: 1.755\n",
      "    step: 150; loss: 2.897; l2dist: 1.557\n",
      "    step: 200; loss: 2.670; l2dist: 1.497\n",
      "    step: 250; loss: 2.588; l2dist: 1.481\n",
      "    step: 300; loss: 2.527; l2dist: 1.468\n",
      "    step: 350; loss: 2.529; l2dist: 1.466\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.318; l2dist: 0.000\n",
      "    step: 50; loss: 8.329; l2dist: 2.299\n",
      "    step: 100; loss: 3.784; l2dist: 1.762\n",
      "    step: 150; loss: 2.909; l2dist: 1.563\n",
      "    step: 200; loss: 2.681; l2dist: 1.513\n",
      "    step: 250; loss: 2.578; l2dist: 1.485\n",
      "    step: 300; loss: 2.541; l2dist: 1.471\n",
      "    step: 350; loss: 2.517; l2dist: 1.472\n",
      "    step: 400; loss: 2.495; l2dist: 1.460\n",
      "    step: 450; loss: 2.520; l2dist: 1.470\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 134.058; l2dist: 0.000\n",
      "    step: 50; loss: 14.105; l2dist: 3.522\n",
      "    step: 100; loss: 6.442; l2dist: 2.452\n",
      "    step: 150; loss: 4.303; l2dist: 1.988\n",
      "    step: 200; loss: 3.443; l2dist: 1.771\n",
      "    step: 250; loss: 3.050; l2dist: 1.662\n",
      "    step: 300; loss: 2.892; l2dist: 1.606\n",
      "    step: 350; loss: 2.804; l2dist: 1.597\n",
      "    step: 400; loss: 2.836; l2dist: 1.598\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 118.534; l2dist: 0.000\n",
      "    step: 50; loss: 12.694; l2dist: 3.290\n",
      "    step: 100; loss: 5.705; l2dist: 2.263\n",
      "    step: 150; loss: 3.912; l2dist: 1.861\n",
      "    step: 200; loss: 3.268; l2dist: 1.700\n",
      "    step: 250; loss: 2.987; l2dist: 1.631\n",
      "    step: 300; loss: 2.864; l2dist: 1.597\n",
      "    step: 350; loss: 2.748; l2dist: 1.552\n",
      "    step: 400; loss: 2.665; l2dist: 1.533\n",
      "    step: 450; loss: 2.622; l2dist: 1.530\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.121; l2dist: 0.000\n",
      "    step: 50; loss: 9.917; l2dist: 2.822\n",
      "    step: 100; loss: 4.343; l2dist: 1.980\n",
      "    step: 150; loss: 3.215; l2dist: 1.687\n",
      "    step: 200; loss: 2.881; l2dist: 1.598\n",
      "    step: 250; loss: 2.680; l2dist: 1.540\n",
      "    step: 300; loss: 2.576; l2dist: 1.516\n",
      "    step: 350; loss: 2.555; l2dist: 1.502\n",
      "    step: 400; loss: 2.506; l2dist: 1.490\n",
      "    step: 450; loss: 2.542; l2dist: 1.506\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.886; l2dist: 0.000\n",
      "    step: 50; loss: 8.353; l2dist: 2.532\n",
      "    step: 100; loss: 3.894; l2dist: 1.861\n",
      "    step: 150; loss: 2.953; l2dist: 1.616\n",
      "    step: 200; loss: 2.689; l2dist: 1.550\n",
      "    step: 250; loss: 2.577; l2dist: 1.513\n",
      "    step: 300; loss: 2.513; l2dist: 1.493\n",
      "    step: 350; loss: 2.521; l2dist: 1.494\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.524; l2dist: 0.000\n",
      "    step: 50; loss: 7.460; l2dist: 2.355\n",
      "    step: 100; loss: 3.783; l2dist: 1.776\n",
      "    step: 150; loss: 2.832; l2dist: 1.569\n",
      "    step: 200; loss: 2.595; l2dist: 1.510\n",
      "    step: 250; loss: 2.497; l2dist: 1.477\n",
      "    step: 300; loss: 2.440; l2dist: 1.469\n",
      "    step: 350; loss: 2.482; l2dist: 1.464\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.736; l2dist: 0.000\n",
      "    step: 50; loss: 7.251; l2dist: 2.329\n",
      "    step: 100; loss: 3.705; l2dist: 1.762\n",
      "    step: 150; loss: 2.811; l2dist: 1.563\n",
      "    step: 200; loss: 2.585; l2dist: 1.503\n",
      "    step: 250; loss: 2.506; l2dist: 1.474\n",
      "    step: 300; loss: 2.459; l2dist: 1.460\n",
      "    step: 350; loss: 2.450; l2dist: 1.468\n",
      "    step: 400; loss: 2.435; l2dist: 1.453\n",
      "    step: 450; loss: 2.426; l2dist: 1.452\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.443; l2dist: 0.000\n",
      "    step: 50; loss: 7.142; l2dist: 2.299\n",
      "    step: 100; loss: 3.691; l2dist: 1.751\n",
      "    step: 150; loss: 2.793; l2dist: 1.549\n",
      "    step: 200; loss: 2.592; l2dist: 1.500\n",
      "    step: 250; loss: 2.499; l2dist: 1.478\n",
      "    step: 300; loss: 2.474; l2dist: 1.469\n",
      "    step: 350; loss: 2.442; l2dist: 1.461\n",
      "    step: 400; loss: 2.434; l2dist: 1.453\n",
      "    step: 450; loss: 2.449; l2dist: 1.456\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.568; l2dist: 0.000\n",
      "    step: 50; loss: 7.117; l2dist: 2.295\n",
      "    step: 100; loss: 3.682; l2dist: 1.751\n",
      "    step: 150; loss: 2.790; l2dist: 1.553\n",
      "    step: 200; loss: 2.573; l2dist: 1.496\n",
      "    step: 250; loss: 2.492; l2dist: 1.478\n",
      "    step: 300; loss: 2.480; l2dist: 1.462\n",
      "    step: 350; loss: 2.447; l2dist: 1.466\n",
      "    step: 400; loss: 2.432; l2dist: 1.458\n",
      "    step: 450; loss: 2.430; l2dist: 1.452\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.069; l2dist: 0.000\n",
      "    step: 50; loss: 7.102; l2dist: 2.290\n",
      "    step: 100; loss: 3.676; l2dist: 1.750\n",
      "    step: 150; loss: 2.787; l2dist: 1.556\n",
      "    step: 200; loss: 2.578; l2dist: 1.505\n",
      "    step: 250; loss: 2.514; l2dist: 1.474\n",
      "    step: 300; loss: 2.465; l2dist: 1.466\n",
      "    step: 350; loss: 2.451; l2dist: 1.467\n",
      "    step: 400; loss: 2.455; l2dist: 1.460\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.292; l2dist: 0.000\n",
      "    step: 50; loss: 7.134; l2dist: 2.298\n",
      "    step: 100; loss: 3.676; l2dist: 1.754\n",
      "    step: 150; loss: 2.792; l2dist: 1.554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 200; loss: 2.574; l2dist: 1.506\n",
      "    step: 250; loss: 2.502; l2dist: 1.479\n",
      "    step: 300; loss: 2.460; l2dist: 1.466\n",
      "    step: 350; loss: 2.460; l2dist: 1.464\n",
      "    step: 400; loss: 2.438; l2dist: 1.464\n",
      "    step: 450; loss: 2.442; l2dist: 1.460\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 104.523; l2dist: 0.000\n",
      "    step: 50; loss: 10.339; l2dist: 3.045\n",
      "    step: 100; loss: 4.607; l2dist: 2.048\n",
      "    step: 150; loss: 3.169; l2dist: 1.679\n",
      "    step: 200; loss: 2.554; l2dist: 1.500\n",
      "    step: 250; loss: 2.287; l2dist: 1.411\n",
      "    step: 300; loss: 2.168; l2dist: 1.374\n",
      "    step: 350; loss: 2.066; l2dist: 1.346\n",
      "    step: 400; loss: 2.065; l2dist: 1.340\n",
      "    step: 450; loss: 2.096; l2dist: 1.339\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 116.862; l2dist: 0.000\n",
      "    step: 50; loss: 10.312; l2dist: 2.989\n",
      "    step: 100; loss: 4.617; l2dist: 2.020\n",
      "    step: 150; loss: 3.180; l2dist: 1.651\n",
      "    step: 200; loss: 2.610; l2dist: 1.497\n",
      "    step: 250; loss: 2.372; l2dist: 1.425\n",
      "    step: 300; loss: 2.276; l2dist: 1.395\n",
      "    step: 350; loss: 2.145; l2dist: 1.358\n",
      "    step: 400; loss: 2.110; l2dist: 1.342\n",
      "    step: 450; loss: 2.065; l2dist: 1.328\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 67.423; l2dist: 0.000\n",
      "    step: 50; loss: 8.128; l2dist: 2.578\n",
      "    step: 100; loss: 3.568; l2dist: 1.757\n",
      "    step: 150; loss: 2.608; l2dist: 1.495\n",
      "    step: 200; loss: 2.268; l2dist: 1.399\n",
      "    step: 250; loss: 2.136; l2dist: 1.355\n",
      "    step: 300; loss: 2.014; l2dist: 1.320\n",
      "    step: 350; loss: 1.986; l2dist: 1.308\n",
      "    step: 400; loss: 1.993; l2dist: 1.310\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.051; l2dist: 0.000\n",
      "    step: 50; loss: 6.854; l2dist: 2.273\n",
      "    step: 100; loss: 3.084; l2dist: 1.630\n",
      "    step: 150; loss: 2.327; l2dist: 1.422\n",
      "    step: 200; loss: 2.078; l2dist: 1.341\n",
      "    step: 250; loss: 1.966; l2dist: 1.306\n",
      "    step: 300; loss: 1.935; l2dist: 1.287\n",
      "    step: 350; loss: 1.864; l2dist: 1.271\n",
      "    step: 400; loss: 1.859; l2dist: 1.277\n",
      "    step: 450; loss: 1.847; l2dist: 1.266\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.118; l2dist: 0.000\n",
      "    step: 50; loss: 6.086; l2dist: 2.082\n",
      "    step: 100; loss: 2.944; l2dist: 1.559\n",
      "    step: 150; loss: 2.245; l2dist: 1.380\n",
      "    step: 200; loss: 2.020; l2dist: 1.306\n",
      "    step: 250; loss: 1.925; l2dist: 1.280\n",
      "    step: 300; loss: 1.866; l2dist: 1.264\n",
      "    step: 350; loss: 1.848; l2dist: 1.254\n",
      "    step: 400; loss: 1.843; l2dist: 1.257\n",
      "    step: 450; loss: 1.841; l2dist: 1.254\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.714; l2dist: 0.000\n",
      "    step: 50; loss: 5.675; l2dist: 1.986\n",
      "    step: 100; loss: 2.846; l2dist: 1.503\n",
      "    step: 150; loss: 2.171; l2dist: 1.332\n",
      "    step: 200; loss: 1.970; l2dist: 1.279\n",
      "    step: 250; loss: 1.870; l2dist: 1.249\n",
      "    step: 300; loss: 1.833; l2dist: 1.248\n",
      "    step: 350; loss: 1.808; l2dist: 1.234\n",
      "    step: 400; loss: 1.809; l2dist: 1.234\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.424; l2dist: 0.000\n",
      "    step: 50; loss: 5.577; l2dist: 1.957\n",
      "    step: 100; loss: 2.809; l2dist: 1.496\n",
      "    step: 150; loss: 2.140; l2dist: 1.325\n",
      "    step: 200; loss: 1.954; l2dist: 1.269\n",
      "    step: 250; loss: 1.859; l2dist: 1.245\n",
      "    step: 300; loss: 1.816; l2dist: 1.239\n",
      "    step: 350; loss: 1.819; l2dist: 1.236\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.354; l2dist: 0.000\n",
      "    step: 50; loss: 5.541; l2dist: 1.950\n",
      "    step: 100; loss: 2.803; l2dist: 1.487\n",
      "    step: 150; loss: 2.144; l2dist: 1.323\n",
      "    step: 200; loss: 1.937; l2dist: 1.265\n",
      "    step: 250; loss: 1.858; l2dist: 1.246\n",
      "    step: 300; loss: 1.836; l2dist: 1.237\n",
      "    step: 350; loss: 1.812; l2dist: 1.229\n",
      "    step: 400; loss: 1.801; l2dist: 1.224\n",
      "    step: 450; loss: 1.788; l2dist: 1.226\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.788; l2dist: 0.000\n",
      "    step: 50; loss: 5.523; l2dist: 1.942\n",
      "    step: 100; loss: 2.799; l2dist: 1.487\n",
      "    step: 150; loss: 2.131; l2dist: 1.326\n",
      "    step: 200; loss: 1.931; l2dist: 1.270\n",
      "    step: 250; loss: 1.855; l2dist: 1.250\n",
      "    step: 300; loss: 1.822; l2dist: 1.235\n",
      "    step: 350; loss: 1.817; l2dist: 1.237\n",
      "    step: 400; loss: 1.793; l2dist: 1.232\n",
      "    step: 450; loss: 1.796; l2dist: 1.229\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.946; l2dist: 0.000\n",
      "    step: 50; loss: 5.539; l2dist: 1.952\n",
      "    step: 100; loss: 2.797; l2dist: 1.498\n",
      "    step: 150; loss: 2.120; l2dist: 1.327\n",
      "    step: 200; loss: 1.924; l2dist: 1.276\n",
      "    step: 250; loss: 1.871; l2dist: 1.259\n",
      "    step: 300; loss: 1.821; l2dist: 1.240\n",
      "    step: 350; loss: 1.815; l2dist: 1.237\n",
      "    step: 400; loss: 1.794; l2dist: 1.232\n",
      "    step: 450; loss: 1.802; l2dist: 1.238\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 112.517; l2dist: 0.000\n",
      "    step: 50; loss: 13.808; l2dist: 3.117\n",
      "    step: 100; loss: 5.133; l2dist: 2.161\n",
      "    step: 150; loss: 3.551; l2dist: 1.776\n",
      "    step: 200; loss: 2.985; l2dist: 1.608\n",
      "    step: 250; loss: 2.687; l2dist: 1.515\n",
      "    step: 300; loss: 2.583; l2dist: 1.490\n",
      "    step: 350; loss: 2.450; l2dist: 1.450\n",
      "    step: 400; loss: 2.388; l2dist: 1.435\n",
      "    step: 450; loss: 2.418; l2dist: 1.432\n",
      "binary step: 0; number of successful adv: 91/100\n",
      "    step: 0; loss: 158.186; l2dist: 0.000\n",
      "    step: 50; loss: 14.116; l2dist: 3.193\n",
      "    step: 100; loss: 5.905; l2dist: 2.265\n",
      "    step: 150; loss: 4.149; l2dist: 1.871\n",
      "    step: 200; loss: 3.342; l2dist: 1.671\n",
      "    step: 250; loss: 2.968; l2dist: 1.582\n",
      "    step: 300; loss: 2.707; l2dist: 1.511\n",
      "    step: 350; loss: 2.612; l2dist: 1.483\n",
      "    step: 400; loss: 2.515; l2dist: 1.453\n",
      "    step: 450; loss: 2.512; l2dist: 1.448\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.907; l2dist: 0.000\n",
      "    step: 50; loss: 10.789; l2dist: 2.737\n",
      "    step: 100; loss: 4.401; l2dist: 1.941\n",
      "    step: 150; loss: 3.183; l2dist: 1.639\n",
      "    step: 200; loss: 2.717; l2dist: 1.515\n",
      "    step: 250; loss: 2.492; l2dist: 1.453\n",
      "    step: 300; loss: 2.382; l2dist: 1.422\n",
      "    step: 350; loss: 2.363; l2dist: 1.409\n",
      "    step: 400; loss: 2.308; l2dist: 1.405\n",
      "    step: 450; loss: 2.289; l2dist: 1.395\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.469; l2dist: 0.000\n",
      "    step: 50; loss: 8.763; l2dist: 2.413\n",
      "    step: 100; loss: 3.733; l2dist: 1.774\n",
      "    step: 150; loss: 2.775; l2dist: 1.531\n",
      "    step: 200; loss: 2.517; l2dist: 1.457\n",
      "    step: 250; loss: 2.386; l2dist: 1.421\n",
      "    step: 300; loss: 2.320; l2dist: 1.397\n",
      "    step: 350; loss: 2.293; l2dist: 1.391\n",
      "    step: 400; loss: 2.263; l2dist: 1.379\n",
      "    step: 450; loss: 2.265; l2dist: 1.383\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.307; l2dist: 0.000\n",
      "    step: 50; loss: 7.385; l2dist: 2.110\n",
      "    step: 100; loss: 3.396; l2dist: 1.626\n",
      "    step: 150; loss: 2.617; l2dist: 1.454\n",
      "    step: 200; loss: 2.380; l2dist: 1.395\n",
      "    step: 250; loss: 2.290; l2dist: 1.369\n",
      "    step: 300; loss: 2.249; l2dist: 1.362\n",
      "    step: 350; loss: 2.222; l2dist: 1.348\n",
      "    step: 400; loss: 2.200; l2dist: 1.353\n",
      "    step: 450; loss: 2.186; l2dist: 1.343\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.333; l2dist: 0.000\n",
      "    step: 50; loss: 6.857; l2dist: 2.000\n",
      "    step: 100; loss: 3.298; l2dist: 1.572\n",
      "    step: 150; loss: 2.528; l2dist: 1.414\n",
      "    step: 200; loss: 2.335; l2dist: 1.361\n",
      "    step: 250; loss: 2.248; l2dist: 1.344\n",
      "    step: 300; loss: 2.198; l2dist: 1.337\n",
      "    step: 350; loss: 2.183; l2dist: 1.325\n",
      "    step: 400; loss: 2.175; l2dist: 1.324\n",
      "    step: 450; loss: 2.151; l2dist: 1.325\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.853; l2dist: 0.000\n",
      "    step: 50; loss: 6.596; l2dist: 1.944\n",
      "    step: 100; loss: 3.256; l2dist: 1.555\n",
      "    step: 150; loss: 2.511; l2dist: 1.396\n",
      "    step: 200; loss: 2.305; l2dist: 1.359\n",
      "    step: 250; loss: 2.226; l2dist: 1.337\n",
      "    step: 300; loss: 2.184; l2dist: 1.330\n",
      "    step: 350; loss: 2.174; l2dist: 1.327\n",
      "    step: 400; loss: 2.153; l2dist: 1.320\n",
      "    step: 450; loss: 2.147; l2dist: 1.318\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.342; l2dist: 0.000\n",
      "    step: 50; loss: 6.514; l2dist: 1.927\n",
      "    step: 100; loss: 3.245; l2dist: 1.554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 2.494; l2dist: 1.401\n",
      "    step: 200; loss: 2.302; l2dist: 1.357\n",
      "    step: 250; loss: 2.248; l2dist: 1.342\n",
      "    step: 300; loss: 2.187; l2dist: 1.330\n",
      "    step: 350; loss: 2.167; l2dist: 1.324\n",
      "    step: 400; loss: 2.150; l2dist: 1.316\n",
      "    step: 450; loss: 2.143; l2dist: 1.318\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.506; l2dist: 0.000\n",
      "    step: 50; loss: 6.450; l2dist: 1.919\n",
      "    step: 100; loss: 3.246; l2dist: 1.549\n",
      "    step: 150; loss: 2.495; l2dist: 1.405\n",
      "    step: 200; loss: 2.301; l2dist: 1.362\n",
      "    step: 250; loss: 2.231; l2dist: 1.340\n",
      "    step: 300; loss: 2.194; l2dist: 1.333\n",
      "    step: 350; loss: 2.164; l2dist: 1.326\n",
      "    step: 400; loss: 2.154; l2dist: 1.326\n",
      "    step: 450; loss: 2.140; l2dist: 1.318\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.700; l2dist: 0.000\n",
      "    step: 50; loss: 6.492; l2dist: 1.930\n",
      "    step: 100; loss: 3.252; l2dist: 1.558\n",
      "    step: 150; loss: 2.496; l2dist: 1.409\n",
      "    step: 200; loss: 2.303; l2dist: 1.363\n",
      "    step: 250; loss: 2.228; l2dist: 1.343\n",
      "    step: 300; loss: 2.196; l2dist: 1.329\n",
      "    step: 350; loss: 2.163; l2dist: 1.328\n",
      "    step: 400; loss: 2.163; l2dist: 1.327\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 150.636; l2dist: 0.000\n",
      "    step: 50; loss: 17.404; l2dist: 3.771\n",
      "    step: 100; loss: 7.665; l2dist: 2.703\n",
      "    step: 150; loss: 5.419; l2dist: 2.250\n",
      "    step: 200; loss: 4.511; l2dist: 2.052\n",
      "    step: 250; loss: 4.086; l2dist: 1.950\n",
      "    step: 300; loss: 3.886; l2dist: 1.894\n",
      "    step: 350; loss: 3.775; l2dist: 1.864\n",
      "    step: 400; loss: 3.715; l2dist: 1.843\n",
      "    step: 450; loss: 3.725; l2dist: 1.847\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 137.971; l2dist: 0.000\n",
      "    step: 50; loss: 15.307; l2dist: 3.544\n",
      "    step: 100; loss: 6.870; l2dist: 2.520\n",
      "    step: 150; loss: 5.024; l2dist: 2.140\n",
      "    step: 200; loss: 4.274; l2dist: 1.964\n",
      "    step: 250; loss: 3.965; l2dist: 1.892\n",
      "    step: 300; loss: 3.760; l2dist: 1.846\n",
      "    step: 350; loss: 3.597; l2dist: 1.814\n",
      "    step: 400; loss: 3.528; l2dist: 1.790\n",
      "    step: 450; loss: 3.537; l2dist: 1.792\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 79.218; l2dist: 0.000\n",
      "    step: 50; loss: 11.853; l2dist: 3.064\n",
      "    step: 100; loss: 5.457; l2dist: 2.236\n",
      "    step: 150; loss: 4.227; l2dist: 1.952\n",
      "    step: 200; loss: 3.819; l2dist: 1.859\n",
      "    step: 250; loss: 3.588; l2dist: 1.803\n",
      "    step: 300; loss: 3.490; l2dist: 1.787\n",
      "    step: 350; loss: 3.442; l2dist: 1.778\n",
      "    step: 400; loss: 3.417; l2dist: 1.765\n",
      "    step: 450; loss: 3.438; l2dist: 1.764\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.869; l2dist: 0.000\n",
      "    step: 50; loss: 10.442; l2dist: 2.815\n",
      "    step: 100; loss: 5.062; l2dist: 2.129\n",
      "    step: 150; loss: 3.994; l2dist: 1.902\n",
      "    step: 200; loss: 3.653; l2dist: 1.818\n",
      "    step: 250; loss: 3.531; l2dist: 1.790\n",
      "    step: 300; loss: 3.471; l2dist: 1.769\n",
      "    step: 350; loss: 3.448; l2dist: 1.764\n",
      "    step: 400; loss: 3.429; l2dist: 1.758\n",
      "    step: 450; loss: 3.437; l2dist: 1.765\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.783; l2dist: 0.000\n",
      "    step: 50; loss: 9.801; l2dist: 2.695\n",
      "    step: 100; loss: 4.869; l2dist: 2.057\n",
      "    step: 150; loss: 3.853; l2dist: 1.848\n",
      "    step: 200; loss: 3.581; l2dist: 1.795\n",
      "    step: 250; loss: 3.464; l2dist: 1.768\n",
      "    step: 300; loss: 3.422; l2dist: 1.755\n",
      "    step: 350; loss: 3.406; l2dist: 1.746\n",
      "    step: 400; loss: 3.389; l2dist: 1.749\n",
      "    step: 450; loss: 3.397; l2dist: 1.744\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.979; l2dist: 0.000\n",
      "    step: 50; loss: 9.490; l2dist: 2.650\n",
      "    step: 100; loss: 4.780; l2dist: 2.056\n",
      "    step: 150; loss: 3.810; l2dist: 1.844\n",
      "    step: 200; loss: 3.540; l2dist: 1.786\n",
      "    step: 250; loss: 3.445; l2dist: 1.766\n",
      "    step: 300; loss: 3.412; l2dist: 1.751\n",
      "    step: 350; loss: 3.388; l2dist: 1.750\n",
      "    step: 400; loss: 3.374; l2dist: 1.745\n",
      "    step: 450; loss: 3.355; l2dist: 1.748\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.504; l2dist: 0.000\n",
      "    step: 50; loss: 9.409; l2dist: 2.645\n",
      "    step: 100; loss: 4.751; l2dist: 2.041\n",
      "    step: 150; loss: 3.793; l2dist: 1.846\n",
      "    step: 200; loss: 3.550; l2dist: 1.790\n",
      "    step: 250; loss: 3.467; l2dist: 1.770\n",
      "    step: 300; loss: 3.405; l2dist: 1.751\n",
      "    step: 350; loss: 3.379; l2dist: 1.751\n",
      "    step: 400; loss: 3.359; l2dist: 1.741\n",
      "    step: 450; loss: 3.369; l2dist: 1.737\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.488; l2dist: 0.000\n",
      "    step: 50; loss: 9.346; l2dist: 2.640\n",
      "    step: 100; loss: 4.737; l2dist: 2.046\n",
      "    step: 150; loss: 3.783; l2dist: 1.838\n",
      "    step: 200; loss: 3.521; l2dist: 1.779\n",
      "    step: 250; loss: 3.438; l2dist: 1.758\n",
      "    step: 300; loss: 3.412; l2dist: 1.748\n",
      "    step: 350; loss: 3.383; l2dist: 1.742\n",
      "    step: 400; loss: 3.395; l2dist: 1.745\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.876; l2dist: 0.000\n",
      "    step: 50; loss: 9.315; l2dist: 2.632\n",
      "    step: 100; loss: 4.721; l2dist: 2.038\n",
      "    step: 150; loss: 3.778; l2dist: 1.837\n",
      "    step: 200; loss: 3.530; l2dist: 1.783\n",
      "    step: 250; loss: 3.460; l2dist: 1.757\n",
      "    step: 300; loss: 3.438; l2dist: 1.746\n",
      "    step: 350; loss: 3.387; l2dist: 1.742\n",
      "    step: 400; loss: 3.374; l2dist: 1.744\n",
      "    step: 450; loss: 3.370; l2dist: 1.740\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.037; l2dist: 0.000\n",
      "    step: 50; loss: 9.337; l2dist: 2.639\n",
      "    step: 100; loss: 4.731; l2dist: 2.045\n",
      "    step: 150; loss: 3.785; l2dist: 1.841\n",
      "    step: 200; loss: 3.541; l2dist: 1.781\n",
      "    step: 250; loss: 3.444; l2dist: 1.758\n",
      "    step: 300; loss: 3.408; l2dist: 1.758\n",
      "    step: 350; loss: 3.379; l2dist: 1.751\n",
      "    step: 400; loss: 3.384; l2dist: 1.743\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 168.486; l2dist: 0.000\n",
      "    step: 50; loss: 19.911; l2dist: 4.031\n",
      "    step: 100; loss: 8.519; l2dist: 2.886\n",
      "    step: 150; loss: 5.684; l2dist: 2.344\n",
      "    step: 200; loss: 4.578; l2dist: 2.084\n",
      "    step: 250; loss: 4.069; l2dist: 1.951\n",
      "    step: 300; loss: 3.830; l2dist: 1.898\n",
      "    step: 350; loss: 3.672; l2dist: 1.847\n",
      "    step: 400; loss: 3.517; l2dist: 1.819\n",
      "    step: 450; loss: 3.501; l2dist: 1.802\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 100.782; l2dist: 0.000\n",
      "    step: 50; loss: 15.445; l2dist: 3.575\n",
      "    step: 100; loss: 6.113; l2dist: 2.411\n",
      "    step: 150; loss: 4.444; l2dist: 2.033\n",
      "    step: 200; loss: 3.854; l2dist: 1.891\n",
      "    step: 250; loss: 3.616; l2dist: 1.827\n",
      "    step: 300; loss: 3.458; l2dist: 1.787\n",
      "    step: 350; loss: 3.425; l2dist: 1.775\n",
      "    step: 400; loss: 3.336; l2dist: 1.765\n",
      "    step: 450; loss: 3.352; l2dist: 1.756\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 60.301; l2dist: 0.000\n",
      "    step: 50; loss: 12.289; l2dist: 3.117\n",
      "    step: 100; loss: 4.987; l2dist: 2.168\n",
      "    step: 150; loss: 3.813; l2dist: 1.885\n",
      "    step: 200; loss: 3.460; l2dist: 1.786\n",
      "    step: 250; loss: 3.295; l2dist: 1.746\n",
      "    step: 300; loss: 3.261; l2dist: 1.733\n",
      "    step: 350; loss: 3.201; l2dist: 1.728\n",
      "    step: 400; loss: 3.194; l2dist: 1.722\n",
      "    step: 450; loss: 3.166; l2dist: 1.719\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.638; l2dist: 0.000\n",
      "    step: 50; loss: 10.993; l2dist: 2.865\n",
      "    step: 100; loss: 4.729; l2dist: 2.077\n",
      "    step: 150; loss: 3.677; l2dist: 1.844\n",
      "    step: 200; loss: 3.382; l2dist: 1.771\n",
      "    step: 250; loss: 3.262; l2dist: 1.733\n",
      "    step: 300; loss: 3.228; l2dist: 1.732\n",
      "    step: 350; loss: 3.216; l2dist: 1.728\n",
      "    step: 400; loss: 3.177; l2dist: 1.720\n",
      "    step: 450; loss: 3.184; l2dist: 1.720\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.941; l2dist: 0.000\n",
      "    step: 50; loss: 9.985; l2dist: 2.668\n",
      "    step: 100; loss: 4.584; l2dist: 1.993\n",
      "    step: 150; loss: 3.539; l2dist: 1.786\n",
      "    step: 200; loss: 3.280; l2dist: 1.727\n",
      "    step: 250; loss: 3.190; l2dist: 1.700\n",
      "    step: 300; loss: 3.157; l2dist: 1.690\n",
      "    step: 350; loss: 3.118; l2dist: 1.682\n",
      "    step: 400; loss: 3.101; l2dist: 1.685\n",
      "    step: 450; loss: 3.105; l2dist: 1.687\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.095; l2dist: 0.000\n",
      "    step: 50; loss: 9.693; l2dist: 2.609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 4.524; l2dist: 1.967\n",
      "    step: 150; loss: 3.529; l2dist: 1.777\n",
      "    step: 200; loss: 3.267; l2dist: 1.716\n",
      "    step: 250; loss: 3.159; l2dist: 1.691\n",
      "    step: 300; loss: 3.122; l2dist: 1.682\n",
      "    step: 350; loss: 3.100; l2dist: 1.684\n",
      "    step: 400; loss: 3.088; l2dist: 1.680\n",
      "    step: 450; loss: 3.082; l2dist: 1.676\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.066; l2dist: 0.000\n",
      "    step: 50; loss: 9.664; l2dist: 2.609\n",
      "    step: 100; loss: 4.523; l2dist: 1.975\n",
      "    step: 150; loss: 3.528; l2dist: 1.781\n",
      "    step: 200; loss: 3.272; l2dist: 1.728\n",
      "    step: 250; loss: 3.192; l2dist: 1.696\n",
      "    step: 300; loss: 3.152; l2dist: 1.693\n",
      "    step: 350; loss: 3.110; l2dist: 1.685\n",
      "    step: 400; loss: 3.097; l2dist: 1.683\n",
      "    step: 450; loss: 3.089; l2dist: 1.681\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.575; l2dist: 0.000\n",
      "    step: 50; loss: 9.659; l2dist: 2.608\n",
      "    step: 100; loss: 4.532; l2dist: 1.979\n",
      "    step: 150; loss: 3.533; l2dist: 1.784\n",
      "    step: 200; loss: 3.286; l2dist: 1.724\n",
      "    step: 250; loss: 3.184; l2dist: 1.710\n",
      "    step: 300; loss: 3.142; l2dist: 1.698\n",
      "    step: 350; loss: 3.124; l2dist: 1.690\n",
      "    step: 400; loss: 3.113; l2dist: 1.692\n",
      "    step: 450; loss: 3.118; l2dist: 1.689\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.322; l2dist: 0.000\n",
      "    step: 50; loss: 9.640; l2dist: 2.605\n",
      "    step: 100; loss: 4.560; l2dist: 1.984\n",
      "    step: 150; loss: 3.551; l2dist: 1.787\n",
      "    step: 200; loss: 3.300; l2dist: 1.733\n",
      "    step: 250; loss: 3.201; l2dist: 1.718\n",
      "    step: 300; loss: 3.160; l2dist: 1.699\n",
      "    step: 350; loss: 3.145; l2dist: 1.700\n",
      "    step: 400; loss: 3.123; l2dist: 1.697\n",
      "    step: 450; loss: 3.124; l2dist: 1.691\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.626; l2dist: 0.000\n",
      "    step: 50; loss: 9.692; l2dist: 2.620\n",
      "    step: 100; loss: 4.565; l2dist: 1.994\n",
      "    step: 150; loss: 3.551; l2dist: 1.794\n",
      "    step: 200; loss: 3.297; l2dist: 1.735\n",
      "    step: 250; loss: 3.207; l2dist: 1.714\n",
      "    step: 300; loss: 3.168; l2dist: 1.710\n",
      "    step: 350; loss: 3.142; l2dist: 1.704\n",
      "    step: 400; loss: 3.135; l2dist: 1.693\n",
      "    step: 450; loss: 3.119; l2dist: 1.699\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 162.691; l2dist: 0.000\n",
      "    step: 50; loss: 20.302; l2dist: 4.086\n",
      "    step: 100; loss: 8.655; l2dist: 2.880\n",
      "    step: 150; loss: 5.919; l2dist: 2.364\n",
      "    step: 200; loss: 4.740; l2dist: 2.103\n",
      "    step: 250; loss: 4.318; l2dist: 1.991\n",
      "    step: 300; loss: 4.006; l2dist: 1.925\n",
      "    step: 350; loss: 3.851; l2dist: 1.882\n",
      "    step: 400; loss: 3.806; l2dist: 1.863\n",
      "    step: 450; loss: 3.708; l2dist: 1.834\n",
      "binary step: 0; number of successful adv: 91/100\n",
      "    step: 0; loss: 225.794; l2dist: 0.000\n",
      "    step: 50; loss: 21.403; l2dist: 4.099\n",
      "    step: 100; loss: 9.906; l2dist: 3.004\n",
      "    step: 150; loss: 6.911; l2dist: 2.479\n",
      "    step: 200; loss: 5.547; l2dist: 2.207\n",
      "    step: 250; loss: 4.693; l2dist: 2.040\n",
      "    step: 300; loss: 4.301; l2dist: 1.964\n",
      "    step: 350; loss: 4.081; l2dist: 1.913\n",
      "    step: 400; loss: 3.966; l2dist: 1.881\n",
      "    step: 450; loss: 3.900; l2dist: 1.865\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 128.435; l2dist: 0.000\n",
      "    step: 50; loss: 16.861; l2dist: 3.598\n",
      "    step: 100; loss: 7.274; l2dist: 2.553\n",
      "    step: 150; loss: 5.208; l2dist: 2.152\n",
      "    step: 200; loss: 4.339; l2dist: 1.969\n",
      "    step: 250; loss: 4.004; l2dist: 1.897\n",
      "    step: 300; loss: 3.817; l2dist: 1.858\n",
      "    step: 350; loss: 3.664; l2dist: 1.822\n",
      "    step: 400; loss: 3.639; l2dist: 1.814\n",
      "    step: 450; loss: 3.608; l2dist: 1.810\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 85.377; l2dist: 0.000\n",
      "    step: 50; loss: 13.808; l2dist: 3.203\n",
      "    step: 100; loss: 6.092; l2dist: 2.292\n",
      "    step: 150; loss: 4.422; l2dist: 1.976\n",
      "    step: 200; loss: 3.936; l2dist: 1.878\n",
      "    step: 250; loss: 3.694; l2dist: 1.824\n",
      "    step: 300; loss: 3.582; l2dist: 1.790\n",
      "    step: 350; loss: 3.523; l2dist: 1.784\n",
      "    step: 400; loss: 3.511; l2dist: 1.779\n",
      "    step: 450; loss: 3.471; l2dist: 1.759\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.980; l2dist: 0.000\n",
      "    step: 50; loss: 12.141; l2dist: 2.914\n",
      "    step: 100; loss: 5.372; l2dist: 2.123\n",
      "    step: 150; loss: 4.094; l2dist: 1.874\n",
      "    step: 200; loss: 3.732; l2dist: 1.795\n",
      "    step: 250; loss: 3.561; l2dist: 1.762\n",
      "    step: 300; loss: 3.465; l2dist: 1.745\n",
      "    step: 350; loss: 3.408; l2dist: 1.734\n",
      "    step: 400; loss: 3.393; l2dist: 1.728\n",
      "    step: 450; loss: 3.395; l2dist: 1.730\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.095; l2dist: 0.000\n",
      "    step: 50; loss: 11.546; l2dist: 2.816\n",
      "    step: 100; loss: 5.186; l2dist: 2.075\n",
      "    step: 150; loss: 3.987; l2dist: 1.851\n",
      "    step: 200; loss: 3.643; l2dist: 1.787\n",
      "    step: 250; loss: 3.499; l2dist: 1.757\n",
      "    step: 300; loss: 3.426; l2dist: 1.743\n",
      "    step: 350; loss: 3.394; l2dist: 1.735\n",
      "    step: 400; loss: 3.341; l2dist: 1.723\n",
      "    step: 450; loss: 3.349; l2dist: 1.725\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.581; l2dist: 0.000\n",
      "    step: 50; loss: 11.345; l2dist: 2.786\n",
      "    step: 100; loss: 5.093; l2dist: 2.061\n",
      "    step: 150; loss: 3.929; l2dist: 1.846\n",
      "    step: 200; loss: 3.624; l2dist: 1.777\n",
      "    step: 250; loss: 3.462; l2dist: 1.753\n",
      "    step: 300; loss: 3.431; l2dist: 1.735\n",
      "    step: 350; loss: 3.363; l2dist: 1.728\n",
      "    step: 400; loss: 3.338; l2dist: 1.726\n",
      "    step: 450; loss: 3.334; l2dist: 1.722\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.197; l2dist: 0.000\n",
      "    step: 50; loss: 11.194; l2dist: 2.763\n",
      "    step: 100; loss: 5.047; l2dist: 2.053\n",
      "    step: 150; loss: 3.913; l2dist: 1.834\n",
      "    step: 200; loss: 3.594; l2dist: 1.775\n",
      "    step: 250; loss: 3.464; l2dist: 1.747\n",
      "    step: 300; loss: 3.411; l2dist: 1.732\n",
      "    step: 350; loss: 3.386; l2dist: 1.733\n",
      "    step: 400; loss: 3.363; l2dist: 1.721\n",
      "    step: 450; loss: 3.322; l2dist: 1.714\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.266; l2dist: 0.000\n",
      "    step: 50; loss: 11.133; l2dist: 2.753\n",
      "    step: 100; loss: 5.027; l2dist: 2.054\n",
      "    step: 150; loss: 3.888; l2dist: 1.837\n",
      "    step: 200; loss: 3.585; l2dist: 1.776\n",
      "    step: 250; loss: 3.465; l2dist: 1.754\n",
      "    step: 300; loss: 3.392; l2dist: 1.734\n",
      "    step: 350; loss: 3.352; l2dist: 1.724\n",
      "    step: 400; loss: 3.352; l2dist: 1.724\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.708; l2dist: 0.000\n",
      "    step: 50; loss: 11.192; l2dist: 2.763\n",
      "    step: 100; loss: 5.023; l2dist: 2.056\n",
      "    step: 150; loss: 3.912; l2dist: 1.840\n",
      "    step: 200; loss: 3.581; l2dist: 1.776\n",
      "    step: 250; loss: 3.452; l2dist: 1.751\n",
      "    step: 300; loss: 3.383; l2dist: 1.735\n",
      "    step: 350; loss: 3.387; l2dist: 1.742\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 157.596; l2dist: 0.000\n",
      "    step: 50; loss: 18.342; l2dist: 3.890\n",
      "    step: 100; loss: 8.096; l2dist: 2.768\n",
      "    step: 150; loss: 5.483; l2dist: 2.262\n",
      "    step: 200; loss: 4.460; l2dist: 2.025\n",
      "    step: 250; loss: 3.978; l2dist: 1.905\n",
      "    step: 300; loss: 3.683; l2dist: 1.828\n",
      "    step: 350; loss: 3.671; l2dist: 1.814\n",
      "    step: 400; loss: 3.578; l2dist: 1.791\n",
      "    step: 450; loss: 3.529; l2dist: 1.790\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 172.794; l2dist: 0.000\n",
      "    step: 50; loss: 17.964; l2dist: 3.830\n",
      "    step: 100; loss: 8.234; l2dist: 2.722\n",
      "    step: 150; loss: 5.760; l2dist: 2.244\n",
      "    step: 200; loss: 4.628; l2dist: 2.010\n",
      "    step: 250; loss: 4.175; l2dist: 1.914\n",
      "    step: 300; loss: 3.877; l2dist: 1.849\n",
      "    step: 350; loss: 3.691; l2dist: 1.805\n",
      "    step: 400; loss: 3.691; l2dist: 1.798\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 102.248; l2dist: 0.000\n",
      "    step: 50; loss: 14.121; l2dist: 3.350\n",
      "    step: 100; loss: 6.174; l2dist: 2.335\n",
      "    step: 150; loss: 4.489; l2dist: 1.990\n",
      "    step: 200; loss: 3.903; l2dist: 1.856\n",
      "    step: 250; loss: 3.611; l2dist: 1.787\n",
      "    step: 300; loss: 3.453; l2dist: 1.750\n",
      "    step: 350; loss: 3.409; l2dist: 1.744\n",
      "    step: 400; loss: 3.401; l2dist: 1.735\n",
      "    step: 450; loss: 3.383; l2dist: 1.738\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.699; l2dist: 0.000\n",
      "    step: 50; loss: 11.721; l2dist: 2.919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 5.278; l2dist: 2.148\n",
      "    step: 150; loss: 4.001; l2dist: 1.874\n",
      "    step: 200; loss: 3.579; l2dist: 1.784\n",
      "    step: 250; loss: 3.413; l2dist: 1.739\n",
      "    step: 300; loss: 3.320; l2dist: 1.727\n",
      "    step: 350; loss: 3.305; l2dist: 1.720\n",
      "    step: 400; loss: 3.264; l2dist: 1.704\n",
      "    step: 450; loss: 3.272; l2dist: 1.706\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.007; l2dist: 0.000\n",
      "    step: 50; loss: 10.655; l2dist: 2.726\n",
      "    step: 100; loss: 4.984; l2dist: 2.070\n",
      "    step: 150; loss: 3.835; l2dist: 1.843\n",
      "    step: 200; loss: 3.484; l2dist: 1.748\n",
      "    step: 250; loss: 3.376; l2dist: 1.718\n",
      "    step: 300; loss: 3.331; l2dist: 1.724\n",
      "    step: 350; loss: 3.270; l2dist: 1.702\n",
      "    step: 400; loss: 3.255; l2dist: 1.698\n",
      "    step: 450; loss: 3.242; l2dist: 1.701\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.037; l2dist: 0.000\n",
      "    step: 50; loss: 10.103; l2dist: 2.641\n",
      "    step: 100; loss: 4.765; l2dist: 1.999\n",
      "    step: 150; loss: 3.692; l2dist: 1.785\n",
      "    step: 200; loss: 3.413; l2dist: 1.721\n",
      "    step: 250; loss: 3.301; l2dist: 1.692\n",
      "    step: 300; loss: 3.245; l2dist: 1.678\n",
      "    step: 350; loss: 3.223; l2dist: 1.671\n",
      "    step: 400; loss: 3.209; l2dist: 1.667\n",
      "    step: 450; loss: 3.194; l2dist: 1.674\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.584; l2dist: 0.000\n",
      "    step: 50; loss: 9.842; l2dist: 2.589\n",
      "    step: 100; loss: 4.743; l2dist: 1.986\n",
      "    step: 150; loss: 3.683; l2dist: 1.780\n",
      "    step: 200; loss: 3.419; l2dist: 1.721\n",
      "    step: 250; loss: 3.288; l2dist: 1.696\n",
      "    step: 300; loss: 3.245; l2dist: 1.686\n",
      "    step: 350; loss: 3.211; l2dist: 1.681\n",
      "    step: 400; loss: 3.203; l2dist: 1.682\n",
      "    step: 450; loss: 3.202; l2dist: 1.668\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.572; l2dist: 0.000\n",
      "    step: 50; loss: 9.799; l2dist: 2.589\n",
      "    step: 100; loss: 4.740; l2dist: 1.994\n",
      "    step: 150; loss: 3.685; l2dist: 1.783\n",
      "    step: 200; loss: 3.415; l2dist: 1.723\n",
      "    step: 250; loss: 3.293; l2dist: 1.701\n",
      "    step: 300; loss: 3.242; l2dist: 1.684\n",
      "    step: 350; loss: 3.215; l2dist: 1.684\n",
      "    step: 400; loss: 3.202; l2dist: 1.680\n",
      "    step: 450; loss: 3.201; l2dist: 1.680\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.769; l2dist: 0.000\n",
      "    step: 50; loss: 9.742; l2dist: 2.584\n",
      "    step: 100; loss: 4.728; l2dist: 1.991\n",
      "    step: 150; loss: 3.662; l2dist: 1.786\n",
      "    step: 200; loss: 3.385; l2dist: 1.724\n",
      "    step: 250; loss: 3.280; l2dist: 1.700\n",
      "    step: 300; loss: 3.236; l2dist: 1.696\n",
      "    step: 350; loss: 3.205; l2dist: 1.685\n",
      "    step: 400; loss: 3.191; l2dist: 1.678\n",
      "    step: 450; loss: 3.182; l2dist: 1.678\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.973; l2dist: 0.000\n",
      "    step: 50; loss: 9.784; l2dist: 2.595\n",
      "    step: 100; loss: 4.727; l2dist: 2.003\n",
      "    step: 150; loss: 3.684; l2dist: 1.793\n",
      "    step: 200; loss: 3.389; l2dist: 1.730\n",
      "    step: 250; loss: 3.292; l2dist: 1.712\n",
      "    step: 300; loss: 3.245; l2dist: 1.704\n",
      "    step: 350; loss: 3.222; l2dist: 1.693\n",
      "    step: 400; loss: 3.193; l2dist: 1.687\n",
      "    step: 450; loss: 3.197; l2dist: 1.692\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 115.523; l2dist: 0.000\n",
      "    step: 50; loss: 13.740; l2dist: 3.240\n",
      "    step: 100; loss: 5.343; l2dist: 2.180\n",
      "    step: 150; loss: 3.667; l2dist: 1.779\n",
      "    step: 200; loss: 3.009; l2dist: 1.603\n",
      "    step: 250; loss: 2.686; l2dist: 1.506\n",
      "    step: 300; loss: 2.549; l2dist: 1.470\n",
      "    step: 350; loss: 2.446; l2dist: 1.425\n",
      "    step: 400; loss: 2.431; l2dist: 1.425\n",
      "    step: 450; loss: 2.396; l2dist: 1.419\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 109.355; l2dist: 0.000\n",
      "    step: 50; loss: 12.205; l2dist: 2.994\n",
      "    step: 100; loss: 4.976; l2dist: 2.043\n",
      "    step: 150; loss: 3.466; l2dist: 1.688\n",
      "    step: 200; loss: 2.861; l2dist: 1.527\n",
      "    step: 250; loss: 2.576; l2dist: 1.455\n",
      "    step: 300; loss: 2.463; l2dist: 1.431\n",
      "    step: 350; loss: 2.361; l2dist: 1.390\n",
      "    step: 400; loss: 2.329; l2dist: 1.382\n",
      "    step: 450; loss: 2.256; l2dist: 1.373\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.852; l2dist: 0.000\n",
      "    step: 50; loss: 9.476; l2dist: 2.586\n",
      "    step: 100; loss: 3.832; l2dist: 1.790\n",
      "    step: 150; loss: 2.798; l2dist: 1.523\n",
      "    step: 200; loss: 2.449; l2dist: 1.423\n",
      "    step: 250; loss: 2.303; l2dist: 1.387\n",
      "    step: 300; loss: 2.217; l2dist: 1.354\n",
      "    step: 350; loss: 2.187; l2dist: 1.350\n",
      "    step: 400; loss: 2.168; l2dist: 1.346\n",
      "    step: 450; loss: 2.204; l2dist: 1.357\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.055; l2dist: 0.000\n",
      "    step: 50; loss: 7.893; l2dist: 2.279\n",
      "    step: 100; loss: 3.351; l2dist: 1.665\n",
      "    step: 150; loss: 2.499; l2dist: 1.445\n",
      "    step: 200; loss: 2.281; l2dist: 1.378\n",
      "    step: 250; loss: 2.164; l2dist: 1.343\n",
      "    step: 300; loss: 2.159; l2dist: 1.331\n",
      "    step: 350; loss: 2.133; l2dist: 1.325\n",
      "    step: 400; loss: 2.092; l2dist: 1.315\n",
      "    step: 450; loss: 2.074; l2dist: 1.316\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.527; l2dist: 0.000\n",
      "    step: 50; loss: 6.984; l2dist: 2.058\n",
      "    step: 100; loss: 3.252; l2dist: 1.583\n",
      "    step: 150; loss: 2.456; l2dist: 1.413\n",
      "    step: 200; loss: 2.246; l2dist: 1.354\n",
      "    step: 250; loss: 2.154; l2dist: 1.326\n",
      "    step: 300; loss: 2.116; l2dist: 1.319\n",
      "    step: 350; loss: 2.096; l2dist: 1.309\n",
      "    step: 400; loss: 2.101; l2dist: 1.304\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.361; l2dist: 0.000\n",
      "    step: 50; loss: 6.685; l2dist: 1.979\n",
      "    step: 100; loss: 3.104; l2dist: 1.514\n",
      "    step: 150; loss: 2.388; l2dist: 1.357\n",
      "    step: 200; loss: 2.197; l2dist: 1.312\n",
      "    step: 250; loss: 2.105; l2dist: 1.295\n",
      "    step: 300; loss: 2.085; l2dist: 1.284\n",
      "    step: 350; loss: 2.058; l2dist: 1.282\n",
      "    step: 400; loss: 2.055; l2dist: 1.276\n",
      "    step: 450; loss: 2.053; l2dist: 1.274\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.259; l2dist: 0.000\n",
      "    step: 50; loss: 6.503; l2dist: 1.923\n",
      "    step: 100; loss: 3.085; l2dist: 1.486\n",
      "    step: 150; loss: 2.366; l2dist: 1.330\n",
      "    step: 200; loss: 2.168; l2dist: 1.283\n",
      "    step: 250; loss: 2.092; l2dist: 1.263\n",
      "    step: 300; loss: 2.073; l2dist: 1.262\n",
      "    step: 350; loss: 2.050; l2dist: 1.255\n",
      "    step: 400; loss: 2.039; l2dist: 1.253\n",
      "    step: 450; loss: 2.033; l2dist: 1.253\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.337; l2dist: 0.000\n",
      "    step: 50; loss: 6.459; l2dist: 1.908\n",
      "    step: 100; loss: 3.098; l2dist: 1.485\n",
      "    step: 150; loss: 2.366; l2dist: 1.333\n",
      "    step: 200; loss: 2.178; l2dist: 1.291\n",
      "    step: 250; loss: 2.100; l2dist: 1.272\n",
      "    step: 300; loss: 2.066; l2dist: 1.263\n",
      "    step: 350; loss: 2.045; l2dist: 1.263\n",
      "    step: 400; loss: 2.043; l2dist: 1.259\n",
      "    step: 450; loss: 2.031; l2dist: 1.255\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.795; l2dist: 0.000\n",
      "    step: 50; loss: 6.414; l2dist: 1.895\n",
      "    step: 100; loss: 3.084; l2dist: 1.481\n",
      "    step: 150; loss: 2.351; l2dist: 1.324\n",
      "    step: 200; loss: 2.173; l2dist: 1.279\n",
      "    step: 250; loss: 2.094; l2dist: 1.266\n",
      "    step: 300; loss: 2.071; l2dist: 1.253\n",
      "    step: 350; loss: 2.047; l2dist: 1.250\n",
      "    step: 400; loss: 2.040; l2dist: 1.250\n",
      "    step: 450; loss: 2.034; l2dist: 1.252\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.024; l2dist: 0.000\n",
      "    step: 50; loss: 6.449; l2dist: 1.905\n",
      "    step: 100; loss: 3.091; l2dist: 1.489\n",
      "    step: 150; loss: 2.369; l2dist: 1.328\n",
      "    step: 200; loss: 2.174; l2dist: 1.285\n",
      "    step: 250; loss: 2.091; l2dist: 1.266\n",
      "    step: 300; loss: 2.068; l2dist: 1.264\n",
      "    step: 350; loss: 2.048; l2dist: 1.257\n",
      "    step: 400; loss: 2.042; l2dist: 1.256\n",
      "    step: 450; loss: 2.033; l2dist: 1.252\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 125.112; l2dist: 0.000\n",
      "    step: 50; loss: 13.109; l2dist: 3.440\n",
      "    step: 100; loss: 5.891; l2dist: 2.342\n",
      "    step: 150; loss: 3.924; l2dist: 1.894\n",
      "    step: 200; loss: 3.045; l2dist: 1.664\n",
      "    step: 250; loss: 2.746; l2dist: 1.570\n",
      "    step: 300; loss: 2.573; l2dist: 1.517\n",
      "    step: 350; loss: 2.506; l2dist: 1.499\n",
      "    step: 400; loss: 2.420; l2dist: 1.473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 450; loss: 2.436; l2dist: 1.474\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 134.265; l2dist: 0.000\n",
      "    step: 50; loss: 12.922; l2dist: 3.261\n",
      "    step: 100; loss: 5.963; l2dist: 2.262\n",
      "    step: 150; loss: 3.959; l2dist: 1.838\n",
      "    step: 200; loss: 3.117; l2dist: 1.638\n",
      "    step: 250; loss: 2.781; l2dist: 1.548\n",
      "    step: 300; loss: 2.557; l2dist: 1.498\n",
      "    step: 350; loss: 2.492; l2dist: 1.485\n",
      "    step: 400; loss: 2.399; l2dist: 1.449\n",
      "    step: 450; loss: 2.370; l2dist: 1.447\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 73.690; l2dist: 0.000\n",
      "    step: 50; loss: 9.981; l2dist: 2.812\n",
      "    step: 100; loss: 4.358; l2dist: 1.943\n",
      "    step: 150; loss: 3.068; l2dist: 1.630\n",
      "    step: 200; loss: 2.641; l2dist: 1.524\n",
      "    step: 250; loss: 2.444; l2dist: 1.471\n",
      "    step: 300; loss: 2.351; l2dist: 1.448\n",
      "    step: 350; loss: 2.284; l2dist: 1.427\n",
      "    step: 400; loss: 2.313; l2dist: 1.424\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.888; l2dist: 0.000\n",
      "    step: 50; loss: 8.333; l2dist: 2.490\n",
      "    step: 100; loss: 3.750; l2dist: 1.810\n",
      "    step: 150; loss: 2.782; l2dist: 1.563\n",
      "    step: 200; loss: 2.462; l2dist: 1.475\n",
      "    step: 250; loss: 2.314; l2dist: 1.432\n",
      "    step: 300; loss: 2.292; l2dist: 1.421\n",
      "    step: 350; loss: 2.244; l2dist: 1.416\n",
      "    step: 400; loss: 2.210; l2dist: 1.401\n",
      "    step: 450; loss: 2.210; l2dist: 1.401\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.215; l2dist: 0.000\n",
      "    step: 50; loss: 7.393; l2dist: 2.262\n",
      "    step: 100; loss: 3.543; l2dist: 1.726\n",
      "    step: 150; loss: 2.660; l2dist: 1.515\n",
      "    step: 200; loss: 2.368; l2dist: 1.443\n",
      "    step: 250; loss: 2.267; l2dist: 1.419\n",
      "    step: 300; loss: 2.209; l2dist: 1.399\n",
      "    step: 350; loss: 2.196; l2dist: 1.391\n",
      "    step: 400; loss: 2.182; l2dist: 1.384\n",
      "    step: 450; loss: 2.165; l2dist: 1.386\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.140; l2dist: 0.000\n",
      "    step: 50; loss: 6.870; l2dist: 2.121\n",
      "    step: 100; loss: 3.435; l2dist: 1.642\n",
      "    step: 150; loss: 2.560; l2dist: 1.464\n",
      "    step: 200; loss: 2.318; l2dist: 1.405\n",
      "    step: 250; loss: 2.218; l2dist: 1.379\n",
      "    step: 300; loss: 2.174; l2dist: 1.367\n",
      "    step: 350; loss: 2.158; l2dist: 1.366\n",
      "    step: 400; loss: 2.144; l2dist: 1.358\n",
      "    step: 450; loss: 2.143; l2dist: 1.359\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.419; l2dist: 0.000\n",
      "    step: 50; loss: 6.702; l2dist: 2.081\n",
      "    step: 100; loss: 3.413; l2dist: 1.627\n",
      "    step: 150; loss: 2.534; l2dist: 1.445\n",
      "    step: 200; loss: 2.314; l2dist: 1.386\n",
      "    step: 250; loss: 2.211; l2dist: 1.367\n",
      "    step: 300; loss: 2.179; l2dist: 1.357\n",
      "    step: 350; loss: 2.164; l2dist: 1.349\n",
      "    step: 400; loss: 2.158; l2dist: 1.358\n",
      "    step: 450; loss: 2.127; l2dist: 1.347\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.290; l2dist: 0.000\n",
      "    step: 50; loss: 6.650; l2dist: 2.069\n",
      "    step: 100; loss: 3.417; l2dist: 1.628\n",
      "    step: 150; loss: 2.535; l2dist: 1.447\n",
      "    step: 200; loss: 2.307; l2dist: 1.389\n",
      "    step: 250; loss: 2.225; l2dist: 1.364\n",
      "    step: 300; loss: 2.169; l2dist: 1.360\n",
      "    step: 350; loss: 2.150; l2dist: 1.353\n",
      "    step: 400; loss: 2.147; l2dist: 1.354\n",
      "    step: 450; loss: 2.130; l2dist: 1.350\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.584; l2dist: 0.000\n",
      "    step: 50; loss: 6.581; l2dist: 2.055\n",
      "    step: 100; loss: 3.398; l2dist: 1.624\n",
      "    step: 150; loss: 2.513; l2dist: 1.436\n",
      "    step: 200; loss: 2.299; l2dist: 1.385\n",
      "    step: 250; loss: 2.205; l2dist: 1.368\n",
      "    step: 300; loss: 2.158; l2dist: 1.359\n",
      "    step: 350; loss: 2.141; l2dist: 1.349\n",
      "    step: 400; loss: 2.123; l2dist: 1.346\n",
      "    step: 450; loss: 2.118; l2dist: 1.349\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.978; l2dist: 0.000\n",
      "    step: 50; loss: 6.627; l2dist: 2.069\n",
      "    step: 100; loss: 3.403; l2dist: 1.630\n",
      "    step: 150; loss: 2.530; l2dist: 1.450\n",
      "    step: 200; loss: 2.295; l2dist: 1.393\n",
      "    step: 250; loss: 2.203; l2dist: 1.374\n",
      "    step: 300; loss: 2.169; l2dist: 1.362\n",
      "    step: 350; loss: 2.152; l2dist: 1.352\n",
      "    step: 400; loss: 2.126; l2dist: 1.353\n",
      "    step: 450; loss: 2.126; l2dist: 1.351\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 128.264; l2dist: 0.000\n",
      "    step: 50; loss: 17.676; l2dist: 3.601\n",
      "    step: 100; loss: 6.686; l2dist: 2.505\n",
      "    step: 150; loss: 4.610; l2dist: 2.053\n",
      "    step: 200; loss: 3.753; l2dist: 1.841\n",
      "    step: 250; loss: 3.385; l2dist: 1.748\n",
      "    step: 300; loss: 3.183; l2dist: 1.688\n",
      "    step: 350; loss: 3.049; l2dist: 1.649\n",
      "    step: 400; loss: 2.985; l2dist: 1.629\n",
      "    step: 450; loss: 2.925; l2dist: 1.622\n",
      "binary step: 0; number of successful adv: 92/100\n",
      "    step: 0; loss: 160.836; l2dist: 0.000\n",
      "    step: 50; loss: 16.317; l2dist: 3.472\n",
      "    step: 100; loss: 6.786; l2dist: 2.462\n",
      "    step: 150; loss: 4.879; l2dist: 2.060\n",
      "    step: 200; loss: 3.973; l2dist: 1.858\n",
      "    step: 250; loss: 3.500; l2dist: 1.751\n",
      "    step: 300; loss: 3.294; l2dist: 1.693\n",
      "    step: 350; loss: 3.112; l2dist: 1.661\n",
      "    step: 400; loss: 3.007; l2dist: 1.624\n",
      "    step: 450; loss: 2.907; l2dist: 1.611\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.417; l2dist: 0.000\n",
      "    step: 50; loss: 12.243; l2dist: 2.965\n",
      "    step: 100; loss: 5.175; l2dist: 2.131\n",
      "    step: 150; loss: 3.844; l2dist: 1.829\n",
      "    step: 200; loss: 3.266; l2dist: 1.696\n",
      "    step: 250; loss: 3.020; l2dist: 1.637\n",
      "    step: 300; loss: 2.939; l2dist: 1.604\n",
      "    step: 350; loss: 2.855; l2dist: 1.592\n",
      "    step: 400; loss: 2.794; l2dist: 1.578\n",
      "    step: 450; loss: 2.773; l2dist: 1.568\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.074; l2dist: 0.000\n",
      "    step: 50; loss: 9.863; l2dist: 2.590\n",
      "    step: 100; loss: 4.503; l2dist: 1.971\n",
      "    step: 150; loss: 3.369; l2dist: 1.722\n",
      "    step: 200; loss: 3.006; l2dist: 1.632\n",
      "    step: 250; loss: 2.864; l2dist: 1.591\n",
      "    step: 300; loss: 2.785; l2dist: 1.569\n",
      "    step: 350; loss: 2.736; l2dist: 1.560\n",
      "    step: 400; loss: 2.727; l2dist: 1.547\n",
      "    step: 450; loss: 2.681; l2dist: 1.535\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.872; l2dist: 0.000\n",
      "    step: 50; loss: 8.816; l2dist: 2.381\n",
      "    step: 100; loss: 4.242; l2dist: 1.868\n",
      "    step: 150; loss: 3.203; l2dist: 1.658\n",
      "    step: 200; loss: 2.949; l2dist: 1.594\n",
      "    step: 250; loss: 2.815; l2dist: 1.559\n",
      "    step: 300; loss: 2.754; l2dist: 1.548\n",
      "    step: 350; loss: 2.737; l2dist: 1.537\n",
      "    step: 400; loss: 2.705; l2dist: 1.534\n",
      "    step: 450; loss: 2.689; l2dist: 1.529\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.575; l2dist: 0.000\n",
      "    step: 50; loss: 8.273; l2dist: 2.252\n",
      "    step: 100; loss: 4.042; l2dist: 1.785\n",
      "    step: 150; loss: 3.103; l2dist: 1.600\n",
      "    step: 200; loss: 2.876; l2dist: 1.546\n",
      "    step: 250; loss: 2.765; l2dist: 1.524\n",
      "    step: 300; loss: 2.729; l2dist: 1.519\n",
      "    step: 350; loss: 2.677; l2dist: 1.505\n",
      "    step: 400; loss: 2.663; l2dist: 1.501\n",
      "    step: 450; loss: 2.645; l2dist: 1.500\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.566; l2dist: 0.000\n",
      "    step: 50; loss: 8.192; l2dist: 2.237\n",
      "    step: 100; loss: 4.046; l2dist: 1.795\n",
      "    step: 150; loss: 3.098; l2dist: 1.613\n",
      "    step: 200; loss: 2.882; l2dist: 1.564\n",
      "    step: 250; loss: 2.774; l2dist: 1.537\n",
      "    step: 300; loss: 2.723; l2dist: 1.537\n",
      "    step: 350; loss: 2.686; l2dist: 1.523\n",
      "    step: 400; loss: 2.671; l2dist: 1.516\n",
      "    step: 450; loss: 2.674; l2dist: 1.516\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.141; l2dist: 0.000\n",
      "    step: 50; loss: 8.124; l2dist: 2.228\n",
      "    step: 100; loss: 4.035; l2dist: 1.798\n",
      "    step: 150; loss: 3.084; l2dist: 1.616\n",
      "    step: 200; loss: 2.883; l2dist: 1.565\n",
      "    step: 250; loss: 2.768; l2dist: 1.550\n",
      "    step: 300; loss: 2.733; l2dist: 1.536\n",
      "    step: 350; loss: 2.674; l2dist: 1.524\n",
      "    step: 400; loss: 2.687; l2dist: 1.524\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.505; l2dist: 0.000\n",
      "    step: 50; loss: 8.071; l2dist: 2.218\n",
      "    step: 100; loss: 4.031; l2dist: 1.799\n",
      "    step: 150; loss: 3.080; l2dist: 1.612\n",
      "    step: 200; loss: 2.856; l2dist: 1.561\n",
      "    step: 250; loss: 2.759; l2dist: 1.536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 2.707; l2dist: 1.536\n",
      "    step: 350; loss: 2.691; l2dist: 1.526\n",
      "    step: 400; loss: 2.673; l2dist: 1.519\n",
      "    step: 450; loss: 2.653; l2dist: 1.517\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.698; l2dist: 0.000\n",
      "    step: 50; loss: 8.127; l2dist: 2.229\n",
      "    step: 100; loss: 4.038; l2dist: 1.803\n",
      "    step: 150; loss: 3.105; l2dist: 1.620\n",
      "    step: 200; loss: 2.862; l2dist: 1.569\n",
      "    step: 250; loss: 2.767; l2dist: 1.548\n",
      "    step: 300; loss: 2.717; l2dist: 1.534\n",
      "    step: 350; loss: 2.706; l2dist: 1.528\n",
      "    step: 400; loss: 2.691; l2dist: 1.522\n",
      "    step: 450; loss: 2.679; l2dist: 1.529\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 140.648; l2dist: 0.000\n",
      "    step: 50; loss: 15.508; l2dist: 3.734\n",
      "    step: 100; loss: 7.026; l2dist: 2.585\n",
      "    step: 150; loss: 4.568; l2dist: 2.075\n",
      "    step: 200; loss: 3.566; l2dist: 1.820\n",
      "    step: 250; loss: 3.146; l2dist: 1.703\n",
      "    step: 300; loss: 2.919; l2dist: 1.644\n",
      "    step: 350; loss: 2.798; l2dist: 1.600\n",
      "    step: 400; loss: 2.768; l2dist: 1.586\n",
      "    step: 450; loss: 2.683; l2dist: 1.571\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 140.698; l2dist: 0.000\n",
      "    step: 50; loss: 14.725; l2dist: 3.619\n",
      "    step: 100; loss: 6.889; l2dist: 2.503\n",
      "    step: 150; loss: 4.570; l2dist: 2.023\n",
      "    step: 200; loss: 3.614; l2dist: 1.798\n",
      "    step: 250; loss: 3.149; l2dist: 1.686\n",
      "    step: 300; loss: 2.926; l2dist: 1.626\n",
      "    step: 350; loss: 2.803; l2dist: 1.591\n",
      "    step: 400; loss: 2.729; l2dist: 1.579\n",
      "    step: 450; loss: 2.676; l2dist: 1.556\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 77.749; l2dist: 0.000\n",
      "    step: 50; loss: 11.550; l2dist: 3.137\n",
      "    step: 100; loss: 4.923; l2dist: 2.104\n",
      "    step: 150; loss: 3.465; l2dist: 1.760\n",
      "    step: 200; loss: 2.982; l2dist: 1.640\n",
      "    step: 250; loss: 2.797; l2dist: 1.594\n",
      "    step: 300; loss: 2.661; l2dist: 1.552\n",
      "    step: 350; loss: 2.620; l2dist: 1.543\n",
      "    step: 400; loss: 2.595; l2dist: 1.531\n",
      "    step: 450; loss: 2.567; l2dist: 1.528\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.553; l2dist: 0.000\n",
      "    step: 50; loss: 9.716; l2dist: 2.761\n",
      "    step: 100; loss: 4.194; l2dist: 1.937\n",
      "    step: 150; loss: 3.081; l2dist: 1.664\n",
      "    step: 200; loss: 2.770; l2dist: 1.580\n",
      "    step: 250; loss: 2.652; l2dist: 1.549\n",
      "    step: 300; loss: 2.572; l2dist: 1.524\n",
      "    step: 350; loss: 2.506; l2dist: 1.508\n",
      "    step: 400; loss: 2.476; l2dist: 1.503\n",
      "    step: 450; loss: 2.479; l2dist: 1.504\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.260; l2dist: 0.000\n",
      "    step: 50; loss: 8.678; l2dist: 2.504\n",
      "    step: 100; loss: 3.902; l2dist: 1.822\n",
      "    step: 150; loss: 2.930; l2dist: 1.619\n",
      "    step: 200; loss: 2.644; l2dist: 1.544\n",
      "    step: 250; loss: 2.543; l2dist: 1.513\n",
      "    step: 300; loss: 2.481; l2dist: 1.501\n",
      "    step: 350; loss: 2.464; l2dist: 1.497\n",
      "    step: 400; loss: 2.463; l2dist: 1.493\n",
      "    step: 450; loss: 2.438; l2dist: 1.483\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.398; l2dist: 0.000\n",
      "    step: 50; loss: 8.256; l2dist: 2.428\n",
      "    step: 100; loss: 3.833; l2dist: 1.790\n",
      "    step: 150; loss: 2.881; l2dist: 1.586\n",
      "    step: 200; loss: 2.632; l2dist: 1.528\n",
      "    step: 250; loss: 2.529; l2dist: 1.502\n",
      "    step: 300; loss: 2.472; l2dist: 1.483\n",
      "    step: 350; loss: 2.464; l2dist: 1.480\n",
      "    step: 400; loss: 2.444; l2dist: 1.482\n",
      "    step: 450; loss: 2.432; l2dist: 1.473\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.302; l2dist: 0.000\n",
      "    step: 50; loss: 8.140; l2dist: 2.391\n",
      "    step: 100; loss: 3.826; l2dist: 1.782\n",
      "    step: 150; loss: 2.864; l2dist: 1.592\n",
      "    step: 200; loss: 2.624; l2dist: 1.524\n",
      "    step: 250; loss: 2.516; l2dist: 1.504\n",
      "    step: 300; loss: 2.478; l2dist: 1.497\n",
      "    step: 350; loss: 2.457; l2dist: 1.487\n",
      "    step: 400; loss: 2.441; l2dist: 1.472\n",
      "    step: 450; loss: 2.426; l2dist: 1.476\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.618; l2dist: 0.000\n",
      "    step: 50; loss: 8.103; l2dist: 2.386\n",
      "    step: 100; loss: 3.818; l2dist: 1.791\n",
      "    step: 150; loss: 2.850; l2dist: 1.595\n",
      "    step: 200; loss: 2.631; l2dist: 1.525\n",
      "    step: 250; loss: 2.515; l2dist: 1.504\n",
      "    step: 300; loss: 2.464; l2dist: 1.494\n",
      "    step: 350; loss: 2.459; l2dist: 1.490\n",
      "    step: 400; loss: 2.426; l2dist: 1.486\n",
      "    step: 450; loss: 2.434; l2dist: 1.489\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.901; l2dist: 0.000\n",
      "    step: 50; loss: 8.044; l2dist: 2.370\n",
      "    step: 100; loss: 3.805; l2dist: 1.788\n",
      "    step: 150; loss: 2.853; l2dist: 1.594\n",
      "    step: 200; loss: 2.617; l2dist: 1.533\n",
      "    step: 250; loss: 2.525; l2dist: 1.508\n",
      "    step: 300; loss: 2.476; l2dist: 1.497\n",
      "    step: 350; loss: 2.445; l2dist: 1.488\n",
      "    step: 400; loss: 2.438; l2dist: 1.484\n",
      "    step: 450; loss: 2.427; l2dist: 1.483\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.157; l2dist: 0.000\n",
      "    step: 50; loss: 8.069; l2dist: 2.379\n",
      "    step: 100; loss: 3.810; l2dist: 1.798\n",
      "    step: 150; loss: 2.849; l2dist: 1.593\n",
      "    step: 200; loss: 2.618; l2dist: 1.538\n",
      "    step: 250; loss: 2.519; l2dist: 1.506\n",
      "    step: 300; loss: 2.477; l2dist: 1.494\n",
      "    step: 350; loss: 2.447; l2dist: 1.486\n",
      "    step: 400; loss: 2.441; l2dist: 1.492\n",
      "    step: 450; loss: 2.430; l2dist: 1.481\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 146.501; l2dist: 0.000\n",
      "    step: 50; loss: 20.939; l2dist: 3.780\n",
      "    step: 100; loss: 7.487; l2dist: 2.680\n",
      "    step: 150; loss: 5.057; l2dist: 2.183\n",
      "    step: 200; loss: 3.959; l2dist: 1.913\n",
      "    step: 250; loss: 3.483; l2dist: 1.774\n",
      "    step: 300; loss: 3.228; l2dist: 1.721\n",
      "    step: 350; loss: 3.032; l2dist: 1.657\n",
      "    step: 400; loss: 2.981; l2dist: 1.635\n",
      "    step: 450; loss: 2.959; l2dist: 1.643\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 104.889; l2dist: 0.000\n",
      "    step: 50; loss: 17.433; l2dist: 3.471\n",
      "    step: 100; loss: 5.974; l2dist: 2.337\n",
      "    step: 150; loss: 4.164; l2dist: 1.929\n",
      "    step: 200; loss: 3.485; l2dist: 1.766\n",
      "    step: 250; loss: 3.193; l2dist: 1.686\n",
      "    step: 300; loss: 3.071; l2dist: 1.654\n",
      "    step: 350; loss: 2.927; l2dist: 1.614\n",
      "    step: 400; loss: 2.860; l2dist: 1.603\n",
      "    step: 450; loss: 2.832; l2dist: 1.598\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.170; l2dist: 0.000\n",
      "    step: 50; loss: 14.011; l2dist: 3.090\n",
      "    step: 100; loss: 4.731; l2dist: 2.063\n",
      "    step: 150; loss: 3.544; l2dist: 1.764\n",
      "    step: 200; loss: 3.138; l2dist: 1.667\n",
      "    step: 250; loss: 2.955; l2dist: 1.617\n",
      "    step: 300; loss: 2.837; l2dist: 1.594\n",
      "    step: 350; loss: 2.811; l2dist: 1.584\n",
      "    step: 400; loss: 2.722; l2dist: 1.571\n",
      "    step: 450; loss: 2.687; l2dist: 1.556\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.464; l2dist: 0.000\n",
      "    step: 50; loss: 11.993; l2dist: 2.756\n",
      "    step: 100; loss: 4.253; l2dist: 1.928\n",
      "    step: 150; loss: 3.280; l2dist: 1.700\n",
      "    step: 200; loss: 2.976; l2dist: 1.621\n",
      "    step: 250; loss: 2.823; l2dist: 1.584\n",
      "    step: 300; loss: 2.739; l2dist: 1.566\n",
      "    step: 350; loss: 2.738; l2dist: 1.559\n",
      "    step: 400; loss: 2.669; l2dist: 1.551\n",
      "    step: 450; loss: 2.646; l2dist: 1.543\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.548; l2dist: 0.000\n",
      "    step: 50; loss: 10.472; l2dist: 2.495\n",
      "    step: 100; loss: 4.016; l2dist: 1.821\n",
      "    step: 150; loss: 3.126; l2dist: 1.633\n",
      "    step: 200; loss: 2.854; l2dist: 1.575\n",
      "    step: 250; loss: 2.742; l2dist: 1.546\n",
      "    step: 300; loss: 2.670; l2dist: 1.532\n",
      "    step: 350; loss: 2.628; l2dist: 1.521\n",
      "    step: 400; loss: 2.643; l2dist: 1.520\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.044; l2dist: 0.000\n",
      "    step: 50; loss: 10.271; l2dist: 2.450\n",
      "    step: 100; loss: 4.012; l2dist: 1.815\n",
      "    step: 150; loss: 3.116; l2dist: 1.629\n",
      "    step: 200; loss: 2.853; l2dist: 1.572\n",
      "    step: 250; loss: 2.741; l2dist: 1.546\n",
      "    step: 300; loss: 2.674; l2dist: 1.534\n",
      "    step: 350; loss: 2.638; l2dist: 1.525\n",
      "    step: 400; loss: 2.633; l2dist: 1.519\n",
      "    step: 450; loss: 2.617; l2dist: 1.516\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.063; l2dist: 0.000\n",
      "    step: 50; loss: 10.239; l2dist: 2.446\n",
      "    step: 100; loss: 4.024; l2dist: 1.817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 3.125; l2dist: 1.630\n",
      "    step: 200; loss: 2.866; l2dist: 1.578\n",
      "    step: 250; loss: 2.761; l2dist: 1.554\n",
      "    step: 300; loss: 2.686; l2dist: 1.537\n",
      "    step: 350; loss: 2.653; l2dist: 1.532\n",
      "    step: 400; loss: 2.634; l2dist: 1.527\n",
      "    step: 450; loss: 2.639; l2dist: 1.526\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.724; l2dist: 0.000\n",
      "    step: 50; loss: 10.223; l2dist: 2.447\n",
      "    step: 100; loss: 4.023; l2dist: 1.822\n",
      "    step: 150; loss: 3.109; l2dist: 1.631\n",
      "    step: 200; loss: 2.857; l2dist: 1.571\n",
      "    step: 250; loss: 2.742; l2dist: 1.550\n",
      "    step: 300; loss: 2.683; l2dist: 1.536\n",
      "    step: 350; loss: 2.646; l2dist: 1.526\n",
      "    step: 400; loss: 2.622; l2dist: 1.526\n",
      "    step: 450; loss: 2.620; l2dist: 1.520\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.512; l2dist: 0.000\n",
      "    step: 50; loss: 10.197; l2dist: 2.446\n",
      "    step: 100; loss: 4.036; l2dist: 1.823\n",
      "    step: 150; loss: 3.119; l2dist: 1.637\n",
      "    step: 200; loss: 2.862; l2dist: 1.577\n",
      "    step: 250; loss: 2.746; l2dist: 1.559\n",
      "    step: 300; loss: 2.690; l2dist: 1.548\n",
      "    step: 350; loss: 2.658; l2dist: 1.528\n",
      "    step: 400; loss: 2.640; l2dist: 1.534\n",
      "    step: 450; loss: 2.614; l2dist: 1.525\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.770; l2dist: 0.000\n",
      "    step: 50; loss: 10.246; l2dist: 2.455\n",
      "    step: 100; loss: 4.041; l2dist: 1.827\n",
      "    step: 150; loss: 3.110; l2dist: 1.640\n",
      "    step: 200; loss: 2.865; l2dist: 1.581\n",
      "    step: 250; loss: 2.746; l2dist: 1.558\n",
      "    step: 300; loss: 2.696; l2dist: 1.544\n",
      "    step: 350; loss: 2.657; l2dist: 1.538\n",
      "    step: 400; loss: 2.645; l2dist: 1.534\n",
      "    step: 450; loss: 2.626; l2dist: 1.529\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 154.033; l2dist: 0.000\n",
      "    step: 50; loss: 17.603; l2dist: 3.870\n",
      "    step: 100; loss: 7.841; l2dist: 2.728\n",
      "    step: 150; loss: 5.217; l2dist: 2.208\n",
      "    step: 200; loss: 4.097; l2dist: 1.947\n",
      "    step: 250; loss: 3.648; l2dist: 1.824\n",
      "    step: 300; loss: 3.408; l2dist: 1.747\n",
      "    step: 350; loss: 3.313; l2dist: 1.735\n",
      "    step: 400; loss: 3.237; l2dist: 1.706\n",
      "    step: 450; loss: 3.207; l2dist: 1.697\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 107.313; l2dist: 0.000\n",
      "    step: 50; loss: 13.856; l2dist: 3.435\n",
      "    step: 100; loss: 5.760; l2dist: 2.313\n",
      "    step: 150; loss: 4.131; l2dist: 1.942\n",
      "    step: 200; loss: 3.581; l2dist: 1.794\n",
      "    step: 250; loss: 3.300; l2dist: 1.728\n",
      "    step: 300; loss: 3.141; l2dist: 1.689\n",
      "    step: 350; loss: 3.108; l2dist: 1.677\n",
      "    step: 400; loss: 3.081; l2dist: 1.670\n",
      "    step: 450; loss: 3.020; l2dist: 1.657\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.204; l2dist: 0.000\n",
      "    step: 50; loss: 11.090; l2dist: 2.980\n",
      "    step: 100; loss: 4.655; l2dist: 2.062\n",
      "    step: 150; loss: 3.536; l2dist: 1.791\n",
      "    step: 200; loss: 3.221; l2dist: 1.713\n",
      "    step: 250; loss: 3.069; l2dist: 1.665\n",
      "    step: 300; loss: 3.008; l2dist: 1.652\n",
      "    step: 350; loss: 2.965; l2dist: 1.641\n",
      "    step: 400; loss: 2.957; l2dist: 1.629\n",
      "    step: 450; loss: 2.919; l2dist: 1.623\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.947; l2dist: 0.000\n",
      "    step: 50; loss: 9.262; l2dist: 2.583\n",
      "    step: 100; loss: 4.275; l2dist: 1.950\n",
      "    step: 150; loss: 3.301; l2dist: 1.726\n",
      "    step: 200; loss: 3.077; l2dist: 1.661\n",
      "    step: 250; loss: 2.980; l2dist: 1.632\n",
      "    step: 300; loss: 2.911; l2dist: 1.627\n",
      "    step: 350; loss: 2.893; l2dist: 1.613\n",
      "    step: 400; loss: 2.877; l2dist: 1.612\n",
      "    step: 450; loss: 2.867; l2dist: 1.605\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.226; l2dist: 0.000\n",
      "    step: 50; loss: 8.893; l2dist: 2.477\n",
      "    step: 100; loss: 4.274; l2dist: 1.915\n",
      "    step: 150; loss: 3.275; l2dist: 1.706\n",
      "    step: 200; loss: 3.053; l2dist: 1.650\n",
      "    step: 250; loss: 2.959; l2dist: 1.623\n",
      "    step: 300; loss: 2.918; l2dist: 1.613\n",
      "    step: 350; loss: 2.891; l2dist: 1.608\n",
      "    step: 400; loss: 2.882; l2dist: 1.607\n",
      "    step: 450; loss: 2.877; l2dist: 1.600\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.589; l2dist: 0.000\n",
      "    step: 50; loss: 8.428; l2dist: 2.341\n",
      "    step: 100; loss: 4.197; l2dist: 1.848\n",
      "    step: 150; loss: 3.235; l2dist: 1.662\n",
      "    step: 200; loss: 3.018; l2dist: 1.614\n",
      "    step: 250; loss: 2.925; l2dist: 1.596\n",
      "    step: 300; loss: 2.887; l2dist: 1.588\n",
      "    step: 350; loss: 2.859; l2dist: 1.580\n",
      "    step: 400; loss: 2.866; l2dist: 1.578\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.459; l2dist: 0.000\n",
      "    step: 50; loss: 8.407; l2dist: 2.344\n",
      "    step: 100; loss: 4.223; l2dist: 1.864\n",
      "    step: 150; loss: 3.243; l2dist: 1.675\n",
      "    step: 200; loss: 3.034; l2dist: 1.626\n",
      "    step: 250; loss: 2.947; l2dist: 1.612\n",
      "    step: 300; loss: 2.918; l2dist: 1.608\n",
      "    step: 350; loss: 2.885; l2dist: 1.592\n",
      "    step: 400; loss: 2.891; l2dist: 1.597\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.752; l2dist: 0.000\n",
      "    step: 50; loss: 8.380; l2dist: 2.340\n",
      "    step: 100; loss: 4.205; l2dist: 1.862\n",
      "    step: 150; loss: 3.236; l2dist: 1.676\n",
      "    step: 200; loss: 3.034; l2dist: 1.623\n",
      "    step: 250; loss: 2.935; l2dist: 1.611\n",
      "    step: 300; loss: 2.924; l2dist: 1.597\n",
      "    step: 350; loss: 2.893; l2dist: 1.596\n",
      "    step: 400; loss: 2.880; l2dist: 1.596\n",
      "    step: 450; loss: 2.863; l2dist: 1.592\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.563; l2dist: 0.000\n",
      "    step: 50; loss: 8.400; l2dist: 2.340\n",
      "    step: 100; loss: 4.216; l2dist: 1.866\n",
      "    step: 150; loss: 3.238; l2dist: 1.676\n",
      "    step: 200; loss: 3.025; l2dist: 1.627\n",
      "    step: 250; loss: 2.944; l2dist: 1.612\n",
      "    step: 300; loss: 2.912; l2dist: 1.604\n",
      "    step: 350; loss: 2.893; l2dist: 1.594\n",
      "    step: 400; loss: 2.872; l2dist: 1.591\n",
      "    step: 450; loss: 2.872; l2dist: 1.595\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.807; l2dist: 0.000\n",
      "    step: 50; loss: 8.440; l2dist: 2.356\n",
      "    step: 100; loss: 4.207; l2dist: 1.876\n",
      "    step: 150; loss: 3.232; l2dist: 1.678\n",
      "    step: 200; loss: 3.039; l2dist: 1.637\n",
      "    step: 250; loss: 2.945; l2dist: 1.617\n",
      "    step: 300; loss: 2.901; l2dist: 1.603\n",
      "    step: 350; loss: 2.893; l2dist: 1.611\n",
      "    step: 400; loss: 2.878; l2dist: 1.596\n",
      "    step: 450; loss: 2.863; l2dist: 1.595\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 155.293; l2dist: 0.000\n",
      "    step: 50; loss: 17.182; l2dist: 3.924\n",
      "    step: 100; loss: 7.841; l2dist: 2.734\n",
      "    step: 150; loss: 5.337; l2dist: 2.240\n",
      "    step: 200; loss: 4.264; l2dist: 1.985\n",
      "    step: 250; loss: 3.819; l2dist: 1.875\n",
      "    step: 300; loss: 3.584; l2dist: 1.809\n",
      "    step: 350; loss: 3.425; l2dist: 1.773\n",
      "    step: 400; loss: 3.335; l2dist: 1.753\n",
      "    step: 450; loss: 3.254; l2dist: 1.732\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 189.429; l2dist: 0.000\n",
      "    step: 50; loss: 17.273; l2dist: 3.859\n",
      "    step: 100; loss: 8.089; l2dist: 2.711\n",
      "    step: 150; loss: 5.644; l2dist: 2.240\n",
      "    step: 200; loss: 4.536; l2dist: 2.010\n",
      "    step: 250; loss: 3.975; l2dist: 1.891\n",
      "    step: 300; loss: 3.696; l2dist: 1.829\n",
      "    step: 350; loss: 3.521; l2dist: 1.784\n",
      "    step: 400; loss: 3.414; l2dist: 1.757\n",
      "    step: 450; loss: 3.302; l2dist: 1.739\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 104.059; l2dist: 0.000\n",
      "    step: 50; loss: 13.299; l2dist: 3.297\n",
      "    step: 100; loss: 5.900; l2dist: 2.300\n",
      "    step: 150; loss: 4.289; l2dist: 1.967\n",
      "    step: 200; loss: 3.690; l2dist: 1.824\n",
      "    step: 250; loss: 3.414; l2dist: 1.764\n",
      "    step: 300; loss: 3.251; l2dist: 1.720\n",
      "    step: 350; loss: 3.218; l2dist: 1.708\n",
      "    step: 400; loss: 3.164; l2dist: 1.697\n",
      "    step: 450; loss: 3.114; l2dist: 1.685\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 65.148; l2dist: 0.000\n",
      "    step: 50; loss: 10.980; l2dist: 2.909\n",
      "    step: 100; loss: 5.039; l2dist: 2.136\n",
      "    step: 150; loss: 3.789; l2dist: 1.851\n",
      "    step: 200; loss: 3.376; l2dist: 1.756\n",
      "    step: 250; loss: 3.210; l2dist: 1.716\n",
      "    step: 300; loss: 3.131; l2dist: 1.686\n",
      "    step: 350; loss: 3.080; l2dist: 1.672\n",
      "    step: 400; loss: 3.091; l2dist: 1.679\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.069; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 9.723; l2dist: 2.641\n",
      "    step: 100; loss: 4.668; l2dist: 1.991\n",
      "    step: 150; loss: 3.541; l2dist: 1.768\n",
      "    step: 200; loss: 3.221; l2dist: 1.689\n",
      "    step: 250; loss: 3.098; l2dist: 1.658\n",
      "    step: 300; loss: 3.028; l2dist: 1.654\n",
      "    step: 350; loss: 3.005; l2dist: 1.638\n",
      "    step: 400; loss: 2.969; l2dist: 1.635\n",
      "    step: 450; loss: 2.972; l2dist: 1.627\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.160; l2dist: 0.000\n",
      "    step: 50; loss: 9.425; l2dist: 2.592\n",
      "    step: 100; loss: 4.471; l2dist: 1.939\n",
      "    step: 150; loss: 3.447; l2dist: 1.724\n",
      "    step: 200; loss: 3.165; l2dist: 1.660\n",
      "    step: 250; loss: 3.041; l2dist: 1.630\n",
      "    step: 300; loss: 2.980; l2dist: 1.617\n",
      "    step: 350; loss: 2.959; l2dist: 1.607\n",
      "    step: 400; loss: 2.941; l2dist: 1.604\n",
      "    step: 450; loss: 2.944; l2dist: 1.612\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.022; l2dist: 0.000\n",
      "    step: 50; loss: 9.259; l2dist: 2.550\n",
      "    step: 100; loss: 4.448; l2dist: 1.933\n",
      "    step: 150; loss: 3.405; l2dist: 1.712\n",
      "    step: 200; loss: 3.153; l2dist: 1.657\n",
      "    step: 250; loss: 3.026; l2dist: 1.628\n",
      "    step: 300; loss: 3.017; l2dist: 1.612\n",
      "    step: 350; loss: 2.957; l2dist: 1.611\n",
      "    step: 400; loss: 2.956; l2dist: 1.618\n",
      "    step: 450; loss: 2.933; l2dist: 1.612\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.798; l2dist: 0.000\n",
      "    step: 50; loss: 9.242; l2dist: 2.545\n",
      "    step: 100; loss: 4.432; l2dist: 1.925\n",
      "    step: 150; loss: 3.411; l2dist: 1.724\n",
      "    step: 200; loss: 3.147; l2dist: 1.667\n",
      "    step: 250; loss: 3.048; l2dist: 1.645\n",
      "    step: 300; loss: 2.996; l2dist: 1.624\n",
      "    step: 350; loss: 2.970; l2dist: 1.618\n",
      "    step: 400; loss: 2.948; l2dist: 1.619\n",
      "    step: 450; loss: 2.944; l2dist: 1.623\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.130; l2dist: 0.000\n",
      "    step: 50; loss: 9.205; l2dist: 2.540\n",
      "    step: 100; loss: 4.424; l2dist: 1.927\n",
      "    step: 150; loss: 3.396; l2dist: 1.717\n",
      "    step: 200; loss: 3.132; l2dist: 1.662\n",
      "    step: 250; loss: 3.034; l2dist: 1.634\n",
      "    step: 300; loss: 2.994; l2dist: 1.625\n",
      "    step: 350; loss: 2.956; l2dist: 1.624\n",
      "    step: 400; loss: 2.953; l2dist: 1.621\n",
      "    step: 450; loss: 2.930; l2dist: 1.618\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.310; l2dist: 0.000\n",
      "    step: 50; loss: 9.243; l2dist: 2.550\n",
      "    step: 100; loss: 4.427; l2dist: 1.932\n",
      "    step: 150; loss: 3.392; l2dist: 1.729\n",
      "    step: 200; loss: 3.133; l2dist: 1.661\n",
      "    step: 250; loss: 3.040; l2dist: 1.638\n",
      "    step: 300; loss: 2.991; l2dist: 1.638\n",
      "    step: 350; loss: 2.957; l2dist: 1.628\n",
      "    step: 400; loss: 2.949; l2dist: 1.626\n",
      "    step: 450; loss: 2.946; l2dist: 1.621\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 134.890; l2dist: 0.000\n",
      "    step: 50; loss: 14.088; l2dist: 3.531\n",
      "    step: 100; loss: 6.168; l2dist: 2.419\n",
      "    step: 150; loss: 4.016; l2dist: 1.930\n",
      "    step: 200; loss: 3.153; l2dist: 1.701\n",
      "    step: 250; loss: 2.782; l2dist: 1.592\n",
      "    step: 300; loss: 2.608; l2dist: 1.527\n",
      "    step: 350; loss: 2.465; l2dist: 1.501\n",
      "    step: 400; loss: 2.443; l2dist: 1.479\n",
      "    step: 450; loss: 2.429; l2dist: 1.475\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 77.023; l2dist: 0.000\n",
      "    step: 50; loss: 10.746; l2dist: 3.025\n",
      "    step: 100; loss: 4.333; l2dist: 2.005\n",
      "    step: 150; loss: 3.086; l2dist: 1.680\n",
      "    step: 200; loss: 2.604; l2dist: 1.540\n",
      "    step: 250; loss: 2.432; l2dist: 1.484\n",
      "    step: 300; loss: 2.364; l2dist: 1.466\n",
      "    step: 350; loss: 2.360; l2dist: 1.454\n",
      "    step: 400; loss: 2.288; l2dist: 1.437\n",
      "    step: 450; loss: 2.297; l2dist: 1.439\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.104; l2dist: 0.000\n",
      "    step: 50; loss: 8.828; l2dist: 2.656\n",
      "    step: 100; loss: 3.591; l2dist: 1.824\n",
      "    step: 150; loss: 2.647; l2dist: 1.552\n",
      "    step: 200; loss: 2.414; l2dist: 1.479\n",
      "    step: 250; loss: 2.313; l2dist: 1.450\n",
      "    step: 300; loss: 2.266; l2dist: 1.432\n",
      "    step: 350; loss: 2.244; l2dist: 1.417\n",
      "    step: 400; loss: 2.212; l2dist: 1.410\n",
      "    step: 450; loss: 2.213; l2dist: 1.412\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.735; l2dist: 0.000\n",
      "    step: 50; loss: 7.800; l2dist: 2.403\n",
      "    step: 100; loss: 3.383; l2dist: 1.745\n",
      "    step: 150; loss: 2.554; l2dist: 1.518\n",
      "    step: 200; loss: 2.345; l2dist: 1.455\n",
      "    step: 250; loss: 2.270; l2dist: 1.433\n",
      "    step: 300; loss: 2.224; l2dist: 1.418\n",
      "    step: 350; loss: 2.199; l2dist: 1.406\n",
      "    step: 400; loss: 2.190; l2dist: 1.402\n",
      "    step: 450; loss: 2.175; l2dist: 1.400\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.291; l2dist: 0.000\n",
      "    step: 50; loss: 7.264; l2dist: 2.236\n",
      "    step: 100; loss: 3.328; l2dist: 1.678\n",
      "    step: 150; loss: 2.520; l2dist: 1.484\n",
      "    step: 200; loss: 2.333; l2dist: 1.428\n",
      "    step: 250; loss: 2.249; l2dist: 1.407\n",
      "    step: 300; loss: 2.213; l2dist: 1.393\n",
      "    step: 350; loss: 2.197; l2dist: 1.386\n",
      "    step: 400; loss: 2.178; l2dist: 1.383\n",
      "    step: 450; loss: 2.169; l2dist: 1.385\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.729; l2dist: 0.000\n",
      "    step: 50; loss: 7.094; l2dist: 2.184\n",
      "    step: 100; loss: 3.281; l2dist: 1.634\n",
      "    step: 150; loss: 2.505; l2dist: 1.441\n",
      "    step: 200; loss: 2.318; l2dist: 1.396\n",
      "    step: 250; loss: 2.234; l2dist: 1.375\n",
      "    step: 300; loss: 2.200; l2dist: 1.368\n",
      "    step: 350; loss: 2.178; l2dist: 1.365\n",
      "    step: 400; loss: 2.174; l2dist: 1.370\n",
      "    step: 450; loss: 2.159; l2dist: 1.360\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.630; l2dist: 0.000\n",
      "    step: 50; loss: 7.025; l2dist: 2.164\n",
      "    step: 100; loss: 3.328; l2dist: 1.640\n",
      "    step: 150; loss: 2.529; l2dist: 1.463\n",
      "    step: 200; loss: 2.330; l2dist: 1.418\n",
      "    step: 250; loss: 2.249; l2dist: 1.397\n",
      "    step: 300; loss: 2.214; l2dist: 1.382\n",
      "    step: 350; loss: 2.210; l2dist: 1.377\n",
      "    step: 400; loss: 2.177; l2dist: 1.377\n",
      "    step: 450; loss: 2.178; l2dist: 1.372\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.216; l2dist: 0.000\n",
      "    step: 50; loss: 7.031; l2dist: 2.150\n",
      "    step: 100; loss: 3.303; l2dist: 1.640\n",
      "    step: 150; loss: 2.513; l2dist: 1.454\n",
      "    step: 200; loss: 2.318; l2dist: 1.419\n",
      "    step: 250; loss: 2.242; l2dist: 1.396\n",
      "    step: 300; loss: 2.208; l2dist: 1.381\n",
      "    step: 350; loss: 2.186; l2dist: 1.377\n",
      "    step: 400; loss: 2.174; l2dist: 1.377\n",
      "    step: 450; loss: 2.162; l2dist: 1.375\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.978; l2dist: 0.000\n",
      "    step: 50; loss: 7.008; l2dist: 2.149\n",
      "    step: 100; loss: 3.316; l2dist: 1.638\n",
      "    step: 150; loss: 2.488; l2dist: 1.460\n",
      "    step: 200; loss: 2.317; l2dist: 1.417\n",
      "    step: 250; loss: 2.239; l2dist: 1.390\n",
      "    step: 300; loss: 2.197; l2dist: 1.386\n",
      "    step: 350; loss: 2.175; l2dist: 1.377\n",
      "    step: 400; loss: 2.169; l2dist: 1.374\n",
      "    step: 450; loss: 2.174; l2dist: 1.375\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.167; l2dist: 0.000\n",
      "    step: 50; loss: 7.043; l2dist: 2.161\n",
      "    step: 100; loss: 3.317; l2dist: 1.648\n",
      "    step: 150; loss: 2.503; l2dist: 1.460\n",
      "    step: 200; loss: 2.324; l2dist: 1.419\n",
      "    step: 250; loss: 2.239; l2dist: 1.398\n",
      "    step: 300; loss: 2.208; l2dist: 1.393\n",
      "    step: 350; loss: 2.189; l2dist: 1.381\n",
      "    step: 400; loss: 2.175; l2dist: 1.380\n",
      "    step: 450; loss: 2.167; l2dist: 1.373\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 154.749; l2dist: 0.000\n",
      "    step: 50; loss: 16.723; l2dist: 3.696\n",
      "    step: 100; loss: 7.460; l2dist: 2.687\n",
      "    step: 150; loss: 5.184; l2dist: 2.219\n",
      "    step: 200; loss: 4.262; l2dist: 2.008\n",
      "    step: 250; loss: 3.779; l2dist: 1.883\n",
      "    step: 300; loss: 3.550; l2dist: 1.818\n",
      "    step: 350; loss: 3.425; l2dist: 1.776\n",
      "    step: 400; loss: 3.330; l2dist: 1.752\n",
      "    step: 450; loss: 3.305; l2dist: 1.747\n",
      "binary step: 0; number of successful adv: 90/100\n",
      "    step: 0; loss: 238.166; l2dist: 0.000\n",
      "    step: 50; loss: 18.506; l2dist: 3.908\n",
      "    step: 100; loss: 9.043; l2dist: 2.894\n",
      "    step: 150; loss: 6.226; l2dist: 2.372\n",
      "    step: 200; loss: 5.007; l2dist: 2.124\n",
      "    step: 250; loss: 4.337; l2dist: 1.973\n",
      "    step: 300; loss: 3.919; l2dist: 1.883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 3.656; l2dist: 1.828\n",
      "    step: 400; loss: 3.489; l2dist: 1.792\n",
      "    step: 450; loss: 3.423; l2dist: 1.777\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 130.740; l2dist: 0.000\n",
      "    step: 50; loss: 14.087; l2dist: 3.406\n",
      "    step: 100; loss: 6.431; l2dist: 2.417\n",
      "    step: 150; loss: 4.630; l2dist: 2.041\n",
      "    step: 200; loss: 3.922; l2dist: 1.887\n",
      "    step: 250; loss: 3.571; l2dist: 1.808\n",
      "    step: 300; loss: 3.374; l2dist: 1.756\n",
      "    step: 350; loss: 3.234; l2dist: 1.731\n",
      "    step: 400; loss: 3.204; l2dist: 1.723\n",
      "    step: 450; loss: 3.136; l2dist: 1.702\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 85.301; l2dist: 0.000\n",
      "    step: 50; loss: 11.404; l2dist: 3.022\n",
      "    step: 100; loss: 5.256; l2dist: 2.169\n",
      "    step: 150; loss: 3.938; l2dist: 1.889\n",
      "    step: 200; loss: 3.492; l2dist: 1.779\n",
      "    step: 250; loss: 3.266; l2dist: 1.726\n",
      "    step: 300; loss: 3.159; l2dist: 1.703\n",
      "    step: 350; loss: 3.099; l2dist: 1.680\n",
      "    step: 400; loss: 3.071; l2dist: 1.678\n",
      "    step: 450; loss: 3.025; l2dist: 1.667\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.247; l2dist: 0.000\n",
      "    step: 50; loss: 10.265; l2dist: 2.820\n",
      "    step: 100; loss: 4.772; l2dist: 2.054\n",
      "    step: 150; loss: 3.681; l2dist: 1.820\n",
      "    step: 200; loss: 3.324; l2dist: 1.728\n",
      "    step: 250; loss: 3.151; l2dist: 1.693\n",
      "    step: 300; loss: 3.089; l2dist: 1.675\n",
      "    step: 350; loss: 3.039; l2dist: 1.662\n",
      "    step: 400; loss: 3.002; l2dist: 1.654\n",
      "    step: 450; loss: 2.988; l2dist: 1.650\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 52.573; l2dist: 0.000\n",
      "    step: 50; loss: 9.592; l2dist: 2.694\n",
      "    step: 100; loss: 4.569; l2dist: 2.017\n",
      "    step: 150; loss: 3.562; l2dist: 1.790\n",
      "    step: 200; loss: 3.271; l2dist: 1.712\n",
      "    step: 250; loss: 3.144; l2dist: 1.685\n",
      "    step: 300; loss: 3.089; l2dist: 1.674\n",
      "    step: 350; loss: 3.024; l2dist: 1.657\n",
      "    step: 400; loss: 2.994; l2dist: 1.650\n",
      "    step: 450; loss: 3.004; l2dist: 1.645\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.444; l2dist: 0.000\n",
      "    step: 50; loss: 9.390; l2dist: 2.675\n",
      "    step: 100; loss: 4.547; l2dist: 2.011\n",
      "    step: 150; loss: 3.531; l2dist: 1.785\n",
      "    step: 200; loss: 3.274; l2dist: 1.715\n",
      "    step: 250; loss: 3.157; l2dist: 1.688\n",
      "    step: 300; loss: 3.083; l2dist: 1.671\n",
      "    step: 350; loss: 3.060; l2dist: 1.660\n",
      "    step: 400; loss: 3.019; l2dist: 1.655\n",
      "    step: 450; loss: 2.999; l2dist: 1.651\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.618; l2dist: 0.000\n",
      "    step: 50; loss: 9.206; l2dist: 2.649\n",
      "    step: 100; loss: 4.489; l2dist: 2.009\n",
      "    step: 150; loss: 3.510; l2dist: 1.786\n",
      "    step: 200; loss: 3.257; l2dist: 1.720\n",
      "    step: 250; loss: 3.157; l2dist: 1.690\n",
      "    step: 300; loss: 3.099; l2dist: 1.674\n",
      "    step: 350; loss: 3.041; l2dist: 1.666\n",
      "    step: 400; loss: 3.024; l2dist: 1.660\n",
      "    step: 450; loss: 3.011; l2dist: 1.654\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.248; l2dist: 0.000\n",
      "    step: 50; loss: 9.114; l2dist: 2.635\n",
      "    step: 100; loss: 4.445; l2dist: 1.991\n",
      "    step: 150; loss: 3.503; l2dist: 1.779\n",
      "    step: 200; loss: 3.245; l2dist: 1.718\n",
      "    step: 250; loss: 3.135; l2dist: 1.689\n",
      "    step: 300; loss: 3.081; l2dist: 1.671\n",
      "    step: 350; loss: 3.045; l2dist: 1.662\n",
      "    step: 400; loss: 3.034; l2dist: 1.671\n",
      "    step: 450; loss: 3.017; l2dist: 1.657\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.465; l2dist: 0.000\n",
      "    step: 50; loss: 9.135; l2dist: 2.643\n",
      "    step: 100; loss: 4.450; l2dist: 1.990\n",
      "    step: 150; loss: 3.492; l2dist: 1.780\n",
      "    step: 200; loss: 3.257; l2dist: 1.717\n",
      "    step: 250; loss: 3.140; l2dist: 1.686\n",
      "    step: 300; loss: 3.074; l2dist: 1.676\n",
      "    step: 350; loss: 3.033; l2dist: 1.665\n",
      "    step: 400; loss: 3.028; l2dist: 1.656\n",
      "    step: 450; loss: 3.003; l2dist: 1.649\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 127.149; l2dist: 0.000\n",
      "    step: 50; loss: 13.761; l2dist: 3.517\n",
      "    step: 100; loss: 6.087; l2dist: 2.392\n",
      "    step: 150; loss: 4.000; l2dist: 1.915\n",
      "    step: 200; loss: 3.192; l2dist: 1.694\n",
      "    step: 250; loss: 2.916; l2dist: 1.618\n",
      "    step: 300; loss: 2.718; l2dist: 1.554\n",
      "    step: 350; loss: 2.673; l2dist: 1.539\n",
      "    step: 400; loss: 2.603; l2dist: 1.498\n",
      "    step: 450; loss: 2.498; l2dist: 1.486\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 121.193; l2dist: 0.000\n",
      "    step: 50; loss: 13.127; l2dist: 3.345\n",
      "    step: 100; loss: 5.960; l2dist: 2.298\n",
      "    step: 150; loss: 3.976; l2dist: 1.861\n",
      "    step: 200; loss: 3.237; l2dist: 1.679\n",
      "    step: 250; loss: 2.910; l2dist: 1.588\n",
      "    step: 300; loss: 2.718; l2dist: 1.538\n",
      "    step: 350; loss: 2.605; l2dist: 1.501\n",
      "    step: 400; loss: 2.528; l2dist: 1.479\n",
      "    step: 450; loss: 2.539; l2dist: 1.487\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.476; l2dist: 0.000\n",
      "    step: 50; loss: 10.388; l2dist: 2.920\n",
      "    step: 100; loss: 4.434; l2dist: 1.971\n",
      "    step: 150; loss: 3.194; l2dist: 1.668\n",
      "    step: 200; loss: 2.833; l2dist: 1.564\n",
      "    step: 250; loss: 2.563; l2dist: 1.497\n",
      "    step: 300; loss: 2.457; l2dist: 1.463\n",
      "    step: 350; loss: 2.426; l2dist: 1.452\n",
      "    step: 400; loss: 2.399; l2dist: 1.447\n",
      "    step: 450; loss: 2.372; l2dist: 1.436\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.786; l2dist: 0.000\n",
      "    step: 50; loss: 8.917; l2dist: 2.616\n",
      "    step: 100; loss: 3.912; l2dist: 1.843\n",
      "    step: 150; loss: 2.905; l2dist: 1.583\n",
      "    step: 200; loss: 2.591; l2dist: 1.497\n",
      "    step: 250; loss: 2.429; l2dist: 1.452\n",
      "    step: 300; loss: 2.344; l2dist: 1.429\n",
      "    step: 350; loss: 2.299; l2dist: 1.418\n",
      "    step: 400; loss: 2.264; l2dist: 1.408\n",
      "    step: 450; loss: 2.261; l2dist: 1.400\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.915; l2dist: 0.000\n",
      "    step: 50; loss: 7.955; l2dist: 2.374\n",
      "    step: 100; loss: 3.744; l2dist: 1.756\n",
      "    step: 150; loss: 2.844; l2dist: 1.547\n",
      "    step: 200; loss: 2.513; l2dist: 1.469\n",
      "    step: 250; loss: 2.379; l2dist: 1.432\n",
      "    step: 300; loss: 2.334; l2dist: 1.420\n",
      "    step: 350; loss: 2.300; l2dist: 1.408\n",
      "    step: 400; loss: 2.288; l2dist: 1.409\n",
      "    step: 450; loss: 2.278; l2dist: 1.402\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.967; l2dist: 0.000\n",
      "    step: 50; loss: 7.468; l2dist: 2.256\n",
      "    step: 100; loss: 3.661; l2dist: 1.682\n",
      "    step: 150; loss: 2.789; l2dist: 1.493\n",
      "    step: 200; loss: 2.485; l2dist: 1.429\n",
      "    step: 250; loss: 2.386; l2dist: 1.402\n",
      "    step: 300; loss: 2.330; l2dist: 1.391\n",
      "    step: 350; loss: 2.289; l2dist: 1.382\n",
      "    step: 400; loss: 2.291; l2dist: 1.376\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.873; l2dist: 0.000\n",
      "    step: 50; loss: 7.390; l2dist: 2.221\n",
      "    step: 100; loss: 3.624; l2dist: 1.669\n",
      "    step: 150; loss: 2.760; l2dist: 1.483\n",
      "    step: 200; loss: 2.487; l2dist: 1.430\n",
      "    step: 250; loss: 2.382; l2dist: 1.403\n",
      "    step: 300; loss: 2.314; l2dist: 1.390\n",
      "    step: 350; loss: 2.296; l2dist: 1.388\n",
      "    step: 400; loss: 2.249; l2dist: 1.375\n",
      "    step: 450; loss: 2.278; l2dist: 1.377\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.274; l2dist: 0.000\n",
      "    step: 50; loss: 7.353; l2dist: 2.213\n",
      "    step: 100; loss: 3.617; l2dist: 1.667\n",
      "    step: 150; loss: 2.740; l2dist: 1.480\n",
      "    step: 200; loss: 2.470; l2dist: 1.425\n",
      "    step: 250; loss: 2.372; l2dist: 1.403\n",
      "    step: 300; loss: 2.329; l2dist: 1.393\n",
      "    step: 350; loss: 2.287; l2dist: 1.382\n",
      "    step: 400; loss: 2.273; l2dist: 1.381\n",
      "    step: 450; loss: 2.274; l2dist: 1.384\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.700; l2dist: 0.000\n",
      "    step: 50; loss: 7.321; l2dist: 2.209\n",
      "    step: 100; loss: 3.628; l2dist: 1.674\n",
      "    step: 150; loss: 2.744; l2dist: 1.484\n",
      "    step: 200; loss: 2.474; l2dist: 1.433\n",
      "    step: 250; loss: 2.372; l2dist: 1.404\n",
      "    step: 300; loss: 2.323; l2dist: 1.393\n",
      "    step: 350; loss: 2.305; l2dist: 1.394\n",
      "    step: 400; loss: 2.278; l2dist: 1.385\n",
      "    step: 450; loss: 2.258; l2dist: 1.382\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.919; l2dist: 0.000\n",
      "    step: 50; loss: 7.373; l2dist: 2.221\n",
      "    step: 100; loss: 3.641; l2dist: 1.685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 2.742; l2dist: 1.498\n",
      "    step: 200; loss: 2.486; l2dist: 1.436\n",
      "    step: 250; loss: 2.366; l2dist: 1.409\n",
      "    step: 300; loss: 2.309; l2dist: 1.392\n",
      "    step: 350; loss: 2.282; l2dist: 1.389\n",
      "    step: 400; loss: 2.264; l2dist: 1.392\n",
      "    step: 450; loss: 2.256; l2dist: 1.386\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 135.294; l2dist: 0.000\n",
      "    step: 50; loss: 15.036; l2dist: 3.616\n",
      "    step: 100; loss: 6.657; l2dist: 2.516\n",
      "    step: 150; loss: 4.491; l2dist: 2.042\n",
      "    step: 200; loss: 3.584; l2dist: 1.821\n",
      "    step: 250; loss: 3.215; l2dist: 1.709\n",
      "    step: 300; loss: 3.025; l2dist: 1.660\n",
      "    step: 350; loss: 2.924; l2dist: 1.624\n",
      "    step: 400; loss: 2.961; l2dist: 1.640\n",
      "binary step: 0; number of successful adv: 98/100\n",
      "    step: 0; loss: 97.289; l2dist: 0.000\n",
      "    step: 50; loss: 12.987; l2dist: 3.360\n",
      "    step: 100; loss: 5.373; l2dist: 2.231\n",
      "    step: 150; loss: 3.750; l2dist: 1.844\n",
      "    step: 200; loss: 3.169; l2dist: 1.692\n",
      "    step: 250; loss: 2.974; l2dist: 1.626\n",
      "    step: 300; loss: 2.862; l2dist: 1.589\n",
      "    step: 350; loss: 2.817; l2dist: 1.571\n",
      "    step: 400; loss: 2.765; l2dist: 1.575\n",
      "    step: 450; loss: 2.724; l2dist: 1.561\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.525; l2dist: 0.000\n",
      "    step: 50; loss: 10.432; l2dist: 2.924\n",
      "    step: 100; loss: 4.330; l2dist: 1.991\n",
      "    step: 150; loss: 3.294; l2dist: 1.713\n",
      "    step: 200; loss: 2.946; l2dist: 1.621\n",
      "    step: 250; loss: 2.806; l2dist: 1.576\n",
      "    step: 300; loss: 2.719; l2dist: 1.553\n",
      "    step: 350; loss: 2.667; l2dist: 1.544\n",
      "    step: 400; loss: 2.640; l2dist: 1.537\n",
      "    step: 450; loss: 2.663; l2dist: 1.544\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.766; l2dist: 0.000\n",
      "    step: 50; loss: 9.343; l2dist: 2.686\n",
      "    step: 100; loss: 3.983; l2dist: 1.894\n",
      "    step: 150; loss: 3.059; l2dist: 1.651\n",
      "    step: 200; loss: 2.782; l2dist: 1.576\n",
      "    step: 250; loss: 2.680; l2dist: 1.544\n",
      "    step: 300; loss: 2.619; l2dist: 1.527\n",
      "    step: 350; loss: 2.582; l2dist: 1.522\n",
      "    step: 400; loss: 2.546; l2dist: 1.507\n",
      "    step: 450; loss: 2.521; l2dist: 1.503\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.512; l2dist: 0.000\n",
      "    step: 50; loss: 8.499; l2dist: 2.481\n",
      "    step: 100; loss: 3.859; l2dist: 1.826\n",
      "    step: 150; loss: 2.962; l2dist: 1.612\n",
      "    step: 200; loss: 2.729; l2dist: 1.550\n",
      "    step: 250; loss: 2.622; l2dist: 1.517\n",
      "    step: 300; loss: 2.581; l2dist: 1.504\n",
      "    step: 350; loss: 2.542; l2dist: 1.497\n",
      "    step: 400; loss: 2.513; l2dist: 1.490\n",
      "    step: 450; loss: 2.505; l2dist: 1.489\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.279; l2dist: 0.000\n",
      "    step: 50; loss: 8.210; l2dist: 2.413\n",
      "    step: 100; loss: 3.791; l2dist: 1.787\n",
      "    step: 150; loss: 2.943; l2dist: 1.581\n",
      "    step: 200; loss: 2.693; l2dist: 1.520\n",
      "    step: 250; loss: 2.588; l2dist: 1.500\n",
      "    step: 300; loss: 2.539; l2dist: 1.492\n",
      "    step: 350; loss: 2.507; l2dist: 1.477\n",
      "    step: 400; loss: 2.514; l2dist: 1.480\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.982; l2dist: 0.000\n",
      "    step: 50; loss: 8.054; l2dist: 2.396\n",
      "    step: 100; loss: 3.776; l2dist: 1.794\n",
      "    step: 150; loss: 2.925; l2dist: 1.589\n",
      "    step: 200; loss: 2.671; l2dist: 1.515\n",
      "    step: 250; loss: 2.581; l2dist: 1.495\n",
      "    step: 300; loss: 2.547; l2dist: 1.484\n",
      "    step: 350; loss: 2.507; l2dist: 1.472\n",
      "    step: 400; loss: 2.489; l2dist: 1.470\n",
      "    step: 450; loss: 2.489; l2dist: 1.473\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.396; l2dist: 0.000\n",
      "    step: 50; loss: 8.015; l2dist: 2.387\n",
      "    step: 100; loss: 3.800; l2dist: 1.794\n",
      "    step: 150; loss: 2.949; l2dist: 1.590\n",
      "    step: 200; loss: 2.673; l2dist: 1.524\n",
      "    step: 250; loss: 2.597; l2dist: 1.492\n",
      "    step: 300; loss: 2.543; l2dist: 1.492\n",
      "    step: 350; loss: 2.540; l2dist: 1.478\n",
      "    step: 400; loss: 2.505; l2dist: 1.480\n",
      "    step: 450; loss: 2.481; l2dist: 1.472\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.007; l2dist: 0.000\n",
      "    step: 50; loss: 7.977; l2dist: 2.381\n",
      "    step: 100; loss: 3.792; l2dist: 1.787\n",
      "    step: 150; loss: 2.920; l2dist: 1.581\n",
      "    step: 200; loss: 2.677; l2dist: 1.515\n",
      "    step: 250; loss: 2.585; l2dist: 1.504\n",
      "    step: 300; loss: 2.531; l2dist: 1.487\n",
      "    step: 350; loss: 2.520; l2dist: 1.478\n",
      "    step: 400; loss: 2.500; l2dist: 1.479\n",
      "    step: 450; loss: 2.490; l2dist: 1.477\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.139; l2dist: 0.000\n",
      "    step: 50; loss: 7.998; l2dist: 2.390\n",
      "    step: 100; loss: 3.806; l2dist: 1.796\n",
      "    step: 150; loss: 2.925; l2dist: 1.585\n",
      "    step: 200; loss: 2.692; l2dist: 1.522\n",
      "    step: 250; loss: 2.583; l2dist: 1.505\n",
      "    step: 300; loss: 2.550; l2dist: 1.489\n",
      "    step: 350; loss: 2.498; l2dist: 1.486\n",
      "    step: 400; loss: 2.516; l2dist: 1.480\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 139.619; l2dist: 0.000\n",
      "    step: 50; loss: 14.453; l2dist: 3.627\n",
      "    step: 100; loss: 6.653; l2dist: 2.517\n",
      "    step: 150; loss: 4.354; l2dist: 2.019\n",
      "    step: 200; loss: 3.444; l2dist: 1.786\n",
      "    step: 250; loss: 3.034; l2dist: 1.673\n",
      "    step: 300; loss: 2.862; l2dist: 1.612\n",
      "    step: 350; loss: 2.789; l2dist: 1.594\n",
      "    step: 400; loss: 2.690; l2dist: 1.564\n",
      "    step: 450; loss: 2.659; l2dist: 1.550\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 151.357; l2dist: 0.000\n",
      "    step: 50; loss: 13.345; l2dist: 3.461\n",
      "    step: 100; loss: 6.336; l2dist: 2.397\n",
      "    step: 150; loss: 4.292; l2dist: 1.951\n",
      "    step: 200; loss: 3.470; l2dist: 1.746\n",
      "    step: 250; loss: 3.067; l2dist: 1.650\n",
      "    step: 300; loss: 2.834; l2dist: 1.594\n",
      "    step: 350; loss: 2.737; l2dist: 1.551\n",
      "    step: 400; loss: 2.685; l2dist: 1.547\n",
      "    step: 450; loss: 2.688; l2dist: 1.543\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.552; l2dist: 0.000\n",
      "    step: 50; loss: 10.564; l2dist: 3.011\n",
      "    step: 100; loss: 4.800; l2dist: 2.065\n",
      "    step: 150; loss: 3.408; l2dist: 1.742\n",
      "    step: 200; loss: 2.936; l2dist: 1.614\n",
      "    step: 250; loss: 2.734; l2dist: 1.569\n",
      "    step: 300; loss: 2.619; l2dist: 1.534\n",
      "    step: 350; loss: 2.566; l2dist: 1.521\n",
      "    step: 400; loss: 2.555; l2dist: 1.519\n",
      "    step: 450; loss: 2.559; l2dist: 1.521\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.935; l2dist: 0.000\n",
      "    step: 50; loss: 8.968; l2dist: 2.672\n",
      "    step: 100; loss: 4.084; l2dist: 1.901\n",
      "    step: 150; loss: 3.073; l2dist: 1.641\n",
      "    step: 200; loss: 2.738; l2dist: 1.556\n",
      "    step: 250; loss: 2.605; l2dist: 1.522\n",
      "    step: 300; loss: 2.538; l2dist: 1.503\n",
      "    step: 350; loss: 2.503; l2dist: 1.495\n",
      "    step: 400; loss: 2.479; l2dist: 1.491\n",
      "    step: 450; loss: 2.491; l2dist: 1.483\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.072; l2dist: 0.000\n",
      "    step: 50; loss: 8.560; l2dist: 2.595\n",
      "    step: 100; loss: 3.937; l2dist: 1.853\n",
      "    step: 150; loss: 2.969; l2dist: 1.621\n",
      "    step: 200; loss: 2.699; l2dist: 1.545\n",
      "    step: 250; loss: 2.583; l2dist: 1.517\n",
      "    step: 300; loss: 2.523; l2dist: 1.502\n",
      "    step: 350; loss: 2.487; l2dist: 1.495\n",
      "    step: 400; loss: 2.459; l2dist: 1.484\n",
      "    step: 450; loss: 2.473; l2dist: 1.490\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.177; l2dist: 0.000\n",
      "    step: 50; loss: 8.106; l2dist: 2.496\n",
      "    step: 100; loss: 3.790; l2dist: 1.793\n",
      "    step: 150; loss: 2.911; l2dist: 1.582\n",
      "    step: 200; loss: 2.663; l2dist: 1.521\n",
      "    step: 250; loss: 2.566; l2dist: 1.494\n",
      "    step: 300; loss: 2.504; l2dist: 1.483\n",
      "    step: 350; loss: 2.473; l2dist: 1.470\n",
      "    step: 400; loss: 2.436; l2dist: 1.465\n",
      "    step: 450; loss: 2.426; l2dist: 1.462\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.784; l2dist: 0.000\n",
      "    step: 50; loss: 8.016; l2dist: 2.475\n",
      "    step: 100; loss: 3.783; l2dist: 1.796\n",
      "    step: 150; loss: 2.889; l2dist: 1.591\n",
      "    step: 200; loss: 2.640; l2dist: 1.525\n",
      "    step: 250; loss: 2.528; l2dist: 1.500\n",
      "    step: 300; loss: 2.479; l2dist: 1.489\n",
      "    step: 350; loss: 2.463; l2dist: 1.482\n",
      "    step: 400; loss: 2.449; l2dist: 1.464\n",
      "    step: 450; loss: 2.428; l2dist: 1.467\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.076; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 7.927; l2dist: 2.450\n",
      "    step: 100; loss: 3.766; l2dist: 1.783\n",
      "    step: 150; loss: 2.899; l2dist: 1.588\n",
      "    step: 200; loss: 2.638; l2dist: 1.521\n",
      "    step: 250; loss: 2.533; l2dist: 1.496\n",
      "    step: 300; loss: 2.500; l2dist: 1.489\n",
      "    step: 350; loss: 2.474; l2dist: 1.474\n",
      "    step: 400; loss: 2.443; l2dist: 1.476\n",
      "    step: 450; loss: 2.456; l2dist: 1.477\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.294; l2dist: 0.000\n",
      "    step: 50; loss: 7.862; l2dist: 2.439\n",
      "    step: 100; loss: 3.746; l2dist: 1.786\n",
      "    step: 150; loss: 2.867; l2dist: 1.579\n",
      "    step: 200; loss: 2.622; l2dist: 1.518\n",
      "    step: 250; loss: 2.544; l2dist: 1.496\n",
      "    step: 300; loss: 2.505; l2dist: 1.489\n",
      "    step: 350; loss: 2.444; l2dist: 1.474\n",
      "    step: 400; loss: 2.432; l2dist: 1.465\n",
      "    step: 450; loss: 2.446; l2dist: 1.475\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.456; l2dist: 0.000\n",
      "    step: 50; loss: 7.889; l2dist: 2.446\n",
      "    step: 100; loss: 3.740; l2dist: 1.784\n",
      "    step: 150; loss: 2.871; l2dist: 1.582\n",
      "    step: 200; loss: 2.635; l2dist: 1.521\n",
      "    step: 250; loss: 2.527; l2dist: 1.501\n",
      "    step: 300; loss: 2.477; l2dist: 1.482\n",
      "    step: 350; loss: 2.468; l2dist: 1.477\n",
      "    step: 400; loss: 2.433; l2dist: 1.475\n",
      "    step: 450; loss: 2.421; l2dist: 1.467\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 146.792; l2dist: 0.000\n",
      "    step: 50; loss: 18.560; l2dist: 3.742\n",
      "    step: 100; loss: 7.411; l2dist: 2.666\n",
      "    step: 150; loss: 5.111; l2dist: 2.192\n",
      "    step: 200; loss: 4.220; l2dist: 1.983\n",
      "    step: 250; loss: 3.702; l2dist: 1.849\n",
      "    step: 300; loss: 3.539; l2dist: 1.808\n",
      "    step: 350; loss: 3.438; l2dist: 1.769\n",
      "    step: 400; loss: 3.360; l2dist: 1.761\n",
      "    step: 450; loss: 3.297; l2dist: 1.739\n",
      "binary step: 0; number of successful adv: 92/100\n",
      "    step: 0; loss: 183.420; l2dist: 0.000\n",
      "    step: 50; loss: 18.756; l2dist: 3.730\n",
      "    step: 100; loss: 8.087; l2dist: 2.701\n",
      "    step: 150; loss: 5.590; l2dist: 2.225\n",
      "    step: 200; loss: 4.565; l2dist: 2.011\n",
      "    step: 250; loss: 4.072; l2dist: 1.905\n",
      "    step: 300; loss: 3.742; l2dist: 1.831\n",
      "    step: 350; loss: 3.553; l2dist: 1.789\n",
      "    step: 400; loss: 3.400; l2dist: 1.753\n",
      "    step: 450; loss: 3.368; l2dist: 1.750\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 103.044; l2dist: 0.000\n",
      "    step: 50; loss: 14.192; l2dist: 3.259\n",
      "    step: 100; loss: 5.910; l2dist: 2.300\n",
      "    step: 150; loss: 4.348; l2dist: 1.965\n",
      "    step: 200; loss: 3.768; l2dist: 1.834\n",
      "    step: 250; loss: 3.452; l2dist: 1.766\n",
      "    step: 300; loss: 3.293; l2dist: 1.730\n",
      "    step: 350; loss: 3.203; l2dist: 1.711\n",
      "    step: 400; loss: 3.273; l2dist: 1.723\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.963; l2dist: 0.000\n",
      "    step: 50; loss: 11.613; l2dist: 2.869\n",
      "    step: 100; loss: 5.003; l2dist: 2.111\n",
      "    step: 150; loss: 3.808; l2dist: 1.844\n",
      "    step: 200; loss: 3.389; l2dist: 1.751\n",
      "    step: 250; loss: 3.259; l2dist: 1.717\n",
      "    step: 300; loss: 3.169; l2dist: 1.693\n",
      "    step: 350; loss: 3.100; l2dist: 1.682\n",
      "    step: 400; loss: 3.070; l2dist: 1.666\n",
      "    step: 450; loss: 3.091; l2dist: 1.676\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.814; l2dist: 0.000\n",
      "    step: 50; loss: 10.234; l2dist: 2.638\n",
      "    step: 100; loss: 4.609; l2dist: 2.008\n",
      "    step: 150; loss: 3.598; l2dist: 1.784\n",
      "    step: 200; loss: 3.302; l2dist: 1.720\n",
      "    step: 250; loss: 3.147; l2dist: 1.689\n",
      "    step: 300; loss: 3.094; l2dist: 1.673\n",
      "    step: 350; loss: 3.067; l2dist: 1.660\n",
      "    step: 400; loss: 3.057; l2dist: 1.661\n",
      "    step: 450; loss: 3.051; l2dist: 1.657\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.881; l2dist: 0.000\n",
      "    step: 50; loss: 9.565; l2dist: 2.540\n",
      "    step: 100; loss: 4.449; l2dist: 1.955\n",
      "    step: 150; loss: 3.442; l2dist: 1.738\n",
      "    step: 200; loss: 3.199; l2dist: 1.679\n",
      "    step: 250; loss: 3.132; l2dist: 1.667\n",
      "    step: 300; loss: 3.058; l2dist: 1.650\n",
      "    step: 350; loss: 3.035; l2dist: 1.640\n",
      "    step: 400; loss: 3.020; l2dist: 1.641\n",
      "    step: 450; loss: 3.012; l2dist: 1.638\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.431; l2dist: 0.000\n",
      "    step: 50; loss: 9.352; l2dist: 2.518\n",
      "    step: 100; loss: 4.367; l2dist: 1.953\n",
      "    step: 150; loss: 3.422; l2dist: 1.736\n",
      "    step: 200; loss: 3.199; l2dist: 1.681\n",
      "    step: 250; loss: 3.079; l2dist: 1.662\n",
      "    step: 300; loss: 3.051; l2dist: 1.659\n",
      "    step: 350; loss: 3.015; l2dist: 1.644\n",
      "    step: 400; loss: 3.005; l2dist: 1.647\n",
      "    step: 450; loss: 2.999; l2dist: 1.635\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.950; l2dist: 0.000\n",
      "    step: 50; loss: 9.259; l2dist: 2.508\n",
      "    step: 100; loss: 4.360; l2dist: 1.943\n",
      "    step: 150; loss: 3.423; l2dist: 1.744\n",
      "    step: 200; loss: 3.201; l2dist: 1.696\n",
      "    step: 250; loss: 3.102; l2dist: 1.661\n",
      "    step: 300; loss: 3.053; l2dist: 1.659\n",
      "    step: 350; loss: 3.058; l2dist: 1.650\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.500; l2dist: 0.000\n",
      "    step: 50; loss: 9.232; l2dist: 2.506\n",
      "    step: 100; loss: 4.345; l2dist: 1.946\n",
      "    step: 150; loss: 3.388; l2dist: 1.741\n",
      "    step: 200; loss: 3.165; l2dist: 1.687\n",
      "    step: 250; loss: 3.098; l2dist: 1.664\n",
      "    step: 300; loss: 3.057; l2dist: 1.651\n",
      "    step: 350; loss: 3.010; l2dist: 1.641\n",
      "    step: 400; loss: 3.026; l2dist: 1.638\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.870; l2dist: 0.000\n",
      "    step: 50; loss: 9.277; l2dist: 2.515\n",
      "    step: 100; loss: 4.359; l2dist: 1.952\n",
      "    step: 150; loss: 3.399; l2dist: 1.741\n",
      "    step: 200; loss: 3.185; l2dist: 1.693\n",
      "    step: 250; loss: 3.087; l2dist: 1.669\n",
      "    step: 300; loss: 3.034; l2dist: 1.655\n",
      "    step: 350; loss: 3.022; l2dist: 1.651\n",
      "    step: 400; loss: 3.017; l2dist: 1.649\n",
      "    step: 450; loss: 2.994; l2dist: 1.650\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 119.864; l2dist: 0.000\n",
      "    step: 50; loss: 13.200; l2dist: 3.288\n",
      "    step: 100; loss: 5.775; l2dist: 2.306\n",
      "    step: 150; loss: 4.004; l2dist: 1.912\n",
      "    step: 200; loss: 3.279; l2dist: 1.724\n",
      "    step: 250; loss: 3.010; l2dist: 1.644\n",
      "    step: 300; loss: 2.838; l2dist: 1.597\n",
      "    step: 350; loss: 2.772; l2dist: 1.580\n",
      "    step: 400; loss: 2.731; l2dist: 1.569\n",
      "    step: 450; loss: 2.736; l2dist: 1.555\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 145.921; l2dist: 0.000\n",
      "    step: 50; loss: 13.249; l2dist: 3.323\n",
      "    step: 100; loss: 5.977; l2dist: 2.306\n",
      "    step: 150; loss: 4.194; l2dist: 1.916\n",
      "    step: 200; loss: 3.493; l2dist: 1.741\n",
      "    step: 250; loss: 3.145; l2dist: 1.656\n",
      "    step: 300; loss: 2.960; l2dist: 1.611\n",
      "    step: 350; loss: 2.864; l2dist: 1.582\n",
      "    step: 400; loss: 2.868; l2dist: 1.579\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 86.632; l2dist: 0.000\n",
      "    step: 50; loss: 10.747; l2dist: 2.912\n",
      "    step: 100; loss: 4.682; l2dist: 2.027\n",
      "    step: 150; loss: 3.450; l2dist: 1.735\n",
      "    step: 200; loss: 3.054; l2dist: 1.631\n",
      "    step: 250; loss: 2.863; l2dist: 1.586\n",
      "    step: 300; loss: 2.737; l2dist: 1.557\n",
      "    step: 350; loss: 2.686; l2dist: 1.542\n",
      "    step: 400; loss: 2.640; l2dist: 1.532\n",
      "    step: 450; loss: 2.641; l2dist: 1.523\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.392; l2dist: 0.000\n",
      "    step: 50; loss: 9.069; l2dist: 2.566\n",
      "    step: 100; loss: 4.083; l2dist: 1.879\n",
      "    step: 150; loss: 3.190; l2dist: 1.664\n",
      "    step: 200; loss: 2.882; l2dist: 1.593\n",
      "    step: 250; loss: 2.755; l2dist: 1.555\n",
      "    step: 300; loss: 2.683; l2dist: 1.543\n",
      "    step: 350; loss: 2.631; l2dist: 1.528\n",
      "    step: 400; loss: 2.600; l2dist: 1.514\n",
      "    step: 450; loss: 2.572; l2dist: 1.513\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.886; l2dist: 0.000\n",
      "    step: 50; loss: 8.150; l2dist: 2.360\n",
      "    step: 100; loss: 3.840; l2dist: 1.788\n",
      "    step: 150; loss: 2.996; l2dist: 1.597\n",
      "    step: 200; loss: 2.752; l2dist: 1.541\n",
      "    step: 250; loss: 2.650; l2dist: 1.514\n",
      "    step: 300; loss: 2.583; l2dist: 1.497\n",
      "    step: 350; loss: 2.541; l2dist: 1.493\n",
      "    step: 400; loss: 2.531; l2dist: 1.489\n",
      "    step: 450; loss: 2.531; l2dist: 1.481\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.277; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 7.853; l2dist: 2.295\n",
      "    step: 100; loss: 3.747; l2dist: 1.754\n",
      "    step: 150; loss: 2.965; l2dist: 1.570\n",
      "    step: 200; loss: 2.734; l2dist: 1.511\n",
      "    step: 250; loss: 2.632; l2dist: 1.495\n",
      "    step: 300; loss: 2.573; l2dist: 1.490\n",
      "    step: 350; loss: 2.536; l2dist: 1.480\n",
      "    step: 400; loss: 2.522; l2dist: 1.473\n",
      "    step: 450; loss: 2.506; l2dist: 1.474\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.172; l2dist: 0.000\n",
      "    step: 50; loss: 7.667; l2dist: 2.259\n",
      "    step: 100; loss: 3.700; l2dist: 1.738\n",
      "    step: 150; loss: 2.907; l2dist: 1.566\n",
      "    step: 200; loss: 2.656; l2dist: 1.496\n",
      "    step: 250; loss: 2.579; l2dist: 1.482\n",
      "    step: 300; loss: 2.531; l2dist: 1.471\n",
      "    step: 350; loss: 2.480; l2dist: 1.460\n",
      "    step: 400; loss: 2.492; l2dist: 1.462\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.759; l2dist: 0.000\n",
      "    step: 50; loss: 7.567; l2dist: 2.242\n",
      "    step: 100; loss: 3.688; l2dist: 1.736\n",
      "    step: 150; loss: 2.865; l2dist: 1.551\n",
      "    step: 200; loss: 2.656; l2dist: 1.498\n",
      "    step: 250; loss: 2.556; l2dist: 1.478\n",
      "    step: 300; loss: 2.503; l2dist: 1.470\n",
      "    step: 350; loss: 2.489; l2dist: 1.462\n",
      "    step: 400; loss: 2.489; l2dist: 1.458\n",
      "    step: 450; loss: 2.468; l2dist: 1.463\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.172; l2dist: 0.000\n",
      "    step: 50; loss: 7.558; l2dist: 2.239\n",
      "    step: 100; loss: 3.682; l2dist: 1.734\n",
      "    step: 150; loss: 2.879; l2dist: 1.551\n",
      "    step: 200; loss: 2.666; l2dist: 1.499\n",
      "    step: 250; loss: 2.553; l2dist: 1.479\n",
      "    step: 300; loss: 2.520; l2dist: 1.471\n",
      "    step: 350; loss: 2.505; l2dist: 1.472\n",
      "    step: 400; loss: 2.485; l2dist: 1.458\n",
      "    step: 450; loss: 2.480; l2dist: 1.467\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.502; l2dist: 0.000\n",
      "    step: 50; loss: 7.610; l2dist: 2.254\n",
      "    step: 100; loss: 3.699; l2dist: 1.740\n",
      "    step: 150; loss: 2.879; l2dist: 1.552\n",
      "    step: 200; loss: 2.664; l2dist: 1.506\n",
      "    step: 250; loss: 2.570; l2dist: 1.492\n",
      "    step: 300; loss: 2.524; l2dist: 1.479\n",
      "    step: 350; loss: 2.494; l2dist: 1.473\n",
      "    step: 400; loss: 2.480; l2dist: 1.464\n",
      "    step: 450; loss: 2.480; l2dist: 1.462\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 130.965; l2dist: 0.000\n",
      "    step: 50; loss: 14.711; l2dist: 3.537\n",
      "    step: 100; loss: 6.596; l2dist: 2.471\n",
      "    step: 150; loss: 4.621; l2dist: 2.039\n",
      "    step: 200; loss: 3.868; l2dist: 1.860\n",
      "    step: 250; loss: 3.571; l2dist: 1.783\n",
      "    step: 300; loss: 3.379; l2dist: 1.736\n",
      "    step: 350; loss: 3.290; l2dist: 1.704\n",
      "    step: 400; loss: 3.301; l2dist: 1.718\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 76.970; l2dist: 0.000\n",
      "    step: 50; loss: 11.467; l2dist: 3.071\n",
      "    step: 100; loss: 5.001; l2dist: 2.120\n",
      "    step: 150; loss: 3.801; l2dist: 1.835\n",
      "    step: 200; loss: 3.408; l2dist: 1.743\n",
      "    step: 250; loss: 3.235; l2dist: 1.683\n",
      "    step: 300; loss: 3.168; l2dist: 1.673\n",
      "    step: 350; loss: 3.115; l2dist: 1.658\n",
      "    step: 400; loss: 3.093; l2dist: 1.654\n",
      "    step: 450; loss: 3.062; l2dist: 1.651\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.023; l2dist: 0.000\n",
      "    step: 50; loss: 9.331; l2dist: 2.724\n",
      "    step: 100; loss: 4.377; l2dist: 1.970\n",
      "    step: 150; loss: 3.456; l2dist: 1.745\n",
      "    step: 200; loss: 3.226; l2dist: 1.679\n",
      "    step: 250; loss: 3.094; l2dist: 1.647\n",
      "    step: 300; loss: 3.038; l2dist: 1.632\n",
      "    step: 350; loss: 3.007; l2dist: 1.627\n",
      "    step: 400; loss: 3.002; l2dist: 1.625\n",
      "    step: 450; loss: 2.986; l2dist: 1.619\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.444; l2dist: 0.000\n",
      "    step: 50; loss: 8.291; l2dist: 2.458\n",
      "    step: 100; loss: 4.268; l2dist: 1.893\n",
      "    step: 150; loss: 3.354; l2dist: 1.697\n",
      "    step: 200; loss: 3.128; l2dist: 1.646\n",
      "    step: 250; loss: 3.040; l2dist: 1.629\n",
      "    step: 300; loss: 2.991; l2dist: 1.610\n",
      "    step: 350; loss: 2.980; l2dist: 1.611\n",
      "    step: 400; loss: 2.963; l2dist: 1.608\n",
      "    step: 450; loss: 2.962; l2dist: 1.604\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.388; l2dist: 0.000\n",
      "    step: 50; loss: 7.706; l2dist: 2.299\n",
      "    step: 100; loss: 4.115; l2dist: 1.813\n",
      "    step: 150; loss: 3.273; l2dist: 1.637\n",
      "    step: 200; loss: 3.069; l2dist: 1.606\n",
      "    step: 250; loss: 2.993; l2dist: 1.588\n",
      "    step: 300; loss: 2.960; l2dist: 1.580\n",
      "    step: 350; loss: 2.942; l2dist: 1.574\n",
      "    step: 400; loss: 2.927; l2dist: 1.569\n",
      "    step: 450; loss: 2.928; l2dist: 1.569\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.870; l2dist: 0.000\n",
      "    step: 50; loss: 7.609; l2dist: 2.242\n",
      "    step: 100; loss: 4.140; l2dist: 1.798\n",
      "    step: 150; loss: 3.257; l2dist: 1.634\n",
      "    step: 200; loss: 3.059; l2dist: 1.596\n",
      "    step: 250; loss: 2.995; l2dist: 1.587\n",
      "    step: 300; loss: 2.963; l2dist: 1.581\n",
      "    step: 350; loss: 2.940; l2dist: 1.570\n",
      "    step: 400; loss: 2.924; l2dist: 1.569\n",
      "    step: 450; loss: 2.928; l2dist: 1.569\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.931; l2dist: 0.000\n",
      "    step: 50; loss: 7.498; l2dist: 2.206\n",
      "    step: 100; loss: 4.098; l2dist: 1.774\n",
      "    step: 150; loss: 3.249; l2dist: 1.622\n",
      "    step: 200; loss: 3.059; l2dist: 1.585\n",
      "    step: 250; loss: 2.988; l2dist: 1.565\n",
      "    step: 300; loss: 2.949; l2dist: 1.562\n",
      "    step: 350; loss: 2.925; l2dist: 1.562\n",
      "    step: 400; loss: 2.921; l2dist: 1.556\n",
      "    step: 450; loss: 2.911; l2dist: 1.554\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.453; l2dist: 0.000\n",
      "    step: 50; loss: 7.478; l2dist: 2.193\n",
      "    step: 100; loss: 4.108; l2dist: 1.779\n",
      "    step: 150; loss: 3.240; l2dist: 1.628\n",
      "    step: 200; loss: 3.055; l2dist: 1.591\n",
      "    step: 250; loss: 2.991; l2dist: 1.571\n",
      "    step: 300; loss: 2.952; l2dist: 1.562\n",
      "    step: 350; loss: 2.936; l2dist: 1.561\n",
      "    step: 400; loss: 2.921; l2dist: 1.561\n",
      "    step: 450; loss: 2.916; l2dist: 1.555\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.266; l2dist: 0.000\n",
      "    step: 50; loss: 7.463; l2dist: 2.188\n",
      "    step: 100; loss: 4.109; l2dist: 1.787\n",
      "    step: 150; loss: 3.246; l2dist: 1.621\n",
      "    step: 200; loss: 3.064; l2dist: 1.582\n",
      "    step: 250; loss: 2.987; l2dist: 1.577\n",
      "    step: 300; loss: 2.954; l2dist: 1.569\n",
      "    step: 350; loss: 2.926; l2dist: 1.562\n",
      "    step: 400; loss: 2.928; l2dist: 1.558\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.537; l2dist: 0.000\n",
      "    step: 50; loss: 7.506; l2dist: 2.203\n",
      "    step: 100; loss: 4.111; l2dist: 1.783\n",
      "    step: 150; loss: 3.242; l2dist: 1.626\n",
      "    step: 200; loss: 3.061; l2dist: 1.591\n",
      "    step: 250; loss: 3.002; l2dist: 1.576\n",
      "    step: 300; loss: 2.952; l2dist: 1.569\n",
      "    step: 350; loss: 2.933; l2dist: 1.566\n",
      "    step: 400; loss: 2.929; l2dist: 1.562\n",
      "    step: 450; loss: 2.919; l2dist: 1.563\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 129.372; l2dist: 0.000\n",
      "    step: 50; loss: 14.266; l2dist: 3.415\n",
      "    step: 100; loss: 6.327; l2dist: 2.402\n",
      "    step: 150; loss: 4.384; l2dist: 1.985\n",
      "    step: 200; loss: 3.609; l2dist: 1.789\n",
      "    step: 250; loss: 3.297; l2dist: 1.700\n",
      "    step: 300; loss: 3.150; l2dist: 1.660\n",
      "    step: 350; loss: 3.032; l2dist: 1.628\n",
      "    step: 400; loss: 2.978; l2dist: 1.609\n",
      "    step: 450; loss: 2.954; l2dist: 1.606\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 133.737; l2dist: 0.000\n",
      "    step: 50; loss: 13.136; l2dist: 3.280\n",
      "    step: 100; loss: 5.924; l2dist: 2.270\n",
      "    step: 150; loss: 4.229; l2dist: 1.900\n",
      "    step: 200; loss: 3.573; l2dist: 1.739\n",
      "    step: 250; loss: 3.254; l2dist: 1.660\n",
      "    step: 300; loss: 3.091; l2dist: 1.614\n",
      "    step: 350; loss: 2.969; l2dist: 1.589\n",
      "    step: 400; loss: 2.896; l2dist: 1.582\n",
      "    step: 450; loss: 2.904; l2dist: 1.576\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 76.721; l2dist: 0.000\n",
      "    step: 50; loss: 10.223; l2dist: 2.825\n",
      "    step: 100; loss: 4.660; l2dist: 1.995\n",
      "    step: 150; loss: 3.505; l2dist: 1.733\n",
      "    step: 200; loss: 3.113; l2dist: 1.629\n",
      "    step: 250; loss: 2.942; l2dist: 1.589\n",
      "    step: 300; loss: 2.851; l2dist: 1.565\n",
      "    step: 350; loss: 2.783; l2dist: 1.551\n",
      "    step: 400; loss: 2.770; l2dist: 1.546\n",
      "    step: 450; loss: 2.739; l2dist: 1.541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.948; l2dist: 0.000\n",
      "    step: 50; loss: 8.614; l2dist: 2.495\n",
      "    step: 100; loss: 4.116; l2dist: 1.871\n",
      "    step: 150; loss: 3.199; l2dist: 1.648\n",
      "    step: 200; loss: 2.906; l2dist: 1.579\n",
      "    step: 250; loss: 2.802; l2dist: 1.557\n",
      "    step: 300; loss: 2.735; l2dist: 1.539\n",
      "    step: 350; loss: 2.713; l2dist: 1.534\n",
      "    step: 400; loss: 2.703; l2dist: 1.527\n",
      "    step: 450; loss: 2.685; l2dist: 1.521\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.760; l2dist: 0.000\n",
      "    step: 50; loss: 7.881; l2dist: 2.329\n",
      "    step: 100; loss: 3.962; l2dist: 1.798\n",
      "    step: 150; loss: 3.112; l2dist: 1.620\n",
      "    step: 200; loss: 2.876; l2dist: 1.571\n",
      "    step: 250; loss: 2.778; l2dist: 1.549\n",
      "    step: 300; loss: 2.743; l2dist: 1.533\n",
      "    step: 350; loss: 2.709; l2dist: 1.529\n",
      "    step: 400; loss: 2.699; l2dist: 1.518\n",
      "    step: 450; loss: 2.697; l2dist: 1.526\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.930; l2dist: 0.000\n",
      "    step: 50; loss: 7.756; l2dist: 2.310\n",
      "    step: 100; loss: 3.850; l2dist: 1.781\n",
      "    step: 150; loss: 3.007; l2dist: 1.585\n",
      "    step: 200; loss: 2.809; l2dist: 1.536\n",
      "    step: 250; loss: 2.722; l2dist: 1.516\n",
      "    step: 300; loss: 2.680; l2dist: 1.500\n",
      "    step: 350; loss: 2.704; l2dist: 1.503\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.534; l2dist: 0.000\n",
      "    step: 50; loss: 7.680; l2dist: 2.293\n",
      "    step: 100; loss: 3.862; l2dist: 1.780\n",
      "    step: 150; loss: 3.042; l2dist: 1.597\n",
      "    step: 200; loss: 2.827; l2dist: 1.545\n",
      "    step: 250; loss: 2.743; l2dist: 1.523\n",
      "    step: 300; loss: 2.729; l2dist: 1.517\n",
      "    step: 350; loss: 2.693; l2dist: 1.511\n",
      "    step: 400; loss: 2.689; l2dist: 1.505\n",
      "    step: 450; loss: 2.685; l2dist: 1.507\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.025; l2dist: 0.000\n",
      "    step: 50; loss: 7.632; l2dist: 2.281\n",
      "    step: 100; loss: 3.855; l2dist: 1.779\n",
      "    step: 150; loss: 3.031; l2dist: 1.596\n",
      "    step: 200; loss: 2.822; l2dist: 1.545\n",
      "    step: 250; loss: 2.741; l2dist: 1.524\n",
      "    step: 300; loss: 2.718; l2dist: 1.516\n",
      "    step: 350; loss: 2.685; l2dist: 1.516\n",
      "    step: 400; loss: 2.690; l2dist: 1.511\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.525; l2dist: 0.000\n",
      "    step: 50; loss: 7.593; l2dist: 2.272\n",
      "    step: 100; loss: 3.846; l2dist: 1.779\n",
      "    step: 150; loss: 3.021; l2dist: 1.593\n",
      "    step: 200; loss: 2.813; l2dist: 1.550\n",
      "    step: 250; loss: 2.753; l2dist: 1.524\n",
      "    step: 300; loss: 2.704; l2dist: 1.518\n",
      "    step: 350; loss: 2.695; l2dist: 1.514\n",
      "    step: 400; loss: 2.669; l2dist: 1.506\n",
      "    step: 450; loss: 2.668; l2dist: 1.505\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.691; l2dist: 0.000\n",
      "    step: 50; loss: 7.617; l2dist: 2.279\n",
      "    step: 100; loss: 3.850; l2dist: 1.777\n",
      "    step: 150; loss: 3.029; l2dist: 1.598\n",
      "    step: 200; loss: 2.821; l2dist: 1.547\n",
      "    step: 250; loss: 2.757; l2dist: 1.531\n",
      "    step: 300; loss: 2.712; l2dist: 1.512\n",
      "    step: 350; loss: 2.697; l2dist: 1.511\n",
      "    step: 400; loss: 2.672; l2dist: 1.511\n",
      "    step: 450; loss: 2.669; l2dist: 1.513\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 145.148; l2dist: 0.000\n",
      "    step: 50; loss: 18.470; l2dist: 3.811\n",
      "    step: 100; loss: 7.670; l2dist: 2.680\n",
      "    step: 150; loss: 5.264; l2dist: 2.180\n",
      "    step: 200; loss: 4.279; l2dist: 1.949\n",
      "    step: 250; loss: 3.802; l2dist: 1.838\n",
      "    step: 300; loss: 3.663; l2dist: 1.802\n",
      "    step: 350; loss: 3.507; l2dist: 1.755\n",
      "    step: 400; loss: 3.412; l2dist: 1.733\n",
      "    step: 450; loss: 3.386; l2dist: 1.728\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 168.340; l2dist: 0.000\n",
      "    step: 50; loss: 17.259; l2dist: 3.703\n",
      "    step: 100; loss: 7.696; l2dist: 2.614\n",
      "    step: 150; loss: 5.347; l2dist: 2.156\n",
      "    step: 200; loss: 4.363; l2dist: 1.945\n",
      "    step: 250; loss: 3.864; l2dist: 1.835\n",
      "    step: 300; loss: 3.621; l2dist: 1.770\n",
      "    step: 350; loss: 3.464; l2dist: 1.733\n",
      "    step: 400; loss: 3.373; l2dist: 1.717\n",
      "    step: 450; loss: 3.317; l2dist: 1.705\n",
      "binary step: 1; number of successful adv: 99/100\n",
      "    step: 0; loss: 253.010; l2dist: 0.000\n",
      "    step: 50; loss: 18.050; l2dist: 3.858\n",
      "    step: 100; loss: 8.923; l2dist: 2.817\n",
      "    step: 150; loss: 6.110; l2dist: 2.299\n",
      "    step: 200; loss: 4.826; l2dist: 2.036\n",
      "    step: 250; loss: 4.163; l2dist: 1.891\n",
      "    step: 300; loss: 3.817; l2dist: 1.811\n",
      "    step: 350; loss: 3.592; l2dist: 1.759\n",
      "    step: 400; loss: 3.515; l2dist: 1.744\n",
      "    step: 450; loss: 3.401; l2dist: 1.715\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 146.640; l2dist: 0.000\n",
      "    step: 50; loss: 14.266; l2dist: 3.342\n",
      "    step: 100; loss: 6.530; l2dist: 2.384\n",
      "    step: 150; loss: 4.657; l2dist: 1.998\n",
      "    step: 200; loss: 3.916; l2dist: 1.831\n",
      "    step: 250; loss: 3.548; l2dist: 1.751\n",
      "    step: 300; loss: 3.401; l2dist: 1.718\n",
      "    step: 350; loss: 3.318; l2dist: 1.696\n",
      "    step: 400; loss: 3.282; l2dist: 1.682\n",
      "    step: 450; loss: 3.230; l2dist: 1.679\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 94.534; l2dist: 0.000\n",
      "    step: 50; loss: 11.817; l2dist: 2.986\n",
      "    step: 100; loss: 5.385; l2dist: 2.143\n",
      "    step: 150; loss: 3.941; l2dist: 1.834\n",
      "    step: 200; loss: 3.484; l2dist: 1.724\n",
      "    step: 250; loss: 3.290; l2dist: 1.676\n",
      "    step: 300; loss: 3.215; l2dist: 1.652\n",
      "    step: 350; loss: 3.154; l2dist: 1.636\n",
      "    step: 400; loss: 3.130; l2dist: 1.640\n",
      "    step: 450; loss: 3.109; l2dist: 1.633\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 69.955; l2dist: 0.000\n",
      "    step: 50; loss: 10.780; l2dist: 2.795\n",
      "    step: 100; loss: 4.911; l2dist: 2.029\n",
      "    step: 150; loss: 3.719; l2dist: 1.772\n",
      "    step: 200; loss: 3.377; l2dist: 1.688\n",
      "    step: 250; loss: 3.268; l2dist: 1.661\n",
      "    step: 300; loss: 3.185; l2dist: 1.645\n",
      "    step: 350; loss: 3.142; l2dist: 1.634\n",
      "    step: 400; loss: 3.111; l2dist: 1.638\n",
      "    step: 450; loss: 3.098; l2dist: 1.623\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 57.396; l2dist: 0.000\n",
      "    step: 50; loss: 10.237; l2dist: 2.680\n",
      "    step: 100; loss: 4.702; l2dist: 1.969\n",
      "    step: 150; loss: 3.591; l2dist: 1.737\n",
      "    step: 200; loss: 3.284; l2dist: 1.668\n",
      "    step: 250; loss: 3.182; l2dist: 1.633\n",
      "    step: 300; loss: 3.120; l2dist: 1.621\n",
      "    step: 350; loss: 3.068; l2dist: 1.618\n",
      "    step: 400; loss: 3.063; l2dist: 1.619\n",
      "    step: 450; loss: 3.062; l2dist: 1.620\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.287; l2dist: 0.000\n",
      "    step: 50; loss: 10.032; l2dist: 2.634\n",
      "    step: 100; loss: 4.574; l2dist: 1.951\n",
      "    step: 150; loss: 3.547; l2dist: 1.724\n",
      "    step: 200; loss: 3.255; l2dist: 1.662\n",
      "    step: 250; loss: 3.175; l2dist: 1.632\n",
      "    step: 300; loss: 3.072; l2dist: 1.618\n",
      "    step: 350; loss: 3.093; l2dist: 1.621\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.223; l2dist: 0.000\n",
      "    step: 50; loss: 9.931; l2dist: 2.612\n",
      "    step: 100; loss: 4.544; l2dist: 1.940\n",
      "    step: 150; loss: 3.516; l2dist: 1.724\n",
      "    step: 200; loss: 3.249; l2dist: 1.661\n",
      "    step: 250; loss: 3.128; l2dist: 1.631\n",
      "    step: 300; loss: 3.082; l2dist: 1.623\n",
      "    step: 350; loss: 3.074; l2dist: 1.617\n",
      "    step: 400; loss: 3.031; l2dist: 1.610\n",
      "    step: 450; loss: 3.034; l2dist: 1.610\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 48.462; l2dist: 0.000\n",
      "    step: 50; loss: 9.985; l2dist: 2.623\n",
      "    step: 100; loss: 4.557; l2dist: 1.952\n",
      "    step: 150; loss: 3.522; l2dist: 1.730\n",
      "    step: 200; loss: 3.239; l2dist: 1.668\n",
      "    step: 250; loss: 3.131; l2dist: 1.639\n",
      "    step: 300; loss: 3.105; l2dist: 1.633\n",
      "    step: 350; loss: 3.078; l2dist: 1.621\n",
      "    step: 400; loss: 3.048; l2dist: 1.619\n",
      "    step: 450; loss: 3.045; l2dist: 1.614\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 135.869; l2dist: 0.000\n",
      "    step: 50; loss: 14.284; l2dist: 3.580\n",
      "    step: 100; loss: 6.466; l2dist: 2.472\n",
      "    step: 150; loss: 4.314; l2dist: 1.999\n",
      "    step: 200; loss: 3.465; l2dist: 1.786\n",
      "    step: 250; loss: 3.170; l2dist: 1.695\n",
      "    step: 300; loss: 2.936; l2dist: 1.638\n",
      "    step: 350; loss: 2.876; l2dist: 1.619\n",
      "    step: 400; loss: 2.847; l2dist: 1.607\n",
      "    step: 450; loss: 2.811; l2dist: 1.590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 120.554; l2dist: 0.000\n",
      "    step: 50; loss: 12.345; l2dist: 3.276\n",
      "    step: 100; loss: 5.314; l2dist: 2.205\n",
      "    step: 150; loss: 3.758; l2dist: 1.832\n",
      "    step: 200; loss: 3.221; l2dist: 1.696\n",
      "    step: 250; loss: 2.975; l2dist: 1.638\n",
      "    step: 300; loss: 2.863; l2dist: 1.606\n",
      "    step: 350; loss: 2.765; l2dist: 1.578\n",
      "    step: 400; loss: 2.705; l2dist: 1.568\n",
      "    step: 450; loss: 2.671; l2dist: 1.560\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 68.938; l2dist: 0.000\n",
      "    step: 50; loss: 9.887; l2dist: 2.845\n",
      "    step: 100; loss: 4.265; l2dist: 1.961\n",
      "    step: 150; loss: 3.213; l2dist: 1.695\n",
      "    step: 200; loss: 2.877; l2dist: 1.610\n",
      "    step: 250; loss: 2.749; l2dist: 1.575\n",
      "    step: 300; loss: 2.675; l2dist: 1.559\n",
      "    step: 350; loss: 2.623; l2dist: 1.540\n",
      "    step: 400; loss: 2.592; l2dist: 1.538\n",
      "    step: 450; loss: 2.615; l2dist: 1.538\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.177; l2dist: 0.000\n",
      "    step: 50; loss: 8.529; l2dist: 2.566\n",
      "    step: 100; loss: 3.902; l2dist: 1.865\n",
      "    step: 150; loss: 2.998; l2dist: 1.641\n",
      "    step: 200; loss: 2.745; l2dist: 1.575\n",
      "    step: 250; loss: 2.641; l2dist: 1.550\n",
      "    step: 300; loss: 2.586; l2dist: 1.533\n",
      "    step: 350; loss: 2.543; l2dist: 1.519\n",
      "    step: 400; loss: 2.523; l2dist: 1.517\n",
      "    step: 450; loss: 2.540; l2dist: 1.518\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.617; l2dist: 0.000\n",
      "    step: 50; loss: 7.667; l2dist: 2.339\n",
      "    step: 100; loss: 3.789; l2dist: 1.783\n",
      "    step: 150; loss: 2.906; l2dist: 1.588\n",
      "    step: 200; loss: 2.683; l2dist: 1.532\n",
      "    step: 250; loss: 2.578; l2dist: 1.511\n",
      "    step: 300; loss: 2.535; l2dist: 1.491\n",
      "    step: 350; loss: 2.487; l2dist: 1.484\n",
      "    step: 400; loss: 2.484; l2dist: 1.487\n",
      "    step: 450; loss: 2.477; l2dist: 1.478\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.789; l2dist: 0.000\n",
      "    step: 50; loss: 7.164; l2dist: 2.191\n",
      "    step: 100; loss: 3.705; l2dist: 1.700\n",
      "    step: 150; loss: 2.879; l2dist: 1.537\n",
      "    step: 200; loss: 2.667; l2dist: 1.497\n",
      "    step: 250; loss: 2.565; l2dist: 1.479\n",
      "    step: 300; loss: 2.517; l2dist: 1.462\n",
      "    step: 350; loss: 2.491; l2dist: 1.462\n",
      "    step: 400; loss: 2.493; l2dist: 1.462\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.897; l2dist: 0.000\n",
      "    step: 50; loss: 7.082; l2dist: 2.178\n",
      "    step: 100; loss: 3.685; l2dist: 1.698\n",
      "    step: 150; loss: 2.855; l2dist: 1.533\n",
      "    step: 200; loss: 2.625; l2dist: 1.497\n",
      "    step: 250; loss: 2.548; l2dist: 1.470\n",
      "    step: 300; loss: 2.504; l2dist: 1.472\n",
      "    step: 350; loss: 2.488; l2dist: 1.458\n",
      "    step: 400; loss: 2.471; l2dist: 1.461\n",
      "    step: 450; loss: 2.450; l2dist: 1.461\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.890; l2dist: 0.000\n",
      "    step: 50; loss: 7.038; l2dist: 2.167\n",
      "    step: 100; loss: 3.696; l2dist: 1.692\n",
      "    step: 150; loss: 2.855; l2dist: 1.545\n",
      "    step: 200; loss: 2.634; l2dist: 1.500\n",
      "    step: 250; loss: 2.543; l2dist: 1.486\n",
      "    step: 300; loss: 2.499; l2dist: 1.470\n",
      "    step: 350; loss: 2.491; l2dist: 1.463\n",
      "    step: 400; loss: 2.469; l2dist: 1.460\n",
      "    step: 450; loss: 2.474; l2dist: 1.467\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.413; l2dist: 0.000\n",
      "    step: 50; loss: 7.012; l2dist: 2.161\n",
      "    step: 100; loss: 3.697; l2dist: 1.687\n",
      "    step: 150; loss: 2.844; l2dist: 1.543\n",
      "    step: 200; loss: 2.627; l2dist: 1.504\n",
      "    step: 250; loss: 2.539; l2dist: 1.484\n",
      "    step: 300; loss: 2.517; l2dist: 1.485\n",
      "    step: 350; loss: 2.493; l2dist: 1.481\n",
      "    step: 400; loss: 2.471; l2dist: 1.467\n",
      "    step: 450; loss: 2.460; l2dist: 1.473\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.647; l2dist: 0.000\n",
      "    step: 50; loss: 7.072; l2dist: 2.176\n",
      "    step: 100; loss: 3.713; l2dist: 1.701\n",
      "    step: 150; loss: 2.853; l2dist: 1.559\n",
      "    step: 200; loss: 2.630; l2dist: 1.507\n",
      "    step: 250; loss: 2.546; l2dist: 1.495\n",
      "    step: 300; loss: 2.527; l2dist: 1.490\n",
      "    step: 350; loss: 2.485; l2dist: 1.479\n",
      "    step: 400; loss: 2.483; l2dist: 1.476\n",
      "    step: 450; loss: 2.459; l2dist: 1.474\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 140.690; l2dist: 0.000\n",
      "    step: 50; loss: 17.048; l2dist: 3.696\n",
      "    step: 100; loss: 7.246; l2dist: 2.614\n",
      "    step: 150; loss: 4.984; l2dist: 2.148\n",
      "    step: 200; loss: 4.018; l2dist: 1.923\n",
      "    step: 250; loss: 3.548; l2dist: 1.808\n",
      "    step: 300; loss: 3.337; l2dist: 1.750\n",
      "    step: 350; loss: 3.218; l2dist: 1.716\n",
      "    step: 400; loss: 3.134; l2dist: 1.698\n",
      "    step: 450; loss: 3.106; l2dist: 1.687\n",
      "binary step: 0; number of successful adv: 99/100\n",
      "    step: 0; loss: 84.165; l2dist: 0.000\n",
      "    step: 50; loss: 13.089; l2dist: 3.215\n",
      "    step: 100; loss: 5.411; l2dist: 2.217\n",
      "    step: 150; loss: 3.919; l2dist: 1.879\n",
      "    step: 200; loss: 3.407; l2dist: 1.757\n",
      "    step: 250; loss: 3.181; l2dist: 1.699\n",
      "    step: 300; loss: 3.059; l2dist: 1.665\n",
      "    step: 350; loss: 2.988; l2dist: 1.654\n",
      "    step: 400; loss: 2.970; l2dist: 1.644\n",
      "    step: 450; loss: 2.948; l2dist: 1.643\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 46.561; l2dist: 0.000\n",
      "    step: 50; loss: 10.498; l2dist: 2.839\n",
      "    step: 100; loss: 4.427; l2dist: 2.013\n",
      "    step: 150; loss: 3.387; l2dist: 1.749\n",
      "    step: 200; loss: 3.095; l2dist: 1.672\n",
      "    step: 250; loss: 2.967; l2dist: 1.646\n",
      "    step: 300; loss: 2.907; l2dist: 1.623\n",
      "    step: 350; loss: 2.864; l2dist: 1.617\n",
      "    step: 400; loss: 2.860; l2dist: 1.615\n",
      "    step: 450; loss: 2.828; l2dist: 1.605\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.881; l2dist: 0.000\n",
      "    step: 50; loss: 9.110; l2dist: 2.547\n",
      "    step: 100; loss: 4.150; l2dist: 1.930\n",
      "    step: 150; loss: 3.159; l2dist: 1.700\n",
      "    step: 200; loss: 2.938; l2dist: 1.626\n",
      "    step: 250; loss: 2.838; l2dist: 1.614\n",
      "    step: 300; loss: 2.803; l2dist: 1.597\n",
      "    step: 350; loss: 2.795; l2dist: 1.603\n",
      "    step: 400; loss: 2.773; l2dist: 1.585\n",
      "    step: 450; loss: 2.766; l2dist: 1.593\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.406; l2dist: 0.000\n",
      "    step: 50; loss: 8.407; l2dist: 2.353\n",
      "    step: 100; loss: 4.076; l2dist: 1.848\n",
      "    step: 150; loss: 3.126; l2dist: 1.662\n",
      "    step: 200; loss: 2.910; l2dist: 1.611\n",
      "    step: 250; loss: 2.836; l2dist: 1.596\n",
      "    step: 300; loss: 2.788; l2dist: 1.586\n",
      "    step: 350; loss: 2.767; l2dist: 1.579\n",
      "    step: 400; loss: 2.758; l2dist: 1.573\n",
      "    step: 450; loss: 2.751; l2dist: 1.575\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.521; l2dist: 0.000\n",
      "    step: 50; loss: 8.184; l2dist: 2.314\n",
      "    step: 100; loss: 4.070; l2dist: 1.832\n",
      "    step: 150; loss: 3.114; l2dist: 1.639\n",
      "    step: 200; loss: 2.920; l2dist: 1.599\n",
      "    step: 250; loss: 2.844; l2dist: 1.584\n",
      "    step: 300; loss: 2.807; l2dist: 1.578\n",
      "    step: 350; loss: 2.800; l2dist: 1.565\n",
      "    step: 400; loss: 2.778; l2dist: 1.566\n",
      "    step: 450; loss: 2.782; l2dist: 1.559\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.573; l2dist: 0.000\n",
      "    step: 50; loss: 8.099; l2dist: 2.292\n",
      "    step: 100; loss: 4.064; l2dist: 1.834\n",
      "    step: 150; loss: 3.092; l2dist: 1.637\n",
      "    step: 200; loss: 2.892; l2dist: 1.597\n",
      "    step: 250; loss: 2.816; l2dist: 1.580\n",
      "    step: 300; loss: 2.785; l2dist: 1.569\n",
      "    step: 350; loss: 2.768; l2dist: 1.562\n",
      "    step: 400; loss: 2.763; l2dist: 1.560\n",
      "    step: 450; loss: 2.747; l2dist: 1.561\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.052; l2dist: 0.000\n",
      "    step: 50; loss: 8.037; l2dist: 2.270\n",
      "    step: 100; loss: 4.052; l2dist: 1.826\n",
      "    step: 150; loss: 3.092; l2dist: 1.641\n",
      "    step: 200; loss: 2.897; l2dist: 1.593\n",
      "    step: 250; loss: 2.829; l2dist: 1.568\n",
      "    step: 300; loss: 2.790; l2dist: 1.562\n",
      "    step: 350; loss: 2.777; l2dist: 1.559\n",
      "    step: 400; loss: 2.769; l2dist: 1.569\n",
      "    step: 450; loss: 2.745; l2dist: 1.562\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.060; l2dist: 0.000\n",
      "    step: 50; loss: 8.035; l2dist: 2.269\n",
      "    step: 100; loss: 4.057; l2dist: 1.830\n",
      "    step: 150; loss: 3.094; l2dist: 1.632\n",
      "    step: 200; loss: 2.898; l2dist: 1.590\n",
      "    step: 250; loss: 2.827; l2dist: 1.574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 300; loss: 2.787; l2dist: 1.566\n",
      "    step: 350; loss: 2.760; l2dist: 1.566\n",
      "    step: 400; loss: 2.755; l2dist: 1.569\n",
      "    step: 450; loss: 2.759; l2dist: 1.559\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.262; l2dist: 0.000\n",
      "    step: 50; loss: 8.081; l2dist: 2.279\n",
      "    step: 100; loss: 4.060; l2dist: 1.836\n",
      "    step: 150; loss: 3.096; l2dist: 1.640\n",
      "    step: 200; loss: 2.900; l2dist: 1.593\n",
      "    step: 250; loss: 2.833; l2dist: 1.577\n",
      "    step: 300; loss: 2.799; l2dist: 1.571\n",
      "    step: 350; loss: 2.765; l2dist: 1.569\n",
      "    step: 400; loss: 2.752; l2dist: 1.565\n",
      "    step: 450; loss: 2.761; l2dist: 1.559\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 144.595; l2dist: 0.000\n",
      "    step: 50; loss: 16.906; l2dist: 3.706\n",
      "    step: 100; loss: 7.138; l2dist: 2.590\n",
      "    step: 150; loss: 4.933; l2dist: 2.133\n",
      "    step: 200; loss: 4.080; l2dist: 1.927\n",
      "    step: 250; loss: 3.687; l2dist: 1.819\n",
      "    step: 300; loss: 3.452; l2dist: 1.761\n",
      "    step: 350; loss: 3.358; l2dist: 1.736\n",
      "    step: 400; loss: 3.241; l2dist: 1.708\n",
      "    step: 450; loss: 3.286; l2dist: 1.709\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 160.166; l2dist: 0.000\n",
      "    step: 50; loss: 16.188; l2dist: 3.603\n",
      "    step: 100; loss: 6.917; l2dist: 2.491\n",
      "    step: 150; loss: 4.891; l2dist: 2.074\n",
      "    step: 200; loss: 4.067; l2dist: 1.881\n",
      "    step: 250; loss: 3.656; l2dist: 1.786\n",
      "    step: 300; loss: 3.492; l2dist: 1.734\n",
      "    step: 350; loss: 3.336; l2dist: 1.711\n",
      "    step: 400; loss: 3.301; l2dist: 1.697\n",
      "    step: 450; loss: 3.210; l2dist: 1.687\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 89.089; l2dist: 0.000\n",
      "    step: 50; loss: 12.600; l2dist: 3.136\n",
      "    step: 100; loss: 5.342; l2dist: 2.172\n",
      "    step: 150; loss: 3.985; l2dist: 1.859\n",
      "    step: 200; loss: 3.516; l2dist: 1.761\n",
      "    step: 250; loss: 3.324; l2dist: 1.713\n",
      "    step: 300; loss: 3.210; l2dist: 1.680\n",
      "    step: 350; loss: 3.154; l2dist: 1.668\n",
      "    step: 400; loss: 3.095; l2dist: 1.657\n",
      "    step: 450; loss: 3.076; l2dist: 1.653\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 55.877; l2dist: 0.000\n",
      "    step: 50; loss: 10.596; l2dist: 2.776\n",
      "    step: 100; loss: 4.654; l2dist: 2.014\n",
      "    step: 150; loss: 3.605; l2dist: 1.777\n",
      "    step: 200; loss: 3.295; l2dist: 1.698\n",
      "    step: 250; loss: 3.141; l2dist: 1.671\n",
      "    step: 300; loss: 3.060; l2dist: 1.650\n",
      "    step: 350; loss: 3.015; l2dist: 1.640\n",
      "    step: 400; loss: 2.978; l2dist: 1.620\n",
      "    step: 450; loss: 2.945; l2dist: 1.622\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.004; l2dist: 0.000\n",
      "    step: 50; loss: 9.913; l2dist: 2.636\n",
      "    step: 100; loss: 4.525; l2dist: 1.948\n",
      "    step: 150; loss: 3.558; l2dist: 1.740\n",
      "    step: 200; loss: 3.277; l2dist: 1.673\n",
      "    step: 250; loss: 3.154; l2dist: 1.646\n",
      "    step: 300; loss: 3.072; l2dist: 1.627\n",
      "    step: 350; loss: 3.041; l2dist: 1.618\n",
      "    step: 400; loss: 2.994; l2dist: 1.619\n",
      "    step: 450; loss: 2.985; l2dist: 1.610\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.181; l2dist: 0.000\n",
      "    step: 50; loss: 9.527; l2dist: 2.555\n",
      "    step: 100; loss: 4.432; l2dist: 1.920\n",
      "    step: 150; loss: 3.510; l2dist: 1.712\n",
      "    step: 200; loss: 3.227; l2dist: 1.662\n",
      "    step: 250; loss: 3.145; l2dist: 1.638\n",
      "    step: 300; loss: 3.052; l2dist: 1.624\n",
      "    step: 350; loss: 3.016; l2dist: 1.607\n",
      "    step: 400; loss: 3.006; l2dist: 1.617\n",
      "    step: 450; loss: 2.988; l2dist: 1.605\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.687; l2dist: 0.000\n",
      "    step: 50; loss: 9.294; l2dist: 2.506\n",
      "    step: 100; loss: 4.382; l2dist: 1.902\n",
      "    step: 150; loss: 3.465; l2dist: 1.707\n",
      "    step: 200; loss: 3.199; l2dist: 1.653\n",
      "    step: 250; loss: 3.097; l2dist: 1.626\n",
      "    step: 300; loss: 3.047; l2dist: 1.625\n",
      "    step: 350; loss: 3.021; l2dist: 1.608\n",
      "    step: 400; loss: 3.000; l2dist: 1.605\n",
      "    step: 450; loss: 2.991; l2dist: 1.606\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.619; l2dist: 0.000\n",
      "    step: 50; loss: 9.266; l2dist: 2.500\n",
      "    step: 100; loss: 4.381; l2dist: 1.900\n",
      "    step: 150; loss: 3.455; l2dist: 1.712\n",
      "    step: 200; loss: 3.204; l2dist: 1.663\n",
      "    step: 250; loss: 3.099; l2dist: 1.632\n",
      "    step: 300; loss: 3.033; l2dist: 1.619\n",
      "    step: 350; loss: 3.011; l2dist: 1.611\n",
      "    step: 400; loss: 2.991; l2dist: 1.609\n",
      "    step: 450; loss: 2.990; l2dist: 1.609\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.826; l2dist: 0.000\n",
      "    step: 50; loss: 9.209; l2dist: 2.489\n",
      "    step: 100; loss: 4.403; l2dist: 1.902\n",
      "    step: 150; loss: 3.475; l2dist: 1.716\n",
      "    step: 200; loss: 3.240; l2dist: 1.668\n",
      "    step: 250; loss: 3.107; l2dist: 1.642\n",
      "    step: 300; loss: 3.057; l2dist: 1.625\n",
      "    step: 350; loss: 3.034; l2dist: 1.620\n",
      "    step: 400; loss: 3.012; l2dist: 1.617\n",
      "    step: 450; loss: 3.016; l2dist: 1.611\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.277; l2dist: 0.000\n",
      "    step: 50; loss: 9.272; l2dist: 2.505\n",
      "    step: 100; loss: 4.411; l2dist: 1.916\n",
      "    step: 150; loss: 3.478; l2dist: 1.720\n",
      "    step: 200; loss: 3.226; l2dist: 1.669\n",
      "    step: 250; loss: 3.160; l2dist: 1.649\n",
      "    step: 300; loss: 3.060; l2dist: 1.625\n",
      "    step: 350; loss: 3.018; l2dist: 1.620\n",
      "    step: 400; loss: 3.011; l2dist: 1.617\n",
      "    step: 450; loss: 3.018; l2dist: 1.619\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 127.703; l2dist: 0.000\n",
      "    step: 50; loss: 16.087; l2dist: 3.471\n",
      "    step: 100; loss: 6.198; l2dist: 2.414\n",
      "    step: 150; loss: 4.333; l2dist: 1.990\n",
      "    step: 200; loss: 3.558; l2dist: 1.796\n",
      "    step: 250; loss: 3.156; l2dist: 1.685\n",
      "    step: 300; loss: 3.011; l2dist: 1.639\n",
      "    step: 350; loss: 2.889; l2dist: 1.608\n",
      "    step: 400; loss: 2.833; l2dist: 1.587\n",
      "    step: 450; loss: 2.819; l2dist: 1.586\n",
      "binary step: 0; number of successful adv: 96/100\n",
      "    step: 0; loss: 115.951; l2dist: 0.000\n",
      "    step: 50; loss: 13.470; l2dist: 3.179\n",
      "    step: 100; loss: 5.531; l2dist: 2.213\n",
      "    step: 150; loss: 3.994; l2dist: 1.867\n",
      "    step: 200; loss: 3.390; l2dist: 1.714\n",
      "    step: 250; loss: 3.089; l2dist: 1.639\n",
      "    step: 300; loss: 2.952; l2dist: 1.609\n",
      "    step: 350; loss: 2.843; l2dist: 1.574\n",
      "    step: 400; loss: 2.742; l2dist: 1.565\n",
      "    step: 450; loss: 2.737; l2dist: 1.556\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 63.297; l2dist: 0.000\n",
      "    step: 50; loss: 10.285; l2dist: 2.719\n",
      "    step: 100; loss: 4.358; l2dist: 1.946\n",
      "    step: 150; loss: 3.302; l2dist: 1.698\n",
      "    step: 200; loss: 2.940; l2dist: 1.609\n",
      "    step: 250; loss: 2.772; l2dist: 1.569\n",
      "    step: 300; loss: 2.689; l2dist: 1.537\n",
      "    step: 350; loss: 2.618; l2dist: 1.528\n",
      "    step: 400; loss: 2.575; l2dist: 1.511\n",
      "    step: 450; loss: 2.583; l2dist: 1.519\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.944; l2dist: 0.000\n",
      "    step: 50; loss: 8.474; l2dist: 2.413\n",
      "    step: 100; loss: 3.926; l2dist: 1.839\n",
      "    step: 150; loss: 3.043; l2dist: 1.629\n",
      "    step: 200; loss: 2.768; l2dist: 1.563\n",
      "    step: 250; loss: 2.641; l2dist: 1.527\n",
      "    step: 300; loss: 2.600; l2dist: 1.518\n",
      "    step: 350; loss: 2.563; l2dist: 1.507\n",
      "    step: 400; loss: 2.533; l2dist: 1.499\n",
      "    step: 450; loss: 2.532; l2dist: 1.491\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.568; l2dist: 0.000\n",
      "    step: 50; loss: 7.664; l2dist: 2.223\n",
      "    step: 100; loss: 3.821; l2dist: 1.790\n",
      "    step: 150; loss: 2.920; l2dist: 1.574\n",
      "    step: 200; loss: 2.689; l2dist: 1.527\n",
      "    step: 250; loss: 2.575; l2dist: 1.495\n",
      "    step: 300; loss: 2.548; l2dist: 1.481\n",
      "    step: 350; loss: 2.518; l2dist: 1.473\n",
      "    step: 400; loss: 2.518; l2dist: 1.478\n",
      "    step: 450; loss: 2.494; l2dist: 1.469\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 25.361; l2dist: 0.000\n",
      "    step: 50; loss: 7.369; l2dist: 2.135\n",
      "    step: 100; loss: 3.760; l2dist: 1.734\n",
      "    step: 150; loss: 2.857; l2dist: 1.550\n",
      "    step: 200; loss: 2.657; l2dist: 1.495\n",
      "    step: 250; loss: 2.553; l2dist: 1.478\n",
      "    step: 300; loss: 2.549; l2dist: 1.480\n",
      "    step: 350; loss: 2.495; l2dist: 1.466\n",
      "    step: 400; loss: 2.527; l2dist: 1.473\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.797; l2dist: 0.000\n",
      "    step: 50; loss: 7.339; l2dist: 2.123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 3.746; l2dist: 1.739\n",
      "    step: 150; loss: 2.855; l2dist: 1.557\n",
      "    step: 200; loss: 2.629; l2dist: 1.501\n",
      "    step: 250; loss: 2.558; l2dist: 1.480\n",
      "    step: 300; loss: 2.520; l2dist: 1.477\n",
      "    step: 350; loss: 2.503; l2dist: 1.472\n",
      "    step: 400; loss: 2.484; l2dist: 1.464\n",
      "    step: 450; loss: 2.485; l2dist: 1.466\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.722; l2dist: 0.000\n",
      "    step: 50; loss: 7.241; l2dist: 2.105\n",
      "    step: 100; loss: 3.726; l2dist: 1.733\n",
      "    step: 150; loss: 2.830; l2dist: 1.552\n",
      "    step: 200; loss: 2.620; l2dist: 1.498\n",
      "    step: 250; loss: 2.556; l2dist: 1.481\n",
      "    step: 300; loss: 2.506; l2dist: 1.471\n",
      "    step: 350; loss: 2.516; l2dist: 1.469\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.276; l2dist: 0.000\n",
      "    step: 50; loss: 7.239; l2dist: 2.098\n",
      "    step: 100; loss: 3.726; l2dist: 1.729\n",
      "    step: 150; loss: 2.829; l2dist: 1.549\n",
      "    step: 200; loss: 2.638; l2dist: 1.502\n",
      "    step: 250; loss: 2.542; l2dist: 1.481\n",
      "    step: 300; loss: 2.526; l2dist: 1.470\n",
      "    step: 350; loss: 2.502; l2dist: 1.470\n",
      "    step: 400; loss: 2.506; l2dist: 1.472\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.494; l2dist: 0.000\n",
      "    step: 50; loss: 7.290; l2dist: 2.114\n",
      "    step: 100; loss: 3.720; l2dist: 1.740\n",
      "    step: 150; loss: 2.827; l2dist: 1.550\n",
      "    step: 200; loss: 2.615; l2dist: 1.506\n",
      "    step: 250; loss: 2.557; l2dist: 1.480\n",
      "    step: 300; loss: 2.535; l2dist: 1.475\n",
      "    step: 350; loss: 2.508; l2dist: 1.476\n",
      "    step: 400; loss: 2.492; l2dist: 1.476\n",
      "    step: 450; loss: 2.489; l2dist: 1.470\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 148.207; l2dist: 0.000\n",
      "    step: 50; loss: 15.764; l2dist: 3.764\n",
      "    step: 100; loss: 7.000; l2dist: 2.608\n",
      "    step: 150; loss: 4.682; l2dist: 2.121\n",
      "    step: 200; loss: 3.755; l2dist: 1.890\n",
      "    step: 250; loss: 3.311; l2dist: 1.770\n",
      "    step: 300; loss: 3.088; l2dist: 1.708\n",
      "    step: 350; loss: 3.009; l2dist: 1.679\n",
      "    step: 400; loss: 2.934; l2dist: 1.658\n",
      "    step: 450; loss: 2.938; l2dist: 1.662\n",
      "binary step: 0; number of successful adv: 100/100\n",
      "    step: 0; loss: 74.103; l2dist: 0.000\n",
      "    step: 50; loss: 11.726; l2dist: 3.202\n",
      "    step: 100; loss: 4.801; l2dist: 2.144\n",
      "    step: 150; loss: 3.521; l2dist: 1.818\n",
      "    step: 200; loss: 3.107; l2dist: 1.708\n",
      "    step: 250; loss: 2.954; l2dist: 1.653\n",
      "    step: 300; loss: 2.908; l2dist: 1.643\n",
      "    step: 350; loss: 2.836; l2dist: 1.618\n",
      "    step: 400; loss: 2.796; l2dist: 1.615\n",
      "    step: 450; loss: 2.792; l2dist: 1.613\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 42.071; l2dist: 0.000\n",
      "    step: 50; loss: 9.651; l2dist: 2.856\n",
      "    step: 100; loss: 4.056; l2dist: 1.959\n",
      "    step: 150; loss: 3.171; l2dist: 1.712\n",
      "    step: 200; loss: 2.924; l2dist: 1.646\n",
      "    step: 250; loss: 2.812; l2dist: 1.615\n",
      "    step: 300; loss: 2.762; l2dist: 1.605\n",
      "    step: 350; loss: 2.729; l2dist: 1.599\n",
      "    step: 400; loss: 2.724; l2dist: 1.598\n",
      "    step: 450; loss: 2.734; l2dist: 1.600\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.638; l2dist: 0.000\n",
      "    step: 50; loss: 8.405; l2dist: 2.547\n",
      "    step: 100; loss: 3.919; l2dist: 1.910\n",
      "    step: 150; loss: 3.074; l2dist: 1.685\n",
      "    step: 200; loss: 2.817; l2dist: 1.623\n",
      "    step: 250; loss: 2.752; l2dist: 1.602\n",
      "    step: 300; loss: 2.711; l2dist: 1.588\n",
      "    step: 350; loss: 2.708; l2dist: 1.583\n",
      "    step: 400; loss: 2.691; l2dist: 1.583\n",
      "    step: 450; loss: 2.685; l2dist: 1.578\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.136; l2dist: 0.000\n",
      "    step: 50; loss: 7.786; l2dist: 2.384\n",
      "    step: 100; loss: 3.962; l2dist: 1.858\n",
      "    step: 150; loss: 3.061; l2dist: 1.668\n",
      "    step: 200; loss: 2.837; l2dist: 1.621\n",
      "    step: 250; loss: 2.773; l2dist: 1.591\n",
      "    step: 300; loss: 2.730; l2dist: 1.596\n",
      "    step: 350; loss: 2.704; l2dist: 1.573\n",
      "    step: 400; loss: 2.686; l2dist: 1.578\n",
      "    step: 450; loss: 2.680; l2dist: 1.573\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.767; l2dist: 0.000\n",
      "    step: 50; loss: 7.568; l2dist: 2.325\n",
      "    step: 100; loss: 3.927; l2dist: 1.842\n",
      "    step: 150; loss: 3.020; l2dist: 1.636\n",
      "    step: 200; loss: 2.822; l2dist: 1.584\n",
      "    step: 250; loss: 2.752; l2dist: 1.564\n",
      "    step: 300; loss: 2.715; l2dist: 1.556\n",
      "    step: 350; loss: 2.711; l2dist: 1.550\n",
      "    step: 400; loss: 2.684; l2dist: 1.546\n",
      "    step: 450; loss: 2.685; l2dist: 1.547\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.483; l2dist: 0.000\n",
      "    step: 50; loss: 7.629; l2dist: 2.327\n",
      "    step: 100; loss: 3.937; l2dist: 1.850\n",
      "    step: 150; loss: 3.033; l2dist: 1.651\n",
      "    step: 200; loss: 2.811; l2dist: 1.600\n",
      "    step: 250; loss: 2.763; l2dist: 1.579\n",
      "    step: 300; loss: 2.706; l2dist: 1.575\n",
      "    step: 350; loss: 2.694; l2dist: 1.569\n",
      "    step: 400; loss: 2.683; l2dist: 1.563\n",
      "    step: 450; loss: 2.668; l2dist: 1.561\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.960; l2dist: 0.000\n",
      "    step: 50; loss: 7.578; l2dist: 2.316\n",
      "    step: 100; loss: 3.967; l2dist: 1.850\n",
      "    step: 150; loss: 3.051; l2dist: 1.652\n",
      "    step: 200; loss: 2.845; l2dist: 1.607\n",
      "    step: 250; loss: 2.763; l2dist: 1.587\n",
      "    step: 300; loss: 2.719; l2dist: 1.580\n",
      "    step: 350; loss: 2.708; l2dist: 1.570\n",
      "    step: 400; loss: 2.698; l2dist: 1.568\n",
      "    step: 450; loss: 2.686; l2dist: 1.564\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.942; l2dist: 0.000\n",
      "    step: 50; loss: 7.606; l2dist: 2.321\n",
      "    step: 100; loss: 3.955; l2dist: 1.861\n",
      "    step: 150; loss: 3.042; l2dist: 1.654\n",
      "    step: 200; loss: 2.840; l2dist: 1.606\n",
      "    step: 250; loss: 2.762; l2dist: 1.587\n",
      "    step: 300; loss: 2.725; l2dist: 1.584\n",
      "    step: 350; loss: 2.710; l2dist: 1.578\n",
      "    step: 400; loss: 2.691; l2dist: 1.575\n",
      "    step: 450; loss: 2.676; l2dist: 1.571\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.047; l2dist: 0.000\n",
      "    step: 50; loss: 7.623; l2dist: 2.330\n",
      "    step: 100; loss: 3.941; l2dist: 1.865\n",
      "    step: 150; loss: 3.023; l2dist: 1.657\n",
      "    step: 200; loss: 2.822; l2dist: 1.604\n",
      "    step: 250; loss: 2.746; l2dist: 1.593\n",
      "    step: 300; loss: 2.705; l2dist: 1.584\n",
      "    step: 350; loss: 2.693; l2dist: 1.570\n",
      "    step: 400; loss: 2.675; l2dist: 1.567\n",
      "    step: 450; loss: 2.663; l2dist: 1.567\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 167.189; l2dist: 0.000\n",
      "    step: 50; loss: 18.739; l2dist: 4.053\n",
      "    step: 100; loss: 8.899; l2dist: 2.930\n",
      "    step: 150; loss: 6.010; l2dist: 2.392\n",
      "    step: 200; loss: 4.865; l2dist: 2.143\n",
      "    step: 250; loss: 4.309; l2dist: 2.010\n",
      "    step: 300; loss: 4.117; l2dist: 1.948\n",
      "    step: 350; loss: 3.973; l2dist: 1.915\n",
      "    step: 400; loss: 3.877; l2dist: 1.891\n",
      "    step: 450; loss: 3.862; l2dist: 1.881\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 162.479; l2dist: 0.000\n",
      "    step: 50; loss: 17.381; l2dist: 3.874\n",
      "    step: 100; loss: 8.137; l2dist: 2.741\n",
      "    step: 150; loss: 5.620; l2dist: 2.258\n",
      "    step: 200; loss: 4.628; l2dist: 2.050\n",
      "    step: 250; loss: 4.216; l2dist: 1.947\n",
      "    step: 300; loss: 4.015; l2dist: 1.888\n",
      "    step: 350; loss: 3.998; l2dist: 1.897\n",
      "    step: 400; loss: 3.746; l2dist: 1.849\n",
      "    step: 450; loss: 3.718; l2dist: 1.832\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 94.947; l2dist: 0.000\n",
      "    step: 50; loss: 13.589; l2dist: 3.408\n",
      "    step: 100; loss: 6.081; l2dist: 2.356\n",
      "    step: 150; loss: 4.534; l2dist: 2.030\n",
      "    step: 200; loss: 3.985; l2dist: 1.900\n",
      "    step: 250; loss: 3.770; l2dist: 1.858\n",
      "    step: 300; loss: 3.643; l2dist: 1.822\n",
      "    step: 350; loss: 3.567; l2dist: 1.802\n",
      "    step: 400; loss: 3.543; l2dist: 1.800\n",
      "    step: 450; loss: 3.554; l2dist: 1.802\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 59.676; l2dist: 0.000\n",
      "    step: 50; loss: 11.270; l2dist: 2.965\n",
      "    step: 100; loss: 5.336; l2dist: 2.187\n",
      "    step: 150; loss: 4.151; l2dist: 1.935\n",
      "    step: 200; loss: 3.780; l2dist: 1.849\n",
      "    step: 250; loss: 3.644; l2dist: 1.817\n",
      "    step: 300; loss: 3.560; l2dist: 1.807\n",
      "    step: 350; loss: 3.532; l2dist: 1.800\n",
      "    step: 400; loss: 3.521; l2dist: 1.778\n",
      "    step: 450; loss: 3.496; l2dist: 1.782\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 49.655; l2dist: 0.000\n",
      "    step: 50; loss: 10.423; l2dist: 2.768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 100; loss: 5.168; l2dist: 2.102\n",
      "    step: 150; loss: 4.019; l2dist: 1.887\n",
      "    step: 200; loss: 3.702; l2dist: 1.822\n",
      "    step: 250; loss: 3.577; l2dist: 1.792\n",
      "    step: 300; loss: 3.486; l2dist: 1.780\n",
      "    step: 350; loss: 3.459; l2dist: 1.765\n",
      "    step: 400; loss: 3.408; l2dist: 1.759\n",
      "    step: 450; loss: 3.409; l2dist: 1.755\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.913; l2dist: 0.000\n",
      "    step: 50; loss: 10.066; l2dist: 2.726\n",
      "    step: 100; loss: 5.034; l2dist: 2.088\n",
      "    step: 150; loss: 3.935; l2dist: 1.859\n",
      "    step: 200; loss: 3.645; l2dist: 1.793\n",
      "    step: 250; loss: 3.519; l2dist: 1.770\n",
      "    step: 300; loss: 3.445; l2dist: 1.748\n",
      "    step: 350; loss: 3.414; l2dist: 1.748\n",
      "    step: 400; loss: 3.426; l2dist: 1.746\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.366; l2dist: 0.000\n",
      "    step: 50; loss: 10.005; l2dist: 2.711\n",
      "    step: 100; loss: 5.022; l2dist: 2.081\n",
      "    step: 150; loss: 3.921; l2dist: 1.866\n",
      "    step: 200; loss: 3.635; l2dist: 1.801\n",
      "    step: 250; loss: 3.503; l2dist: 1.774\n",
      "    step: 300; loss: 3.448; l2dist: 1.761\n",
      "    step: 350; loss: 3.416; l2dist: 1.756\n",
      "    step: 400; loss: 3.405; l2dist: 1.755\n",
      "    step: 450; loss: 3.395; l2dist: 1.752\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.965; l2dist: 0.000\n",
      "    step: 50; loss: 9.983; l2dist: 2.701\n",
      "    step: 100; loss: 5.031; l2dist: 2.084\n",
      "    step: 150; loss: 3.943; l2dist: 1.875\n",
      "    step: 200; loss: 3.635; l2dist: 1.810\n",
      "    step: 250; loss: 3.513; l2dist: 1.780\n",
      "    step: 300; loss: 3.442; l2dist: 1.770\n",
      "    step: 350; loss: 3.421; l2dist: 1.763\n",
      "    step: 400; loss: 3.426; l2dist: 1.763\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.177; l2dist: 0.000\n",
      "    step: 50; loss: 9.943; l2dist: 2.694\n",
      "    step: 100; loss: 5.025; l2dist: 2.081\n",
      "    step: 150; loss: 3.924; l2dist: 1.871\n",
      "    step: 200; loss: 3.639; l2dist: 1.811\n",
      "    step: 250; loss: 3.496; l2dist: 1.777\n",
      "    step: 300; loss: 3.446; l2dist: 1.770\n",
      "    step: 350; loss: 3.427; l2dist: 1.763\n",
      "    step: 400; loss: 3.404; l2dist: 1.767\n",
      "    step: 450; loss: 3.391; l2dist: 1.762\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.627; l2dist: 0.000\n",
      "    step: 50; loss: 9.990; l2dist: 2.703\n",
      "    step: 100; loss: 5.027; l2dist: 2.091\n",
      "    step: 150; loss: 3.929; l2dist: 1.873\n",
      "    step: 200; loss: 3.645; l2dist: 1.811\n",
      "    step: 250; loss: 3.523; l2dist: 1.783\n",
      "    step: 300; loss: 3.437; l2dist: 1.772\n",
      "    step: 350; loss: 3.411; l2dist: 1.764\n",
      "    step: 400; loss: 3.410; l2dist: 1.768\n",
      "    step: 450; loss: 3.412; l2dist: 1.764\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 176.941; l2dist: 0.000\n",
      "    step: 50; loss: 20.939; l2dist: 4.318\n",
      "    step: 100; loss: 10.012; l2dist: 3.123\n",
      "    step: 150; loss: 6.630; l2dist: 2.530\n",
      "    step: 200; loss: 5.232; l2dist: 2.234\n",
      "    step: 250; loss: 4.508; l2dist: 2.078\n",
      "    step: 300; loss: 4.153; l2dist: 1.988\n",
      "    step: 350; loss: 3.963; l2dist: 1.941\n",
      "    step: 400; loss: 3.942; l2dist: 1.925\n",
      "    step: 450; loss: 3.894; l2dist: 1.919\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 164.554; l2dist: 0.000\n",
      "    step: 50; loss: 18.681; l2dist: 4.049\n",
      "    step: 100; loss: 8.398; l2dist: 2.816\n",
      "    step: 150; loss: 5.852; l2dist: 2.330\n",
      "    step: 200; loss: 4.810; l2dist: 2.116\n",
      "    step: 250; loss: 4.314; l2dist: 2.007\n",
      "    step: 300; loss: 4.085; l2dist: 1.960\n",
      "    step: 350; loss: 3.926; l2dist: 1.912\n",
      "    step: 400; loss: 3.831; l2dist: 1.892\n",
      "    step: 450; loss: 3.746; l2dist: 1.876\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 91.728; l2dist: 0.000\n",
      "    step: 50; loss: 14.675; l2dist: 3.545\n",
      "    step: 100; loss: 6.259; l2dist: 2.425\n",
      "    step: 150; loss: 4.649; l2dist: 2.088\n",
      "    step: 200; loss: 4.129; l2dist: 1.967\n",
      "    step: 250; loss: 3.858; l2dist: 1.907\n",
      "    step: 300; loss: 3.760; l2dist: 1.877\n",
      "    step: 350; loss: 3.674; l2dist: 1.863\n",
      "    step: 400; loss: 3.629; l2dist: 1.843\n",
      "    step: 450; loss: 3.606; l2dist: 1.839\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.913; l2dist: 0.000\n",
      "    step: 50; loss: 12.505; l2dist: 3.100\n",
      "    step: 100; loss: 5.510; l2dist: 2.249\n",
      "    step: 150; loss: 4.238; l2dist: 1.981\n",
      "    step: 200; loss: 3.864; l2dist: 1.885\n",
      "    step: 250; loss: 3.646; l2dist: 1.847\n",
      "    step: 300; loss: 3.594; l2dist: 1.832\n",
      "    step: 350; loss: 3.521; l2dist: 1.817\n",
      "    step: 400; loss: 3.502; l2dist: 1.811\n",
      "    step: 450; loss: 3.505; l2dist: 1.805\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 45.397; l2dist: 0.000\n",
      "    step: 50; loss: 11.584; l2dist: 2.935\n",
      "    step: 100; loss: 5.230; l2dist: 2.149\n",
      "    step: 150; loss: 4.073; l2dist: 1.927\n",
      "    step: 200; loss: 3.718; l2dist: 1.847\n",
      "    step: 250; loss: 3.584; l2dist: 1.812\n",
      "    step: 300; loss: 3.535; l2dist: 1.814\n",
      "    step: 350; loss: 3.498; l2dist: 1.795\n",
      "    step: 400; loss: 3.475; l2dist: 1.794\n",
      "    step: 450; loss: 3.459; l2dist: 1.789\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.881; l2dist: 0.000\n",
      "    step: 50; loss: 11.191; l2dist: 2.859\n",
      "    step: 100; loss: 5.155; l2dist: 2.121\n",
      "    step: 150; loss: 4.016; l2dist: 1.886\n",
      "    step: 200; loss: 3.688; l2dist: 1.825\n",
      "    step: 250; loss: 3.571; l2dist: 1.797\n",
      "    step: 300; loss: 3.524; l2dist: 1.798\n",
      "    step: 350; loss: 3.503; l2dist: 1.781\n",
      "    step: 400; loss: 3.470; l2dist: 1.786\n",
      "    step: 450; loss: 3.470; l2dist: 1.785\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.153; l2dist: 0.000\n",
      "    step: 50; loss: 11.082; l2dist: 2.846\n",
      "    step: 100; loss: 5.103; l2dist: 2.113\n",
      "    step: 150; loss: 3.964; l2dist: 1.894\n",
      "    step: 200; loss: 3.669; l2dist: 1.832\n",
      "    step: 250; loss: 3.561; l2dist: 1.801\n",
      "    step: 300; loss: 3.511; l2dist: 1.794\n",
      "    step: 350; loss: 3.483; l2dist: 1.795\n",
      "    step: 400; loss: 3.473; l2dist: 1.790\n",
      "    step: 450; loss: 3.450; l2dist: 1.785\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.705; l2dist: 0.000\n",
      "    step: 50; loss: 11.058; l2dist: 2.846\n",
      "    step: 100; loss: 5.108; l2dist: 2.122\n",
      "    step: 150; loss: 3.973; l2dist: 1.899\n",
      "    step: 200; loss: 3.666; l2dist: 1.837\n",
      "    step: 250; loss: 3.573; l2dist: 1.820\n",
      "    step: 300; loss: 3.513; l2dist: 1.800\n",
      "    step: 350; loss: 3.474; l2dist: 1.787\n",
      "    step: 400; loss: 3.469; l2dist: 1.789\n",
      "    step: 450; loss: 3.462; l2dist: 1.792\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 34.917; l2dist: 0.000\n",
      "    step: 50; loss: 11.011; l2dist: 2.837\n",
      "    step: 100; loss: 5.107; l2dist: 2.115\n",
      "    step: 150; loss: 3.956; l2dist: 1.903\n",
      "    step: 200; loss: 3.658; l2dist: 1.832\n",
      "    step: 250; loss: 3.562; l2dist: 1.813\n",
      "    step: 300; loss: 3.512; l2dist: 1.805\n",
      "    step: 350; loss: 3.469; l2dist: 1.790\n",
      "    step: 400; loss: 3.472; l2dist: 1.794\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.199; l2dist: 0.000\n",
      "    step: 50; loss: 11.060; l2dist: 2.848\n",
      "    step: 100; loss: 5.115; l2dist: 2.129\n",
      "    step: 150; loss: 3.969; l2dist: 1.903\n",
      "    step: 200; loss: 3.678; l2dist: 1.833\n",
      "    step: 250; loss: 3.573; l2dist: 1.810\n",
      "    step: 300; loss: 3.517; l2dist: 1.804\n",
      "    step: 350; loss: 3.466; l2dist: 1.797\n",
      "    step: 400; loss: 3.465; l2dist: 1.789\n",
      "    step: 450; loss: 3.511; l2dist: 1.792\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 172.979; l2dist: 0.000\n",
      "    step: 50; loss: 22.307; l2dist: 4.240\n",
      "    step: 100; loss: 9.540; l2dist: 3.050\n",
      "    step: 150; loss: 6.505; l2dist: 2.501\n",
      "    step: 200; loss: 5.166; l2dist: 2.221\n",
      "    step: 250; loss: 4.500; l2dist: 2.077\n",
      "    step: 300; loss: 4.251; l2dist: 2.008\n",
      "    step: 350; loss: 3.998; l2dist: 1.950\n",
      "    step: 400; loss: 3.877; l2dist: 1.916\n",
      "    step: 450; loss: 3.838; l2dist: 1.902\n",
      "binary step: 0; number of successful adv: 92/100\n",
      "    step: 0; loss: 220.639; l2dist: 0.000\n",
      "    step: 50; loss: 22.299; l2dist: 4.203\n",
      "    step: 100; loss: 10.446; l2dist: 3.145\n",
      "    step: 150; loss: 7.176; l2dist: 2.582\n",
      "    step: 200; loss: 5.770; l2dist: 2.295\n",
      "    step: 250; loss: 5.041; l2dist: 2.152\n",
      "    step: 300; loss: 4.611; l2dist: 2.062\n",
      "    step: 350; loss: 4.340; l2dist: 2.004\n",
      "    step: 400; loss: 4.192; l2dist: 1.966\n",
      "    step: 450; loss: 4.022; l2dist: 1.948\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 123.330; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 17.020; l2dist: 3.669\n",
      "    step: 100; loss: 7.459; l2dist: 2.638\n",
      "    step: 150; loss: 5.391; l2dist: 2.236\n",
      "    step: 200; loss: 4.597; l2dist: 2.065\n",
      "    step: 250; loss: 4.230; l2dist: 1.984\n",
      "    step: 300; loss: 3.969; l2dist: 1.926\n",
      "    step: 350; loss: 3.861; l2dist: 1.900\n",
      "    step: 400; loss: 3.769; l2dist: 1.883\n",
      "    step: 450; loss: 3.730; l2dist: 1.874\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 78.126; l2dist: 0.000\n",
      "    step: 50; loss: 14.096; l2dist: 3.281\n",
      "    step: 100; loss: 6.221; l2dist: 2.370\n",
      "    step: 150; loss: 4.664; l2dist: 2.066\n",
      "    step: 200; loss: 4.104; l2dist: 1.942\n",
      "    step: 250; loss: 3.878; l2dist: 1.893\n",
      "    step: 300; loss: 3.759; l2dist: 1.861\n",
      "    step: 350; loss: 3.676; l2dist: 1.842\n",
      "    step: 400; loss: 3.619; l2dist: 1.835\n",
      "    step: 450; loss: 3.594; l2dist: 1.832\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 53.689; l2dist: 0.000\n",
      "    step: 50; loss: 12.153; l2dist: 2.960\n",
      "    step: 100; loss: 5.553; l2dist: 2.190\n",
      "    step: 150; loss: 4.246; l2dist: 1.938\n",
      "    step: 200; loss: 3.894; l2dist: 1.863\n",
      "    step: 250; loss: 3.706; l2dist: 1.823\n",
      "    step: 300; loss: 3.584; l2dist: 1.803\n",
      "    step: 350; loss: 3.540; l2dist: 1.796\n",
      "    step: 400; loss: 3.520; l2dist: 1.780\n",
      "    step: 450; loss: 3.479; l2dist: 1.779\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.850; l2dist: 0.000\n",
      "    step: 50; loss: 11.661; l2dist: 2.873\n",
      "    step: 100; loss: 5.304; l2dist: 2.144\n",
      "    step: 150; loss: 4.095; l2dist: 1.898\n",
      "    step: 200; loss: 3.749; l2dist: 1.827\n",
      "    step: 250; loss: 3.609; l2dist: 1.800\n",
      "    step: 300; loss: 3.520; l2dist: 1.778\n",
      "    step: 350; loss: 3.479; l2dist: 1.765\n",
      "    step: 400; loss: 3.484; l2dist: 1.773\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.846; l2dist: 0.000\n",
      "    step: 50; loss: 11.507; l2dist: 2.837\n",
      "    step: 100; loss: 5.282; l2dist: 2.135\n",
      "    step: 150; loss: 4.062; l2dist: 1.905\n",
      "    step: 200; loss: 3.757; l2dist: 1.834\n",
      "    step: 250; loss: 3.631; l2dist: 1.812\n",
      "    step: 300; loss: 3.568; l2dist: 1.800\n",
      "    step: 350; loss: 3.499; l2dist: 1.783\n",
      "    step: 400; loss: 3.476; l2dist: 1.779\n",
      "    step: 450; loss: 3.464; l2dist: 1.773\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 38.690; l2dist: 0.000\n",
      "    step: 50; loss: 11.385; l2dist: 2.811\n",
      "    step: 100; loss: 5.227; l2dist: 2.133\n",
      "    step: 150; loss: 4.051; l2dist: 1.901\n",
      "    step: 200; loss: 3.744; l2dist: 1.840\n",
      "    step: 250; loss: 3.591; l2dist: 1.812\n",
      "    step: 300; loss: 3.525; l2dist: 1.797\n",
      "    step: 350; loss: 3.475; l2dist: 1.783\n",
      "    step: 400; loss: 3.481; l2dist: 1.784\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.567; l2dist: 0.000\n",
      "    step: 50; loss: 11.330; l2dist: 2.802\n",
      "    step: 100; loss: 5.202; l2dist: 2.127\n",
      "    step: 150; loss: 4.021; l2dist: 1.899\n",
      "    step: 200; loss: 3.720; l2dist: 1.831\n",
      "    step: 250; loss: 3.573; l2dist: 1.804\n",
      "    step: 300; loss: 3.512; l2dist: 1.787\n",
      "    step: 350; loss: 3.480; l2dist: 1.786\n",
      "    step: 400; loss: 3.449; l2dist: 1.780\n",
      "    step: 450; loss: 3.447; l2dist: 1.778\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 37.865; l2dist: 0.000\n",
      "    step: 50; loss: 11.383; l2dist: 2.811\n",
      "    step: 100; loss: 5.212; l2dist: 2.134\n",
      "    step: 150; loss: 4.042; l2dist: 1.903\n",
      "    step: 200; loss: 3.729; l2dist: 1.836\n",
      "    step: 250; loss: 3.615; l2dist: 1.817\n",
      "    step: 300; loss: 3.538; l2dist: 1.812\n",
      "    step: 350; loss: 3.502; l2dist: 1.790\n",
      "    step: 400; loss: 3.477; l2dist: 1.797\n",
      "    step: 450; loss: 3.452; l2dist: 1.790\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 142.156; l2dist: 0.000\n",
      "    step: 50; loss: 19.115; l2dist: 3.742\n",
      "    step: 100; loss: 7.416; l2dist: 2.630\n",
      "    step: 150; loss: 5.079; l2dist: 2.155\n",
      "    step: 200; loss: 4.188; l2dist: 1.954\n",
      "    step: 250; loss: 3.714; l2dist: 1.833\n",
      "    step: 300; loss: 3.494; l2dist: 1.775\n",
      "    step: 350; loss: 3.495; l2dist: 1.773\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 146.769; l2dist: 0.000\n",
      "    step: 50; loss: 17.684; l2dist: 3.572\n",
      "    step: 100; loss: 7.217; l2dist: 2.536\n",
      "    step: 150; loss: 5.000; l2dist: 2.092\n",
      "    step: 200; loss: 4.168; l2dist: 1.909\n",
      "    step: 250; loss: 3.737; l2dist: 1.816\n",
      "    step: 300; loss: 3.567; l2dist: 1.770\n",
      "    step: 350; loss: 3.465; l2dist: 1.742\n",
      "    step: 400; loss: 3.362; l2dist: 1.729\n",
      "    step: 450; loss: 3.296; l2dist: 1.710\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 85.880; l2dist: 0.000\n",
      "    step: 50; loss: 13.357; l2dist: 3.081\n",
      "    step: 100; loss: 5.519; l2dist: 2.201\n",
      "    step: 150; loss: 4.083; l2dist: 1.900\n",
      "    step: 200; loss: 3.538; l2dist: 1.776\n",
      "    step: 250; loss: 3.332; l2dist: 1.731\n",
      "    step: 300; loss: 3.248; l2dist: 1.699\n",
      "    step: 350; loss: 3.135; l2dist: 1.678\n",
      "    step: 400; loss: 3.142; l2dist: 1.671\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 58.230; l2dist: 0.000\n",
      "    step: 50; loss: 11.213; l2dist: 2.764\n",
      "    step: 100; loss: 4.941; l2dist: 2.074\n",
      "    step: 150; loss: 3.751; l2dist: 1.806\n",
      "    step: 200; loss: 3.385; l2dist: 1.733\n",
      "    step: 250; loss: 3.216; l2dist: 1.693\n",
      "    step: 300; loss: 3.110; l2dist: 1.670\n",
      "    step: 350; loss: 3.070; l2dist: 1.651\n",
      "    step: 400; loss: 3.052; l2dist: 1.651\n",
      "    step: 450; loss: 3.054; l2dist: 1.648\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 47.718; l2dist: 0.000\n",
      "    step: 50; loss: 10.277; l2dist: 2.625\n",
      "    step: 100; loss: 4.815; l2dist: 2.012\n",
      "    step: 150; loss: 3.650; l2dist: 1.780\n",
      "    step: 200; loss: 3.337; l2dist: 1.706\n",
      "    step: 250; loss: 3.199; l2dist: 1.669\n",
      "    step: 300; loss: 3.116; l2dist: 1.655\n",
      "    step: 350; loss: 3.093; l2dist: 1.642\n",
      "    step: 400; loss: 3.047; l2dist: 1.640\n",
      "    step: 450; loss: 3.047; l2dist: 1.634\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.995; l2dist: 0.000\n",
      "    step: 50; loss: 10.040; l2dist: 2.590\n",
      "    step: 100; loss: 4.734; l2dist: 1.967\n",
      "    step: 150; loss: 3.610; l2dist: 1.752\n",
      "    step: 200; loss: 3.298; l2dist: 1.679\n",
      "    step: 250; loss: 3.154; l2dist: 1.647\n",
      "    step: 300; loss: 3.062; l2dist: 1.629\n",
      "    step: 350; loss: 3.039; l2dist: 1.622\n",
      "    step: 400; loss: 3.021; l2dist: 1.613\n",
      "    step: 450; loss: 3.028; l2dist: 1.618\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.937; l2dist: 0.000\n",
      "    step: 50; loss: 9.850; l2dist: 2.556\n",
      "    step: 100; loss: 4.689; l2dist: 1.959\n",
      "    step: 150; loss: 3.568; l2dist: 1.744\n",
      "    step: 200; loss: 3.290; l2dist: 1.674\n",
      "    step: 250; loss: 3.125; l2dist: 1.643\n",
      "    step: 300; loss: 3.080; l2dist: 1.638\n",
      "    step: 350; loss: 3.020; l2dist: 1.618\n",
      "    step: 400; loss: 3.014; l2dist: 1.621\n",
      "    step: 450; loss: 3.025; l2dist: 1.621\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.370; l2dist: 0.000\n",
      "    step: 50; loss: 9.728; l2dist: 2.538\n",
      "    step: 100; loss: 4.674; l2dist: 1.959\n",
      "    step: 150; loss: 3.564; l2dist: 1.742\n",
      "    step: 200; loss: 3.295; l2dist: 1.683\n",
      "    step: 250; loss: 3.136; l2dist: 1.649\n",
      "    step: 300; loss: 3.052; l2dist: 1.627\n",
      "    step: 350; loss: 3.035; l2dist: 1.628\n",
      "    step: 400; loss: 3.025; l2dist: 1.630\n",
      "    step: 450; loss: 2.989; l2dist: 1.614\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.731; l2dist: 0.000\n",
      "    step: 50; loss: 9.683; l2dist: 2.531\n",
      "    step: 100; loss: 4.656; l2dist: 1.960\n",
      "    step: 150; loss: 3.565; l2dist: 1.740\n",
      "    step: 200; loss: 3.259; l2dist: 1.678\n",
      "    step: 250; loss: 3.149; l2dist: 1.659\n",
      "    step: 300; loss: 3.089; l2dist: 1.645\n",
      "    step: 350; loss: 3.041; l2dist: 1.625\n",
      "    step: 400; loss: 3.016; l2dist: 1.618\n",
      "    step: 450; loss: 2.988; l2dist: 1.619\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 39.893; l2dist: 0.000\n",
      "    step: 50; loss: 9.713; l2dist: 2.540\n",
      "    step: 100; loss: 4.658; l2dist: 1.963\n",
      "    step: 150; loss: 3.576; l2dist: 1.751\n",
      "    step: 200; loss: 3.255; l2dist: 1.681\n",
      "    step: 250; loss: 3.136; l2dist: 1.651\n",
      "    step: 300; loss: 3.072; l2dist: 1.630\n",
      "    step: 350; loss: 3.022; l2dist: 1.628\n",
      "    step: 400; loss: 3.036; l2dist: 1.625\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 158.645; l2dist: 0.000\n",
      "    step: 50; loss: 18.207; l2dist: 3.912\n",
      "    step: 100; loss: 7.780; l2dist: 2.730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 150; loss: 5.312; l2dist: 2.229\n",
      "    step: 200; loss: 4.387; l2dist: 2.013\n",
      "    step: 250; loss: 3.887; l2dist: 1.889\n",
      "    step: 300; loss: 3.704; l2dist: 1.842\n",
      "    step: 350; loss: 3.638; l2dist: 1.813\n",
      "    step: 400; loss: 3.576; l2dist: 1.800\n",
      "    step: 450; loss: 3.489; l2dist: 1.785\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 157.259; l2dist: 0.000\n",
      "    step: 50; loss: 16.064; l2dist: 3.670\n",
      "    step: 100; loss: 7.085; l2dist: 2.542\n",
      "    step: 150; loss: 5.025; l2dist: 2.123\n",
      "    step: 200; loss: 4.192; l2dist: 1.935\n",
      "    step: 250; loss: 3.810; l2dist: 1.851\n",
      "    step: 300; loss: 3.585; l2dist: 1.798\n",
      "    step: 350; loss: 3.500; l2dist: 1.782\n",
      "    step: 400; loss: 3.476; l2dist: 1.769\n",
      "    step: 450; loss: 3.359; l2dist: 1.739\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 88.131; l2dist: 0.000\n",
      "    step: 50; loss: 12.431; l2dist: 3.189\n",
      "    step: 100; loss: 5.529; l2dist: 2.229\n",
      "    step: 150; loss: 4.172; l2dist: 1.924\n",
      "    step: 200; loss: 3.698; l2dist: 1.811\n",
      "    step: 250; loss: 3.460; l2dist: 1.768\n",
      "    step: 300; loss: 3.372; l2dist: 1.744\n",
      "    step: 350; loss: 3.308; l2dist: 1.728\n",
      "    step: 400; loss: 3.280; l2dist: 1.722\n",
      "    step: 450; loss: 3.263; l2dist: 1.727\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 56.911; l2dist: 0.000\n",
      "    step: 50; loss: 10.552; l2dist: 2.803\n",
      "    step: 100; loss: 4.930; l2dist: 2.090\n",
      "    step: 150; loss: 3.792; l2dist: 1.840\n",
      "    step: 200; loss: 3.464; l2dist: 1.767\n",
      "    step: 250; loss: 3.306; l2dist: 1.729\n",
      "    step: 300; loss: 3.251; l2dist: 1.715\n",
      "    step: 350; loss: 3.214; l2dist: 1.700\n",
      "    step: 400; loss: 3.209; l2dist: 1.698\n",
      "    step: 450; loss: 3.189; l2dist: 1.691\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 41.842; l2dist: 0.000\n",
      "    step: 50; loss: 9.574; l2dist: 2.653\n",
      "    step: 100; loss: 4.750; l2dist: 2.030\n",
      "    step: 150; loss: 3.658; l2dist: 1.810\n",
      "    step: 200; loss: 3.399; l2dist: 1.736\n",
      "    step: 250; loss: 3.284; l2dist: 1.705\n",
      "    step: 300; loss: 3.229; l2dist: 1.701\n",
      "    step: 350; loss: 3.202; l2dist: 1.696\n",
      "    step: 400; loss: 3.233; l2dist: 1.698\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.605; l2dist: 0.000\n",
      "    step: 50; loss: 9.240; l2dist: 2.557\n",
      "    step: 100; loss: 4.652; l2dist: 1.967\n",
      "    step: 150; loss: 3.643; l2dist: 1.770\n",
      "    step: 200; loss: 3.377; l2dist: 1.720\n",
      "    step: 250; loss: 3.269; l2dist: 1.689\n",
      "    step: 300; loss: 3.252; l2dist: 1.687\n",
      "    step: 350; loss: 3.213; l2dist: 1.682\n",
      "    step: 400; loss: 3.187; l2dist: 1.681\n",
      "    step: 450; loss: 3.168; l2dist: 1.675\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.745; l2dist: 0.000\n",
      "    step: 50; loss: 9.170; l2dist: 2.544\n",
      "    step: 100; loss: 4.617; l2dist: 1.986\n",
      "    step: 150; loss: 3.607; l2dist: 1.785\n",
      "    step: 200; loss: 3.353; l2dist: 1.734\n",
      "    step: 250; loss: 3.272; l2dist: 1.710\n",
      "    step: 300; loss: 3.225; l2dist: 1.702\n",
      "    step: 350; loss: 3.217; l2dist: 1.696\n",
      "    step: 400; loss: 3.196; l2dist: 1.685\n",
      "    step: 450; loss: 3.195; l2dist: 1.686\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.031; l2dist: 0.000\n",
      "    step: 50; loss: 9.059; l2dist: 2.522\n",
      "    step: 100; loss: 4.608; l2dist: 1.976\n",
      "    step: 150; loss: 3.605; l2dist: 1.778\n",
      "    step: 200; loss: 3.369; l2dist: 1.724\n",
      "    step: 250; loss: 3.265; l2dist: 1.707\n",
      "    step: 300; loss: 3.259; l2dist: 1.697\n",
      "    step: 350; loss: 3.203; l2dist: 1.692\n",
      "    step: 400; loss: 3.182; l2dist: 1.694\n",
      "    step: 450; loss: 3.168; l2dist: 1.684\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.441; l2dist: 0.000\n",
      "    step: 50; loss: 9.014; l2dist: 2.513\n",
      "    step: 100; loss: 4.613; l2dist: 1.982\n",
      "    step: 150; loss: 3.608; l2dist: 1.792\n",
      "    step: 200; loss: 3.354; l2dist: 1.725\n",
      "    step: 250; loss: 3.264; l2dist: 1.705\n",
      "    step: 300; loss: 3.246; l2dist: 1.691\n",
      "    step: 350; loss: 3.207; l2dist: 1.693\n",
      "    step: 400; loss: 3.189; l2dist: 1.690\n",
      "    step: 450; loss: 3.182; l2dist: 1.690\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.753; l2dist: 0.000\n",
      "    step: 50; loss: 9.054; l2dist: 2.522\n",
      "    step: 100; loss: 4.621; l2dist: 1.987\n",
      "    step: 150; loss: 3.610; l2dist: 1.776\n",
      "    step: 200; loss: 3.369; l2dist: 1.731\n",
      "    step: 250; loss: 3.267; l2dist: 1.714\n",
      "    step: 300; loss: 3.228; l2dist: 1.697\n",
      "    step: 350; loss: 3.206; l2dist: 1.698\n",
      "    step: 400; loss: 3.193; l2dist: 1.697\n",
      "    step: 450; loss: 3.184; l2dist: 1.690\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 151.942; l2dist: 0.000\n",
      "    step: 50; loss: 15.919; l2dist: 3.843\n",
      "    step: 100; loss: 7.708; l2dist: 2.704\n",
      "    step: 150; loss: 5.184; l2dist: 2.199\n",
      "    step: 200; loss: 4.211; l2dist: 1.969\n",
      "    step: 250; loss: 3.772; l2dist: 1.856\n",
      "    step: 300; loss: 3.586; l2dist: 1.800\n",
      "    step: 350; loss: 3.494; l2dist: 1.779\n",
      "    step: 400; loss: 3.437; l2dist: 1.756\n",
      "    step: 450; loss: 3.430; l2dist: 1.758\n",
      "binary step: 0; number of successful adv: 93/100\n",
      "    step: 0; loss: 190.806; l2dist: 0.000\n",
      "    step: 50; loss: 16.613; l2dist: 3.893\n",
      "    step: 100; loss: 8.145; l2dist: 2.743\n",
      "    step: 150; loss: 5.503; l2dist: 2.224\n",
      "    step: 200; loss: 4.425; l2dist: 1.990\n",
      "    step: 250; loss: 3.863; l2dist: 1.850\n",
      "    step: 300; loss: 3.578; l2dist: 1.795\n",
      "    step: 350; loss: 3.449; l2dist: 1.751\n",
      "    step: 400; loss: 3.369; l2dist: 1.739\n",
      "    step: 450; loss: 3.278; l2dist: 1.718\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 105.204; l2dist: 0.000\n",
      "    step: 50; loss: 12.519; l2dist: 3.337\n",
      "    step: 100; loss: 5.698; l2dist: 2.277\n",
      "    step: 150; loss: 4.165; l2dist: 1.930\n",
      "    step: 200; loss: 3.597; l2dist: 1.795\n",
      "    step: 250; loss: 3.392; l2dist: 1.746\n",
      "    step: 300; loss: 3.253; l2dist: 1.707\n",
      "    step: 350; loss: 3.152; l2dist: 1.690\n",
      "    step: 400; loss: 3.130; l2dist: 1.678\n",
      "    step: 450; loss: 3.103; l2dist: 1.677\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 64.008; l2dist: 0.000\n",
      "    step: 50; loss: 10.164; l2dist: 2.876\n",
      "    step: 100; loss: 4.918; l2dist: 2.073\n",
      "    step: 150; loss: 3.708; l2dist: 1.806\n",
      "    step: 200; loss: 3.336; l2dist: 1.716\n",
      "    step: 250; loss: 3.201; l2dist: 1.688\n",
      "    step: 300; loss: 3.162; l2dist: 1.678\n",
      "    step: 350; loss: 3.109; l2dist: 1.654\n",
      "    step: 400; loss: 3.066; l2dist: 1.646\n",
      "    step: 450; loss: 3.054; l2dist: 1.641\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 44.730; l2dist: 0.000\n",
      "    step: 50; loss: 8.959; l2dist: 2.629\n",
      "    step: 100; loss: 4.616; l2dist: 1.974\n",
      "    step: 150; loss: 3.567; l2dist: 1.752\n",
      "    step: 200; loss: 3.279; l2dist: 1.690\n",
      "    step: 250; loss: 3.132; l2dist: 1.654\n",
      "    step: 300; loss: 3.070; l2dist: 1.641\n",
      "    step: 350; loss: 3.036; l2dist: 1.635\n",
      "    step: 400; loss: 3.013; l2dist: 1.633\n",
      "    step: 450; loss: 3.016; l2dist: 1.633\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.392; l2dist: 0.000\n",
      "    step: 50; loss: 8.457; l2dist: 2.518\n",
      "    step: 100; loss: 4.445; l2dist: 1.912\n",
      "    step: 150; loss: 3.425; l2dist: 1.705\n",
      "    step: 200; loss: 3.192; l2dist: 1.643\n",
      "    step: 250; loss: 3.071; l2dist: 1.622\n",
      "    step: 300; loss: 3.010; l2dist: 1.609\n",
      "    step: 350; loss: 2.970; l2dist: 1.603\n",
      "    step: 400; loss: 2.973; l2dist: 1.602\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.345; l2dist: 0.000\n",
      "    step: 50; loss: 8.374; l2dist: 2.507\n",
      "    step: 100; loss: 4.421; l2dist: 1.907\n",
      "    step: 150; loss: 3.414; l2dist: 1.708\n",
      "    step: 200; loss: 3.164; l2dist: 1.656\n",
      "    step: 250; loss: 3.072; l2dist: 1.635\n",
      "    step: 300; loss: 3.030; l2dist: 1.626\n",
      "    step: 350; loss: 2.981; l2dist: 1.622\n",
      "    step: 400; loss: 2.958; l2dist: 1.613\n",
      "    step: 450; loss: 2.956; l2dist: 1.610\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.403; l2dist: 0.000\n",
      "    step: 50; loss: 8.332; l2dist: 2.508\n",
      "    step: 100; loss: 4.410; l2dist: 1.908\n",
      "    step: 150; loss: 3.416; l2dist: 1.705\n",
      "    step: 200; loss: 3.168; l2dist: 1.653\n",
      "    step: 250; loss: 3.081; l2dist: 1.633\n",
      "    step: 300; loss: 3.026; l2dist: 1.623\n",
      "    step: 350; loss: 2.988; l2dist: 1.618\n",
      "    step: 400; loss: 2.980; l2dist: 1.608\n",
      "    step: 450; loss: 2.966; l2dist: 1.612\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.429; l2dist: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 50; loss: 8.300; l2dist: 2.501\n",
      "    step: 100; loss: 4.396; l2dist: 1.905\n",
      "    step: 150; loss: 3.393; l2dist: 1.711\n",
      "    step: 200; loss: 3.166; l2dist: 1.660\n",
      "    step: 250; loss: 3.075; l2dist: 1.633\n",
      "    step: 300; loss: 3.015; l2dist: 1.622\n",
      "    step: 350; loss: 2.989; l2dist: 1.620\n",
      "    step: 400; loss: 2.961; l2dist: 1.613\n",
      "    step: 450; loss: 2.954; l2dist: 1.615\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 31.762; l2dist: 0.000\n",
      "    step: 50; loss: 8.338; l2dist: 2.512\n",
      "    step: 100; loss: 4.410; l2dist: 1.917\n",
      "    step: 150; loss: 3.403; l2dist: 1.712\n",
      "    step: 200; loss: 3.168; l2dist: 1.658\n",
      "    step: 250; loss: 3.076; l2dist: 1.637\n",
      "    step: 300; loss: 3.011; l2dist: 1.626\n",
      "    step: 350; loss: 2.988; l2dist: 1.625\n",
      "    step: 400; loss: 2.975; l2dist: 1.615\n",
      "    step: 450; loss: 2.955; l2dist: 1.618\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 141.499; l2dist: 0.000\n",
      "    step: 50; loss: 17.050; l2dist: 3.785\n",
      "    step: 100; loss: 7.109; l2dist: 2.602\n",
      "    step: 150; loss: 4.721; l2dist: 2.087\n",
      "    step: 200; loss: 3.632; l2dist: 1.830\n",
      "    step: 250; loss: 3.190; l2dist: 1.689\n",
      "    step: 300; loss: 2.953; l2dist: 1.639\n",
      "    step: 350; loss: 2.755; l2dist: 1.575\n",
      "    step: 400; loss: 2.662; l2dist: 1.549\n",
      "    step: 450; loss: 2.626; l2dist: 1.547\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 140.995; l2dist: 0.000\n",
      "    step: 50; loss: 15.709; l2dist: 3.589\n",
      "    step: 100; loss: 6.702; l2dist: 2.456\n",
      "    step: 150; loss: 4.507; l2dist: 1.988\n",
      "    step: 200; loss: 3.595; l2dist: 1.773\n",
      "    step: 250; loss: 3.144; l2dist: 1.663\n",
      "    step: 300; loss: 2.890; l2dist: 1.600\n",
      "    step: 350; loss: 2.730; l2dist: 1.552\n",
      "    step: 400; loss: 2.652; l2dist: 1.533\n",
      "    step: 450; loss: 2.611; l2dist: 1.526\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 82.179; l2dist: 0.000\n",
      "    step: 50; loss: 12.394; l2dist: 3.151\n",
      "    step: 100; loss: 5.034; l2dist: 2.112\n",
      "    step: 150; loss: 3.518; l2dist: 1.759\n",
      "    step: 200; loss: 2.966; l2dist: 1.615\n",
      "    step: 250; loss: 2.708; l2dist: 1.553\n",
      "    step: 300; loss: 2.612; l2dist: 1.532\n",
      "    step: 350; loss: 2.497; l2dist: 1.498\n",
      "    step: 400; loss: 2.499; l2dist: 1.492\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.545; l2dist: 0.000\n",
      "    step: 50; loss: 10.373; l2dist: 2.777\n",
      "    step: 100; loss: 4.241; l2dist: 1.930\n",
      "    step: 150; loss: 3.043; l2dist: 1.648\n",
      "    step: 200; loss: 2.696; l2dist: 1.552\n",
      "    step: 250; loss: 2.542; l2dist: 1.504\n",
      "    step: 300; loss: 2.488; l2dist: 1.488\n",
      "    step: 350; loss: 2.443; l2dist: 1.477\n",
      "    step: 400; loss: 2.428; l2dist: 1.464\n",
      "    step: 450; loss: 2.392; l2dist: 1.464\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.704; l2dist: 0.000\n",
      "    step: 50; loss: 9.195; l2dist: 2.503\n",
      "    step: 100; loss: 3.906; l2dist: 1.807\n",
      "    step: 150; loss: 2.890; l2dist: 1.586\n",
      "    step: 200; loss: 2.596; l2dist: 1.513\n",
      "    step: 250; loss: 2.488; l2dist: 1.484\n",
      "    step: 300; loss: 2.441; l2dist: 1.463\n",
      "    step: 350; loss: 2.438; l2dist: 1.465\n",
      "    step: 400; loss: 2.390; l2dist: 1.452\n",
      "    step: 450; loss: 2.376; l2dist: 1.446\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.988; l2dist: 0.000\n",
      "    step: 50; loss: 8.751; l2dist: 2.402\n",
      "    step: 100; loss: 3.757; l2dist: 1.734\n",
      "    step: 150; loss: 2.805; l2dist: 1.526\n",
      "    step: 200; loss: 2.544; l2dist: 1.460\n",
      "    step: 250; loss: 2.455; l2dist: 1.446\n",
      "    step: 300; loss: 2.404; l2dist: 1.432\n",
      "    step: 350; loss: 2.377; l2dist: 1.424\n",
      "    step: 400; loss: 2.371; l2dist: 1.426\n",
      "    step: 450; loss: 2.349; l2dist: 1.418\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.900; l2dist: 0.000\n",
      "    step: 50; loss: 8.696; l2dist: 2.402\n",
      "    step: 100; loss: 3.784; l2dist: 1.764\n",
      "    step: 150; loss: 2.800; l2dist: 1.550\n",
      "    step: 200; loss: 2.569; l2dist: 1.488\n",
      "    step: 250; loss: 2.481; l2dist: 1.464\n",
      "    step: 300; loss: 2.414; l2dist: 1.451\n",
      "    step: 350; loss: 2.401; l2dist: 1.442\n",
      "    step: 400; loss: 2.378; l2dist: 1.440\n",
      "    step: 450; loss: 2.368; l2dist: 1.440\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.185; l2dist: 0.000\n",
      "    step: 50; loss: 8.682; l2dist: 2.410\n",
      "    step: 100; loss: 3.773; l2dist: 1.763\n",
      "    step: 150; loss: 2.793; l2dist: 1.545\n",
      "    step: 200; loss: 2.561; l2dist: 1.485\n",
      "    step: 250; loss: 2.465; l2dist: 1.464\n",
      "    step: 300; loss: 2.425; l2dist: 1.450\n",
      "    step: 350; loss: 2.411; l2dist: 1.443\n",
      "    step: 400; loss: 2.380; l2dist: 1.440\n",
      "    step: 450; loss: 2.373; l2dist: 1.437\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.083; l2dist: 0.000\n",
      "    step: 50; loss: 8.661; l2dist: 2.404\n",
      "    step: 100; loss: 3.765; l2dist: 1.762\n",
      "    step: 150; loss: 2.799; l2dist: 1.548\n",
      "    step: 200; loss: 2.548; l2dist: 1.485\n",
      "    step: 250; loss: 2.454; l2dist: 1.466\n",
      "    step: 300; loss: 2.414; l2dist: 1.448\n",
      "    step: 350; loss: 2.396; l2dist: 1.447\n",
      "    step: 400; loss: 2.377; l2dist: 1.442\n",
      "    step: 450; loss: 2.395; l2dist: 1.443\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.348; l2dist: 0.000\n",
      "    step: 50; loss: 8.712; l2dist: 2.417\n",
      "    step: 100; loss: 3.770; l2dist: 1.770\n",
      "    step: 150; loss: 2.802; l2dist: 1.556\n",
      "    step: 200; loss: 2.553; l2dist: 1.492\n",
      "    step: 250; loss: 2.482; l2dist: 1.478\n",
      "    step: 300; loss: 2.420; l2dist: 1.458\n",
      "    step: 350; loss: 2.408; l2dist: 1.451\n",
      "    step: 400; loss: 2.400; l2dist: 1.440\n",
      "    step: 450; loss: 2.388; l2dist: 1.449\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 133.354; l2dist: 0.000\n",
      "    step: 50; loss: 15.070; l2dist: 3.592\n",
      "    step: 100; loss: 6.263; l2dist: 2.424\n",
      "    step: 150; loss: 4.134; l2dist: 1.952\n",
      "    step: 200; loss: 3.319; l2dist: 1.744\n",
      "    step: 250; loss: 2.974; l2dist: 1.636\n",
      "    step: 300; loss: 2.852; l2dist: 1.584\n",
      "    step: 350; loss: 2.739; l2dist: 1.576\n",
      "    step: 400; loss: 2.622; l2dist: 1.536\n",
      "    step: 450; loss: 2.639; l2dist: 1.545\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 111.433; l2dist: 0.000\n",
      "    step: 50; loss: 12.676; l2dist: 3.268\n",
      "    step: 100; loss: 5.174; l2dist: 2.169\n",
      "    step: 150; loss: 3.596; l2dist: 1.798\n",
      "    step: 200; loss: 3.078; l2dist: 1.646\n",
      "    step: 250; loss: 2.801; l2dist: 1.585\n",
      "    step: 300; loss: 2.684; l2dist: 1.553\n",
      "    step: 350; loss: 2.688; l2dist: 1.549\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 62.991; l2dist: 0.000\n",
      "    step: 50; loss: 10.083; l2dist: 2.846\n",
      "    step: 100; loss: 4.128; l2dist: 1.932\n",
      "    step: 150; loss: 3.063; l2dist: 1.657\n",
      "    step: 200; loss: 2.730; l2dist: 1.562\n",
      "    step: 250; loss: 2.587; l2dist: 1.522\n",
      "    step: 300; loss: 2.510; l2dist: 1.501\n",
      "    step: 350; loss: 2.465; l2dist: 1.487\n",
      "    step: 400; loss: 2.448; l2dist: 1.489\n",
      "    step: 450; loss: 2.407; l2dist: 1.481\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.069; l2dist: 0.000\n",
      "    step: 50; loss: 8.973; l2dist: 2.591\n",
      "    step: 100; loss: 3.869; l2dist: 1.833\n",
      "    step: 150; loss: 2.903; l2dist: 1.602\n",
      "    step: 200; loss: 2.631; l2dist: 1.529\n",
      "    step: 250; loss: 2.504; l2dist: 1.491\n",
      "    step: 300; loss: 2.449; l2dist: 1.481\n",
      "    step: 350; loss: 2.409; l2dist: 1.465\n",
      "    step: 400; loss: 2.387; l2dist: 1.463\n",
      "    step: 450; loss: 2.398; l2dist: 1.456\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.541; l2dist: 0.000\n",
      "    step: 50; loss: 8.442; l2dist: 2.439\n",
      "    step: 100; loss: 3.729; l2dist: 1.760\n",
      "    step: 150; loss: 2.824; l2dist: 1.561\n",
      "    step: 200; loss: 2.565; l2dist: 1.500\n",
      "    step: 250; loss: 2.460; l2dist: 1.470\n",
      "    step: 300; loss: 2.428; l2dist: 1.462\n",
      "    step: 350; loss: 2.388; l2dist: 1.453\n",
      "    step: 400; loss: 2.376; l2dist: 1.445\n",
      "    step: 450; loss: 2.353; l2dist: 1.446\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 26.291; l2dist: 0.000\n",
      "    step: 50; loss: 8.101; l2dist: 2.366\n",
      "    step: 100; loss: 3.621; l2dist: 1.720\n",
      "    step: 150; loss: 2.759; l2dist: 1.526\n",
      "    step: 200; loss: 2.506; l2dist: 1.469\n",
      "    step: 250; loss: 2.416; l2dist: 1.449\n",
      "    step: 300; loss: 2.385; l2dist: 1.437\n",
      "    step: 350; loss: 2.353; l2dist: 1.430\n",
      "    step: 400; loss: 2.340; l2dist: 1.425\n",
      "    step: 450; loss: 2.351; l2dist: 1.423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.832; l2dist: 0.000\n",
      "    step: 50; loss: 8.052; l2dist: 2.358\n",
      "    step: 100; loss: 3.607; l2dist: 1.724\n",
      "    step: 150; loss: 2.729; l2dist: 1.522\n",
      "    step: 200; loss: 2.505; l2dist: 1.477\n",
      "    step: 250; loss: 2.408; l2dist: 1.450\n",
      "    step: 300; loss: 2.380; l2dist: 1.441\n",
      "    step: 350; loss: 2.354; l2dist: 1.433\n",
      "    step: 400; loss: 2.358; l2dist: 1.438\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.835; l2dist: 0.000\n",
      "    step: 50; loss: 7.995; l2dist: 2.345\n",
      "    step: 100; loss: 3.608; l2dist: 1.725\n",
      "    step: 150; loss: 2.722; l2dist: 1.522\n",
      "    step: 200; loss: 2.496; l2dist: 1.472\n",
      "    step: 250; loss: 2.416; l2dist: 1.456\n",
      "    step: 300; loss: 2.375; l2dist: 1.445\n",
      "    step: 350; loss: 2.357; l2dist: 1.440\n",
      "    step: 400; loss: 2.341; l2dist: 1.435\n",
      "    step: 450; loss: 2.363; l2dist: 1.434\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.400; l2dist: 0.000\n",
      "    step: 50; loss: 7.984; l2dist: 2.341\n",
      "    step: 100; loss: 3.598; l2dist: 1.720\n",
      "    step: 150; loss: 2.729; l2dist: 1.524\n",
      "    step: 200; loss: 2.501; l2dist: 1.469\n",
      "    step: 250; loss: 2.416; l2dist: 1.453\n",
      "    step: 300; loss: 2.385; l2dist: 1.442\n",
      "    step: 350; loss: 2.362; l2dist: 1.442\n",
      "    step: 400; loss: 2.348; l2dist: 1.433\n",
      "    step: 450; loss: 2.363; l2dist: 1.439\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.605; l2dist: 0.000\n",
      "    step: 50; loss: 8.000; l2dist: 2.350\n",
      "    step: 100; loss: 3.610; l2dist: 1.735\n",
      "    step: 150; loss: 2.725; l2dist: 1.522\n",
      "    step: 200; loss: 2.490; l2dist: 1.474\n",
      "    step: 250; loss: 2.416; l2dist: 1.456\n",
      "    step: 300; loss: 2.376; l2dist: 1.451\n",
      "    step: 350; loss: 2.358; l2dist: 1.433\n",
      "    step: 400; loss: 2.357; l2dist: 1.444\n",
      "    step: 450; loss: 2.338; l2dist: 1.431\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 139.335; l2dist: 0.000\n",
      "    step: 50; loss: 16.597; l2dist: 3.579\n",
      "    step: 100; loss: 6.699; l2dist: 2.500\n",
      "    step: 150; loss: 4.584; l2dist: 2.052\n",
      "    step: 200; loss: 3.725; l2dist: 1.835\n",
      "    step: 250; loss: 3.285; l2dist: 1.715\n",
      "    step: 300; loss: 3.083; l2dist: 1.660\n",
      "    step: 350; loss: 3.003; l2dist: 1.626\n",
      "    step: 400; loss: 2.944; l2dist: 1.612\n",
      "    step: 450; loss: 2.913; l2dist: 1.611\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 156.366; l2dist: 0.000\n",
      "    step: 50; loss: 15.629; l2dist: 3.508\n",
      "    step: 100; loss: 6.637; l2dist: 2.435\n",
      "    step: 150; loss: 4.520; l2dist: 1.991\n",
      "    step: 200; loss: 3.707; l2dist: 1.804\n",
      "    step: 250; loss: 3.297; l2dist: 1.694\n",
      "    step: 300; loss: 3.128; l2dist: 1.655\n",
      "    step: 350; loss: 3.020; l2dist: 1.628\n",
      "    step: 400; loss: 2.895; l2dist: 1.592\n",
      "    step: 450; loss: 2.840; l2dist: 1.581\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 83.427; l2dist: 0.000\n",
      "    step: 50; loss: 11.656; l2dist: 2.986\n",
      "    step: 100; loss: 4.768; l2dist: 2.048\n",
      "    step: 150; loss: 3.511; l2dist: 1.747\n",
      "    step: 200; loss: 3.096; l2dist: 1.646\n",
      "    step: 250; loss: 2.902; l2dist: 1.600\n",
      "    step: 300; loss: 2.810; l2dist: 1.576\n",
      "    step: 350; loss: 2.762; l2dist: 1.557\n",
      "    step: 400; loss: 2.757; l2dist: 1.558\n",
      "    step: 450; loss: 2.708; l2dist: 1.541\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 51.882; l2dist: 0.000\n",
      "    step: 50; loss: 9.699; l2dist: 2.631\n",
      "    step: 100; loss: 4.270; l2dist: 1.920\n",
      "    step: 150; loss: 3.227; l2dist: 1.675\n",
      "    step: 200; loss: 2.949; l2dist: 1.598\n",
      "    step: 250; loss: 2.838; l2dist: 1.569\n",
      "    step: 300; loss: 2.782; l2dist: 1.556\n",
      "    step: 350; loss: 2.760; l2dist: 1.557\n",
      "    step: 400; loss: 2.731; l2dist: 1.546\n",
      "    step: 450; loss: 2.731; l2dist: 1.542\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 36.705; l2dist: 0.000\n",
      "    step: 50; loss: 8.569; l2dist: 2.395\n",
      "    step: 100; loss: 3.980; l2dist: 1.816\n",
      "    step: 150; loss: 3.107; l2dist: 1.618\n",
      "    step: 200; loss: 2.873; l2dist: 1.566\n",
      "    step: 250; loss: 2.792; l2dist: 1.547\n",
      "    step: 300; loss: 2.770; l2dist: 1.545\n",
      "    step: 350; loss: 2.721; l2dist: 1.527\n",
      "    step: 400; loss: 2.728; l2dist: 1.527\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 32.712; l2dist: 0.000\n",
      "    step: 50; loss: 8.267; l2dist: 2.353\n",
      "    step: 100; loss: 3.892; l2dist: 1.792\n",
      "    step: 150; loss: 3.038; l2dist: 1.611\n",
      "    step: 200; loss: 2.822; l2dist: 1.556\n",
      "    step: 250; loss: 2.743; l2dist: 1.538\n",
      "    step: 300; loss: 2.712; l2dist: 1.525\n",
      "    step: 350; loss: 2.693; l2dist: 1.517\n",
      "    step: 400; loss: 2.662; l2dist: 1.518\n",
      "    step: 450; loss: 2.664; l2dist: 1.515\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.603; l2dist: 0.000\n",
      "    step: 50; loss: 8.082; l2dist: 2.313\n",
      "    step: 100; loss: 3.867; l2dist: 1.772\n",
      "    step: 150; loss: 3.021; l2dist: 1.592\n",
      "    step: 200; loss: 2.802; l2dist: 1.543\n",
      "    step: 250; loss: 2.728; l2dist: 1.523\n",
      "    step: 300; loss: 2.698; l2dist: 1.518\n",
      "    step: 350; loss: 2.659; l2dist: 1.508\n",
      "    step: 400; loss: 2.649; l2dist: 1.508\n",
      "    step: 450; loss: 2.631; l2dist: 1.505\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.971; l2dist: 0.000\n",
      "    step: 50; loss: 7.987; l2dist: 2.295\n",
      "    step: 100; loss: 3.839; l2dist: 1.755\n",
      "    step: 150; loss: 2.990; l2dist: 1.588\n",
      "    step: 200; loss: 2.794; l2dist: 1.542\n",
      "    step: 250; loss: 2.714; l2dist: 1.521\n",
      "    step: 300; loss: 2.672; l2dist: 1.514\n",
      "    step: 350; loss: 2.654; l2dist: 1.513\n",
      "    step: 400; loss: 2.637; l2dist: 1.507\n",
      "    step: 450; loss: 2.635; l2dist: 1.509\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.255; l2dist: 0.000\n",
      "    step: 50; loss: 7.961; l2dist: 2.288\n",
      "    step: 100; loss: 3.826; l2dist: 1.767\n",
      "    step: 150; loss: 2.980; l2dist: 1.585\n",
      "    step: 200; loss: 2.786; l2dist: 1.543\n",
      "    step: 250; loss: 2.699; l2dist: 1.522\n",
      "    step: 300; loss: 2.670; l2dist: 1.514\n",
      "    step: 350; loss: 2.648; l2dist: 1.511\n",
      "    step: 400; loss: 2.636; l2dist: 1.507\n",
      "    step: 450; loss: 2.628; l2dist: 1.504\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.423; l2dist: 0.000\n",
      "    step: 50; loss: 7.995; l2dist: 2.298\n",
      "    step: 100; loss: 3.832; l2dist: 1.768\n",
      "    step: 150; loss: 2.997; l2dist: 1.588\n",
      "    step: 200; loss: 2.783; l2dist: 1.541\n",
      "    step: 250; loss: 2.700; l2dist: 1.521\n",
      "    step: 300; loss: 2.655; l2dist: 1.517\n",
      "    step: 350; loss: 2.648; l2dist: 1.508\n",
      "    step: 400; loss: 2.653; l2dist: 1.509\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 124.922; l2dist: 0.000\n",
      "    step: 50; loss: 12.643; l2dist: 3.421\n",
      "    step: 100; loss: 5.836; l2dist: 2.320\n",
      "    step: 150; loss: 3.820; l2dist: 1.869\n",
      "    step: 200; loss: 3.085; l2dist: 1.668\n",
      "    step: 250; loss: 2.794; l2dist: 1.575\n",
      "    step: 300; loss: 2.665; l2dist: 1.533\n",
      "    step: 350; loss: 2.624; l2dist: 1.504\n",
      "    step: 400; loss: 2.570; l2dist: 1.511\n",
      "    step: 450; loss: 2.558; l2dist: 1.486\n",
      "binary step: 0; number of successful adv: 95/100\n",
      "    step: 0; loss: 129.262; l2dist: 0.000\n",
      "    step: 50; loss: 12.347; l2dist: 3.257\n",
      "    step: 100; loss: 5.874; l2dist: 2.266\n",
      "    step: 150; loss: 3.892; l2dist: 1.825\n",
      "    step: 200; loss: 3.164; l2dist: 1.654\n",
      "    step: 250; loss: 2.865; l2dist: 1.565\n",
      "    step: 300; loss: 2.734; l2dist: 1.532\n",
      "    step: 350; loss: 2.624; l2dist: 1.507\n",
      "    step: 400; loss: 2.568; l2dist: 1.485\n",
      "    step: 450; loss: 2.459; l2dist: 1.464\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 71.776; l2dist: 0.000\n",
      "    step: 50; loss: 9.538; l2dist: 2.805\n",
      "    step: 100; loss: 4.312; l2dist: 1.941\n",
      "    step: 150; loss: 3.082; l2dist: 1.640\n",
      "    step: 200; loss: 2.713; l2dist: 1.534\n",
      "    step: 250; loss: 2.550; l2dist: 1.481\n",
      "    step: 300; loss: 2.455; l2dist: 1.461\n",
      "    step: 350; loss: 2.391; l2dist: 1.443\n",
      "    step: 400; loss: 2.457; l2dist: 1.458\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 43.478; l2dist: 0.000\n",
      "    step: 50; loss: 7.997; l2dist: 2.483\n",
      "    step: 100; loss: 3.750; l2dist: 1.805\n",
      "    step: 150; loss: 2.811; l2dist: 1.552\n",
      "    step: 200; loss: 2.519; l2dist: 1.482\n",
      "    step: 250; loss: 2.412; l2dist: 1.453\n",
      "    step: 300; loss: 2.349; l2dist: 1.431\n",
      "    step: 350; loss: 2.331; l2dist: 1.426\n",
      "    step: 400; loss: 2.309; l2dist: 1.410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 450; loss: 2.300; l2dist: 1.408\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.256; l2dist: 0.000\n",
      "    step: 50; loss: 7.121; l2dist: 2.238\n",
      "    step: 100; loss: 3.639; l2dist: 1.695\n",
      "    step: 150; loss: 2.747; l2dist: 1.514\n",
      "    step: 200; loss: 2.493; l2dist: 1.453\n",
      "    step: 250; loss: 2.400; l2dist: 1.418\n",
      "    step: 300; loss: 2.329; l2dist: 1.407\n",
      "    step: 350; loss: 2.311; l2dist: 1.400\n",
      "    step: 400; loss: 2.306; l2dist: 1.399\n",
      "    step: 450; loss: 2.290; l2dist: 1.396\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.434; l2dist: 0.000\n",
      "    step: 50; loss: 6.542; l2dist: 2.071\n",
      "    step: 100; loss: 3.407; l2dist: 1.559\n",
      "    step: 150; loss: 2.640; l2dist: 1.421\n",
      "    step: 200; loss: 2.401; l2dist: 1.374\n",
      "    step: 250; loss: 2.296; l2dist: 1.355\n",
      "    step: 300; loss: 2.255; l2dist: 1.338\n",
      "    step: 350; loss: 2.232; l2dist: 1.339\n",
      "    step: 400; loss: 2.220; l2dist: 1.330\n",
      "    step: 450; loss: 2.205; l2dist: 1.333\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 21.267; l2dist: 0.000\n",
      "    step: 50; loss: 6.530; l2dist: 2.061\n",
      "    step: 100; loss: 3.410; l2dist: 1.582\n",
      "    step: 150; loss: 2.625; l2dist: 1.435\n",
      "    step: 200; loss: 2.383; l2dist: 1.380\n",
      "    step: 250; loss: 2.291; l2dist: 1.362\n",
      "    step: 300; loss: 2.251; l2dist: 1.351\n",
      "    step: 350; loss: 2.236; l2dist: 1.347\n",
      "    step: 400; loss: 2.219; l2dist: 1.341\n",
      "    step: 450; loss: 2.212; l2dist: 1.345\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.202; l2dist: 0.000\n",
      "    step: 50; loss: 6.498; l2dist: 2.052\n",
      "    step: 100; loss: 3.400; l2dist: 1.578\n",
      "    step: 150; loss: 2.606; l2dist: 1.435\n",
      "    step: 200; loss: 2.377; l2dist: 1.390\n",
      "    step: 250; loss: 2.285; l2dist: 1.371\n",
      "    step: 300; loss: 2.255; l2dist: 1.358\n",
      "    step: 350; loss: 2.221; l2dist: 1.350\n",
      "    step: 400; loss: 2.211; l2dist: 1.347\n",
      "    step: 450; loss: 2.199; l2dist: 1.350\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 19.820; l2dist: 0.000\n",
      "    step: 50; loss: 6.490; l2dist: 2.048\n",
      "    step: 100; loss: 3.408; l2dist: 1.578\n",
      "    step: 150; loss: 2.604; l2dist: 1.431\n",
      "    step: 200; loss: 2.380; l2dist: 1.384\n",
      "    step: 250; loss: 2.292; l2dist: 1.363\n",
      "    step: 300; loss: 2.258; l2dist: 1.357\n",
      "    step: 350; loss: 2.248; l2dist: 1.355\n",
      "    step: 400; loss: 2.221; l2dist: 1.351\n",
      "    step: 450; loss: 2.215; l2dist: 1.346\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.052; l2dist: 0.000\n",
      "    step: 50; loss: 6.532; l2dist: 2.060\n",
      "    step: 100; loss: 3.402; l2dist: 1.592\n",
      "    step: 150; loss: 2.603; l2dist: 1.437\n",
      "    step: 200; loss: 2.375; l2dist: 1.389\n",
      "    step: 250; loss: 2.279; l2dist: 1.370\n",
      "    step: 300; loss: 2.251; l2dist: 1.358\n",
      "    step: 350; loss: 2.225; l2dist: 1.354\n",
      "    step: 400; loss: 2.216; l2dist: 1.353\n",
      "    step: 450; loss: 2.217; l2dist: 1.354\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 118.182; l2dist: 0.000\n",
      "    step: 50; loss: 12.210; l2dist: 3.224\n",
      "    step: 100; loss: 5.315; l2dist: 2.222\n",
      "    step: 150; loss: 3.584; l2dist: 1.803\n",
      "    step: 200; loss: 2.945; l2dist: 1.622\n",
      "    step: 250; loss: 2.656; l2dist: 1.531\n",
      "    step: 300; loss: 2.531; l2dist: 1.488\n",
      "    step: 350; loss: 2.436; l2dist: 1.467\n",
      "    step: 400; loss: 2.391; l2dist: 1.455\n",
      "    step: 450; loss: 2.329; l2dist: 1.438\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 99.285; l2dist: 0.000\n",
      "    step: 50; loss: 10.593; l2dist: 2.984\n",
      "    step: 100; loss: 4.693; l2dist: 2.035\n",
      "    step: 150; loss: 3.254; l2dist: 1.680\n",
      "    step: 200; loss: 2.709; l2dist: 1.532\n",
      "    step: 250; loss: 2.477; l2dist: 1.472\n",
      "    step: 300; loss: 2.379; l2dist: 1.443\n",
      "    step: 350; loss: 2.317; l2dist: 1.429\n",
      "    step: 400; loss: 2.286; l2dist: 1.410\n",
      "    step: 450; loss: 2.252; l2dist: 1.404\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 54.955; l2dist: 0.000\n",
      "    step: 50; loss: 8.207; l2dist: 2.535\n",
      "    step: 100; loss: 3.669; l2dist: 1.798\n",
      "    step: 150; loss: 2.689; l2dist: 1.536\n",
      "    step: 200; loss: 2.386; l2dist: 1.447\n",
      "    step: 250; loss: 2.285; l2dist: 1.417\n",
      "    step: 300; loss: 2.247; l2dist: 1.405\n",
      "    step: 350; loss: 2.204; l2dist: 1.389\n",
      "    step: 400; loss: 2.188; l2dist: 1.386\n",
      "    step: 450; loss: 2.199; l2dist: 1.390\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 33.438; l2dist: 0.000\n",
      "    step: 50; loss: 6.871; l2dist: 2.245\n",
      "    step: 100; loss: 3.283; l2dist: 1.694\n",
      "    step: 150; loss: 2.474; l2dist: 1.468\n",
      "    step: 200; loss: 2.276; l2dist: 1.407\n",
      "    step: 250; loss: 2.206; l2dist: 1.390\n",
      "    step: 300; loss: 2.150; l2dist: 1.369\n",
      "    step: 350; loss: 2.125; l2dist: 1.362\n",
      "    step: 400; loss: 2.118; l2dist: 1.362\n",
      "    step: 450; loss: 2.108; l2dist: 1.364\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.985; l2dist: 0.000\n",
      "    step: 50; loss: 6.200; l2dist: 2.036\n",
      "    step: 100; loss: 3.179; l2dist: 1.600\n",
      "    step: 150; loss: 2.424; l2dist: 1.425\n",
      "    step: 200; loss: 2.215; l2dist: 1.383\n",
      "    step: 250; loss: 2.141; l2dist: 1.355\n",
      "    step: 300; loss: 2.121; l2dist: 1.345\n",
      "    step: 350; loss: 2.089; l2dist: 1.347\n",
      "    step: 400; loss: 2.077; l2dist: 1.339\n",
      "    step: 450; loss: 2.066; l2dist: 1.341\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 20.545; l2dist: 0.000\n",
      "    step: 50; loss: 5.959; l2dist: 1.975\n",
      "    step: 100; loss: 3.103; l2dist: 1.557\n",
      "    step: 150; loss: 2.355; l2dist: 1.393\n",
      "    step: 200; loss: 2.179; l2dist: 1.349\n",
      "    step: 250; loss: 2.097; l2dist: 1.329\n",
      "    step: 300; loss: 2.070; l2dist: 1.325\n",
      "    step: 350; loss: 2.043; l2dist: 1.314\n",
      "    step: 400; loss: 2.035; l2dist: 1.314\n",
      "    step: 450; loss: 2.037; l2dist: 1.311\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.898; l2dist: 0.000\n",
      "    step: 50; loss: 5.808; l2dist: 1.944\n",
      "    step: 100; loss: 3.095; l2dist: 1.535\n",
      "    step: 150; loss: 2.360; l2dist: 1.377\n",
      "    step: 200; loss: 2.176; l2dist: 1.340\n",
      "    step: 250; loss: 2.104; l2dist: 1.319\n",
      "    step: 300; loss: 2.066; l2dist: 1.312\n",
      "    step: 350; loss: 2.072; l2dist: 1.309\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.289; l2dist: 0.000\n",
      "    step: 50; loss: 5.813; l2dist: 1.942\n",
      "    step: 100; loss: 3.106; l2dist: 1.541\n",
      "    step: 150; loss: 2.355; l2dist: 1.384\n",
      "    step: 200; loss: 2.179; l2dist: 1.337\n",
      "    step: 250; loss: 2.126; l2dist: 1.323\n",
      "    step: 300; loss: 2.082; l2dist: 1.316\n",
      "    step: 350; loss: 2.058; l2dist: 1.307\n",
      "    step: 400; loss: 2.050; l2dist: 1.308\n",
      "    step: 450; loss: 2.040; l2dist: 1.305\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 17.905; l2dist: 0.000\n",
      "    step: 50; loss: 5.799; l2dist: 1.940\n",
      "    step: 100; loss: 3.104; l2dist: 1.540\n",
      "    step: 150; loss: 2.351; l2dist: 1.382\n",
      "    step: 200; loss: 2.180; l2dist: 1.339\n",
      "    step: 250; loss: 2.115; l2dist: 1.322\n",
      "    step: 300; loss: 2.074; l2dist: 1.317\n",
      "    step: 350; loss: 2.064; l2dist: 1.315\n",
      "    step: 400; loss: 2.059; l2dist: 1.307\n",
      "    step: 450; loss: 2.039; l2dist: 1.305\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 18.175; l2dist: 0.000\n",
      "    step: 50; loss: 5.837; l2dist: 1.955\n",
      "    step: 100; loss: 3.115; l2dist: 1.550\n",
      "    step: 150; loss: 2.359; l2dist: 1.388\n",
      "    step: 200; loss: 2.186; l2dist: 1.352\n",
      "    step: 250; loss: 2.112; l2dist: 1.327\n",
      "    step: 300; loss: 2.085; l2dist: 1.320\n",
      "    step: 350; loss: 2.063; l2dist: 1.316\n",
      "    step: 400; loss: 2.056; l2dist: 1.310\n",
      "    step: 450; loss: 2.046; l2dist: 1.306\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 140.634; l2dist: 0.000\n",
      "    step: 50; loss: 14.261; l2dist: 3.533\n",
      "    step: 100; loss: 6.359; l2dist: 2.450\n",
      "    step: 150; loss: 4.212; l2dist: 1.967\n",
      "    step: 200; loss: 3.330; l2dist: 1.739\n",
      "    step: 250; loss: 2.907; l2dist: 1.611\n",
      "    step: 300; loss: 2.725; l2dist: 1.557\n",
      "    step: 350; loss: 2.571; l2dist: 1.512\n",
      "    step: 400; loss: 2.557; l2dist: 1.495\n",
      "    step: 450; loss: 2.502; l2dist: 1.480\n",
      "binary step: 0; number of successful adv: 97/100\n",
      "    step: 0; loss: 107.385; l2dist: 0.000\n",
      "    step: 50; loss: 11.833; l2dist: 3.160\n",
      "    step: 100; loss: 5.085; l2dist: 2.153\n",
      "    step: 150; loss: 3.498; l2dist: 1.758\n",
      "    step: 200; loss: 2.904; l2dist: 1.593\n",
      "    step: 250; loss: 2.687; l2dist: 1.528\n",
      "    step: 300; loss: 2.543; l2dist: 1.482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 2.471; l2dist: 1.468\n",
      "    step: 400; loss: 2.425; l2dist: 1.454\n",
      "    step: 450; loss: 2.398; l2dist: 1.442\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 61.829; l2dist: 0.000\n",
      "    step: 50; loss: 9.308; l2dist: 2.750\n",
      "    step: 100; loss: 3.974; l2dist: 1.884\n",
      "    step: 150; loss: 2.908; l2dist: 1.601\n",
      "    step: 200; loss: 2.574; l2dist: 1.495\n",
      "    step: 250; loss: 2.429; l2dist: 1.453\n",
      "    step: 300; loss: 2.386; l2dist: 1.441\n",
      "    step: 350; loss: 2.350; l2dist: 1.433\n",
      "    step: 400; loss: 2.328; l2dist: 1.432\n",
      "    step: 450; loss: 2.341; l2dist: 1.433\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 40.396; l2dist: 0.000\n",
      "    step: 50; loss: 8.028; l2dist: 2.464\n",
      "    step: 100; loss: 3.541; l2dist: 1.771\n",
      "    step: 150; loss: 2.691; l2dist: 1.536\n",
      "    step: 200; loss: 2.446; l2dist: 1.462\n",
      "    step: 250; loss: 2.385; l2dist: 1.428\n",
      "    step: 300; loss: 2.302; l2dist: 1.418\n",
      "    step: 350; loss: 2.263; l2dist: 1.405\n",
      "    step: 400; loss: 2.254; l2dist: 1.403\n",
      "    step: 450; loss: 2.247; l2dist: 1.399\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 29.240; l2dist: 0.000\n",
      "    step: 50; loss: 7.067; l2dist: 2.257\n",
      "    step: 100; loss: 3.404; l2dist: 1.683\n",
      "    step: 150; loss: 2.590; l2dist: 1.488\n",
      "    step: 200; loss: 2.393; l2dist: 1.436\n",
      "    step: 250; loss: 2.293; l2dist: 1.406\n",
      "    step: 300; loss: 2.265; l2dist: 1.394\n",
      "    step: 350; loss: 2.243; l2dist: 1.389\n",
      "    step: 400; loss: 2.218; l2dist: 1.383\n",
      "    step: 450; loss: 2.225; l2dist: 1.388\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 24.332; l2dist: 0.000\n",
      "    step: 50; loss: 6.727; l2dist: 2.146\n",
      "    step: 100; loss: 3.285; l2dist: 1.619\n",
      "    step: 150; loss: 2.520; l2dist: 1.452\n",
      "    step: 200; loss: 2.343; l2dist: 1.398\n",
      "    step: 250; loss: 2.265; l2dist: 1.382\n",
      "    step: 300; loss: 2.222; l2dist: 1.371\n",
      "    step: 350; loss: 2.208; l2dist: 1.364\n",
      "    step: 400; loss: 2.201; l2dist: 1.364\n",
      "    step: 450; loss: 2.196; l2dist: 1.362\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.045; l2dist: 0.000\n",
      "    step: 50; loss: 6.644; l2dist: 2.110\n",
      "    step: 100; loss: 3.268; l2dist: 1.607\n",
      "    step: 150; loss: 2.524; l2dist: 1.437\n",
      "    step: 200; loss: 2.337; l2dist: 1.391\n",
      "    step: 250; loss: 2.257; l2dist: 1.375\n",
      "    step: 300; loss: 2.231; l2dist: 1.363\n",
      "    step: 350; loss: 2.212; l2dist: 1.355\n",
      "    step: 400; loss: 2.201; l2dist: 1.358\n",
      "    step: 450; loss: 2.197; l2dist: 1.356\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 23.000; l2dist: 0.000\n",
      "    step: 50; loss: 6.637; l2dist: 2.113\n",
      "    step: 100; loss: 3.292; l2dist: 1.626\n",
      "    step: 150; loss: 2.529; l2dist: 1.443\n",
      "    step: 200; loss: 2.355; l2dist: 1.399\n",
      "    step: 250; loss: 2.275; l2dist: 1.385\n",
      "    step: 300; loss: 2.241; l2dist: 1.370\n",
      "    step: 350; loss: 2.215; l2dist: 1.370\n",
      "    step: 400; loss: 2.210; l2dist: 1.365\n",
      "    step: 450; loss: 2.206; l2dist: 1.363\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.652; l2dist: 0.000\n",
      "    step: 50; loss: 6.628; l2dist: 2.112\n",
      "    step: 100; loss: 3.305; l2dist: 1.627\n",
      "    step: 150; loss: 2.536; l2dist: 1.454\n",
      "    step: 200; loss: 2.340; l2dist: 1.406\n",
      "    step: 250; loss: 2.280; l2dist: 1.390\n",
      "    step: 300; loss: 2.230; l2dist: 1.377\n",
      "    step: 350; loss: 2.218; l2dist: 1.372\n",
      "    step: 400; loss: 2.212; l2dist: 1.374\n",
      "    step: 450; loss: 2.217; l2dist: 1.368\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 22.904; l2dist: 0.000\n",
      "    step: 50; loss: 6.702; l2dist: 2.123\n",
      "    step: 100; loss: 3.299; l2dist: 1.629\n",
      "    step: 150; loss: 2.540; l2dist: 1.470\n",
      "    step: 200; loss: 2.348; l2dist: 1.418\n",
      "    step: 250; loss: 2.260; l2dist: 1.392\n",
      "    step: 300; loss: 2.238; l2dist: 1.384\n",
      "    step: 350; loss: 2.217; l2dist: 1.371\n",
      "    step: 400; loss: 2.228; l2dist: 1.373\n",
      "binary step: 9; number of successful adv: 100/100\n",
      "    step: 0; loss: 136.020; l2dist: 0.000\n",
      "    step: 50; loss: 14.801; l2dist: 3.589\n",
      "    step: 100; loss: 6.566; l2dist: 2.508\n",
      "    step: 150; loss: 4.385; l2dist: 2.033\n",
      "    step: 200; loss: 3.501; l2dist: 1.812\n",
      "    step: 250; loss: 3.113; l2dist: 1.694\n",
      "    step: 300; loss: 2.966; l2dist: 1.659\n",
      "    step: 350; loss: 2.780; l2dist: 1.603\n",
      "    step: 400; loss: 2.809; l2dist: 1.607\n",
      "binary step: 0; number of successful adv: 94/100\n",
      "    step: 0; loss: 147.192; l2dist: 0.000\n",
      "    step: 50; loss: 14.258; l2dist: 3.475\n",
      "    step: 100; loss: 6.421; l2dist: 2.426\n",
      "    step: 150; loss: 4.357; l2dist: 1.989\n",
      "    step: 200; loss: 3.615; l2dist: 1.803\n",
      "    step: 250; loss: 3.174; l2dist: 1.698\n",
      "    step: 300; loss: 2.991; l2dist: 1.641\n",
      "    step: 350; loss: 2.824; l2dist: 1.610\n",
      "    step: 400; loss: 2.802; l2dist: 1.602\n",
      "    step: 450; loss: 2.739; l2dist: 1.587\n",
      "binary step: 1; number of successful adv: 100/100\n",
      "    step: 0; loss: 81.538; l2dist: 0.000\n",
      "    step: 50; loss: 10.871; l2dist: 2.977\n",
      "    step: 100; loss: 4.772; l2dist: 2.087\n",
      "    step: 150; loss: 3.458; l2dist: 1.772\n",
      "    step: 200; loss: 3.027; l2dist: 1.658\n",
      "    step: 250; loss: 2.794; l2dist: 1.598\n",
      "    step: 300; loss: 2.762; l2dist: 1.593\n",
      "    step: 350; loss: 2.643; l2dist: 1.557\n",
      "    step: 400; loss: 2.676; l2dist: 1.565\n",
      "binary step: 2; number of successful adv: 100/100\n",
      "    step: 0; loss: 50.889; l2dist: 0.000\n",
      "    step: 50; loss: 9.023; l2dist: 2.662\n",
      "    step: 100; loss: 4.137; l2dist: 1.926\n",
      "    step: 150; loss: 3.111; l2dist: 1.671\n",
      "    step: 200; loss: 2.767; l2dist: 1.591\n",
      "    step: 250; loss: 2.648; l2dist: 1.556\n",
      "    step: 300; loss: 2.601; l2dist: 1.534\n",
      "    step: 350; loss: 2.548; l2dist: 1.524\n",
      "    step: 400; loss: 2.540; l2dist: 1.519\n",
      "    step: 450; loss: 2.546; l2dist: 1.531\n",
      "binary step: 3; number of successful adv: 100/100\n",
      "    step: 0; loss: 35.371; l2dist: 0.000\n",
      "    step: 50; loss: 8.008; l2dist: 2.450\n",
      "    step: 100; loss: 3.841; l2dist: 1.832\n",
      "    step: 150; loss: 2.941; l2dist: 1.618\n",
      "    step: 200; loss: 2.686; l2dist: 1.545\n",
      "    step: 250; loss: 2.581; l2dist: 1.522\n",
      "    step: 300; loss: 2.538; l2dist: 1.508\n",
      "    step: 350; loss: 2.528; l2dist: 1.504\n",
      "    step: 400; loss: 2.506; l2dist: 1.501\n",
      "    step: 450; loss: 2.518; l2dist: 1.502\n",
      "binary step: 4; number of successful adv: 100/100\n",
      "    step: 0; loss: 30.666; l2dist: 0.000\n",
      "    step: 50; loss: 7.621; l2dist: 2.362\n",
      "    step: 100; loss: 3.768; l2dist: 1.786\n",
      "    step: 150; loss: 2.880; l2dist: 1.585\n",
      "    step: 200; loss: 2.620; l2dist: 1.516\n",
      "    step: 250; loss: 2.541; l2dist: 1.492\n",
      "    step: 300; loss: 2.500; l2dist: 1.489\n",
      "    step: 350; loss: 2.483; l2dist: 1.479\n",
      "    step: 400; loss: 2.485; l2dist: 1.481\n",
      "binary step: 5; number of successful adv: 100/100\n",
      "    step: 0; loss: 28.705; l2dist: 0.000\n",
      "    step: 50; loss: 7.536; l2dist: 2.329\n",
      "    step: 100; loss: 3.700; l2dist: 1.768\n",
      "    step: 150; loss: 2.829; l2dist: 1.573\n",
      "    step: 200; loss: 2.624; l2dist: 1.515\n",
      "    step: 250; loss: 2.542; l2dist: 1.499\n",
      "    step: 300; loss: 2.479; l2dist: 1.484\n",
      "    step: 350; loss: 2.458; l2dist: 1.478\n",
      "    step: 400; loss: 2.465; l2dist: 1.477\n",
      "binary step: 6; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.974; l2dist: 0.000\n",
      "    step: 50; loss: 7.509; l2dist: 2.326\n",
      "    step: 100; loss: 3.698; l2dist: 1.771\n",
      "    step: 150; loss: 2.850; l2dist: 1.571\n",
      "    step: 200; loss: 2.615; l2dist: 1.520\n",
      "    step: 250; loss: 2.548; l2dist: 1.492\n",
      "    step: 300; loss: 2.509; l2dist: 1.489\n",
      "    step: 350; loss: 2.499; l2dist: 1.492\n",
      "    step: 400; loss: 2.473; l2dist: 1.488\n",
      "    step: 450; loss: 2.459; l2dist: 1.476\n",
      "binary step: 7; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.192; l2dist: 0.000\n",
      "    step: 50; loss: 7.468; l2dist: 2.317\n",
      "    step: 100; loss: 3.681; l2dist: 1.769\n",
      "    step: 150; loss: 2.838; l2dist: 1.574\n",
      "    step: 200; loss: 2.618; l2dist: 1.516\n",
      "    step: 250; loss: 2.532; l2dist: 1.491\n",
      "    step: 300; loss: 2.485; l2dist: 1.488\n",
      "    step: 350; loss: 2.469; l2dist: 1.486\n",
      "    step: 400; loss: 2.446; l2dist: 1.478\n",
      "    step: 450; loss: 2.445; l2dist: 1.474\n",
      "binary step: 8; number of successful adv: 100/100\n",
      "    step: 0; loss: 27.369; l2dist: 0.000\n",
      "    step: 50; loss: 7.500; l2dist: 2.328\n",
      "    step: 100; loss: 3.693; l2dist: 1.778\n",
      "    step: 150; loss: 2.833; l2dist: 1.581\n",
      "    step: 200; loss: 2.609; l2dist: 1.530\n",
      "    step: 250; loss: 2.524; l2dist: 1.498\n",
      "    step: 300; loss: 2.513; l2dist: 1.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 350; loss: 2.478; l2dist: 1.488\n",
      "    step: 400; loss: 2.457; l2dist: 1.485\n",
      "    step: 450; loss: 2.446; l2dist: 1.483\n",
      "binary step: 9; number of successful adv: 100/100\n"
     ]
    }
   ],
   "source": [
    "attack = CWL2Attack()\n",
    "\n",
    "def attack_batch(x, y, batch_size):\n",
    "    x_adv = torch.zeros_like(x)\n",
    "    total_num = x.size(0)\n",
    "    num_batches = total_num // batch_size\n",
    "    for i in range(num_batches):\n",
    "        begin = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x_adv[begin:end] = attack(\n",
    "            net, x[begin:end], y[begin:end], targeted=False,\n",
    "            binary_search_steps=10, max_iterations=500,\n",
    "            confidence=0, learning_rate=1e-1,\n",
    "            initial_const=1e1, abort_early=True)\n",
    "    return x_adv\n",
    "\n",
    "x_adv = attack_batch(x_test[:10000].cuda(), y_test[:10000].cuda(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = net(x_adv)\n",
    "(y_pred.argmax(1).cpu() == y_test[:10000]).numpy().sum() / y_pred.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dknn.classify(x_adv)\n",
    "(y_pred.argmax(1) == y_test[:10000].numpy()).sum() / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([139.,  46.,  23.,  19.,   3.,   3.,   4.,   2.,   2.,   3.]),\n",
       " array([0.0165    , 0.08763333, 0.15876667, 0.2299    , 0.30103333,\n",
       "        0.37216667, 0.4433    , 0.51443333, 0.58556667, 0.6567    ,\n",
       "        0.72783333]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cred = dknn.credibility(y_pred)\n",
    "suc_ind = y_pred.argmax(1) != y_test[:10000].numpy()\n",
    "plt.hist(cred[suc_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5e5ddbbac8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[0].cpu().detach().numpy().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3970, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_adv.cpu() - x_test[:10000]).view(10000, -1).norm(2, 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all random seeds\n",
    "exp_id = 0\n",
    "seed = 2019\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set up model directory\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'train_mnist_cav_exp%d.h5' % exp_id\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "net = ClassAuxVAE((1, 28, 28), num_classes=10, latent_dim=20)\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "net = net.module\n",
    "net.eval()\n",
    "\n",
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test) = load_mnist_all(\n",
    "    '/data', val_size=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "x_valid = x_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassAuxVAE(\n",
       "  (en_conv1): Conv2d(1, 64, kernel_size=(8, 8), stride=(2, 2), padding=(3, 3))\n",
       "  (relu1): ReLU(inplace)\n",
       "  (en_conv2): Conv2d(64, 128, kernel_size=(6, 6), stride=(2, 2), padding=(3, 3))\n",
       "  (relu2): ReLU(inplace)\n",
       "  (en_conv3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu3): ReLU(inplace)\n",
       "  (en_fc1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (relu4): ReLU(inplace)\n",
       "  (en_mu): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (en_logvar): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (de_fc1): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (de_fc2): Linear(in_features=128, out_features=1568, bias=True)\n",
       "  (ax_fc1): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (ax_fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers = ['en_mu']\n",
    "net = net.to(device)\n",
    "with torch.no_grad():\n",
    "    dknn = DKNN(net, x_train, y_train, x_valid, y_valid, layers, \n",
    "                k=10, num_classes=10)\n",
    "    y_pred = dknn.classify(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.964"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred.argmax(1) == y_test.numpy()).sum() / y_test.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 741., 1243.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        8016.]),\n",
       " array([0.01683333, 0.11515   , 0.21346667, 0.31178333, 0.4101    ,\n",
       "        0.50841667, 0.60673333, 0.70505   , 0.80336667, 0.90168333,\n",
       "        1.        ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cred = dknn.credibility(y_pred)\n",
    "plt.hist(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: 0; loss: -10.374; l2dist: 0.000\n",
      "    step: 50; loss: -16.516; l2dist: 2.206\n",
      "    step: 100; loss: -17.532; l2dist: 2.233\n",
      "    step: 150; loss: -21.379; l2dist: 2.383\n",
      "    step: 200; loss: -23.950; l2dist: 2.461\n",
      "    step: 250; loss: -25.525; l2dist: 2.522\n",
      "    step: 300; loss: -26.562; l2dist: 2.575\n",
      "    step: 350; loss: -27.237; l2dist: 2.610\n",
      "    step: 400; loss: -27.690; l2dist: 2.641\n",
      "    step: 450; loss: -27.980; l2dist: 2.662\n",
      "binary step: 0; number of successful adv: 732/1000\n",
      "    step: 0; loss: -31.082; l2dist: 0.000\n",
      "    step: 50; loss: -80.035; l2dist: 2.555\n",
      "    step: 100; loss: -102.080; l2dist: 2.891\n",
      "    step: 150; loss: -113.852; l2dist: 3.048\n",
      "    step: 200; loss: -119.545; l2dist: 3.074\n",
      "    step: 250; loss: -122.952; l2dist: 3.069\n",
      "    step: 300; loss: -125.118; l2dist: 3.057\n",
      "    step: 350; loss: -126.465; l2dist: 3.052\n",
      "    step: 400; loss: -127.398; l2dist: 3.048\n",
      "    step: 450; loss: -127.963; l2dist: 3.048\n",
      "binary step: 1; number of successful adv: 1000/1000\n",
      "    step: 0; loss: -18.453; l2dist: 0.000\n",
      "    step: 50; loss: -41.968; l2dist: 2.307\n",
      "    step: 100; loss: -50.826; l2dist: 2.505\n",
      "    step: 150; loss: -58.404; l2dist: 2.650\n",
      "    step: 200; loss: -62.506; l2dist: 2.697\n",
      "    step: 250; loss: -64.977; l2dist: 2.730\n",
      "    step: 300; loss: -66.444; l2dist: 2.758\n",
      "    step: 350; loss: -67.368; l2dist: 2.784\n",
      "    step: 400; loss: -68.046; l2dist: 2.805\n",
      "    step: 450; loss: -68.504; l2dist: 2.821\n",
      "binary step: 2; number of successful adv: 1000/1000\n",
      "    step: 0; loss: -12.291; l2dist: 0.000\n",
      "    step: 50; loss: -23.981; l2dist: 2.042\n",
      "    step: 100; loss: -27.347; l2dist: 2.139\n",
      "    step: 150; loss: -31.913; l2dist: 2.274\n",
      "    step: 200; loss: -34.881; l2dist: 2.359\n",
      "    step: 250; loss: -36.645; l2dist: 2.417\n",
      "    step: 300; loss: -37.730; l2dist: 2.470\n",
      "    step: 350; loss: -38.446; l2dist: 2.513\n",
      "    step: 400; loss: -38.975; l2dist: 2.552\n",
      "    step: 450; loss: -39.337; l2dist: 2.582\n",
      "binary step: 3; number of successful adv: 1000/1000\n",
      "    step: 0; loss: -9.416; l2dist: 0.000\n",
      "    step: 50; loss: -16.268; l2dist: 1.800\n",
      "    step: 100; loss: -17.725; l2dist: 1.852\n",
      "    step: 150; loss: -20.717; l2dist: 1.967\n",
      "    step: 200; loss: -22.756; l2dist: 2.050\n",
      "    step: 250; loss: -24.101; l2dist: 2.127\n",
      "    step: 300; loss: -24.973; l2dist: 2.191\n",
      "    step: 350; loss: -25.603; l2dist: 2.247\n",
      "    step: 400; loss: -26.060; l2dist: 2.291\n",
      "    step: 450; loss: -26.376; l2dist: 2.325\n",
      "binary step: 4; number of successful adv: 1000/1000\n"
     ]
    }
   ],
   "source": [
    "from lib.dknn_attack import DKNNAttack\n",
    "\n",
    "attack = DKNNAttack()\n",
    "x_adv = attack(dknn, x_test[:1000], y_test[:1000],\n",
    "               guide_layer='en_mu', binary_search_steps=5,\n",
    "               max_iterations=500, learning_rate=1e-1,\n",
    "               initial_const=1, abort_early=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dknn.classify(x_adv)\n",
    "(y_pred.argmax(1) == y_test[:1000].numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 128, 4, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000], device='cuda:0', grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.normalize(x.view(1000, -1), 2, 1).norm(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f452c17f7f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[3].cpu().detach().numpy().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9813, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_adv.cpu() - x_test[:1000]).view(1000, -1).norm(2, 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Soft DkNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_soft_dknn(dknn, x):\n",
    "    \n",
    "    train_reps = dknn.get_activations(dknn.x_train)[dknn.layers[0]]\n",
    "    dknn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_reps = dknn.get_activations(dknn.x_train)[dknn.layers[0]]\n",
    "    train_reps = F.normalize(train_reps.view(dknn.x_train.size(0), -1), 2, 1)\n",
    "    test_reps = dknn.get_activations(x_test)[dknn.layers[0]]\n",
    "    test_reps = F.normalize(test_reps.view(x_test.size(0), -1), 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_reps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a50f5f88d62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#         cos = (tr.unsqueeze(0) * train_reps).sum(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_reps' is not defined"
     ]
    }
   ],
   "source": [
    "temp = 2e-2\n",
    "k = 75\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = torch.empty((x_test.size(0), dknn.num_classes))\n",
    "    for i, tr in enumerate(test_reps):\n",
    "        cos = ((tr.unsqueeze(0) * train_reps).sum(1) / temp).exp()\n",
    "#         cos = (tr.unsqueeze(0) * train_reps).sum(1)\n",
    "        for label in range(dknn.num_classes):\n",
    "#             logits[i, label] = cos[dknn.y_train == label].topk(k)[0].mean()\n",
    "            logits[i, label] = cos[dknn.y_train == label].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LID & Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral norm of jacobian and LID of basic model\n",
    "\n",
    "layers = ['relu1', 'relu2', 'relu3', 'fc']\n",
    "dknn = DKNNL2(net, x_train.cuda(), y_train, x_valid.cuda(), y_valid, layers, \n",
    "              k=75, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_test.requires_grad_(True)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5763, 12.8764, 26.2434, 13.5448\n"
     ]
    }
   ],
   "source": [
    "norms = compute_spnorm(x, dknn, layers)\n",
    "print(', '.join('%.4f' % i for i in norms.mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5957, 11.1405, 8.5105, 4.0548\n"
     ]
    }
   ],
   "source": [
    "lid = np.zeros((x.size(0), len(layers)))\n",
    "reps = dknn.get_activations(x, requires_grad=False)\n",
    "train_reps = dknn.get_activations(x_train, requires_grad=False)\n",
    "\n",
    "for l, layer in enumerate(layers):\n",
    "    lid[:, l] = compute_lid(reps[layer], \n",
    "                         train_reps[layer], \n",
    "                         3000, \n",
    "                         exclude_self=False)\n",
    "print(', '.join('%.4f' % i for i in lid.mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (maybe) spectral norm is not good for comparing between layers\n",
    "# because of difference in dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral norm of jacobian and LID of VAE (exp4)\n",
    "\n",
    "layers = ['relu1', 'relu2', 'relu3', 'en_mu']\n",
    "dknn = DKNNL2(net, x_train.cuda(), y_train, x_valid.cuda(), y_valid, layers, \n",
    "              k=75, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_test.requires_grad_(True)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = compute_spnorm(x, dknn, layers)\n",
    "print(', '.join('%.4f' % i for i in norms.mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lid = np.zeros((x.size(0), len(layers)))\n",
    "reps = dknn.get_activations(x, requires_grad=False)\n",
    "train_reps = dknn.get_activations(x_train, requires_grad=False)\n",
    "\n",
    "for l, layer in enumerate(layers):\n",
    "    lid[:, l] = compute_lid(reps[layer], \n",
    "                         train_reps[layer], \n",
    "                         3000, \n",
    "                         exclude_self=False)\n",
    "print(', '.join('%.4f' % i for i in lid.mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.foolbox_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dknn_fb = DkNNFoolboxModel(dknn, (0, 1), 1, preprocessing=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foolbox.criteria import Misclassification\n",
    "from foolbox.distances import MeanSquaredDistance, Linfinity \n",
    "\n",
    "criterion = Misclassification()\n",
    "distance = MeanSquaredDistance\n",
    "# distance = Linfinity\n",
    "\n",
    "attack = foolbox.attacks.BoundaryAttack(\n",
    "    model=dknn_fb, criterion=criterion, distance=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neither starting_point nor initialization_attack given. Falling back to BlendedUniformNoiseAttack for initialization.\n",
      "Initial spherical_step = 1.00, source_step = 0.10\n",
      "Using 4 threads to create random numbers\n",
      "Step 0: 1.36508e-01, stepsizes = 1.0e+00/1.0e-01: \n",
      "  Boundary too non-linear, decreasing steps: 0.05 (100), 0.00 ( 5)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 6)\n",
      "Step 100: 8.79483e-02, stepsizes = 4.4e-01/4.4e-02:  (took 2.99109 seconds)\n",
      "Initializing generation and prediction time measurements. This can take a few seconds.\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 669.22764\n",
      "   1.0% for generation (6.46957)\n",
      "   15.3% for spherical prediction (102.47140)\n",
      "   70.9% for prediction (474.42754)\n",
      "   0.0% for hyperparameter update (0.01493)\n",
      "   12.8% for the rest (85.84422)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.39345837e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.57342741e-05 1.09945734e-05\n",
      " 3.46500397e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01518669 0.01552219\n",
      " 0.01948805]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.97 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "Using batch size   1, an average step would have taken 4.66979 = 0.06244 + 4.60734 seconds\n",
      "Using batch size   2, an average step would have taken 3.09637 = 0.01279 + 3.08358 seconds\n",
      "Using batch size   3, an average step would have taken 1.91815 = 0.00728 + 1.91087 seconds\n",
      "Using batch size   4, an average step would have taken 1.85581 = 0.00861 + 1.84721 seconds\n",
      "Using batch size   5, an average step would have taken 1.27467 = 0.00870 + 1.26597 seconds\n",
      "Using batch size   6, an average step would have taken 1.34586 = 0.00377 + 1.34209 seconds\n",
      "Using batch size   7, an average step would have taken 1.16225 = 0.00264 + 1.15961 seconds\n",
      "Using batch size   8, an average step would have taken 1.17434 = 0.00453 + 1.16980 seconds\n",
      "Using batch size   9, an average step would have taken 0.89344 = 0.00222 + 0.89122 seconds\n",
      "Using batch size  10, an average step would have taken 0.86904 = 0.00290 + 0.86614 seconds\n",
      "Using batch size  11, an average step would have taken 0.85464 = 0.00143 + 0.85321 seconds\n",
      "Using batch size  12, an average step would have taken 0.86713 = 0.00645 + 0.86069 seconds\n",
      "Using batch size  13, an average step would have taken 0.85672 = 0.00268 + 0.85404 seconds\n",
      "Using batch size  14, an average step would have taken 0.80715 = 0.00222 + 0.80493 seconds\n",
      "Using batch size  15, an average step would have taken 0.86389 = 0.00201 + 0.86188 seconds\n",
      "Using batch size  16, an average step would have taken 0.79313 = 0.00114 + 0.79200 seconds\n",
      "Using batch size  17, an average step would have taken 0.82766 = 0.00092 + 0.82674 seconds\n",
      "Using batch size  18, an average step would have taken 0.90182 = 0.00104 + 0.90078 seconds\n",
      "Using batch size  19, an average step would have taken 0.87731 = 0.00082 + 0.87649 seconds\n",
      "Using batch size  20, an average step would have taken 0.61433 = 0.00211 + 0.61222 seconds\n",
      "Using batch size  21, an average step would have taken 0.63512 = 0.00160 + 0.63352 seconds\n",
      "Using batch size  22, an average step would have taken 0.63183 = 0.00112 + 0.63071 seconds\n",
      "Using batch size  23, an average step would have taken 0.59036 = 0.00121 + 0.58915 seconds\n",
      "Using batch size  24, an average step would have taken 0.55836 = 0.00275 + 0.55562 seconds\n",
      "Using batch size  25, an average step would have taken 0.48807 = 0.00087 + 0.48720 seconds\n",
      "batch size was 1, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.48807\n",
      "improvement compared to old batch size (1): 9.6x\n",
      "improvement compared to worst batch size (1): 9.6x\n",
      "improvement compared to smallest batch size (1): 9.6x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 200\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 7)\n",
      "  Boundary too non-linear, decreasing steps: 0.09 (100), 0.00 (16)\n",
      "  Success rate too low, decreasing source step:  0.20 ( 75), 0.03 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.00 ( 2)\n",
      "Step 200: 5.77934e-02, stepsizes = 1.3e-01/8.8e-03:  (took 0.72004 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 715.59752\n",
      "   1.0% for generation (7.32684)\n",
      "   15.0% for spherical prediction (107.06094)\n",
      "   71.7% for prediction (512.98070)\n",
      "   0.0% for hyperparameter update (0.02662)\n",
      "   12.3% for the rest (88.20242)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.39345837e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.57342741e-05 1.09945734e-05\n",
      " 1.39235884e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01518669 0.01552219\n",
      " 0.01723488]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.91 0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.02 0.\n",
      " 0.01 0.   0.02 0.   0.01 0.   0.   0.   0.02 0.   0.   0.  ]\n",
      "Using batch size   1, an average step would have taken 4.61240 = 0.06168 + 4.55072 seconds\n",
      "Using batch size   2, an average step would have taken 3.05263 = 0.01252 + 3.04011 seconds\n",
      "Using batch size   3, an average step would have taken 1.89339 = 0.00709 + 1.88630 seconds\n",
      "Using batch size   4, an average step would have taken 1.82729 = 0.00839 + 1.81890 seconds\n",
      "Using batch size   5, an average step would have taken 1.26687 = 0.00865 + 1.25822 seconds\n",
      "Using batch size   6, an average step would have taken 1.32257 = 0.00361 + 1.31897 seconds\n",
      "Using batch size   7, an average step would have taken 1.15089 = 0.00260 + 1.14829 seconds\n",
      "Using batch size   8, an average step would have taken 1.15615 = 0.00437 + 1.15179 seconds\n",
      "Using batch size   9, an average step would have taken 0.88445 = 0.00220 + 0.88225 seconds\n",
      "Using batch size  10, an average step would have taken 0.86175 = 0.00283 + 0.85892 seconds\n",
      "Using batch size  11, an average step would have taken 0.84460 = 0.00140 + 0.84320 seconds\n",
      "Using batch size  12, an average step would have taken 0.85221 = 0.00627 + 0.84594 seconds\n",
      "Using batch size  13, an average step would have taken 0.85672 = 0.00268 + 0.85404 seconds\n",
      "Using batch size  14, an average step would have taken 0.80391 = 0.00221 + 0.80170 seconds\n",
      "Using batch size  15, an average step would have taken 0.86077 = 0.00200 + 0.85877 seconds\n",
      "Using batch size  16, an average step would have taken 0.78401 = 0.00111 + 0.78290 seconds\n",
      "Using batch size  17, an average step would have taken 0.81761 = 0.00090 + 0.81671 seconds\n",
      "Using batch size  18, an average step would have taken 0.88980 = 0.00102 + 0.88878 seconds\n",
      "Using batch size  19, an average step would have taken 0.86550 = 0.00081 + 0.86469 seconds\n",
      "Using batch size  20, an average step would have taken 0.60393 = 0.00204 + 0.60189 seconds\n",
      "Using batch size  21, an average step would have taken 0.62376 = 0.00156 + 0.62220 seconds\n",
      "Using batch size  22, an average step would have taken 0.61855 = 0.00108 + 0.61747 seconds\n",
      "Using batch size  23, an average step would have taken 0.57547 = 0.00116 + 0.57431 seconds\n",
      "Using batch size  24, an average step would have taken 0.54689 = 0.00259 + 0.54429 seconds\n",
      "Using batch size  25, an average step would have taken 0.43122 = 0.00035 + 0.43087 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.43122\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 10.7x\n",
      "improvement compared to smallest batch size (1): 10.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.12 (100), 0.14 (14)\n",
      "  Success rate too low, decreasing source step:  0.22 (100), 0.13 (30)\n",
      "Step 300: 3.50126e-02, stepsizes = 8.8e-02/3.9e-03:  (took 0.65270 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.05 (19)\n",
      "  Success rate too low, decreasing source step:  0.28 ( 50), 0.00 (30)\n",
      "Step 400: 2.67988e-02, stepsizes = 5.9e-02/1.7e-03: d. reduced by 0.35% (9.3189e-05) (took 0.67623 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 808.57654\n",
      "   1.1% for generation (8.91197)\n",
      "   14.2% for spherical prediction (115.18854)\n",
      "   72.9% for prediction (589.44378)\n",
      "   0.0% for hyperparameter update (0.04040)\n",
      "   11.7% for the rest (94.99185)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.39345837e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.57342741e-05 1.09945734e-05\n",
      " 1.30980064e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01518669 0.01552219\n",
      " 0.01707519]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.47  0.08  0.025 0.025 0.035 0.03  0.025 0.025 0.025 0.    0.015 0.025\n",
      " 0.03  0.03  0.03  0.015 0.005 0.02  0.015 0.01  0.015 0.    0.025 0.01\n",
      " 0.01  0.005]\n",
      "Using batch size   1, an average step would have taken 3.23595 = 0.04327 + 3.19268 seconds\n",
      "Using batch size   2, an average step would have taken 2.16417 = 0.00856 + 2.15561 seconds\n",
      "Using batch size   3, an average step would have taken 1.34992 = 0.00470 + 1.34522 seconds\n",
      "Using batch size   4, an average step would have taken 1.31941 = 0.00572 + 1.31369 seconds\n",
      "Using batch size   5, an average step would have taken 0.94300 = 0.00644 + 0.93656 seconds\n",
      "Using batch size   6, an average step would have taken 0.97216 = 0.00220 + 0.96997 seconds\n",
      "Using batch size   7, an average step would have taken 0.85558 = 0.00185 + 0.85373 seconds\n",
      "Using batch size   8, an average step would have taken 0.86513 = 0.00282 + 0.86231 seconds\n",
      "Using batch size   9, an average step would have taken 0.68970 = 0.00177 + 0.68793 seconds\n",
      "Using batch size  10, an average step would have taken 0.66944 = 0.00194 + 0.66750 seconds\n",
      "Using batch size  11, an average step would have taken 0.65580 = 0.00102 + 0.65478 seconds\n",
      "Using batch size  12, an average step would have taken 0.66228 = 0.00454 + 0.65774 seconds\n",
      "Using batch size  13, an average step would have taken 0.73969 = 0.00200 + 0.73769 seconds\n",
      "Using batch size  14, an average step would have taken 0.68755 = 0.00206 + 0.68549 seconds\n",
      "Using batch size  15, an average step would have taken 0.74397 = 0.00178 + 0.74219 seconds\n",
      "Using batch size  16, an average step would have taken 0.67458 = 0.00081 + 0.67378 seconds\n",
      "Using batch size  17, an average step would have taken 0.69024 = 0.00063 + 0.68961 seconds\n",
      "Using batch size  18, an average step would have taken 0.77407 = 0.00080 + 0.77327 seconds\n",
      "Using batch size  19, an average step would have taken 0.74888 = 0.00068 + 0.74820 seconds\n",
      "Using batch size  20, an average step would have taken 0.49727 = 0.00131 + 0.49596 seconds\n",
      "Using batch size  21, an average step would have taken 0.50730 = 0.00114 + 0.50616 seconds\n",
      "Using batch size  22, an average step would have taken 0.52672 = 0.00083 + 0.52589 seconds\n",
      "Using batch size  23, an average step would have taken 0.47001 = 0.00079 + 0.46922 seconds\n",
      "Using batch size  24, an average step would have taken 0.46367 = 0.00148 + 0.46219 seconds\n",
      "Using batch size  25, an average step would have taken 0.42721 = 0.00033 + 0.42688 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42721\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 7.6x\n",
      "improvement compared to smallest batch size (1): 7.6x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 800\n",
      "  Success rate too low, decreasing source step:  0.32 (100), 0.13 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.18 (17)\n",
      "Step 500: 2.19464e-02, stepsizes = 3.9e-02/7.7e-04: d. reduced by 0.15% (3.3869e-05) (took 0.66194 seconds)\n",
      "  Success rate too low, decreasing source step:  0.33 (100), 0.13 (30)\n",
      "  Success rate too high, increasing source step: 0.40 (100), 0.63 (30)\n",
      "Step 600: 1.96503e-02, stepsizes = 3.9e-02/7.7e-04:  (took 0.65492 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.14 (100), 0.00 ( 9)\n",
      "Step 700: 1.81176e-02, stepsizes = 2.6e-02/5.1e-04:  (took 0.80920 seconds)\n",
      "  Success rate too low, decreasing source step:  0.35 (100), 0.17 (30)\n",
      "Step 800: 1.69526e-02, stepsizes = 2.6e-02/3.4e-04: d. reduced by 0.07% (1.1620e-05) (took 0.58015 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 994.38740\n",
      "   1.2% for generation (12.14030)\n",
      "   13.2% for spherical prediction (131.45618)\n",
      "   74.5% for prediction (740.56397)\n",
      "   0.0% for hyperparameter update (0.13928)\n",
      "   11.1% for the rest (110.08767)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.39345837e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.57342741e-05 1.09945734e-05\n",
      " 1.29926135e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01518669 0.01552219\n",
      " 0.0169139 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.18   0.1325 0.0625 0.0825 0.0475 0.06   0.0425 0.0375 0.04   0.0325\n",
      " 0.015  0.035  0.03   0.015  0.0125 0.015  0.01   0.015  0.025  0.02\n",
      " 0.02   0.0225 0.015  0.01   0.0075 0.015 ]\n",
      "Using batch size   1, an average step would have taken 2.17085 = 0.02903 + 2.14182 seconds\n",
      "Using batch size   2, an average step would have taken 1.48030 = 0.00561 + 1.47469 seconds\n",
      "Using batch size   3, an average step would have taken 0.92185 = 0.00295 + 0.91890 seconds\n",
      "Using batch size   4, an average step would have taken 0.92711 = 0.00376 + 0.92335 seconds\n",
      "Using batch size   5, an average step would have taken 0.69001 = 0.00471 + 0.68530 seconds\n",
      "Using batch size   6, an average step would have taken 0.70012 = 0.00124 + 0.69888 seconds\n",
      "Using batch size   7, an average step would have taken 0.63272 = 0.00128 + 0.63144 seconds\n",
      "Using batch size   8, an average step would have taken 0.64900 = 0.00177 + 0.64723 seconds\n",
      "Using batch size   9, an average step would have taken 0.53173 = 0.00140 + 0.53033 seconds\n",
      "Using batch size  10, an average step would have taken 0.51589 = 0.00130 + 0.51458 seconds\n",
      "Using batch size  11, an average step would have taken 0.50359 = 0.00073 + 0.50286 seconds\n",
      "Using batch size  12, an average step would have taken 0.51320 = 0.00327 + 0.50993 seconds\n",
      "Using batch size  13, an average step would have taken 0.64933 = 0.00148 + 0.64785 seconds\n",
      "Using batch size  14, an average step would have taken 0.60836 = 0.00196 + 0.60640 seconds\n",
      "Using batch size  15, an average step would have taken 0.66766 = 0.00164 + 0.66603 seconds\n",
      "Using batch size  16, an average step would have taken 0.59859 = 0.00060 + 0.59799 seconds\n",
      "Using batch size  17, an average step would have taken 0.60812 = 0.00046 + 0.60766 seconds\n",
      "Using batch size  18, an average step would have taken 0.69742 = 0.00066 + 0.69676 seconds\n",
      "Using batch size  19, an average step would have taken 0.67064 = 0.00059 + 0.67005 seconds\n",
      "Using batch size  20, an average step would have taken 0.42704 = 0.00083 + 0.42620 seconds\n",
      "Using batch size  21, an average step would have taken 0.42421 = 0.00083 + 0.42338 seconds\n",
      "Using batch size  22, an average step would have taken 0.46421 = 0.00065 + 0.46356 seconds\n",
      "Using batch size  23, an average step would have taken 0.39991 = 0.00054 + 0.39937 seconds\n",
      "Using batch size  24, an average step would have taken 0.41010 = 0.00076 + 0.40934 seconds\n",
      "Using batch size  25, an average step would have taken 0.42317 = 0.00032 + 0.42285 seconds\n",
      "batch size was 25, optimal batch size would have been 23\n",
      "setting batch size to 23: expected step duration: 0.39991\n",
      "improvement compared to old batch size (25): 1.1x\n",
      "improvement compared to worst batch size (1): 5.4x\n",
      "improvement compared to smallest batch size (1): 5.4x\n",
      "improvement compared to largest batch size (25): 1.1x\n",
      "next batch size tuning in 200 steps, after step 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 900: 1.61476e-02, stepsizes = 2.6e-02/3.4e-04: d. reduced by 0.07% (1.1068e-05) (took 0.72218 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.30 (30)\n",
      "Step 1000: 1.56287e-02, stepsizes = 1.7e-02/2.3e-04: d. reduced by 0.05% (7.1404e-06) (took 0.80553 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1099.91178\n",
      "   1.3% for generation (13.83356)\n",
      "   12.8% for spherical prediction (140.92117)\n",
      "   75.2% for prediction (827.04217)\n",
      "   0.0% for hyperparameter update (0.19105)\n",
      "   10.7% for the rest (117.92383)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.08196449e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.48678152e-05 1.09945734e-05\n",
      " 1.29926135e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01516138 0.01552219\n",
      " 0.0169139 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.34  0.07  0.06  0.03  0.055 0.03  0.04  0.02  0.02  0.03  0.05  0.\n",
      " 0.055 0.02  0.015 0.025 0.02  0.01  0.01  0.01  0.02  0.015 0.015 0.01\n",
      " 0.01  0.02 ]\n",
      "Using batch size   1, an average step would have taken 2.83899 = 0.03796 + 2.80102 seconds\n",
      "Using batch size   2, an average step would have taken 1.89852 = 0.00694 + 1.89158 seconds\n",
      "Using batch size   3, an average step would have taken 1.18741 = 0.00402 + 1.18339 seconds\n",
      "Using batch size   4, an average step would have taken 1.16817 = 0.00495 + 1.16321 seconds\n",
      "Using batch size   5, an average step would have taken 0.84284 = 0.00575 + 0.83709 seconds\n",
      "Using batch size   6, an average step would have taken 0.86454 = 0.00181 + 0.86274 seconds\n",
      "Using batch size   7, an average step would have taken 0.77348 = 0.00163 + 0.77185 seconds\n",
      "Using batch size   8, an average step would have taken 0.78447 = 0.00241 + 0.78207 seconds\n",
      "Using batch size   9, an average step would have taken 0.63230 = 0.00164 + 0.63066 seconds\n",
      "Using batch size  10, an average step would have taken 0.60345 = 0.00168 + 0.60177 seconds\n",
      "Using batch size  11, an average step would have taken 0.59965 = 0.00091 + 0.59874 seconds\n",
      "Using batch size  12, an average step would have taken 0.59898 = 0.00400 + 0.59497 seconds\n",
      "Using batch size  13, an average step would have taken 0.70182 = 0.00178 + 0.70004 seconds\n",
      "Using batch size  14, an average step would have taken 0.65684 = 0.00202 + 0.65482 seconds\n",
      "Using batch size  15, an average step would have taken 0.71127 = 0.00172 + 0.70955 seconds\n",
      "Using batch size  16, an average step would have taken 0.63811 = 0.00071 + 0.63740 seconds\n",
      "Using batch size  17, an average step would have taken 0.65337 = 0.00056 + 0.65281 seconds\n",
      "Using batch size  18, an average step would have taken 0.74251 = 0.00075 + 0.74176 seconds\n",
      "Using batch size  19, an average step would have taken 0.71788 = 0.00065 + 0.71724 seconds\n",
      "Using batch size  20, an average step would have taken 0.46866 = 0.00112 + 0.46754 seconds\n",
      "Using batch size  21, an average step would have taken 0.47179 = 0.00101 + 0.47078 seconds\n",
      "Using batch size  22, an average step would have taken 0.50127 = 0.00076 + 0.50052 seconds\n",
      "Using batch size  23, an average step would have taken 0.44085 = 0.00064 + 0.44020 seconds\n",
      "Using batch size  24, an average step would have taken 0.44167 = 0.00118 + 0.44048 seconds\n",
      "Using batch size  25, an average step would have taken 0.42317 = 0.00032 + 0.42285 seconds\n",
      "batch size was 23, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42317\n",
      "improvement compared to old batch size (23): 1.0x\n",
      "improvement compared to worst batch size (1): 6.7x\n",
      "improvement compared to smallest batch size (1): 6.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 1100\n",
      "  Success rate too low, decreasing source step:  0.26 (100), 0.07 (30)\n",
      "  Boundary too linear, increasing steps:     0.58 (100), 0.73 (30)\n",
      "  Success rate too high, increasing source step: 0.58 (100), 0.73 (30)\n",
      "Step 1100: 1.52189e-02, stepsizes = 2.6e-02/3.4e-04: d. reduced by 0.07% (1.0432e-05) (took 0.57642 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1145.68716\n",
      "   1.3% for generation (14.68362)\n",
      "   12.6% for spherical prediction (144.85473)\n",
      "   75.4% for prediction (864.02238)\n",
      "   0.0% for hyperparameter update (0.20639)\n",
      "   10.6% for the rest (121.92004)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.08196449e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.48678152e-05 1.09945734e-05\n",
      " 1.30685647e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01516138 0.01552219\n",
      " 0.01685444]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.32 0.11 0.01 0.02 0.07 0.05 0.01 0.04 0.04 0.07 0.02 0.02 0.04 0.01\n",
      " 0.   0.01 0.05 0.   0.01 0.01 0.01 0.02 0.01 0.02 0.01 0.02]\n",
      "Using batch size   1, an average step would have taken 2.75481 = 0.03684 + 2.71797 seconds\n",
      "Using batch size   2, an average step would have taken 1.85624 = 0.00676 + 1.84948 seconds\n",
      "Using batch size   3, an average step would have taken 1.16035 = 0.00390 + 1.15645 seconds\n",
      "Using batch size   4, an average step would have taken 1.13593 = 0.00480 + 1.13114 seconds\n",
      "Using batch size   5, an average step would have taken 0.82724 = 0.00565 + 0.82159 seconds\n",
      "Using batch size   6, an average step would have taken 0.84743 = 0.00174 + 0.84569 seconds\n",
      "Using batch size   7, an average step would have taken 0.76321 = 0.00160 + 0.76161 seconds\n",
      "Using batch size   8, an average step would have taken 0.76221 = 0.00232 + 0.75989 seconds\n",
      "Using batch size   9, an average step would have taken 0.60653 = 0.00157 + 0.60496 seconds\n",
      "Using batch size  10, an average step would have taken 0.58995 = 0.00164 + 0.58831 seconds\n",
      "Using batch size  11, an average step would have taken 0.57965 = 0.00088 + 0.57877 seconds\n",
      "Using batch size  12, an average step would have taken 0.58138 = 0.00387 + 0.57751 seconds\n",
      "Using batch size  13, an average step would have taken 0.69150 = 0.00172 + 0.68978 seconds\n",
      "Using batch size  14, an average step would have taken 0.65199 = 0.00201 + 0.64998 seconds\n",
      "Using batch size  15, an average step would have taken 0.71127 = 0.00172 + 0.70955 seconds\n",
      "Using batch size  16, an average step would have taken 0.62899 = 0.00068 + 0.62831 seconds\n",
      "Using batch size  17, an average step would have taken 0.64667 = 0.00054 + 0.64612 seconds\n",
      "Using batch size  18, an average step would have taken 0.73650 = 0.00074 + 0.73576 seconds\n",
      "Using batch size  19, an average step would have taken 0.71198 = 0.00064 + 0.71134 seconds\n",
      "Using batch size  20, an average step would have taken 0.46606 = 0.00110 + 0.46496 seconds\n",
      "Using batch size  21, an average step would have taken 0.46753 = 0.00099 + 0.46654 seconds\n",
      "Using batch size  22, an average step would have taken 0.49906 = 0.00075 + 0.49831 seconds\n",
      "Using batch size  23, an average step would have taken 0.43588 = 0.00063 + 0.43526 seconds\n",
      "Using batch size  24, an average step would have taken 0.43784 = 0.00113 + 0.43671 seconds\n",
      "Using batch size  25, an average step would have taken 0.42169 = 0.00033 + 0.42136 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42169\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.5x\n",
      "improvement compared to smallest batch size (1): 6.5x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.03 (100), 0.33 ( 3)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.18 (22)\n",
      "  Success rate too low, decreasing source step:  0.56 ( 25), 0.13 (30)\n",
      "Step 1200: 1.50270e-02, stepsizes = 1.2e-02/1.0e-04: d. reduced by 0.02% (3.0507e-06) (took 0.83964 seconds)\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.17 (30)\n",
      "Step 1300: 1.47830e-02, stepsizes = 1.2e-02/6.8e-05: d. reduced by 0.01% (2.0007e-06) (took 0.70453 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1240.87653\n",
      "   1.3% for generation (16.44455)\n",
      "   12.4% for spherical prediction (153.30529)\n",
      "   75.9% for prediction (941.94015)\n",
      "   0.0% for hyperparameter update (0.29152)\n",
      "   10.4% for the rest (128.89502)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.08196449e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.48678152e-05 1.09945734e-05\n",
      " 1.32721420e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01516138 0.01552219\n",
      " 0.01694616]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.37  0.11  0.08  0.07  0.015 0.035 0.025 0.035 0.025 0.025 0.015 0.02\n",
      " 0.03  0.01  0.015 0.005 0.    0.02  0.015 0.01  0.01  0.005 0.025 0.005\n",
      " 0.01  0.015]\n",
      "Using batch size   1, an average step would have taken 2.73855 = 0.03662 + 2.70193 seconds\n",
      "Using batch size   2, an average step would have taken 1.84500 = 0.00681 + 1.83819 seconds\n",
      "Using batch size   3, an average step would have taken 1.14905 = 0.00396 + 1.14508 seconds\n",
      "Using batch size   4, an average step would have taken 1.14170 = 0.00490 + 1.13680 seconds\n",
      "Using batch size   5, an average step would have taken 0.82463 = 0.00563 + 0.81901 seconds\n",
      "Using batch size   6, an average step would have taken 0.85014 = 0.00185 + 0.84829 seconds\n",
      "Using batch size   7, an average step would have taken 0.75796 = 0.00162 + 0.75634 seconds\n",
      "Using batch size   8, an average step would have taken 0.77417 = 0.00244 + 0.77173 seconds\n",
      "Using batch size   9, an average step would have taken 0.61554 = 0.00159 + 0.61396 seconds\n",
      "Using batch size  10, an average step would have taken 0.59931 = 0.00169 + 0.59762 seconds\n",
      "Using batch size  11, an average step would have taken 0.58791 = 0.00090 + 0.58701 seconds\n",
      "Using batch size  12, an average step would have taken 0.59515 = 0.00402 + 0.59114 seconds\n",
      "Using batch size  13, an average step would have taken 0.69666 = 0.00175 + 0.69491 seconds\n",
      "Using batch size  14, an average step would have taken 0.65199 = 0.00201 + 0.64998 seconds\n",
      "Using batch size  15, an average step would have taken 0.71283 = 0.00172 + 0.71110 seconds\n",
      "Using batch size  16, an average step would have taken 0.64571 = 0.00073 + 0.64498 seconds\n",
      "Using batch size  17, an average step would have taken 0.65840 = 0.00057 + 0.65783 seconds\n",
      "Using batch size  18, an average step would have taken 0.74552 = 0.00075 + 0.74477 seconds\n",
      "Using batch size  19, an average step would have taken 0.72083 = 0.00065 + 0.72019 seconds\n",
      "Using batch size  20, an average step would have taken 0.47386 = 0.00115 + 0.47271 seconds\n",
      "Using batch size  21, an average step would have taken 0.48031 = 0.00104 + 0.47927 seconds\n",
      "Using batch size  22, an average step would have taken 0.50570 = 0.00077 + 0.50493 seconds\n",
      "Using batch size  23, an average step would have taken 0.44705 = 0.00066 + 0.44638 seconds\n",
      "Using batch size  24, an average step would have taken 0.44645 = 0.00125 + 0.44520 seconds\n",
      "Using batch size  25, an average step would have taken 0.42399 = 0.00033 + 0.42365 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42399\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.5x\n",
      "improvement compared to smallest batch size (1): 6.5x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 1700\n",
      "  Success rate too high, increasing source step: 0.45 (100), 0.73 (30)\n",
      "  Success rate too low, decreasing source step:  0.21 (100), 0.13 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.75 ( 4)\n",
      "Step 1400: 1.45872e-02, stepsizes = 7.7e-03/4.5e-05: d. reduced by 0.01% (1.9742e-06) (took 0.70996 seconds)\n",
      "  Success rate too high, increasing source step: 0.34 (100), 0.60 (30)\n",
      "  Success rate too low, decreasing source step:  0.21 (100), 0.17 (30)\n",
      "Step 1500: 1.44725e-02, stepsizes = 7.7e-03/4.5e-05: d. reduced by 0.01% (1.9585e-06) (took 0.68849 seconds)\n",
      "  Success rate too low, decreasing source step:  0.27 (100), 0.17 (30)\n",
      "  Boundary too linear, increasing steps:     0.55 (100), 0.60 (30)\n",
      "  Success rate too high, increasing source step: 0.55 (100), 0.60 (30)\n",
      "Step 1600: 1.43552e-02, stepsizes = 1.2e-02/6.8e-05: d. reduced by 0.01% (1.9429e-06) (took 0.75235 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.11 (100), 0.33 (30)\n",
      "Step 1700: 1.42372e-02, stepsizes = 7.7e-03/4.5e-05:  (took 0.76085 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1428.37038\n",
      "   1.4% for generation (19.74497)\n",
      "   11.9% for spherical prediction (170.20866)\n",
      "   76.6% for prediction (1094.83210)\n",
      "   0.0% for hyperparameter update (0.37186)\n",
      "   10.0% for the rest (143.21279)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 4.08196449e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.48678152e-05 1.09945734e-05\n",
      " 1.32520233e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01516138 0.01552219\n",
      " 0.01695808]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.26   0.1275 0.075  0.0475 0.0625 0.0525 0.0325 0.03   0.0375 0.03\n",
      " 0.0225 0.02   0.02   0.0275 0.0075 0.02   0.0175 0.015  0.015  0.01\n",
      " 0.0075 0.01   0.015  0.015  0.0075 0.015 ]\n",
      "Using batch size   1, an average step would have taken 2.37985 = 0.03182 + 2.34803 seconds\n",
      "Using batch size   2, an average step would have taken 1.61184 = 0.00583 + 1.60601 seconds\n",
      "Using batch size   3, an average step would have taken 1.01294 = 0.00336 + 1.00958 seconds\n",
      "Using batch size   4, an average step would have taken 1.00633 = 0.00420 + 1.00213 seconds\n",
      "Using batch size   5, an average step would have taken 0.73554 = 0.00502 + 0.73052 seconds\n",
      "Using batch size   6, an average step would have taken 0.75676 = 0.00149 + 0.75527 seconds\n",
      "Using batch size   7, an average step would have taken 0.68392 = 0.00142 + 0.68250 seconds\n",
      "Using batch size   8, an average step would have taken 0.69112 = 0.00203 + 0.68909 seconds\n",
      "Using batch size   9, an average step would have taken 0.55968 = 0.00146 + 0.55822 seconds\n",
      "Using batch size  10, an average step would have taken 0.54565 = 0.00145 + 0.54419 seconds\n",
      "Using batch size  11, an average step would have taken 0.53856 = 0.00080 + 0.53776 seconds\n",
      "Using batch size  12, an average step would have taken 0.54915 = 0.00359 + 0.54556 seconds\n",
      "Using batch size  13, an average step would have taken 0.66568 = 0.00157 + 0.66411 seconds\n",
      "Using batch size  14, an average step would have taken 0.62533 = 0.00198 + 0.62335 seconds\n",
      "Using batch size  15, an average step would have taken 0.68246 = 0.00166 + 0.68079 seconds\n",
      "Using batch size  16, an average step would have taken 0.61075 = 0.00063 + 0.61012 seconds\n",
      "Using batch size  17, an average step would have taken 0.62153 = 0.00049 + 0.62104 seconds\n",
      "Using batch size  18, an average step would have taken 0.71245 = 0.00069 + 0.71176 seconds\n",
      "Using batch size  19, an average step would have taken 0.68836 = 0.00061 + 0.68775 seconds\n",
      "Using batch size  20, an average step would have taken 0.44590 = 0.00096 + 0.44493 seconds\n",
      "Using batch size  21, an average step would have taken 0.44835 = 0.00092 + 0.44744 seconds\n",
      "Using batch size  22, an average step would have taken 0.48302 = 0.00071 + 0.48231 seconds\n",
      "Using batch size  23, an average step would have taken 0.41914 = 0.00057 + 0.41857 seconds\n",
      "Using batch size  24, an average step would have taken 0.42541 = 0.00097 + 0.42444 seconds\n",
      "Using batch size  25, an average step would have taken 0.42428 = 0.00033 + 0.42395 seconds\n",
      "batch size was 25, optimal batch size would have been 23\n",
      "setting batch size to 23: expected step duration: 0.41914\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 5.7x\n",
      "improvement compared to smallest batch size (1): 5.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 1900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.20 (30)\n",
      "  Success rate too low, decreasing source step:  0.20 ( 50), 0.13 (30)\n",
      "Step 1800: 1.41669e-02, stepsizes = 5.1e-03/2.0e-05: d. reduced by 0.00% (5.6826e-07) (took 0.60293 seconds)\n",
      "  Success rate too high, increasing source step: 0.40 (100), 0.57 (30)\n",
      "  Success rate too low, decreasing source step:  0.43 (100), 0.07 (30)\n",
      "Step 1900: 1.41043e-02, stepsizes = 5.1e-03/2.0e-05: d. reduced by 0.00% (5.6559e-07) (took 0.69565 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1523.01799\n",
      "   1.4% for generation (21.41045)\n",
      "   11.8% for spherical prediction (179.06828)\n",
      "   76.9% for prediction (1171.81344)\n",
      "   0.0% for hyperparameter update (0.42228)\n",
      "   9.9% for the rest (150.30354)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 5.31754805e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.47278694e-05 1.09945734e-05\n",
      " 1.32520233e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01538129 0.01552219\n",
      " 0.01695808]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.17  0.16  0.115 0.05  0.055 0.065 0.04  0.02  0.035 0.015 0.045 0.02\n",
      " 0.015 0.025 0.01  0.01  0.02  0.015 0.03  0.01  0.    0.015 0.02  0.01\n",
      " 0.02  0.01 ]\n",
      "Using batch size   1, an average step would have taken 2.03072 = 0.02715 + 2.00356 seconds\n",
      "Using batch size   2, an average step would have taken 1.38165 = 0.00623 + 1.37542 seconds\n",
      "Using batch size   3, an average step would have taken 0.87638 = 0.00279 + 0.87359 seconds\n",
      "Using batch size   4, an average step would have taken 0.88376 = 0.00357 + 0.88019 seconds\n",
      "Using batch size   5, an average step would have taken 0.65554 = 0.00447 + 0.65107 seconds\n",
      "Using batch size   6, an average step would have taken 0.66477 = 0.00116 + 0.66361 seconds\n",
      "Using batch size   7, an average step would have taken 0.61389 = 0.00125 + 0.61264 seconds\n",
      "Using batch size   8, an average step would have taken 0.62434 = 0.00168 + 0.62266 seconds\n",
      "Using batch size   9, an average step would have taken 0.51589 = 0.00137 + 0.51453 seconds\n",
      "Using batch size  10, an average step would have taken 0.49979 = 0.00127 + 0.49852 seconds\n",
      "Using batch size  11, an average step would have taken 0.49253 = 0.00071 + 0.49182 seconds\n",
      "Using batch size  12, an average step would have taken 0.50430 = 0.00319 + 0.50111 seconds\n",
      "Using batch size  13, an average step would have taken 0.63986 = 0.00142 + 0.63844 seconds\n",
      "Using batch size  14, an average step would have taken 0.60028 = 0.00194 + 0.59833 seconds\n",
      "Using batch size  15, an average step would have taken 0.66143 = 0.00162 + 0.65981 seconds\n",
      "Using batch size  16, an average step would have taken 0.58947 = 0.00057 + 0.58890 seconds\n",
      "Using batch size  17, an average step would have taken 0.59807 = 0.00044 + 0.59762 seconds\n",
      "Using batch size  18, an average step would have taken 0.68690 = 0.00064 + 0.68626 seconds\n",
      "Using batch size  19, an average step would have taken 0.66326 = 0.00058 + 0.66268 seconds\n",
      "Using batch size  20, an average step would have taken 0.42573 = 0.00082 + 0.42491 seconds\n",
      "Using batch size  21, an average step would have taken 0.42492 = 0.00083 + 0.42409 seconds\n",
      "Using batch size  22, an average step would have taken 0.46366 = 0.00065 + 0.46301 seconds\n",
      "Using batch size  23, an average step would have taken 0.40378 = 0.00055 + 0.40322 seconds\n",
      "Using batch size  24, an average step would have taken 0.40723 = 0.00072 + 0.40651 seconds\n",
      "Using batch size  25, an average step would have taken 0.42428 = 0.00033 + 0.42395 seconds\n",
      "batch size was 23, optimal batch size would have been 23\n",
      "setting batch size to 23: expected step duration: 0.40378\n",
      "improvement compared to old batch size (23): 1.0x\n",
      "improvement compared to worst batch size (1): 5.0x\n",
      "improvement compared to smallest batch size (1): 5.0x\n",
      "improvement compared to largest batch size (25): 1.1x\n",
      "next batch size tuning in 400 steps, after step 2300\n",
      "Step 2000: 1.40529e-02, stepsizes = 5.1e-03/2.0e-05:  (took 0.96835 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.43 (30)\n",
      "Step 2100: 1.40188e-02, stepsizes = 3.4e-03/1.3e-05: d. reduced by 0.00% (3.7490e-07) (took 0.58701 seconds)\n",
      "  Success rate too low, decreasing source step:  0.18 ( 96), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.25 (100), 0.53 (30)\n",
      "Step 2200: 1.39947e-02, stepsizes = 3.4e-03/1.3e-05: d. reduced by 0.00% (2.4947e-07) (took 0.80112 seconds)\n",
      "Step 2300: 1.39663e-02, stepsizes = 3.4e-03/1.3e-05:  (took 0.80567 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1726.21282\n",
      "   1.4% for generation (24.65294)\n",
      "   11.5% for spherical prediction (198.76615)\n",
      "   77.5% for prediction (1338.31684)\n",
      "   0.0% for hyperparameter update (0.49143)\n",
      "   9.5% for the rest (163.98547)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.55814007e-03 6.15931780e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.44688775e-05 1.09945734e-05\n",
      " 1.32520233e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01606127 0.01552219\n",
      " 0.01695808]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.18   0.115  0.0675 0.055  0.05   0.0675 0.0575 0.0525 0.07   0.02\n",
      " 0.0275 0.0225 0.0275 0.02   0.0225 0.015  0.0125 0.01   0.02   0.01\n",
      " 0.0125 0.0125 0.0225 0.01   0.01   0.01  ]\n",
      "Using batch size   1, an average step would have taken 2.14359 = 0.02866 + 2.11492 seconds\n",
      "Using batch size   2, an average step would have taken 1.45592 = 0.00752 + 1.44840 seconds\n",
      "Using batch size   3, an average step would have taken 0.91536 = 0.00292 + 0.91244 seconds\n",
      "Using batch size   4, an average step would have taken 0.91337 = 0.00370 + 0.90967 seconds\n",
      "Using batch size   5, an average step would have taken 0.68091 = 0.00465 + 0.67626 seconds\n",
      "Using batch size   6, an average step would have taken 0.69252 = 0.00122 + 0.69130 seconds\n",
      "Using batch size   7, an average step would have taken 0.62512 = 0.00127 + 0.62385 seconds\n",
      "Using batch size   8, an average step would have taken 0.62710 = 0.00171 + 0.62538 seconds\n",
      "Using batch size   9, an average step would have taken 0.51965 = 0.00137 + 0.51828 seconds\n",
      "Using batch size  10, an average step would have taken 0.50524 = 0.00128 + 0.50396 seconds\n",
      "Using batch size  11, an average step would have taken 0.49738 = 0.00072 + 0.49666 seconds\n",
      "Using batch size  12, an average step would have taken 0.50708 = 0.00322 + 0.50385 seconds\n",
      "Using batch size  13, an average step would have taken 0.64244 = 0.00144 + 0.64101 seconds\n",
      "Using batch size  14, an average step would have taken 0.59866 = 0.00194 + 0.59672 seconds\n",
      "Using batch size  15, an average step would have taken 0.65832 = 0.00162 + 0.65670 seconds\n",
      "Using batch size  16, an average step would have taken 0.58871 = 0.00057 + 0.58814 seconds\n",
      "Using batch size  17, an average step would have taken 0.59890 = 0.00044 + 0.59846 seconds\n",
      "Using batch size  18, an average step would have taken 0.69066 = 0.00065 + 0.69001 seconds\n",
      "Using batch size  19, an average step would have taken 0.66695 = 0.00059 + 0.66636 seconds\n",
      "Using batch size  20, an average step would have taken 0.42573 = 0.00082 + 0.42491 seconds\n",
      "Using batch size  21, an average step would have taken 0.42563 = 0.00084 + 0.42479 seconds\n",
      "Using batch size  22, an average step would have taken 0.46366 = 0.00065 + 0.46301 seconds\n",
      "Using batch size  23, an average step would have taken 0.41944 = 0.00058 + 0.41886 seconds\n",
      "Using batch size  24, an average step would have taken 0.40914 = 0.00075 + 0.40839 seconds\n",
      "Using batch size  25, an average step would have taken 0.42428 = 0.00033 + 0.42395 seconds\n",
      "batch size was 23, optimal batch size would have been 24\n",
      "setting batch size to 24: expected step duration: 0.40914\n",
      "improvement compared to old batch size (23): 1.0x\n",
      "improvement compared to worst batch size (1): 5.2x\n",
      "improvement compared to smallest batch size (1): 5.2x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 2700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.23 (30)\n",
      "  Success rate too low, decreasing source step:  0.20 ( 74), 0.13 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.40 (15)\n",
      "Step 2400: 1.39531e-02, stepsizes = 1.5e-03/4.0e-06: d. reduced by 0.00% (1.1030e-07) (took 0.72685 seconds)\n",
      "  Success rate too high, increasing source step: 0.38 ( 72), 0.63 (30)\n",
      "  Success rate too low, decreasing source step:  0.33 (100), 0.17 (30)\n",
      "Step 2500: 1.39416e-02, stepsizes = 1.5e-03/4.0e-06: d. reduced by 0.00% (1.1042e-07) (took 0.82726 seconds)\n",
      "  Success rate too high, increasing source step: 0.41 (100), 0.53 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.24 (17)\n",
      "  Success rate too low, decreasing source step:  0.27 ( 49), 0.17 (30)\n",
      "Step 2600: 1.39340e-02, stepsizes = 1.0e-03/2.6e-06: d. reduced by 0.00% (7.3738e-08) (took 0.64675 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.52 (21)\n",
      "  Success rate too high, increasing source step: 0.71 ( 24), 0.57 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.32 (25)\n",
      "Step 2700: 1.39289e-02, stepsizes = 4.5e-04/1.8e-06: d. reduced by 0.00% (7.3528e-08) (took 0.64863 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1939.10042\n",
      "   1.4% for generation (27.99003)\n",
      "   11.2% for spherical prediction (217.72148)\n",
      "   78.1% for prediction (1514.15453)\n",
      "   0.0% for hyperparameter update (0.57562)\n",
      "   9.2% for the rest (178.65875)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.52754774e-03 6.15931780e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.44688775e-05 1.35511445e-05\n",
      " 1.32520233e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01606127 0.01572055\n",
      " 0.01695808]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.27   0.1525 0.07   0.055  0.04   0.0675 0.035  0.035  0.0175 0.0175\n",
      " 0.0175 0.005  0.025  0.02   0.0275 0.0225 0.01   0.02   0.015  0.0175\n",
      " 0.0175 0.0125 0.0025 0.005  0.0075 0.015 ]\n",
      "Using batch size   1, an average step would have taken 2.37373 = 0.03137 + 2.34237 seconds\n",
      "Using batch size   2, an average step would have taken 1.61449 = 0.00845 + 1.60604 seconds\n",
      "Using batch size   3, an average step would have taken 1.00931 = 0.00336 + 1.00595 seconds\n",
      "Using batch size   4, an average step would have taken 1.01107 = 0.00423 + 1.00684 seconds\n",
      "Using batch size   5, an average step would have taken 0.73163 = 0.00499 + 0.72664 seconds\n",
      "Using batch size   6, an average step would have taken 0.76014 = 0.00151 + 0.75863 seconds\n",
      "Using batch size   7, an average step would have taken 0.67436 = 0.00140 + 0.67296 seconds\n",
      "Using batch size   8, an average step would have taken 0.69470 = 0.00205 + 0.69265 seconds\n",
      "Using batch size   9, an average step would have taken 0.56346 = 0.00147 + 0.56198 seconds\n",
      "Using batch size  10, an average step would have taken 0.54616 = 0.00144 + 0.54472 seconds\n",
      "Using batch size  11, an average step would have taken 0.54664 = 0.00081 + 0.54583 seconds\n",
      "Using batch size  12, an average step would have taken 0.55794 = 0.00365 + 0.55429 seconds\n",
      "Using batch size  13, an average step would have taken 0.67515 = 0.00163 + 0.67352 seconds\n",
      "Using batch size  14, an average step would have taken 0.62775 = 0.00198 + 0.62577 seconds\n",
      "Using batch size  15, an average step would have taken 0.68402 = 0.00167 + 0.68235 seconds\n",
      "Using batch size  16, an average step would have taken 0.61455 = 0.00064 + 0.61391 seconds\n",
      "Using batch size  17, an average step would have taken 0.62404 = 0.00050 + 0.62355 seconds\n",
      "Using batch size  18, an average step would have taken 0.71471 = 0.00069 + 0.71401 seconds\n",
      "Using batch size  19, an average step would have taken 0.68836 = 0.00061 + 0.68775 seconds\n",
      "Using batch size  20, an average step would have taken 0.44329 = 0.00094 + 0.44235 seconds\n",
      "Using batch size  21, an average step would have taken 0.44480 = 0.00091 + 0.44390 seconds\n",
      "Using batch size  22, an average step would have taken 0.48302 = 0.00071 + 0.48231 seconds\n",
      "Using batch size  23, an average step would have taken 0.44243 = 0.00069 + 0.44174 seconds\n",
      "Using batch size  24, an average step would have taken 0.43213 = 0.00105 + 0.43109 seconds\n",
      "Using batch size  25, an average step would have taken 0.42428 = 0.00033 + 0.42395 seconds\n",
      "batch size was 24, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42428\n",
      "improvement compared to old batch size (24): 1.0x\n",
      "improvement compared to worst batch size (1): 5.6x\n",
      "improvement compared to smallest batch size (1): 5.6x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 3100\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.44 (100), 0.67 (30)\n",
      "Step 2800: 1.39258e-02, stepsizes = 4.5e-04/1.8e-06: d. reduced by 0.00% (4.8989e-08) (took 0.90690 seconds)\n",
      "  Success rate too low, decreasing source step:  0.23 (100), 0.10 (30)\n",
      "  Success rate too high, increasing source step: 0.37 (100), 0.57 (30)\n",
      "Step 2900: 1.39221e-02, stepsizes = 4.5e-04/1.8e-06: d. reduced by 0.00% (4.9017e-08) (took 1.41962 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.12 (100), 0.37 (30)\n",
      "  Success rate too low, decreasing source step:  0.38 ( 50), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.31 (100), 0.53 (30)\n",
      "Step 3000: 1.39198e-02, stepsizes = 3.0e-04/1.2e-06: d. reduced by 0.00% (2.1874e-08) (took 0.78424 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.12 (100), 0.20 (30)\n",
      "Step 3100: 1.39181e-02, stepsizes = 2.0e-04/7.8e-07: d. reduced by 0.00% (2.1734e-08) (took 0.72411 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 2121.51083\n",
      "   1.5% for generation (31.27089)\n",
      "   11.0% for spherical prediction (233.76282)\n",
      "   78.4% for prediction (1663.04005)\n",
      "   0.0% for hyperparameter update (0.70925)\n",
      "   9.1% for the rest (192.72783)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.52754774e-03 6.15931780e-04 2.04192268e-04 2.60367990e-04\n",
      " 3.55072021e-04 5.48428959e-05 7.90917144e-05 8.69520009e-05\n",
      " 9.42077166e-05 5.94210625e-05 3.85784906e-05 1.66851613e-04\n",
      " 5.68945732e-05 1.28919981e-04 9.55666436e-05 1.97105110e-05\n",
      " 1.43760628e-05 2.79155778e-05 2.65139952e-05 1.94364786e-05\n",
      " 2.82712534e-05 2.37837311e-05 1.44688775e-05 1.35511445e-05\n",
      " 1.32234602e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.1887482  0.1236358  0.07355392 0.07075356 0.05167229 0.04915191\n",
      " 0.04286112 0.04181005 0.03368098 0.03108752 0.02934649 0.02851853\n",
      " 0.04016053 0.03512883 0.03735535 0.03112258 0.02954691 0.03387515\n",
      " 0.03107497 0.01808062 0.01709494 0.01893941 0.01606127 0.01572055\n",
      " 0.01686603]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.2875 0.1    0.0725 0.06   0.05   0.05   0.035  0.0275 0.0225 0.0375\n",
      " 0.0175 0.0325 0.0275 0.0225 0.02   0.0175 0.03   0.015  0.0125 0.0075\n",
      " 0.005  0.0175 0.0075 0.0075 0.01   0.0075]\n",
      "Using batch size   1, an average step would have taken 2.48945 = 0.03290 + 2.45656 seconds\n",
      "Using batch size   2, an average step would have taken 1.68599 = 0.00882 + 1.67716 seconds\n",
      "Using batch size   3, an average step would have taken 1.04995 = 0.00350 + 1.04645 seconds\n",
      "Using batch size   4, an average step would have taken 1.04565 = 0.00437 + 1.04128 seconds\n",
      "Using batch size   5, an average step would have taken 0.76480 = 0.00522 + 0.75958 seconds\n",
      "Using batch size   6, an average step would have taken 0.77977 = 0.00155 + 0.77821 seconds\n",
      "Using batch size   7, an average step would have taken 0.70108 = 0.00146 + 0.69962 seconds\n",
      "Using batch size   8, an average step would have taken 0.71337 = 0.00211 + 0.71126 seconds\n",
      "Using batch size   9, an average step would have taken 0.57485 = 0.00150 + 0.57334 seconds\n",
      "Using batch size  10, an average step would have taken 0.56357 = 0.00151 + 0.56206 seconds\n",
      "Using batch size  11, an average step would have taken 0.55239 = 0.00083 + 0.55156 seconds\n",
      "Using batch size  12, an average step would have taken 0.56158 = 0.00368 + 0.55789 seconds\n",
      "Using batch size  13, an average step would have taken 0.67601 = 0.00163 + 0.67438 seconds\n",
      "Using batch size  14, an average step would have taken 0.63098 = 0.00199 + 0.62900 seconds\n",
      "Using batch size  15, an average step would have taken 0.68869 = 0.00168 + 0.68701 seconds\n",
      "Using batch size  16, an average step would have taken 0.61303 = 0.00064 + 0.61239 seconds\n",
      "Using batch size  17, an average step would have taken 0.62404 = 0.00050 + 0.62355 seconds\n",
      "Using batch size  18, an average step would have taken 0.71546 = 0.00070 + 0.71476 seconds\n",
      "Using batch size  19, an average step would have taken 0.69205 = 0.00062 + 0.69143 seconds\n",
      "Using batch size  20, an average step would have taken 0.44980 = 0.00099 + 0.44881 seconds\n",
      "Using batch size  21, an average step would have taken 0.45049 = 0.00093 + 0.44956 seconds\n",
      "Using batch size  22, an average step would have taken 0.48634 = 0.00071 + 0.48562 seconds\n",
      "Using batch size  23, an average step would have taken 0.44554 = 0.00071 + 0.44483 seconds\n",
      "Using batch size  24, an average step would have taken 0.43404 = 0.00107 + 0.43297 seconds\n",
      "Using batch size  25, an average step would have taken 0.42198 = 0.00033 + 0.42165 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.42198\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 5.9x\n",
      "improvement compared to smallest batch size (1): 5.9x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 800 steps, after step 3900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Success rate too low, decreasing source step:  0.27 ( 75), 0.17 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.40 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.30 (30)\n",
      "Step 3200: 1.39172e-02, stepsizes = 8.9e-05/2.3e-07:  (took 0.52095 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.11 (100), 0.20 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.37 (30)\n",
      "Step 3300: 1.39169e-02, stepsizes = 4.0e-05/1.0e-07:  (took 0.54615 seconds)\n",
      "  Success rate too low, decreasing source step:  0.28 ( 50), 0.13 (30)\n",
      "Step 3320: 1.39168e-02, stepsizes = 4.0e-05/6.9e-08: \n",
      "Looks like attack has converged after 3321 steps for the first time. Resetting steps to be sure.\n",
      "  Boundary too non-linear, decreasing steps: 0.14 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), -1.00 ( 0)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 1)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 3)\n",
      "  Boundary too non-linear, decreasing steps: 0.07 (100), 0.00 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.00 (23)\n",
      "  Success rate too low, decreasing source step:  0.26 ( 50), 0.00 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.00 ( 2)\n",
      "  Success rate too low, decreasing source step:  0.29 (100), 0.00 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.00 (19)\n",
      "  Success rate too low, decreasing source step:  0.22 ( 50), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.28 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.32 (100), 0.00 (30)\n",
      "Step 3400: 1.39168e-02, stepsizes = 2.3e-05/2.0e-06:  (took 0.66678 seconds)\n",
      "  Success rate too low, decreasing source step:  0.32 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.25 (100), 0.00 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.00 (18)\n",
      "  Success rate too low, decreasing source step:  0.28 ( 50), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.31 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.31 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.29 (100), 0.00 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.00 (14)\n",
      "Step 3430: 1.39168e-02, stepsizes = 1.0e-05/7.8e-08: \n",
      "Looks like attack has converged after 3431 steps, 100 remaining\n",
      "Step 3431: 1.39168e-02, stepsizes = 1.0e-05/7.8e-08: \n",
      "Looks like attack has converged after 3432 steps, 99 remaining\n",
      "  Success rate too low, decreasing source step:  0.38 ( 50), 0.00 (30)\n",
      "Step 3432: 1.39168e-02, stepsizes = 1.0e-05/5.2e-08: \n",
      "Looks like attack has converged after 3433 steps, 98 remaining\n",
      "Step 3433: 1.39168e-02, stepsizes = 1.0e-05/5.2e-08: \n",
      "Looks like attack has converged after 3434 steps, 97 remaining\n",
      "Step 3434: 1.39168e-02, stepsizes = 1.0e-05/5.2e-08: \n",
      "Looks like attack has converged after 3435 steps, 96 remaining\n",
      "Step 3435: 1.39168e-02, stepsizes = 1.0e-05/5.2e-08: \n",
      "Looks like attack has converged after 3436 steps, 95 remaining\n",
      "  Success rate too low, decreasing source step:  0.32 (100), 0.00 (30)\n",
      "Step 3436: 1.39168e-02, stepsizes = 1.0e-05/3.5e-08: \n",
      "Looks like attack has converged after 3437 steps, 94 remaining\n",
      "Step 3437: 1.39168e-02, stepsizes = 1.0e-05/3.5e-08: \n",
      "Looks like attack has converged after 3438 steps, 93 remaining\n",
      "Step 3438: 1.39168e-02, stepsizes = 1.0e-05/3.5e-08: \n",
      "Looks like attack has converged after 3439 steps, 92 remaining\n",
      "Step 3439: 1.39168e-02, stepsizes = 1.0e-05/3.5e-08: \n",
      "Looks like attack has converged after 3440 steps, 91 remaining\n",
      "  Success rate too low, decreasing source step:  0.31 (100), 0.00 (30)\n",
      "Step 3440: 1.39168e-02, stepsizes = 1.0e-05/2.3e-08: \n",
      "Looks like attack has converged after 3441 steps, 90 remaining\n",
      "Step 3441: 1.39168e-02, stepsizes = 1.0e-05/2.3e-08: \n",
      "Looks like attack has converged after 3442 steps, 89 remaining\n",
      "Step 3442: 1.39168e-02, stepsizes = 1.0e-05/2.3e-08: \n",
      "Looks like attack has converged after 3443 steps, 88 remaining\n",
      "Step 3443: 1.39168e-02, stepsizes = 1.0e-05/2.3e-08: \n",
      "Looks like attack has converged after 3444 steps, 87 remaining\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.07 (30)\n",
      "Step 3444: 1.39168e-02, stepsizes = 1.0e-05/1.5e-08: \n",
      "Looks like attack has converged after 3445 steps, 86 remaining\n",
      "Step 3445: 1.39168e-02, stepsizes = 1.0e-05/1.5e-08: \n",
      "Looks like attack has converged after 3446 steps, 85 remaining\n",
      "Step 3446: 1.39168e-02, stepsizes = 1.0e-05/1.5e-08: \n",
      "Looks like attack has converged after 3447 steps, 84 remaining\n",
      "Step 3447: 1.39168e-02, stepsizes = 1.0e-05/1.5e-08: \n",
      "Looks like attack has converged after 3448 steps, 83 remaining\n",
      "Step 3448: 1.39168e-02, stepsizes = 1.0e-05/1.5e-08: \n",
      "Looks like attack has converged after 3449 steps, 82 remaining\n",
      "  Success rate too low, decreasing source step:  0.25 (100), 0.00 (30)\n",
      "Step 3449: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3450 steps, 81 remaining\n",
      "Step 3450: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3451 steps, 80 remaining\n",
      "Step 3451: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3452 steps, 79 remaining\n",
      "Step 3452: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3453 steps, 78 remaining\n",
      "Step 3453: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3454 steps, 77 remaining\n",
      "Step 3454: 1.39168e-02, stepsizes = 1.0e-05/1.0e-08: \n",
      "Looks like attack has converged after 3455 steps, 76 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.03 (30)\n",
      "  Success rate too low, decreasing source step:  0.18 (100), 0.03 (30)\n",
      "Step 3455: 1.39168e-02, stepsizes = 6.8e-06/4.6e-09: \n",
      "Looks like attack has converged after 3456 steps, 75 remaining\n",
      "Step 3456: 1.39168e-02, stepsizes = 6.8e-06/4.6e-09: \n",
      "Looks like attack has converged after 3457 steps, 74 remaining\n",
      "Step 3457: 1.39168e-02, stepsizes = 6.8e-06/4.6e-09: \n",
      "Looks like attack has converged after 3458 steps, 73 remaining\n",
      "  Success rate too low, decreasing source step:  0.48 ( 75), 0.03 (30)\n",
      "Step 3458: 1.39168e-02, stepsizes = 6.8e-06/3.1e-09: \n",
      "Looks like attack has converged after 3459 steps, 72 remaining\n",
      "Step 3459: 1.39168e-02, stepsizes = 6.8e-06/3.1e-09: \n",
      "Looks like attack has converged after 3460 steps, 71 remaining\n",
      "  Boundary too linear, increasing steps:     0.55 (100), 0.31 (29)\n",
      "Step 3460: 1.39168e-02, stepsizes = 1.0e-05/4.6e-09: \n",
      "Looks like attack has converged after 3461 steps, 70 remaining\n",
      "Step 3461: 1.39168e-02, stepsizes = 1.0e-05/4.6e-09: \n",
      "Looks like attack has converged after 3462 steps, 69 remaining\n",
      "Step 3462: 1.39168e-02, stepsizes = 1.0e-05/4.6e-09: \n",
      "Looks like attack has converged after 3463 steps, 68 remaining\n",
      "Step 3463: 1.39168e-02, stepsizes = 1.0e-05/4.6e-09: \n",
      "Looks like attack has converged after 3464 steps, 67 remaining\n",
      "  Success rate too low, decreasing source step:  0.45 (100), 0.17 (30)\n",
      "Step 3464: 1.39168e-02, stepsizes = 1.0e-05/3.1e-09: \n",
      "Looks like attack has converged after 3465 steps, 66 remaining\n",
      "Step 3465: 1.39168e-02, stepsizes = 1.0e-05/3.1e-09: \n",
      "Looks like attack has converged after 3466 steps, 65 remaining\n",
      "Step 3466: 1.39168e-02, stepsizes = 1.0e-05/3.1e-09: \n",
      "Looks like attack has converged after 3467 steps, 64 remaining\n",
      "Step 3467: 1.39168e-02, stepsizes = 1.0e-05/3.1e-09: \n",
      "Looks like attack has converged after 3468 steps, 63 remaining\n",
      "  Success rate too low, decreasing source step:  0.35 (100), 0.07 (30)\n",
      "Step 3468: 1.39168e-02, stepsizes = 1.0e-05/2.0e-09: \n",
      "Looks like attack has converged after 3469 steps, 62 remaining\n",
      "Step 3469: 1.39168e-02, stepsizes = 1.0e-05/2.0e-09: \n",
      "Looks like attack has converged after 3470 steps, 61 remaining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3470: 1.39168e-02, stepsizes = 1.0e-05/2.0e-09: \n",
      "Looks like attack has converged after 3471 steps, 60 remaining\n",
      "  Boundary too linear, increasing steps:     0.59 (100), 0.23 (30)\n",
      "Step 3471: 1.39168e-02, stepsizes = 1.5e-05/3.1e-09: \n",
      "Looks like attack has converged after 3472 steps, 59 remaining\n",
      "Step 3472: 1.39168e-02, stepsizes = 1.5e-05/3.1e-09: \n",
      "Looks like attack has converged after 3473 steps, 58 remaining\n",
      "  Success rate too high, increasing source step: 0.68 ( 50), 0.53 (30)\n",
      "Step 3473: 1.39168e-02, stepsizes = 1.5e-05/4.6e-09: \n",
      "Looks like attack has converged after 3474 steps, 57 remaining\n",
      "Step 3474: 1.39168e-02, stepsizes = 1.5e-05/4.6e-09: \n",
      "Looks like attack has converged after 3475 steps, 56 remaining\n",
      "  Boundary too linear, increasing steps:     0.67 (100), 0.77 (30)\n",
      "  Success rate too high, increasing source step: 0.67 (100), 0.77 (30)\n",
      "Step 3475: 1.39168e-02, stepsizes = 2.3e-05/1.0e-08: \n",
      "Looks like attack has converged after 3476 steps, 55 remaining\n",
      "Step 3476: 1.39168e-02, stepsizes = 2.3e-05/1.0e-08: \n",
      "Looks like attack has converged after 3477 steps, 54 remaining\n",
      "  Success rate too high, increasing source step: 0.76 ( 50), 0.80 (30)\n",
      "Step 3477: 1.39168e-02, stepsizes = 2.3e-05/1.5e-08: \n",
      "Looks like attack has converged after 3478 steps, 53 remaining\n",
      "Step 3478: 1.39168e-02, stepsizes = 2.3e-05/1.5e-08: \n",
      "Looks like attack has converged after 3479 steps, 52 remaining\n",
      "  Boundary too linear, increasing steps:     0.65 (100), 0.67 (27)\n",
      "Step 3479: 1.39168e-02, stepsizes = 3.4e-05/2.3e-08: \n",
      "Looks like attack has converged after 3480 steps, 51 remaining\n",
      "  Success rate too high, increasing source step: 0.28 ( 25), 0.63 (30)\n",
      "Step 3480: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3481 steps, 50 remaining\n",
      "Step 3481: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3482 steps, 49 remaining\n",
      "Step 3482: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3483 steps, 48 remaining\n",
      "Step 3483: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3484 steps, 47 remaining\n",
      "Step 3484: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3485 steps, 46 remaining\n",
      "Step 3485: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3486 steps, 45 remaining\n",
      "Step 3486: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3487 steps, 44 remaining\n",
      "Step 3487: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3488 steps, 43 remaining\n",
      "Step 3488: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3489 steps, 42 remaining\n",
      "Step 3489: 1.39168e-02, stepsizes = 3.4e-05/3.5e-08: \n",
      "Looks like attack has converged after 3490 steps, 41 remaining\n",
      "  Success rate too high, increasing source step: 0.37 (100), 0.53 (30)\n",
      "Step 3490: 1.39168e-02, stepsizes = 3.4e-05/5.2e-08: \n",
      "Looks like attack has converged after 3491 steps, 40 remaining\n",
      "Step 3491: 1.39168e-02, stepsizes = 3.4e-05/5.2e-08: \n",
      "Looks like attack has converged after 3492 steps, 39 remaining\n",
      "Step 3492: 1.39168e-02, stepsizes = 3.4e-05/5.2e-08: \n",
      "Looks like attack has converged after 3493 steps, 38 remaining\n",
      "  Boundary too linear, increasing steps:     0.61 (100), 0.53 (30)\n",
      "  Success rate too high, increasing source step: 0.61 (100), 0.53 (30)\n",
      "Step 3493: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3494 steps, 37 remaining\n",
      "Step 3494: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3495 steps, 36 remaining\n",
      "Step 3495: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3496 steps, 35 remaining\n",
      "Step 3496: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3497 steps, 34 remaining\n",
      "Step 3497: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3498 steps, 33 remaining\n",
      "Step 3498: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3499 steps, 32 remaining\n",
      "Step 3499: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3500 steps, 31 remaining\n",
      "Step 3500: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: d. reduced by 0.00% (3.2660e-09) (took 0.58886 seconds)\n",
      "Step 3500: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3501 steps, 30 remaining\n",
      "Step 3501: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3502 steps, 29 remaining\n",
      "Step 3502: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3503 steps, 28 remaining\n",
      "Step 3503: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3504 steps, 27 remaining\n",
      "  Success rate too low, decreasing source step:  0.31 (100), 0.13 (30)\n",
      "Step 3504: 1.39168e-02, stepsizes = 5.1e-05/7.8e-08: \n",
      "Looks like attack has converged after 3505 steps, 26 remaining\n",
      "Step 3505: 1.39168e-02, stepsizes = 5.1e-05/7.8e-08: \n",
      "Looks like attack has converged after 3506 steps, 25 remaining\n",
      "Step 3506: 1.39168e-02, stepsizes = 5.1e-05/7.8e-08: \n",
      "Looks like attack has converged after 3507 steps, 24 remaining\n",
      "Step 3507: 1.39168e-02, stepsizes = 5.1e-05/7.8e-08: \n",
      "Looks like attack has converged after 3508 steps, 23 remaining\n",
      "Step 3508: 1.39168e-02, stepsizes = 5.1e-05/7.8e-08: \n",
      "Looks like attack has converged after 3509 steps, 22 remaining\n",
      "  Success rate too high, increasing source step: 0.37 (100), 0.53 (30)\n",
      "Step 3509: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3510 steps, 21 remaining\n",
      "Step 3510: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3511 steps, 20 remaining\n",
      "Step 3511: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3512 steps, 19 remaining\n",
      "Step 3512: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3513 steps, 18 remaining\n",
      "Step 3513: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3514 steps, 17 remaining\n",
      "Step 3514: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3515 steps, 16 remaining\n",
      "Step 3515: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3516 steps, 15 remaining\n",
      "Step 3516: 1.39168e-02, stepsizes = 5.1e-05/1.2e-07: \n",
      "Looks like attack has converged after 3517 steps, 14 remaining\n",
      "  Success rate too high, increasing source step: 0.41 (100), 0.53 (30)\n",
      "Step 3517: 1.39167e-02, stepsizes = 5.1e-05/1.8e-07: \n",
      "Looks like attack has converged after 3518 steps, 13 remaining\n",
      "Step 3518: 1.39167e-02, stepsizes = 5.1e-05/1.8e-07: \n",
      "Looks like attack has converged after 3519 steps, 12 remaining\n",
      "Step 3519: 1.39167e-02, stepsizes = 5.1e-05/1.8e-07: \n",
      "Looks like attack has converged after 3520 steps, 11 remaining\n",
      "  Boundary too linear, increasing steps:     0.51 (100), 0.30 (30)\n",
      "Step 3520: 1.39167e-02, stepsizes = 7.7e-05/2.6e-07: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/foolbox/attacks/boundary_attack.py:357: UserWarning: Attack has not converged!\n",
      "  warnings.warn('Attack has not converged!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.07 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "Step 3600: 1.39167e-02, stepsizes = 3.4e-05/1.2e-07:  (took 0.44585 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.27 (30)\n",
      "Step 3640: 1.39167e-02, stepsizes = 2.3e-05/7.8e-08: \n",
      "Looks like attack has converged after 3641 steps for the first time. Resetting steps to be sure.\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.20 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.20 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.20 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.20 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.20 (30)\n",
      "  Success rate too low, decreasing source step:  0.04 ( 25), 0.17 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), -1.00 ( 0)\n",
      "Step 3700: 1.39167e-02, stepsizes = 2.3e-05/1.5e-05:  (took 0.43982 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 2)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.00 ( 2)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 4)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 5)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 6)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 ( 7)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.00 ( 7)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 9)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.00 ( 9)\n",
      "  Boundary too non-linear, decreasing steps: 0.05 (100), 0.00 (14)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.00 (15)\n",
      "  Boundary too non-linear, decreasing steps: 0.08 (100), 0.00 (23)\n",
      "  Success rate too low, decreasing source step:  0.12 ( 75), 0.00 (30)\n",
      "Step 3751: 1.39167e-02, stepsizes = 1.8e-07/7.8e-08: \n",
      "Looks like attack has converged after 3752 steps, 100 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.10 (100), 0.00 ( 1)\n",
      "Step 3752: 1.39167e-02, stepsizes = 1.2e-07/5.2e-08: \n",
      "Looks like attack has converged after 3753 steps, 99 remaining\n",
      "Step 3753: 1.39167e-02, stepsizes = 1.2e-07/5.2e-08: \n",
      "Looks like attack has converged after 3754 steps, 98 remaining\n",
      "Step 3754: 1.39167e-02, stepsizes = 1.2e-07/5.2e-08: \n",
      "Looks like attack has converged after 3755 steps, 97 remaining\n",
      "Step 3755: 1.39167e-02, stepsizes = 1.2e-07/5.2e-08: \n",
      "Looks like attack has converged after 3756 steps, 96 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.06 (100), 0.00 ( 7)\n",
      "Step 3756: 1.39167e-02, stepsizes = 7.8e-08/3.5e-08: \n",
      "Looks like attack has converged after 3757 steps, 95 remaining\n",
      "Step 3757: 1.39167e-02, stepsizes = 7.8e-08/3.5e-08: \n",
      "Looks like attack has converged after 3758 steps, 94 remaining\n",
      "Step 3758: 1.39167e-02, stepsizes = 7.8e-08/3.5e-08: \n",
      "Looks like attack has converged after 3759 steps, 93 remaining\n",
      "Step 3759: 1.39167e-02, stepsizes = 7.8e-08/3.5e-08: \n",
      "Looks like attack has converged after 3760 steps, 92 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.00 (20)\n",
      "Step 3760: 1.39167e-02, stepsizes = 5.2e-08/2.3e-08: \n",
      "Looks like attack has converged after 3761 steps, 91 remaining\n",
      "Step 3761: 1.39167e-02, stepsizes = 5.2e-08/2.3e-08: \n",
      "Looks like attack has converged after 3762 steps, 90 remaining\n",
      "Step 3762: 1.39167e-02, stepsizes = 5.2e-08/2.3e-08: \n",
      "Looks like attack has converged after 3763 steps, 89 remaining\n",
      "Step 3763: 1.39167e-02, stepsizes = 5.2e-08/2.3e-08: \n",
      "Looks like attack has converged after 3764 steps, 88 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.13 (100), 0.00 (30)\n",
      "Step 3764: 1.39167e-02, stepsizes = 3.5e-08/1.0e-08: \n",
      "Looks like attack has converged after 3765 steps, 87 remaining\n",
      "Step 3765: 1.39167e-02, stepsizes = 3.5e-08/1.0e-08: \n",
      "Looks like attack has converged after 3766 steps, 86 remaining\n",
      "Step 3766: 1.39167e-02, stepsizes = 3.5e-08/1.0e-08: \n",
      "Looks like attack has converged after 3767 steps, 85 remaining\n",
      "Step 3767: 1.39167e-02, stepsizes = 3.5e-08/1.0e-08: \n",
      "Looks like attack has converged after 3768 steps, 84 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.09 (100), 0.00 ( 9)\n",
      "Step 3768: 1.39167e-02, stepsizes = 2.3e-08/6.9e-09: \n",
      "Looks like attack has converged after 3769 steps, 83 remaining\n",
      "Step 3769: 1.39167e-02, stepsizes = 2.3e-08/6.9e-09: \n",
      "Looks like attack has converged after 3770 steps, 82 remaining\n",
      "Step 3770: 1.39167e-02, stepsizes = 2.3e-08/6.9e-09: \n",
      "Looks like attack has converged after 3771 steps, 81 remaining\n",
      "Step 3771: 1.39167e-02, stepsizes = 2.3e-08/6.9e-09: \n",
      "Looks like attack has converged after 3772 steps, 80 remaining\n",
      "  Success rate too low, decreasing source step:  0.21 (100), 0.00 (30)\n",
      "Step 3772: 1.39167e-02, stepsizes = 2.3e-08/4.6e-09: \n",
      "Looks like attack has converged after 3773 steps, 79 remaining\n",
      "Step 3773: 1.39167e-02, stepsizes = 2.3e-08/4.6e-09: \n",
      "Looks like attack has converged after 3774 steps, 78 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.00 ( 8)\n",
      "Step 3774: 1.39167e-02, stepsizes = 1.5e-08/3.1e-09: \n",
      "Looks like attack has converged after 3775 steps, 77 remaining\n",
      "Step 3775: 1.39167e-02, stepsizes = 1.5e-08/3.1e-09: \n",
      "Looks like attack has converged after 3776 steps, 76 remaining\n",
      "Step 3776: 1.39167e-02, stepsizes = 1.5e-08/3.1e-09: \n",
      "Looks like attack has converged after 3777 steps, 75 remaining\n",
      "Step 3777: 1.39167e-02, stepsizes = 1.5e-08/3.1e-09: \n",
      "Looks like attack has converged after 3778 steps, 74 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.00 (26)\n",
      "Step 3778: 1.39167e-02, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 3779 steps, 73 remaining\n",
      "  Success rate too low, decreasing source step:  0.20 ( 25), 0.00 (30)\n",
      "Step 3779: 1.39167e-02, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 3780 steps, 72 remaining\n",
      "Step 3780: 1.39167e-02, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 3781 steps, 71 remaining\n",
      "Step 3781: 1.39167e-02, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 3782 steps, 70 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.00 (13)\n",
      "Step 3782: 1.39167e-02, stepsizes = 6.9e-09/9.0e-10: \n",
      "Looks like attack has converged after 3783 steps, 69 remaining\n",
      "Step 3783: 1.39167e-02, stepsizes = 6.9e-09/9.0e-10: \n",
      "Looks like attack has converged after 3784 steps, 68 remaining\n",
      "Step 3784: 1.39167e-02, stepsizes = 6.9e-09/9.0e-10: \n",
      "Looks like attack has converged after 3785 steps, 67 remaining\n",
      "  Success rate too low, decreasing source step:  0.27 ( 75), 0.00 (30)\n",
      "Step 3785: 1.39167e-02, stepsizes = 6.9e-09/6.0e-10: \n",
      "Looks like attack has converged after 3786 steps, 66 remaining\n",
      "Step 3786: 1.39167e-02, stepsizes = 6.9e-09/6.0e-10: \n",
      "Looks like attack has converged after 3787 steps, 65 remaining\n",
      "Step 3787: 1.39167e-02, stepsizes = 6.9e-09/6.0e-10: \n",
      "Looks like attack has converged after 3788 steps, 64 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.00 ( 9)\n",
      "Step 3788: 1.39167e-02, stepsizes = 4.6e-09/4.0e-10: \n",
      "Looks like attack has converged after 3789 steps, 63 remaining\n",
      "Step 3789: 1.39167e-02, stepsizes = 4.6e-09/4.0e-10: \n",
      "Looks like attack has converged after 3790 steps, 62 remaining\n",
      "Step 3790: 1.39167e-02, stepsizes = 4.6e-09/4.0e-10: \n",
      "Looks like attack has converged after 3791 steps, 61 remaining\n",
      "Step 3791: 1.39167e-02, stepsizes = 4.6e-09/4.0e-10: \n",
      "Looks like attack has converged after 3792 steps, 60 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.14 (100), 0.00 (23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3792: 1.39167e-02, stepsizes = 3.1e-09/2.7e-10: \n",
      "Looks like attack has converged after 3793 steps, 59 remaining\n",
      "Step 3793: 1.39167e-02, stepsizes = 3.1e-09/2.7e-10: \n",
      "Looks like attack has converged after 3794 steps, 58 remaining\n",
      "  Success rate too low, decreasing source step:  0.16 ( 50), 0.00 (30)\n",
      "Step 3794: 1.39167e-02, stepsizes = 3.1e-09/1.8e-10: \n",
      "Looks like attack has converged after 3795 steps, 57 remaining\n",
      "Step 3795: 1.39167e-02, stepsizes = 3.1e-09/1.8e-10: \n",
      "Looks like attack has converged after 3796 steps, 56 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.16 (100), 0.00 ( 8)\n",
      "Step 3796: 1.39167e-02, stepsizes = 2.0e-09/1.2e-10: \n",
      "Looks like attack has converged after 3797 steps, 55 remaining\n",
      "Step 3797: 1.39167e-02, stepsizes = 2.0e-09/1.2e-10: \n",
      "Looks like attack has converged after 3798 steps, 54 remaining\n",
      "Step 3798: 1.39167e-02, stepsizes = 2.0e-09/1.2e-10: \n",
      "Looks like attack has converged after 3799 steps, 53 remaining\n",
      "Step 3799: 1.39167e-02, stepsizes = 2.0e-09/1.2e-10: \n",
      "Looks like attack has converged after 3800 steps, 52 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.00 (23)\n",
      "Step 3800: 1.39167e-02, stepsizes = 1.4e-09/7.9e-11:  (took 0.70639 seconds)\n",
      "Step 3800: 1.39167e-02, stepsizes = 1.4e-09/7.9e-11: \n",
      "Looks like attack has converged after 3801 steps, 51 remaining\n",
      "Step 3801: 1.39167e-02, stepsizes = 1.4e-09/7.9e-11: \n",
      "Looks like attack has converged after 3802 steps, 50 remaining\n",
      "  Success rate too low, decreasing source step:  0.20 ( 50), 0.00 (30)\n",
      "Step 3802: 1.39167e-02, stepsizes = 1.4e-09/5.3e-11: \n",
      "Looks like attack has converged after 3803 steps, 49 remaining\n",
      "Step 3803: 1.39167e-02, stepsizes = 1.4e-09/5.3e-11: \n",
      "Looks like attack has converged after 3804 steps, 48 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.00 ( 5)\n",
      "Step 3804: 1.39167e-02, stepsizes = 9.0e-10/3.5e-11: \n",
      "Looks like attack has converged after 3805 steps, 47 remaining\n",
      "Step 3805: 1.39167e-02, stepsizes = 9.0e-10/3.5e-11: \n",
      "Looks like attack has converged after 3806 steps, 46 remaining\n",
      "Step 3806: 1.39167e-02, stepsizes = 9.0e-10/3.5e-11: \n",
      "Looks like attack has converged after 3807 steps, 45 remaining\n",
      "Step 3807: 1.39167e-02, stepsizes = 9.0e-10/3.5e-11: \n",
      "Looks like attack has converged after 3808 steps, 44 remaining\n",
      "Step 3808: 1.39167e-02, stepsizes = 9.0e-10/3.5e-11: \n",
      "Looks like attack has converged after 3809 steps, 43 remaining\n",
      "  Success rate too low, decreasing source step:  0.20 (100), 0.00 (30)\n",
      "Step 3809: 1.39167e-02, stepsizes = 9.0e-10/2.4e-11: \n",
      "Looks like attack has converged after 3810 steps, 42 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.16 (100), 0.00 ( 4)\n",
      "Step 3810: 1.39167e-02, stepsizes = 6.0e-10/1.6e-11: \n",
      "Looks like attack has converged after 3811 steps, 41 remaining\n",
      "Step 3811: 1.39167e-02, stepsizes = 6.0e-10/1.6e-11: \n",
      "Looks like attack has converged after 3812 steps, 40 remaining\n",
      "Step 3812: 1.39167e-02, stepsizes = 6.0e-10/1.6e-11: \n",
      "Looks like attack has converged after 3813 steps, 39 remaining\n",
      "Step 3813: 1.39167e-02, stepsizes = 6.0e-10/1.6e-11: \n",
      "Looks like attack has converged after 3814 steps, 38 remaining\n",
      "Step 3814: 1.39167e-02, stepsizes = 6.0e-10/1.6e-11: \n",
      "Looks like attack has converged after 3815 steps, 37 remaining\n",
      "  Success rate too low, decreasing source step:  0.21 (100), 0.00 (30)\n",
      "Step 3815: 1.39167e-02, stepsizes = 6.0e-10/1.0e-11: \n",
      "Looks like attack has converged after 3816 steps, 36 remaining\n",
      "Step 3816: 1.39167e-02, stepsizes = 6.0e-10/1.0e-11: \n",
      "Looks like attack has converged after 3817 steps, 35 remaining\n",
      "Step 3817: 1.39167e-02, stepsizes = 6.0e-10/1.0e-11: \n",
      "Looks like attack has converged after 3818 steps, 34 remaining\n",
      "Step 3818: 1.39167e-02, stepsizes = 6.0e-10/1.0e-11: \n",
      "Looks like attack has converged after 3819 steps, 33 remaining\n",
      "Step 3819: 1.39167e-02, stepsizes = 6.0e-10/1.0e-11: \n",
      "Looks like attack has converged after 3820 steps, 32 remaining\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.00 (30)\n",
      "Step 3820: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3821 steps, 31 remaining\n",
      "Step 3821: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3822 steps, 30 remaining\n",
      "Step 3822: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3823 steps, 29 remaining\n",
      "Step 3823: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3824 steps, 28 remaining\n",
      "Step 3824: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3825 steps, 27 remaining\n",
      "Step 3825: 1.39167e-02, stepsizes = 6.0e-10/7.0e-12: \n",
      "Looks like attack has converged after 3826 steps, 26 remaining\n",
      "  Success rate too low, decreasing source step:  0.26 (100), 0.00 (30)\n",
      "Step 3826: 1.39167e-02, stepsizes = 6.0e-10/4.6e-12: \n",
      "Looks like attack has converged after 3827 steps, 25 remaining\n",
      "Step 3827: 1.39167e-02, stepsizes = 6.0e-10/4.6e-12: \n",
      "Looks like attack has converged after 3828 steps, 24 remaining\n",
      "Step 3828: 1.39167e-02, stepsizes = 6.0e-10/4.6e-12: \n",
      "Looks like attack has converged after 3829 steps, 23 remaining\n",
      "Step 3829: 1.39167e-02, stepsizes = 6.0e-10/4.6e-12: \n",
      "Looks like attack has converged after 3830 steps, 22 remaining\n",
      "Step 3830: 1.39167e-02, stepsizes = 6.0e-10/4.6e-12: \n",
      "Looks like attack has converged after 3831 steps, 21 remaining\n",
      "  Success rate too low, decreasing source step:  0.29 (100), 0.00 (30)\n",
      "Step 3831: 1.39167e-02, stepsizes = 6.0e-10/3.1e-12: \n",
      "Looks like attack has converged after 3832 steps, 20 remaining\n",
      "Step 3832: 1.39167e-02, stepsizes = 6.0e-10/3.1e-12: \n",
      "Looks like attack has converged after 3833 steps, 19 remaining\n",
      "Step 3833: 1.39167e-02, stepsizes = 6.0e-10/3.1e-12: \n",
      "Looks like attack has converged after 3834 steps, 18 remaining\n",
      "Step 3834: 1.39167e-02, stepsizes = 6.0e-10/3.1e-12: \n",
      "Looks like attack has converged after 3835 steps, 17 remaining\n",
      "Step 3835: 1.39167e-02, stepsizes = 6.0e-10/3.1e-12: \n",
      "Looks like attack has converged after 3836 steps, 16 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.00 (28)\n",
      "Step 3836: 1.39167e-02, stepsizes = 4.0e-10/2.1e-12: \n",
      "Looks like attack has converged after 3837 steps, 15 remaining\n",
      "  Success rate too low, decreasing source step:  0.24 ( 25), 0.00 (30)\n",
      "Step 3837: 1.39167e-02, stepsizes = 4.0e-10/1.4e-12: \n",
      "Looks like attack has converged after 3838 steps, 14 remaining\n",
      "Step 3838: 1.39167e-02, stepsizes = 4.0e-10/1.4e-12: \n",
      "Looks like attack has converged after 3839 steps, 13 remaining\n",
      "Step 3839: 1.39167e-02, stepsizes = 4.0e-10/1.4e-12: \n",
      "Looks like attack has converged after 3840 steps, 12 remaining\n",
      "Step 3840: 1.39167e-02, stepsizes = 4.0e-10/1.4e-12: \n",
      "Looks like attack has converged after 3841 steps, 11 remaining\n",
      "Step 3841: 1.39167e-02, stepsizes = 4.0e-10/1.4e-12: \n",
      "Looks like attack has converged after 3842 steps, 10 remaining\n",
      "  Success rate too low, decreasing source step:  0.28 (100), 0.00 (30)\n",
      "Step 3842: 1.39167e-02, stepsizes = 4.0e-10/9.2e-13: \n",
      "Looks like attack has converged after 3843 steps, 9 remaining\n",
      "Step 3843: 1.39167e-02, stepsizes = 4.0e-10/9.2e-13: \n",
      "Looks like attack has converged after 3844 steps, 8 remaining\n",
      "Step 3844: 1.39167e-02, stepsizes = 4.0e-10/9.2e-13: \n",
      "Looks like attack has converged after 3845 steps, 7 remaining\n",
      "Step 3845: 1.39167e-02, stepsizes = 4.0e-10/9.2e-13: \n",
      "Looks like attack has converged after 3846 steps, 6 remaining\n",
      "  Success rate too low, decreasing source step:  0.35 (100), 0.00 (30)\n",
      "Step 3846: 1.39167e-02, stepsizes = 4.0e-10/6.1e-13: \n",
      "Looks like attack has converged after 3847 steps, 5 remaining\n",
      "Step 3847: 1.39167e-02, stepsizes = 4.0e-10/6.1e-13: \n",
      "Looks like attack has converged after 3848 steps, 4 remaining\n",
      "Step 3848: 1.39167e-02, stepsizes = 4.0e-10/6.1e-13: \n",
      "Looks like attack has converged after 3849 steps, 3 remaining\n",
      "Step 3849: 1.39167e-02, stepsizes = 4.0e-10/6.1e-13: \n",
      "Looks like attack has converged after 3850 steps, 2 remaining\n",
      "Step 3850: 1.39167e-02, stepsizes = 4.0e-10/6.1e-13: \n",
      "Looks like attack has converged after 3851 steps, 1 remaining\n",
      "Time since beginning: 2527.47226\n",
      "   1.5% for generation (37.22626)\n",
      "   16.5% for spherical prediction (417.51385)\n",
      "   70.7% for prediction (1786.45050)\n",
      "   0.1% for hyperparameter update (1.26638)\n",
      "   11.3% for the rest (285.01528)\n"
     ]
    }
   ],
   "source": [
    "attack_params = {\n",
    "    'iterations': 10000,\n",
    "    'max_directions': 25,\n",
    "    'starting_point': None,\n",
    "    'initialization_attack': None,\n",
    "    'log_every_n_steps': 100,\n",
    "    'spherical_step': 1.0,\n",
    "    'source_step': 0.1,\n",
    "    'step_adaptation': 1.5,\n",
    "    'batch_size': 1,\n",
    "    'tune_batch_size': True, \n",
    "    'threaded_rnd': True, \n",
    "    'threaded_gen': True, \n",
    "    'alternative_generator': False\n",
    "}\n",
    "\n",
    "num = 1\n",
    "x_adv = np.zeros_like(x_test[:num].numpy())\n",
    "for i in range(num):\n",
    "    x_adv[i] = attack(x_test[i].numpy(), label=y_test[i].numpy(), \n",
    "                      unpack=True, verbose=True, **attack_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = dknn.classify(torch.tensor(x_adv))\n",
    "print((y_pred.argmax(1) == y_test[:num].numpy()).sum() / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3031292\n"
     ]
    }
   ],
   "source": [
    "dist = np.sqrt(np.sum((x_adv - x_test[:num].numpy())**2, (1, 2, 3)))\n",
    "print(dist.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f35985edcc0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[0].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neither starting_point nor initialization_attack given. Falling back to BlendedUniformNoiseAttack for initialization.\n",
      "Initial spherical_step = 0.50, source_step = 0.05\n",
      "Using 4 threads to create random numbers\n",
      "Step 0: 1.18628e-01, stepsizes = 5.0e-01/5.0e-02: \n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.33 ( 3)\n",
      "  Boundary too non-linear, decreasing steps: 0.06 (100), 0.22 ( 9)\n",
      "Step 100: 5.23035e-02, stepsizes = 2.2e-01/2.2e-02:  (took 6.82330 seconds)\n",
      "Initializing generation and prediction time measurements. This can take a few seconds.\n",
      "During initialization, a better adversarial has been found. Continuing from there.\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 769.55649\n",
      "   0.8% for generation (6.29083)\n",
      "   16.2% for spherical prediction (125.01746)\n",
      "   69.0% for prediction (531.29734)\n",
      "   0.0% for hyperparameter update (0.00170)\n",
      "   13.9% for the rest (106.94915)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 1.38908625e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 3.34980807e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 4.15630341e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01644211 0.0179603  0.01686172 0.02211609\n",
      " 0.02132043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.91 0.   0.   0.   0.   0.03 0.01 0.   0.   0.   0.   0.   0.   0.01\n",
      " 0.01 0.   0.01 0.01 0.   0.   0.   0.   0.01 0.   0.   0.  ]\n",
      "Using batch size   1, an average step would have taken 5.16179 = 0.06095 + 5.10084 seconds\n",
      "Using batch size   2, an average step would have taken 3.83510 = 0.00604 + 3.82905 seconds\n",
      "Using batch size   3, an average step would have taken 2.54300 = 0.01353 + 2.52947 seconds\n",
      "Using batch size   4, an average step would have taken 2.14147 = 0.00554 + 2.13593 seconds\n",
      "Using batch size   5, an average step would have taken 1.44762 = 0.00206 + 1.44556 seconds\n",
      "Using batch size   6, an average step would have taken 1.54937 = 0.00476 + 1.54461 seconds\n",
      "Using batch size   7, an average step would have taken 1.41372 = 0.00422 + 1.40950 seconds\n",
      "Using batch size   8, an average step would have taken 1.23135 = 0.00370 + 1.22764 seconds\n",
      "Using batch size   9, an average step would have taken 0.98253 = 0.00213 + 0.98040 seconds\n",
      "Using batch size  10, an average step would have taken 0.93446 = 0.00110 + 0.93336 seconds\n",
      "Using batch size  11, an average step would have taken 0.91873 = 0.00333 + 0.91540 seconds\n",
      "Using batch size  12, an average step would have taken 0.91625 = 0.00402 + 0.91223 seconds\n",
      "Using batch size  13, an average step would have taken 0.85709 = 0.00113 + 0.85596 seconds\n",
      "Using batch size  14, an average step would have taken 0.87574 = 0.00138 + 0.87436 seconds\n",
      "Using batch size  15, an average step would have taken 0.93166 = 0.00097 + 0.93069 seconds\n",
      "Using batch size  16, an average step would have taken 0.90052 = 0.00107 + 0.89944 seconds\n",
      "Using batch size  17, an average step would have taken 0.87820 = 0.00082 + 0.87738 seconds\n",
      "Using batch size  18, an average step would have taken 0.94862 = 0.00145 + 0.94717 seconds\n",
      "Using batch size  19, an average step would have taken 0.97661 = 0.00125 + 0.97536 seconds\n",
      "Using batch size  20, an average step would have taken 0.67108 = 0.00116 + 0.66992 seconds\n",
      "Using batch size  21, an average step would have taken 0.65594 = 0.00121 + 0.65473 seconds\n",
      "Using batch size  22, an average step would have taken 0.67406 = 0.00162 + 0.67244 seconds\n",
      "Using batch size  23, an average step would have taken 0.67710 = 0.00073 + 0.67637 seconds\n",
      "Using batch size  24, an average step would have taken 0.72881 = 0.00283 + 0.72598 seconds\n",
      "Using batch size  25, an average step would have taken 0.53405 = 0.00104 + 0.53301 seconds\n",
      "batch size was 1, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.53405\n",
      "improvement compared to old batch size (1): 9.7x\n",
      "improvement compared to worst batch size (1): 9.7x\n",
      "improvement compared to smallest batch size (1): 9.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 200\n",
      "  Boundary too non-linear, decreasing steps: 0.07 (100), 0.12 (16)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.07 (29)\n",
      "  Success rate too low, decreasing source step:  0.12 ( 25), 0.03 (30)\n",
      "Step 200: 3.35918e-02, stepsizes = 9.9e-02/6.6e-03:  (took 0.80635 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 819.75231\n",
      "   0.9% for generation (7.11420)\n",
      "   15.8% for spherical prediction (129.40454)\n",
      "   69.9% for prediction (573.16435)\n",
      "   0.0% for hyperparameter update (0.01590)\n",
      "   13.4% for the rest (110.05331)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 1.38908625e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 3.34980807e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 1.34550189e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01644211 0.0179603  0.01686172 0.02211609\n",
      " 0.018723  ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.87 0.01 0.01 0.   0.   0.   0.01 0.01 0.   0.   0.   0.01 0.   0.\n",
      " 0.   0.   0.01 0.   0.   0.02 0.   0.01 0.02 0.   0.   0.02]\n",
      "Using batch size   1, an average step would have taken 5.14659 = 0.06077 + 5.08582 seconds\n",
      "Using batch size   2, an average step would have taken 3.82441 = 0.00599 + 3.81842 seconds\n",
      "Using batch size   3, an average step would have taken 2.53866 = 0.01348 + 2.52518 seconds\n",
      "Using batch size   4, an average step would have taken 2.13039 = 0.00548 + 2.12491 seconds\n",
      "Using batch size   5, an average step would have taken 1.45366 = 0.00207 + 1.45159 seconds\n",
      "Using batch size   6, an average step would have taken 1.55207 = 0.00472 + 1.54735 seconds\n",
      "Using batch size   7, an average step would have taken 1.41416 = 0.00422 + 1.40993 seconds\n",
      "Using batch size   8, an average step would have taken 1.23058 = 0.00366 + 1.22692 seconds\n",
      "Using batch size   9, an average step would have taken 0.99014 = 0.00215 + 0.98799 seconds\n",
      "Using batch size  10, an average step would have taken 0.93446 = 0.00110 + 0.93336 seconds\n",
      "Using batch size  11, an average step would have taken 0.90934 = 0.00329 + 0.90605 seconds\n",
      "Using batch size  12, an average step would have taken 0.90824 = 0.00396 + 0.90428 seconds\n",
      "Using batch size  13, an average step would have taken 0.85709 = 0.00113 + 0.85596 seconds\n",
      "Using batch size  14, an average step would have taken 0.87901 = 0.00139 + 0.87762 seconds\n",
      "Using batch size  15, an average step would have taken 0.93501 = 0.00098 + 0.93403 seconds\n",
      "Using batch size  16, an average step would have taken 0.90374 = 0.00108 + 0.90266 seconds\n",
      "Using batch size  17, an average step would have taken 0.88536 = 0.00083 + 0.88453 seconds\n",
      "Using batch size  18, an average step would have taken 0.95623 = 0.00147 + 0.95476 seconds\n",
      "Using batch size  19, an average step would have taken 0.97661 = 0.00125 + 0.97536 seconds\n",
      "Using batch size  20, an average step would have taken 0.67108 = 0.00116 + 0.66992 seconds\n",
      "Using batch size  21, an average step would have taken 0.65257 = 0.00121 + 0.65136 seconds\n",
      "Using batch size  22, an average step would have taken 0.66793 = 0.00159 + 0.66634 seconds\n",
      "Using batch size  23, an average step would have taken 0.67075 = 0.00072 + 0.67003 seconds\n",
      "Using batch size  24, an average step would have taken 0.72447 = 0.00278 + 0.72169 seconds\n",
      "Using batch size  25, an average step would have taken 0.46841 = 0.00034 + 0.46808 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.46841\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 11.0x\n",
      "improvement compared to smallest batch size (1): 11.0x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.03 (30)\n",
      "  Success rate too low, decreasing source step:  0.19 (100), 0.03 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.16 (100), 0.18 (22)\n",
      "Step 300: 2.30754e-02, stepsizes = 4.4e-02/2.0e-03:  (took 1.00404 seconds)\n",
      "  Success rate too low, decreasing source step:  0.30 ( 50), 0.13 (30)\n",
      "Step 400: 1.86159e-02, stepsizes = 4.4e-02/1.3e-03: d. reduced by 0.26% (4.8519e-05) (took 0.63047 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 920.39514\n",
      "   0.9% for generation (8.68309)\n",
      "   15.0% for spherical prediction (138.45500)\n",
      "   71.3% for prediction (656.50427)\n",
      "   0.0% for hyperparameter update (0.03242)\n",
      "   12.7% for the rest (116.72037)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 1.38908625e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 3.34980807e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 1.28543917e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01644211 0.0179603  0.01686172 0.02211609\n",
      " 0.01858963]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.395 0.12  0.035 0.065 0.05  0.035 0.02  0.03  0.025 0.025 0.035 0.01\n",
      " 0.02  0.02  0.015 0.01  0.01  0.035 0.015 0.01  0.005 0.005 0.005 0.\n",
      " 0.005 0.   ]\n",
      "Using batch size   1, an average step would have taken 3.11704 = 0.03681 + 3.08024 seconds\n",
      "Using batch size   2, an average step would have taken 2.36001 = 0.00333 + 2.35668 seconds\n",
      "Using batch size   3, an average step would have taken 1.58457 = 0.00817 + 1.57641 seconds\n",
      "Using batch size   4, an average step would have taken 1.35419 = 0.00310 + 1.35108 seconds\n",
      "Using batch size   5, an average step would have taken 0.95198 = 0.00136 + 0.95063 seconds\n",
      "Using batch size   6, an average step would have taken 1.01161 = 0.00268 + 1.00894 seconds\n",
      "Using batch size   7, an average step would have taken 0.95095 = 0.00296 + 0.94799 seconds\n",
      "Using batch size   8, an average step would have taken 0.83517 = 0.00201 + 0.83316 seconds\n",
      "Using batch size   9, an average step would have taken 0.67637 = 0.00132 + 0.67505 seconds\n",
      "Using batch size  10, an average step would have taken 0.64636 = 0.00074 + 0.64563 seconds\n",
      "Using batch size  11, an average step would have taken 0.62867 = 0.00216 + 0.62651 seconds\n",
      "Using batch size  12, an average step would have taken 0.64678 = 0.00233 + 0.64445 seconds\n",
      "Using batch size  13, an average step would have taken 0.69574 = 0.00075 + 0.69499 seconds\n",
      "Using batch size  14, an average step would have taken 0.73042 = 0.00093 + 0.72949 seconds\n",
      "Using batch size  15, an average step would have taken 0.77928 = 0.00081 + 0.77847 seconds\n",
      "Using batch size  16, an average step would have taken 0.75372 = 0.00085 + 0.75286 seconds\n",
      "Using batch size  17, an average step would have taken 0.70650 = 0.00059 + 0.70590 seconds\n",
      "Using batch size  18, an average step would have taken 0.76021 = 0.00081 + 0.75940 seconds\n",
      "Using batch size  19, an average step would have taken 0.79883 = 0.00093 + 0.79790 seconds\n",
      "Using batch size  20, an average step would have taken 0.51695 = 0.00094 + 0.51601 seconds\n",
      "Using batch size  21, an average step would have taken 0.48243 = 0.00093 + 0.48151 seconds\n",
      "Using batch size  22, an average step would have taken 0.51790 = 0.00087 + 0.51702 seconds\n",
      "Using batch size  23, an average step would have taken 0.51522 = 0.00057 + 0.51466 seconds\n",
      "Using batch size  24, an average step would have taken 0.61702 = 0.00151 + 0.61551 seconds\n",
      "Using batch size  25, an average step would have taken 0.46506 = 0.00032 + 0.46474 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.46506\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.7x\n",
      "improvement compared to smallest batch size (1): 6.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 800\n",
      "Step 500: 1.51164e-02, stepsizes = 4.4e-02/1.3e-03: d. reduced by 0.26% (3.9398e-05) (took 0.92866 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.37 (30)\n",
      "  Boundary too linear, increasing steps:     0.51 (100), 0.27 (30)\n",
      "Step 600: 1.33413e-02, stepsizes = 4.4e-02/1.3e-03:  (took 0.61772 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.08 (100), 0.23 (30)\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.17 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), -1.00 ( 0)\n",
      "Step 700: 1.24061e-02, stepsizes = 2.0e-02/3.9e-04:  (took 0.51961 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.16 (100), 0.30 (30)\n",
      "Step 800: 1.16133e-02, stepsizes = 1.3e-02/2.6e-04: d. reduced by 0.05% (5.9694e-06) (took 0.84386 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1123.15431\n",
      "   1.1% for generation (11.92368)\n",
      "   14.0% for spherical prediction (157.32430)\n",
      "   73.2% for prediction (822.08008)\n",
      "   0.0% for hyperparameter update (0.11799)\n",
      "   11.7% for the rest (131.70826)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 1.38908625e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 3.34980807e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 1.29160137e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01644211 0.0179603  0.01686172 0.02211609\n",
      " 0.01848043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.29   0.1    0.0775 0.055  0.065  0.0425 0.04   0.0225 0.025  0.04\n",
      " 0.0225 0.02   0.01   0.0225 0.01   0.0225 0.02   0.0175 0.0175 0.02\n",
      " 0.0125 0.02   0.01   0.005  0.01   0.0025]\n",
      "Using batch size   1, an average step would have taken 2.83377 = 0.03346 + 2.80031 seconds\n",
      "Using batch size   2, an average step would have taken 2.15049 = 0.00288 + 2.14761 seconds\n",
      "Using batch size   3, an average step would have taken 1.44521 = 0.00735 + 1.43786 seconds\n",
      "Using batch size   4, an average step would have taken 1.24181 = 0.00269 + 1.23912 seconds\n",
      "Using batch size   5, an average step would have taken 0.88625 = 0.00126 + 0.88499 seconds\n",
      "Using batch size   6, an average step would have taken 0.93304 = 0.00231 + 0.93072 seconds\n",
      "Using batch size   7, an average step would have taken 0.88531 = 0.00279 + 0.88252 seconds\n",
      "Using batch size   8, an average step would have taken 0.77089 = 0.00169 + 0.76920 seconds\n",
      "Using batch size   9, an average step would have taken 0.63527 = 0.00122 + 0.63405 seconds\n",
      "Using batch size  10, an average step would have taken 0.60771 = 0.00069 + 0.60702 seconds\n",
      "Using batch size  11, an average step would have taken 0.58075 = 0.00197 + 0.57879 seconds\n",
      "Using batch size  12, an average step would have taken 0.60620 = 0.00202 + 0.60418 seconds\n",
      "Using batch size  13, an average step would have taken 0.67649 = 0.00070 + 0.67579 seconds\n",
      "Using batch size  14, an average step would have taken 0.71490 = 0.00088 + 0.71403 seconds\n",
      "Using batch size  15, an average step would have taken 0.75918 = 0.00079 + 0.75839 seconds\n",
      "Using batch size  16, an average step would have taken 0.73113 = 0.00082 + 0.73031 seconds\n",
      "Using batch size  17, an average step would have taken 0.68772 = 0.00057 + 0.68715 seconds\n",
      "Using batch size  18, an average step would have taken 0.73928 = 0.00074 + 0.73854 seconds\n",
      "Using batch size  19, an average step would have taken 0.77595 = 0.00089 + 0.77506 seconds\n",
      "Using batch size  20, an average step would have taken 0.49503 = 0.00091 + 0.49413 seconds\n",
      "Using batch size  21, an average step would have taken 0.45296 = 0.00088 + 0.45208 seconds\n",
      "Using batch size  22, an average step would have taken 0.48957 = 0.00074 + 0.48883 seconds\n",
      "Using batch size  23, an average step would have taken 0.48427 = 0.00053 + 0.48374 seconds\n",
      "Using batch size  24, an average step would have taken 0.59477 = 0.00125 + 0.59353 seconds\n",
      "Using batch size  25, an average step would have taken 0.46233 = 0.00032 + 0.46201 seconds\n",
      "batch size was 25, optimal batch size would have been 21\n",
      "setting batch size to 21: expected step duration: 0.45296\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.3x\n",
      "improvement compared to smallest batch size (1): 6.3x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too linear, increasing steps:     0.51 (100), 0.37 (30)\n",
      "  Success rate too low, decreasing source step:  0.21 ( 71), 0.17 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.44 ( 9)\n",
      "Step 900: 1.10514e-02, stepsizes = 1.3e-02/1.7e-04: d. reduced by 0.05% (5.6806e-06) (took 0.48744 seconds)\n",
      "  Success rate too high, increasing source step: 0.38 (100), 0.63 (30)\n",
      "  Success rate too low, decreasing source step:  0.37 (100), 0.10 (30)\n",
      "Step 1000: 1.06682e-02, stepsizes = 1.3e-02/1.7e-04:  (took 1.29117 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1220.04727\n",
      "   1.1% for generation (13.49967)\n",
      "   13.6% for spherical prediction (166.38187)\n",
      "   73.8% for prediction (900.47613)\n",
      "   0.0% for hyperparameter update (0.15246)\n",
      "   11.4% for the rest (139.53713)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 3.13494574e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.60472022e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 1.29160137e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.0179194  0.0179603  0.01686172 0.02211609\n",
      " 0.01848043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.13  0.165 0.1   0.105 0.075 0.065 0.03  0.045 0.04  0.04  0.015 0.03\n",
      " 0.015 0.015 0.015 0.005 0.02  0.04  0.005 0.    0.01  0.    0.015 0.005\n",
      " 0.005 0.01 ]\n",
      "Using batch size   1, an average step would have taken 1.94815 = 0.02300 + 1.92515 seconds\n",
      "Using batch size   2, an average step would have taken 1.51430 = 0.00187 + 1.51243 seconds\n",
      "Using batch size   3, an average step would have taken 1.03472 = 0.00515 + 1.02956 seconds\n",
      "Using batch size   4, an average step would have taken 0.90478 = 0.00361 + 0.90117 seconds\n",
      "Using batch size   5, an average step would have taken 0.66941 = 0.00095 + 0.66846 seconds\n",
      "Using batch size   6, an average step would have taken 0.70279 = 0.00157 + 0.70122 seconds\n",
      "Using batch size   7, an average step would have taken 0.68752 = 0.00233 + 0.68519 seconds\n",
      "Using batch size   8, an average step would have taken 0.60096 = 0.00112 + 0.59984 seconds\n",
      "Using batch size   9, an average step would have taken 0.49733 = 0.00087 + 0.49646 seconds\n",
      "Using batch size  10, an average step would have taken 0.49194 = 0.00055 + 0.49140 seconds\n",
      "Using batch size  11, an average step would have taken 0.46721 = 0.00153 + 0.46568 seconds\n",
      "Using batch size  12, an average step would have taken 0.49792 = 0.00145 + 0.49647 seconds\n",
      "Using batch size  13, an average step would have taken 0.60407 = 0.00053 + 0.60354 seconds\n",
      "Using batch size  14, an average step would have taken 0.64877 = 0.00067 + 0.64810 seconds\n",
      "Using batch size  15, an average step would have taken 0.69722 = 0.00072 + 0.69650 seconds\n",
      "Using batch size  16, an average step would have taken 0.67144 = 0.00073 + 0.67072 seconds\n",
      "Using batch size  17, an average step would have taken 0.61349 = 0.00047 + 0.61302 seconds\n",
      "Using batch size  18, an average step would have taken 0.66506 = 0.00049 + 0.66457 seconds\n",
      "Using batch size  19, an average step would have taken 0.71434 = 0.00078 + 0.71356 seconds\n",
      "Using batch size  20, an average step would have taken 0.44290 = 0.00083 + 0.44207 seconds\n",
      "Using batch size  21, an average step would have taken 0.43235 = 0.00054 + 0.43181 seconds\n",
      "Using batch size  22, an average step would have taken 0.44135 = 0.00051 + 0.44084 seconds\n",
      "Using batch size  23, an average step would have taken 0.43428 = 0.00048 + 0.43380 seconds\n",
      "Using batch size  24, an average step would have taken 0.56167 = 0.00085 + 0.56082 seconds\n",
      "Using batch size  25, an average step would have taken 0.46233 = 0.00032 + 0.46201 seconds\n",
      "batch size was 21, optimal batch size would have been 21\n",
      "setting batch size to 21: expected step duration: 0.43235\n",
      "improvement compared to old batch size (21): 1.0x\n",
      "improvement compared to worst batch size (1): 4.5x\n",
      "improvement compared to smallest batch size (1): 4.5x\n",
      "improvement compared to largest batch size (25): 1.1x\n",
      "next batch size tuning in 400 steps, after step 1400\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.29 (21)\n",
      "  Success rate too high, increasing source step: 0.43 ( 42), 0.53 (30)\n",
      "Step 1100: 1.03822e-02, stepsizes = 8.7e-03/1.7e-04:  (took 1.42516 seconds)\n",
      "  Success rate too low, decreasing source step:  0.37 (100), 0.10 (30)\n",
      "Step 1200: 1.01304e-02, stepsizes = 8.7e-03/1.1e-04: d. reduced by 0.02% (2.3139e-06) (took 0.66313 seconds)\n",
      "  Success rate too low, decreasing source step:  0.33 (100), 0.13 (30)\n",
      "Step 1300: 9.91978e-03, stepsizes = 8.7e-03/7.6e-05: d. reduced by 0.02% (2.2657e-06) (took 0.63895 seconds)\n",
      "  Boundary too linear, increasing steps:     0.55 (100), 0.63 (30)\n",
      "  Success rate too high, increasing source step: 0.55 (100), 0.63 (30)\n",
      "Step 1400: 9.72352e-03, stepsizes = 1.3e-02/1.7e-04: d. reduced by 0.03% (3.3317e-06) (took 0.59100 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1423.38077\n",
      "   1.2% for generation (16.64117)\n",
      "   13.0% for spherical prediction (184.81929)\n",
      "   74.9% for prediction (1066.12624)\n",
      "   0.0% for hyperparameter update (0.22944)\n",
      "   10.9% for the rest (155.56463)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 1.61886215e-04 4.87354067e-04 2.53410491e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59968701e-05 1.31696709e-05 1.89675726e-05 2.06563208e-05\n",
      " 1.29160137e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01798002 0.0179603  0.01686172 0.02211609\n",
      " 0.01848043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.1725 0.145  0.085  0.0775 0.07   0.045  0.05   0.0375 0.045  0.0225\n",
      " 0.025  0.01   0.0275 0.0275 0.015  0.02   0.015  0.0175 0.02   0.005\n",
      " 0.0175 0.0025 0.0175 0.0125 0.0075 0.01  ]\n",
      "Using batch size   1, an average step would have taken 2.28894 = 0.02703 + 2.26191 seconds\n",
      "Using batch size   2, an average step would have taken 1.75127 = 0.00221 + 1.74906 seconds\n",
      "Using batch size   3, an average step would have taken 1.19245 = 0.00597 + 1.18647 seconds\n",
      "Using batch size   4, an average step would have taken 1.02893 = 0.00344 + 1.02549 seconds\n",
      "Using batch size   5, an average step would have taken 0.75403 = 0.00107 + 0.75296 seconds\n",
      "Using batch size   6, an average step would have taken 0.79034 = 0.00182 + 0.78853 seconds\n",
      "Using batch size   7, an average step would have taken 0.76217 = 0.00253 + 0.75964 seconds\n",
      "Using batch size   8, an average step would have taken 0.65758 = 0.00129 + 0.65629 seconds\n",
      "Using batch size   9, an average step would have taken 0.55220 = 0.00100 + 0.55120 seconds\n",
      "Using batch size  10, an average step would have taken 0.53528 = 0.00060 + 0.53468 seconds\n",
      "Using batch size  11, an average step would have taken 0.51513 = 0.00171 + 0.51342 seconds\n",
      "Using batch size  12, an average step would have taken 0.53832 = 0.00164 + 0.53668 seconds\n",
      "Using batch size  13, an average step would have taken 0.63066 = 0.00060 + 0.63006 seconds\n",
      "Using batch size  14, an average step would have taken 0.67245 = 0.00075 + 0.67170 seconds\n",
      "Using batch size  15, an average step would have taken 0.71648 = 0.00074 + 0.71574 seconds\n",
      "Using batch size  16, an average step would have taken 0.69161 = 0.00076 + 0.69085 seconds\n",
      "Using batch size  17, an average step would have taken 0.64390 = 0.00051 + 0.64338 seconds\n",
      "Using batch size  18, an average step would have taken 0.69170 = 0.00058 + 0.69112 seconds\n",
      "Using batch size  19, an average step would have taken 0.73723 = 0.00082 + 0.73640 seconds\n",
      "Using batch size  20, an average step would have taken 0.46028 = 0.00086 + 0.45942 seconds\n",
      "Using batch size  21, an average step would have taken 0.45214 = 0.00056 + 0.45158 seconds\n",
      "Using batch size  22, an average step would have taken 0.45742 = 0.00059 + 0.45684 seconds\n",
      "Using batch size  23, an average step would have taken 0.44856 = 0.00050 + 0.44807 seconds\n",
      "Using batch size  24, an average step would have taken 0.57090 = 0.00096 + 0.56993 seconds\n",
      "Using batch size  25, an average step would have taken 0.46233 = 0.00032 + 0.46201 seconds\n",
      "batch size was 21, optimal batch size would have been 23\n",
      "setting batch size to 23: expected step duration: 0.44856\n",
      "improvement compared to old batch size (21): 1.0x\n",
      "improvement compared to worst batch size (1): 5.1x\n",
      "improvement compared to smallest batch size (1): 5.1x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Success rate too low, decreasing source step:  0.31 (100), 0.10 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.30 (30)\n",
      "Step 1500: 9.54275e-03, stepsizes = 8.7e-03/7.6e-05: d. reduced by 0.02% (1.4531e-06) (took 0.69538 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.47 (30)\n",
      "  Success rate too high, increasing source step: 0.51 ( 69), 0.57 (30)\n",
      "Step 1600: 9.44063e-03, stepsizes = 5.8e-03/7.6e-05: d. reduced by 0.02% (1.4374e-06) (took 0.65566 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1524.51953\n",
      "   1.2% for generation (18.25969)\n",
      "   12.7% for spherical prediction (193.50963)\n",
      "   75.4% for prediction (1149.34780)\n",
      "   0.0% for hyperparameter update (0.27283)\n",
      "   10.7% for the rest (163.12957)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 2.53410491e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59968701e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.29160137e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01798002 0.0179603  0.01690723 0.02211609\n",
      " 0.01848043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.165 0.14  0.1   0.065 0.115 0.055 0.03  0.045 0.03  0.03  0.035 0.01\n",
      " 0.045 0.02  0.02  0.005 0.025 0.01  0.01  0.005 0.005 0.005 0.005 0.01\n",
      " 0.01  0.005]\n",
      "Using batch size   1, an average step would have taken 2.12072 = 0.02504 + 2.09568 seconds\n",
      "Using batch size   2, an average step would have taken 1.63130 = 0.00938 + 1.62192 seconds\n",
      "Using batch size   3, an average step would have taken 1.12084 = 0.00561 + 1.11523 seconds\n",
      "Using batch size   4, an average step would have taken 0.95790 = 0.00320 + 0.95470 seconds\n",
      "Using batch size   5, an average step would have taken 0.70568 = 0.00101 + 0.70467 seconds\n",
      "Using batch size   6, an average step would have taken 0.74275 = 0.00170 + 0.74104 seconds\n",
      "Using batch size   7, an average step would have taken 0.71663 = 0.00238 + 0.71424 seconds\n",
      "Using batch size   8, an average step would have taken 0.62714 = 0.00122 + 0.62592 seconds\n",
      "Using batch size   9, an average step would have taken 0.52839 = 0.00094 + 0.52745 seconds\n",
      "Using batch size  10, an average step would have taken 0.51424 = 0.00057 + 0.51367 seconds\n",
      "Using batch size  11, an average step would have taken 0.49742 = 0.00165 + 0.49577 seconds\n",
      "Using batch size  12, an average step would have taken 0.51360 = 0.00155 + 0.51205 seconds\n",
      "Using batch size  13, an average step would have taken 0.61140 = 0.00055 + 0.61085 seconds\n",
      "Using batch size  14, an average step would have taken 0.65367 = 0.00069 + 0.65298 seconds\n",
      "Using batch size  15, an average step would have taken 0.70225 = 0.00073 + 0.70152 seconds\n",
      "Using batch size  16, an average step would have taken 0.67467 = 0.00073 + 0.67394 seconds\n",
      "Using batch size  17, an average step would have taken 0.62780 = 0.00049 + 0.62731 seconds\n",
      "Using batch size  18, an average step would have taken 0.67838 = 0.00054 + 0.67784 seconds\n",
      "Using batch size  19, an average step would have taken 0.72490 = 0.00080 + 0.72410 seconds\n",
      "Using batch size  20, an average step would have taken 0.45348 = 0.00085 + 0.45263 seconds\n",
      "Using batch size  21, an average step would have taken 0.44370 = 0.00053 + 0.44317 seconds\n",
      "Using batch size  22, an average step would have taken 0.45359 = 0.00057 + 0.45303 seconds\n",
      "Using batch size  23, an average step would have taken 0.44659 = 0.00065 + 0.44594 seconds\n",
      "Using batch size  24, an average step would have taken 0.56818 = 0.00093 + 0.56725 seconds\n",
      "Using batch size  25, an average step would have taken 0.46233 = 0.00032 + 0.46201 seconds\n",
      "batch size was 23, optimal batch size would have been 21\n",
      "setting batch size to 21: expected step duration: 0.44370\n",
      "improvement compared to old batch size (23): 1.0x\n",
      "improvement compared to worst batch size (1): 4.8x\n",
      "improvement compared to smallest batch size (1): 4.8x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 1700\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.40 (25)\n",
      "  Success rate too low, decreasing source step:  0.34 (100), 0.13 (30)\n",
      "Step 1700: 9.36429e-03, stepsizes = 3.9e-03/3.4e-05: d. reduced by 0.01% (6.3367e-07) (took 0.82177 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1582.74611\n",
      "   1.2% for generation (19.02943)\n",
      "   12.5% for spherical prediction (198.58924)\n",
      "   75.7% for prediction (1197.59842)\n",
      "   0.0% for hyperparameter update (0.30173)\n",
      "   10.6% for the rest (167.22729)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 2.12542666e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.60338336e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.29160137e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802147 0.0179603  0.01690723 0.02211609\n",
      " 0.01848043]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.32 0.07 0.07 0.03 0.06 0.03 0.02 0.03 0.04 0.02 0.02 0.06 0.05 0.01\n",
      " 0.01 0.01 0.02 0.02 0.01 0.   0.03 0.   0.02 0.   0.02 0.03]\n",
      "Using batch size   1, an average step would have taken 3.14960 = 0.03719 + 3.11241 seconds\n",
      "Using batch size   2, an average step would have taken 2.37829 = 0.01382 + 2.36447 seconds\n",
      "Using batch size   3, an average step would have taken 1.59471 = 0.00815 + 1.58656 seconds\n",
      "Using batch size   4, an average step would have taken 1.34384 = 0.00409 + 1.33975 seconds\n",
      "Using batch size   5, an average step would have taken 0.97012 = 0.00138 + 0.96873 seconds\n",
      "Using batch size   6, an average step would have taken 1.00536 = 0.00257 + 1.00280 seconds\n",
      "Using batch size   7, an average step would have taken 0.95743 = 0.00310 + 0.95433 seconds\n",
      "Using batch size   8, an average step would have taken 0.82719 = 0.00189 + 0.82530 seconds\n",
      "Using batch size   9, an average step would have taken 0.68576 = 0.00133 + 0.68443 seconds\n",
      "Using batch size  10, an average step would have taken 0.65707 = 0.00075 + 0.65632 seconds\n",
      "Using batch size  11, an average step would have taken 0.61949 = 0.00212 + 0.61737 seconds\n",
      "Using batch size  12, an average step would have taken 0.62601 = 0.00219 + 0.62383 seconds\n",
      "Using batch size  13, an average step would have taken 0.68841 = 0.00073 + 0.68768 seconds\n",
      "Using batch size  14, an average step would have taken 0.72552 = 0.00091 + 0.72461 seconds\n",
      "Using batch size  15, an average step would have taken 0.77425 = 0.00080 + 0.77345 seconds\n",
      "Using batch size  16, an average step would have taken 0.74565 = 0.00084 + 0.74481 seconds\n",
      "Using batch size  17, an average step would have taken 0.70292 = 0.00059 + 0.70233 seconds\n",
      "Using batch size  18, an average step would have taken 0.75831 = 0.00081 + 0.75750 seconds\n",
      "Using batch size  19, an average step would have taken 0.80059 = 0.00094 + 0.79966 seconds\n",
      "Using batch size  20, an average step would have taken 0.51090 = 0.00093 + 0.50997 seconds\n",
      "Using batch size  21, an average step would have taken 0.51030 = 0.00067 + 0.50963 seconds\n",
      "Using batch size  22, an average step would have taken 0.50871 = 0.00083 + 0.50788 seconds\n",
      "Using batch size  23, an average step would have taken 0.50718 = 0.00099 + 0.50619 seconds\n",
      "Using batch size  24, an average step would have taken 0.60725 = 0.00139 + 0.60586 seconds\n",
      "Using batch size  25, an average step would have taken 0.46233 = 0.00032 + 0.46201 seconds\n",
      "batch size was 21, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.46233\n",
      "improvement compared to old batch size (21): 1.1x\n",
      "improvement compared to worst batch size (1): 6.8x\n",
      "improvement compared to smallest batch size (1): 6.8x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 1800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Success rate too high, increasing source step: 0.47 (100), 0.63 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.16 (100), 0.17 (30)\n",
      "  Success rate too low, decreasing source step:  0.16 (100), 0.17 (30)\n",
      "Step 1800: 9.30334e-03, stepsizes = 2.6e-03/2.3e-05: d. reduced by 0.01% (9.4427e-07) (took 0.79132 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1632.05951\n",
      "   1.2% for generation (19.98013)\n",
      "   12.4% for spherical prediction (202.64156)\n",
      "   75.8% for prediction (1237.83260)\n",
      "   0.0% for hyperparameter update (0.32096)\n",
      "   10.5% for the rest (171.28426)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 2.12542666e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.60338336e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.32025525e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802147 0.0179603  0.01690723 0.02211609\n",
      " 0.01840602]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.26 0.14 0.1  0.04 0.07 0.05 0.03 0.03 0.04 0.02 0.02 0.05 0.   0.01\n",
      " 0.02 0.04 0.   0.04 0.01 0.01 0.   0.01 0.   0.   0.01 0.  ]\n",
      "Using batch size   1, an average step would have taken 2.52663 = 0.02983 + 2.49679 seconds\n",
      "Using batch size   2, an average step would have taken 1.94102 = 0.01124 + 1.92977 seconds\n",
      "Using batch size   3, an average step would have taken 1.30572 = 0.00663 + 1.29909 seconds\n",
      "Using batch size   4, an average step would have taken 1.12873 = 0.00337 + 1.12536 seconds\n",
      "Using batch size   5, an average step would have taken 0.80994 = 0.00115 + 0.80879 seconds\n",
      "Using batch size   6, an average step would have taken 0.85557 = 0.00210 + 0.85347 seconds\n",
      "Using batch size   7, an average step would have taken 0.82181 = 0.00268 + 0.81913 seconds\n",
      "Using batch size   8, an average step would have taken 0.71465 = 0.00154 + 0.71311 seconds\n",
      "Using batch size   9, an average step would have taken 0.58788 = 0.00109 + 0.58679 seconds\n",
      "Using batch size  10, an average step would have taken 0.57359 = 0.00065 + 0.57294 seconds\n",
      "Using batch size  11, an average step would have taken 0.54314 = 0.00183 + 0.54131 seconds\n",
      "Using batch size  12, an average step would have taken 0.57347 = 0.00188 + 0.57160 seconds\n",
      "Using batch size  13, an average step would have taken 0.65541 = 0.00065 + 0.65475 seconds\n",
      "Using batch size  14, an average step would have taken 0.69286 = 0.00081 + 0.69205 seconds\n",
      "Using batch size  15, an average step would have taken 0.73071 = 0.00076 + 0.72996 seconds\n",
      "Using batch size  16, an average step would have taken 0.71016 = 0.00079 + 0.70937 seconds\n",
      "Using batch size  17, an average step would have taken 0.65642 = 0.00053 + 0.65589 seconds\n",
      "Using batch size  18, an average step would have taken 0.70883 = 0.00064 + 0.70819 seconds\n",
      "Using batch size  19, an average step would have taken 0.75131 = 0.00085 + 0.75046 seconds\n",
      "Using batch size  20, an average step would have taken 0.47766 = 0.00088 + 0.47678 seconds\n",
      "Using batch size  21, an average step would have taken 0.46983 = 0.00057 + 0.46927 seconds\n",
      "Using batch size  22, an average step would have taken 0.47809 = 0.00068 + 0.47741 seconds\n",
      "Using batch size  23, an average step would have taken 0.47529 = 0.00081 + 0.47448 seconds\n",
      "Using batch size  24, an average step would have taken 0.58772 = 0.00116 + 0.58656 seconds\n",
      "Using batch size  25, an average step would have taken 0.46048 = 0.00033 + 0.46015 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.46048\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 5.5x\n",
      "improvement compared to smallest batch size (1): 5.5x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 2000\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.47 (30)\n",
      "Step 1900: 9.26314e-03, stepsizes = 1.7e-03/1.5e-05: d. reduced by 0.00% (4.1786e-07) (took 0.92739 seconds)\n",
      "  Success rate too high, increasing source step: 0.40 (100), 0.57 (30)\n",
      "Step 2000: 9.24088e-03, stepsizes = 1.7e-03/2.3e-05: d. reduced by 0.00% (4.1676e-07) (took 0.68977 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1733.85853\n",
      "   1.2% for generation (21.61129)\n",
      "   12.2% for spherical prediction (211.65356)\n",
      "   76.2% for prediction (1321.38706)\n",
      "   0.0% for hyperparameter update (0.37750)\n",
      "   10.3% for the rest (178.82912)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 2.12542666e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.60338336e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.31719256e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802147 0.0179603  0.01690723 0.02211609\n",
      " 0.0184382 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.14  0.15  0.095 0.1   0.045 0.05  0.04  0.04  0.045 0.045 0.05  0.02\n",
      " 0.02  0.015 0.035 0.01  0.02  0.    0.01  0.02  0.    0.015 0.01  0.005\n",
      " 0.    0.02 ]\n",
      "Using batch size   1, an average step would have taken 2.10118 = 0.02481 + 2.07637 seconds\n",
      "Using batch size   2, an average step would have taken 1.62754 = 0.00935 + 1.61819 seconds\n",
      "Using batch size   3, an average step would have taken 1.10336 = 0.00551 + 1.09785 seconds\n",
      "Using batch size   4, an average step would have taken 0.96371 = 0.00275 + 0.96096 seconds\n",
      "Using batch size   5, an average step would have taken 0.70416 = 0.00100 + 0.70316 seconds\n",
      "Using batch size   6, an average step would have taken 0.74938 = 0.00169 + 0.74768 seconds\n",
      "Using batch size   7, an average step would have taken 0.71746 = 0.00236 + 0.71510 seconds\n",
      "Using batch size   8, an average step would have taken 0.62676 = 0.00120 + 0.62557 seconds\n",
      "Using batch size   9, an average step would have taken 0.52839 = 0.00094 + 0.52745 seconds\n",
      "Using batch size  10, an average step would have taken 0.50620 = 0.00056 + 0.50563 seconds\n",
      "Using batch size  11, an average step would have taken 0.48160 = 0.00159 + 0.48002 seconds\n",
      "Using batch size  12, an average step would have taken 0.51143 = 0.00153 + 0.50990 seconds\n",
      "Using batch size  13, an average step would have taken 0.61324 = 0.00055 + 0.61268 seconds\n",
      "Using batch size  14, an average step would have taken 0.65040 = 0.00068 + 0.64973 seconds\n",
      "Using batch size  15, an average step would have taken 0.69722 = 0.00072 + 0.69650 seconds\n",
      "Using batch size  16, an average step would have taken 0.67144 = 0.00073 + 0.67072 seconds\n",
      "Using batch size  17, an average step would have taken 0.62780 = 0.00049 + 0.62731 seconds\n",
      "Using batch size  18, an average step would have taken 0.67838 = 0.00054 + 0.67784 seconds\n",
      "Using batch size  19, an average step would have taken 0.71962 = 0.00079 + 0.71883 seconds\n",
      "Using batch size  20, an average step would have taken 0.45046 = 0.00084 + 0.44961 seconds\n",
      "Using batch size  21, an average step would have taken 0.43780 = 0.00049 + 0.43731 seconds\n",
      "Using batch size  22, an average step would have taken 0.44594 = 0.00053 + 0.44541 seconds\n",
      "Using batch size  23, an average step would have taken 0.44021 = 0.00061 + 0.43960 seconds\n",
      "Using batch size  24, an average step would have taken 0.56601 = 0.00091 + 0.56511 seconds\n",
      "Using batch size  25, an average step would have taken 0.46128 = 0.00033 + 0.46096 seconds\n",
      "batch size was 25, optimal batch size would have been 21\n",
      "setting batch size to 21: expected step duration: 0.43780\n",
      "improvement compared to old batch size (25): 1.1x\n",
      "improvement compared to worst batch size (1): 4.8x\n",
      "improvement compared to smallest batch size (1): 4.8x\n",
      "improvement compared to largest batch size (25): 1.1x\n",
      "next batch size tuning in 100 steps, after step 2100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2100: 9.21258e-03, stepsizes = 1.7e-03/2.3e-05: d. reduced by 0.00% (4.1564e-07) (took 0.73708 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1792.42641\n",
      "   1.2% for generation (22.34715)\n",
      "   12.1% for spherical prediction (216.67831)\n",
      "   76.4% for prediction (1369.85085)\n",
      "   0.0% for hyperparameter update (0.39626)\n",
      "   10.2% for the rest (183.15384)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.31719256e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01690723 0.02211609\n",
      " 0.0184382 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.32 0.03 0.05 0.04 0.02 0.04 0.07 0.01 0.06 0.06 0.02 0.02 0.02 0.03\n",
      " 0.03 0.01 0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.03 0.02 0.01]\n",
      "Using batch size   1, an average step would have taken 3.27984 = 0.03873 + 3.24111 seconds\n",
      "Using batch size   2, an average step would have taken 2.47917 = 0.01436 + 2.46482 seconds\n",
      "Using batch size   3, an average step would have taken 1.63936 = 0.00833 + 1.63103 seconds\n",
      "Using batch size   4, an average step would have taken 1.40991 = 0.00382 + 1.40609 seconds\n",
      "Using batch size   5, an average step would have taken 1.00034 = 0.00143 + 0.99891 seconds\n",
      "Using batch size   6, an average step would have taken 1.03623 = 0.00258 + 1.03365 seconds\n",
      "Using batch size   7, an average step would have taken 0.98403 = 0.00315 + 0.98088 seconds\n",
      "Using batch size   8, an average step would have taken 0.84073 = 0.00187 + 0.83887 seconds\n",
      "Using batch size   9, an average step would have taken 0.68633 = 0.00134 + 0.68500 seconds\n",
      "Using batch size  10, an average step would have taken 0.65674 = 0.00075 + 0.65599 seconds\n",
      "Using batch size  11, an average step would have taken 0.63234 = 0.00217 + 0.63018 seconds\n",
      "Using batch size  12, an average step would have taken 0.64367 = 0.00219 + 0.64149 seconds\n",
      "Using batch size  13, an average step would have taken 0.70308 = 0.00077 + 0.70231 seconds\n",
      "Using batch size  14, an average step would have taken 0.73205 = 0.00093 + 0.73112 seconds\n",
      "Using batch size  15, an average step would have taken 0.78095 = 0.00081 + 0.78014 seconds\n",
      "Using batch size  16, an average step would have taken 0.75210 = 0.00085 + 0.75125 seconds\n",
      "Using batch size  17, an average step would have taken 0.71007 = 0.00060 + 0.70948 seconds\n",
      "Using batch size  18, an average step would have taken 0.76212 = 0.00082 + 0.76130 seconds\n",
      "Using batch size  19, an average step would have taken 0.79707 = 0.00093 + 0.79614 seconds\n",
      "Using batch size  20, an average step would have taken 0.51392 = 0.00093 + 0.51299 seconds\n",
      "Using batch size  21, an average step would have taken 0.51028 = 0.00063 + 0.50965 seconds\n",
      "Using batch size  22, an average step would have taken 0.51177 = 0.00085 + 0.51093 seconds\n",
      "Using batch size  23, an average step would have taken 0.50080 = 0.00095 + 0.49985 seconds\n",
      "Using batch size  24, an average step would have taken 0.60291 = 0.00134 + 0.60157 seconds\n",
      "Using batch size  25, an average step would have taken 0.46128 = 0.00033 + 0.46096 seconds\n",
      "batch size was 21, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.46128\n",
      "improvement compared to old batch size (21): 1.1x\n",
      "improvement compared to worst batch size (1): 7.1x\n",
      "improvement compared to smallest batch size (1): 7.1x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 100 steps, after step 2200\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.17 (30)\n",
      "  Success rate too low, decreasing source step:  0.17 (100), 0.17 (30)\n",
      "Step 2200: 9.19754e-03, stepsizes = 1.1e-03/1.0e-05: d. reduced by 0.00% (1.8428e-07) (took 0.68767 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1838.05904\n",
      "   1.3% for generation (23.23772)\n",
      "   12.0% for spherical prediction (220.51828)\n",
      "   76.6% for prediction (1407.13701)\n",
      "   0.0% for hyperparameter update (0.41888)\n",
      "   10.2% for the rest (186.74715)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.32697557e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01690723 0.02211609\n",
      " 0.0182692 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.31 0.09 0.05 0.06 0.02 0.02 0.02 0.06 0.01 0.06 0.03 0.03 0.   0.03\n",
      " 0.   0.02 0.02 0.   0.03 0.05 0.   0.01 0.04 0.01 0.01 0.02]\n",
      "Using batch size   1, an average step would have taken 3.17999 = 0.03755 + 3.14244 seconds\n",
      "Using batch size   2, an average step would have taken 2.42496 = 0.01405 + 2.41091 seconds\n",
      "Using batch size   3, an average step would have taken 1.61486 = 0.00821 + 1.60665 seconds\n",
      "Using batch size   4, an average step would have taken 1.37957 = 0.00375 + 1.37582 seconds\n",
      "Using batch size   5, an average step would have taken 0.97918 = 0.00139 + 0.97779 seconds\n",
      "Using batch size   6, an average step would have taken 1.03623 = 0.00258 + 1.03365 seconds\n",
      "Using batch size   7, an average step would have taken 0.96881 = 0.00310 + 0.96571 seconds\n",
      "Using batch size   8, an average step would have taken 0.84073 = 0.00187 + 0.83887 seconds\n",
      "Using batch size   9, an average step would have taken 0.69072 = 0.00136 + 0.68936 seconds\n",
      "Using batch size  10, an average step would have taken 0.65004 = 0.00074 + 0.64930 seconds\n",
      "Using batch size  11, an average step would have taken 0.61336 = 0.00209 + 0.61127 seconds\n",
      "Using batch size  12, an average step would have taken 0.64001 = 0.00218 + 0.63783 seconds\n",
      "Using batch size  13, an average step would have taken 0.69941 = 0.00076 + 0.69865 seconds\n",
      "Using batch size  14, an average step would have taken 0.73858 = 0.00095 + 0.73763 seconds\n",
      "Using batch size  15, an average step would have taken 0.78430 = 0.00081 + 0.78349 seconds\n",
      "Using batch size  16, an average step would have taken 0.75533 = 0.00086 + 0.75447 seconds\n",
      "Using batch size  17, an average step would have taken 0.72081 = 0.00061 + 0.72019 seconds\n",
      "Using batch size  18, an average step would have taken 0.76973 = 0.00085 + 0.76888 seconds\n",
      "Using batch size  19, an average step would have taken 0.79355 = 0.00092 + 0.79263 seconds\n",
      "Using batch size  20, an average step would have taken 0.51392 = 0.00093 + 0.51299 seconds\n",
      "Using batch size  21, an average step would have taken 0.51028 = 0.00063 + 0.50965 seconds\n",
      "Using batch size  22, an average step would have taken 0.50259 = 0.00080 + 0.50178 seconds\n",
      "Using batch size  23, an average step would have taken 0.49761 = 0.00093 + 0.49668 seconds\n",
      "Using batch size  24, an average step would have taken 0.60291 = 0.00134 + 0.60157 seconds\n",
      "Using batch size  25, an average step would have taken 0.45706 = 0.00033 + 0.45673 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.45706\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 7.0x\n",
      "improvement compared to smallest batch size (1): 7.0x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Success rate too high, increasing source step: 0.48 (100), 0.80 (30)\n",
      "Step 2300: 9.18427e-03, stepsizes = 1.1e-03/1.5e-05:  (took 0.73214 seconds)\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.10 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.17 (100), 0.30 (30)\n",
      "Step 2400: 9.16937e-03, stepsizes = 7.6e-04/6.7e-06: d. reduced by 0.00% (1.8384e-07) (took 0.86292 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 1938.11571\n",
      "   1.3% for generation (24.89770)\n",
      "   11.9% for spherical prediction (229.76535)\n",
      "   76.8% for prediction (1488.70167)\n",
      "   0.0% for hyperparameter update (0.45575)\n",
      "   10.0% for the rest (194.29523)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.32713063e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01690723 0.02211609\n",
      " 0.01824716]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.285 0.12  0.05  0.04  0.04  0.035 0.035 0.035 0.04  0.03  0.04  0.045\n",
      " 0.015 0.025 0.015 0.015 0.01  0.03  0.01  0.    0.015 0.015 0.015 0.005\n",
      " 0.02  0.015]\n",
      "Using batch size   1, an average step would have taken 2.91843 = 0.03446 + 2.88397 seconds\n",
      "Using batch size   2, an average step would have taken 2.22394 = 0.01288 + 2.21105 seconds\n",
      "Using batch size   3, an average step would have taken 1.48894 = 0.00757 + 1.48137 seconds\n",
      "Using batch size   4, an average step would have taken 1.27698 = 0.00346 + 1.27352 seconds\n",
      "Using batch size   5, an average step would have taken 0.91421 = 0.00130 + 0.91290 seconds\n",
      "Using batch size   6, an average step would have taken 0.95403 = 0.00237 + 0.95166 seconds\n",
      "Using batch size   7, an average step would have taken 0.90628 = 0.00291 + 0.90337 seconds\n",
      "Using batch size   8, an average step would have taken 0.78593 = 0.00173 + 0.78420 seconds\n",
      "Using batch size   9, an average step would have taken 0.64898 = 0.00124 + 0.64774 seconds\n",
      "Using batch size  10, an average step would have taken 0.62137 = 0.00071 + 0.62066 seconds\n",
      "Using batch size  11, an average step would have taken 0.58611 = 0.00199 + 0.58412 seconds\n",
      "Using batch size  12, an average step would have taken 0.60599 = 0.00204 + 0.60396 seconds\n",
      "Using batch size  13, an average step would have taken 0.67374 = 0.00070 + 0.67304 seconds\n",
      "Using batch size  14, an average step would have taken 0.71082 = 0.00087 + 0.70996 seconds\n",
      "Using batch size  15, an average step would have taken 0.75751 = 0.00078 + 0.75672 seconds\n",
      "Using batch size  16, an average step would have taken 0.73275 = 0.00082 + 0.73192 seconds\n",
      "Using batch size  17, an average step would have taken 0.68503 = 0.00057 + 0.68447 seconds\n",
      "Using batch size  18, an average step would have taken 0.73928 = 0.00074 + 0.73854 seconds\n",
      "Using batch size  19, an average step would have taken 0.78299 = 0.00091 + 0.78209 seconds\n",
      "Using batch size  20, an average step would have taken 0.50032 = 0.00091 + 0.49941 seconds\n",
      "Using batch size  21, an average step would have taken 0.49342 = 0.00059 + 0.49283 seconds\n",
      "Using batch size  22, an average step would have taken 0.49493 = 0.00076 + 0.49417 seconds\n",
      "Using batch size  23, an average step would have taken 0.49123 = 0.00090 + 0.49034 seconds\n",
      "Using batch size  24, an average step would have taken 0.59640 = 0.00126 + 0.59514 seconds\n",
      "Using batch size  25, an average step would have taken 0.45651 = 0.00033 + 0.45618 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.45651\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.4x\n",
      "improvement compared to smallest batch size (1): 6.4x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 2800\n",
      "  Success rate too high, increasing source step: 0.37 (100), 0.60 (30)\n",
      "Step 2500: 9.15883e-03, stepsizes = 7.6e-04/1.0e-05: d. reduced by 0.00% (1.8368e-07) (took 0.79139 seconds)\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.42 (100), 0.60 (30)\n",
      "Step 2600: 9.15143e-03, stepsizes = 7.6e-04/1.0e-05: d. reduced by 0.00% (1.8353e-07) (took 0.82041 seconds)\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.07 (30)\n",
      "Step 2700: 9.14348e-03, stepsizes = 7.6e-04/6.7e-06:  (took 0.74502 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.23 (30)\n",
      "Step 2800: 9.13632e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.1425e-08) (took 0.96539 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 2138.30275\n",
      "   1.3% for generation (28.18408)\n",
      "   11.6% for spherical prediction (247.64650)\n",
      "   77.3% for prediction (1652.56177)\n",
      "   0.0% for hyperparameter update (0.53936)\n",
      "   9.8% for the rest (209.37104)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.32417279e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01690723 0.02211609\n",
      " 0.01823766]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.345  0.11   0.06   0.06   0.0425 0.0375 0.0275 0.0125 0.025  0.02\n",
      " 0.0325 0.03   0.015  0.0275 0.0175 0.01   0.015  0.0225 0.0075 0.0175\n",
      " 0.0125 0.0075 0.0225 0.0125 0.     0.01  ]\n",
      "Using batch size   1, an average step would have taken 3.06983 = 0.03625 + 3.03358 seconds\n",
      "Using batch size   2, an average step would have taken 2.33393 = 0.01357 + 2.32036 seconds\n",
      "Using batch size   3, an average step would have taken 1.56364 = 0.00801 + 1.55564 seconds\n",
      "Using batch size   4, an average step would have taken 1.33611 = 0.00370 + 1.33241 seconds\n",
      "Using batch size   5, an average step would have taken 0.94669 = 0.00135 + 0.94535 seconds\n",
      "Using batch size   6, an average step would have taken 0.99853 = 0.00257 + 0.99596 seconds\n",
      "Using batch size   7, an average step would have taken 0.94312 = 0.00302 + 0.94010 seconds\n",
      "Using batch size   8, an average step would have taken 0.82201 = 0.00190 + 0.82012 seconds\n",
      "Using batch size   9, an average step would have taken 0.68054 = 0.00133 + 0.67922 seconds\n",
      "Using batch size  10, an average step would have taken 0.64677 = 0.00074 + 0.64604 seconds\n",
      "Using batch size  11, an average step would have taken 0.61627 = 0.00211 + 0.61416 seconds\n",
      "Using batch size  12, an average step would have taken 0.63718 = 0.00222 + 0.63496 seconds\n",
      "Using batch size  13, an average step would have taken 0.69208 = 0.00074 + 0.69134 seconds\n",
      "Using batch size  14, an average step would have taken 0.72633 = 0.00091 + 0.72542 seconds\n",
      "Using batch size  15, an average step would have taken 0.77509 = 0.00080 + 0.77429 seconds\n",
      "Using batch size  16, an average step would have taken 0.74807 = 0.00084 + 0.74723 seconds\n",
      "Using batch size  17, an average step would have taken 0.70471 = 0.00059 + 0.70412 seconds\n",
      "Using batch size  18, an average step would have taken 0.76117 = 0.00082 + 0.76035 seconds\n",
      "Using batch size  19, an average step would have taken 0.79707 = 0.00093 + 0.79614 seconds\n",
      "Using batch size  20, an average step would have taken 0.51317 = 0.00093 + 0.51223 seconds\n",
      "Using batch size  21, an average step would have taken 0.51028 = 0.00063 + 0.50965 seconds\n",
      "Using batch size  22, an average step would have taken 0.50794 = 0.00083 + 0.50712 seconds\n",
      "Using batch size  23, an average step would have taken 0.50239 = 0.00096 + 0.50143 seconds\n",
      "Using batch size  24, an average step would have taken 0.60834 = 0.00141 + 0.60693 seconds\n",
      "Using batch size  25, an average step would have taken 0.45627 = 0.00033 + 0.45594 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.45627\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 6.7x\n",
      "improvement compared to smallest batch size (1): 6.7x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 800 steps, after step 3600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too linear, increasing steps:     0.63 (100), 0.50 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.40 (30)\n",
      "Step 2900: 9.12883e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.1391e-08) (took 0.58794 seconds)\n",
      "Step 3000: 9.12257e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.1249e-08) (took 0.76017 seconds)\n",
      "Step 3100: 9.11550e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.1233e-08) (took 0.87935 seconds)\n",
      "  Success rate too low, decreasing source step:  0.24 (100), 0.13 (30)\n",
      "  Success rate too high, increasing source step: 0.36 (100), 0.60 (30)\n",
      "  Success rate too low, decreasing source step:  0.38 (100), 0.17 (30)\n",
      "Step 3200: 9.11036e-03, stepsizes = 5.1e-04/3.0e-06:  (took 0.89250 seconds)\n",
      "  Success rate too high, increasing source step: 0.39 (100), 0.73 (30)\n",
      "Step 3300: 9.10530e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.1109e-08) (took 0.71822 seconds)\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.13 (30)\n",
      "  Success rate too high, increasing source step: 0.41 (100), 0.60 (30)\n",
      "Step 3400: 9.09989e-03, stepsizes = 5.1e-04/4.5e-06:  (took 0.78015 seconds)\n",
      "  Success rate too low, decreasing source step:  0.29 (100), 0.13 (30)\n",
      "  Success rate too high, increasing source step: 0.42 (100), 0.70 (30)\n",
      "  Success rate too low, decreasing source step:  0.33 (100), 0.17 (30)\n",
      "Step 3500: 9.09341e-03, stepsizes = 5.1e-04/3.0e-06: d. reduced by 0.00% (8.0966e-08) (took 0.73891 seconds)\n",
      "  Success rate too high, increasing source step: 0.39 (100), 0.70 (30)\n",
      "Step 3600: 9.08765e-03, stepsizes = 5.1e-04/4.5e-06:  (took 0.64684 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 2540.68506\n",
      "   1.4% for generation (34.72876)\n",
      "   11.2% for spherical prediction (284.36337)\n",
      "   78.0% for prediction (1981.35456)\n",
      "   0.0% for hyperparameter update (0.73212)\n",
      "   9.4% for the rest (239.50624)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 8.94709213e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40768963e-05 2.06563208e-05\n",
      " 1.31929928e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01690723 0.02211609\n",
      " 0.0182468 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.19375 0.15    0.095   0.0725  0.05875 0.04625 0.05125 0.02875 0.035\n",
      " 0.03875 0.0275  0.03125 0.01375 0.02375 0.00875 0.02    0.01    0.01625\n",
      " 0.0175  0.00375 0.0075  0.01375 0.01375 0.00875 0.00625 0.0075 ]\n",
      "Using batch size   1, an average step would have taken 2.30576 = 0.02723 + 2.27854 seconds\n",
      "Using batch size   2, an average step would have taken 1.77759 = 0.01025 + 1.76735 seconds\n",
      "Using batch size   3, an average step would have taken 1.19728 = 0.00602 + 1.19126 seconds\n",
      "Using batch size   4, an average step would have taken 1.04486 = 0.00274 + 1.04212 seconds\n",
      "Using batch size   5, an average step would have taken 0.76045 = 0.00108 + 0.75937 seconds\n",
      "Using batch size   6, an average step would have taken 0.79441 = 0.00186 + 0.79255 seconds\n",
      "Using batch size   7, an average step would have taken 0.76881 = 0.00249 + 0.76632 seconds\n",
      "Using batch size   8, an average step would have taken 0.67015 = 0.00135 + 0.66880 seconds\n",
      "Using batch size   9, an average step would have taken 0.55641 = 0.00102 + 0.55539 seconds\n",
      "Using batch size  10, an average step would have taken 0.54128 = 0.00061 + 0.54067 seconds\n",
      "Using batch size  11, an average step would have taken 0.51199 = 0.00171 + 0.51028 seconds\n",
      "Using batch size  12, an average step would have taken 0.53918 = 0.00168 + 0.53750 seconds\n",
      "Using batch size  13, an average step would have taken 0.62882 = 0.00059 + 0.62823 seconds\n",
      "Using batch size  14, an average step would have taken 0.67286 = 0.00075 + 0.67211 seconds\n",
      "Using batch size  15, an average step would have taken 0.71690 = 0.00074 + 0.71616 seconds\n",
      "Using batch size  16, an average step would have taken 0.69363 = 0.00076 + 0.69286 seconds\n",
      "Using batch size  17, an average step would have taken 0.64658 = 0.00051 + 0.64606 seconds\n",
      "Using batch size  18, an average step would have taken 0.69551 = 0.00060 + 0.69491 seconds\n",
      "Using batch size  19, an average step would have taken 0.74119 = 0.00083 + 0.74036 seconds\n",
      "Using batch size  20, an average step would have taken 0.46670 = 0.00087 + 0.46584 seconds\n",
      "Using batch size  21, an average step would have taken 0.45634 = 0.00051 + 0.45584 seconds\n",
      "Using batch size  22, an average step would have taken 0.46163 = 0.00061 + 0.46103 seconds\n",
      "Using batch size  23, an average step would have taken 0.45536 = 0.00070 + 0.45466 seconds\n",
      "Using batch size  24, an average step would have taken 0.57497 = 0.00101 + 0.57395 seconds\n",
      "Using batch size  25, an average step would have taken 0.45650 = 0.00033 + 0.45617 seconds\n",
      "batch size was 25, optimal batch size would have been 23\n",
      "setting batch size to 23: expected step duration: 0.45536\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 5.1x\n",
      "improvement compared to smallest batch size (1): 5.1x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 4000\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.20 (30)\n",
      "  Success rate too low, decreasing source step:  0.18 ( 50), 0.17 (30)\n",
      "  Boundary too linear, increasing steps:     0.52 (100), 0.57 (30)\n",
      "  Success rate too high, increasing source step: 0.52 (100), 0.57 (30)\n",
      "Step 3700: 9.08375e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (3.5927e-08) (took 1.02761 seconds)\n",
      "Step 3800: 9.07841e-03, stepsizes = 5.1e-04/4.5e-06: d. reduced by 0.00% (8.0971e-08) (took 0.73447 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.27 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.20 (30)\n",
      "Step 3900: 9.07480e-03, stepsizes = 2.3e-04/2.0e-06:  (took 0.94020 seconds)\n",
      "  Success rate too low, decreasing source step:  0.04 ( 25), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.35 (100), 0.53 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.25 ( 8)\n",
      "Step 4000: 9.07253e-03, stepsizes = 1.5e-04/1.3e-06: d. reduced by 0.00% (2.3886e-08) (took 1.12495 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 2768.93237\n",
      "   1.4% for generation (37.99286)\n",
      "   11.1% for spherical prediction (306.22283)\n",
      "   78.4% for prediction (2170.45993)\n",
      "   0.0% for hyperparameter update (0.80250)\n",
      "   9.2% for the rest (253.45425)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 6.63739184e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40466803e-05 2.06563208e-05\n",
      " 1.31929928e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01728265 0.02211609\n",
      " 0.0182468 ]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.2875 0.1225 0.085  0.0875 0.05   0.04   0.03   0.0325 0.0275 0.01\n",
      " 0.0175 0.0125 0.0175 0.0175 0.02   0.0175 0.015  0.0125 0.02   0.0225\n",
      " 0.01   0.0025 0.0125 0.0075 0.0075 0.015 ]\n",
      "Using batch size   1, an average step would have taken 2.74532 = 0.03242 + 2.71290 seconds\n",
      "Using batch size   2, an average step would have taken 2.09239 = 0.00922 + 2.08317 seconds\n",
      "Using batch size   3, an average step would have taken 1.40680 = 0.00718 + 1.39963 seconds\n",
      "Using batch size   4, an average step would have taken 1.20421 = 0.00330 + 1.20091 seconds\n",
      "Using batch size   5, an average step would have taken 0.86358 = 0.00123 + 0.86235 seconds\n",
      "Using batch size   6, an average step would have taken 0.91672 = 0.00230 + 0.91442 seconds\n",
      "Using batch size   7, an average step would have taken 0.86580 = 0.00278 + 0.86302 seconds\n",
      "Using batch size   8, an average step would have taken 0.75338 = 0.00169 + 0.75170 seconds\n",
      "Using batch size   9, an average step would have taken 0.62772 = 0.00120 + 0.62652 seconds\n",
      "Using batch size  10, an average step would have taken 0.60201 = 0.00068 + 0.60133 seconds\n",
      "Using batch size  11, an average step would have taken 0.58218 = 0.00198 + 0.58021 seconds\n",
      "Using batch size  12, an average step would have taken 0.60378 = 0.00204 + 0.60175 seconds\n",
      "Using batch size  13, an average step would have taken 0.67374 = 0.00070 + 0.67304 seconds\n",
      "Using batch size  14, an average step would have taken 0.70919 = 0.00086 + 0.70833 seconds\n",
      "Using batch size  15, an average step would have taken 0.75499 = 0.00078 + 0.75421 seconds\n",
      "Using batch size  16, an average step would have taken 0.72871 = 0.00082 + 0.72790 seconds\n",
      "Using batch size  17, an average step would have taken 0.68682 = 0.00057 + 0.68625 seconds\n",
      "Using batch size  18, an average step would have taken 0.73738 = 0.00074 + 0.73664 seconds\n",
      "Using batch size  19, an average step would have taken 0.77331 = 0.00089 + 0.77242 seconds\n",
      "Using batch size  20, an average step would have taken 0.49352 = 0.00090 + 0.49262 seconds\n",
      "Using batch size  21, an average step would have taken 0.49005 = 0.00058 + 0.48947 seconds\n",
      "Using batch size  22, an average step would have taken 0.49263 = 0.00075 + 0.49188 seconds\n",
      "Using batch size  23, an average step would have taken 0.49653 = 0.00073 + 0.49580 seconds\n",
      "Using batch size  24, an average step would have taken 0.59694 = 0.00127 + 0.59567 seconds\n",
      "Using batch size  25, an average step would have taken 0.45650 = 0.00033 + 0.45617 seconds\n",
      "batch size was 23, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.45650\n",
      "improvement compared to old batch size (23): 1.1x\n",
      "improvement compared to worst batch size (1): 6.0x\n",
      "improvement compared to smallest batch size (1): 6.0x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 200 steps, after step 4200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.17 (30)\n",
      "  Success rate too low, decreasing source step:  0.19 (100), 0.17 (30)\n",
      "  Success rate too high, increasing source step: 0.51 ( 75), 0.53 (30)\n",
      "Step 4100: 9.07145e-03, stepsizes = 1.0e-04/8.8e-07:  (took 0.76283 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.13 (100), 0.15 (13)\n",
      "  Boundary too non-linear, decreasing steps: 0.04 (100), 0.12 (17)\n",
      "  Boundary too non-linear, decreasing steps: 0.12 (100), 0.10 (29)\n",
      "  Success rate too low, decreasing source step:  0.12 ( 25), 0.07 (30)\n",
      "Step 4200: 9.07117e-03, stepsizes = 3.0e-05/1.7e-07:  (took 0.90357 seconds)\n",
      "Estimating optimal batch size\n",
      "Time since beginning: 2875.01473\n",
      "   1.4% for generation (39.73211)\n",
      "   11.0% for spherical prediction (316.08568)\n",
      "   78.6% for prediction (2259.24545)\n",
      "   0.0% for hyperparameter update (0.81603)\n",
      "   9.0% for the rest (259.13545)\n",
      "current estimate of the time to generate a candidate depending on the batch size:\n",
      "[2.56306898e-03 6.63739184e-04 4.87354067e-04 1.87010542e-04\n",
      " 8.61072540e-05 1.05440617e-04 1.82516721e-04 5.93326986e-05\n",
      " 5.39355808e-05 3.60035896e-05 9.26108400e-05 7.16265705e-05\n",
      " 2.38432687e-05 3.01331890e-05 4.22329373e-05 3.89022753e-05\n",
      " 2.26406903e-05 1.50328801e-05 3.53414266e-05 3.80820036e-05\n",
      " 1.59711216e-05 1.31696709e-05 1.40466803e-05 2.06563208e-05\n",
      " 1.32463813e-05]\n",
      "current estimate of the time to get predictions for a candidate depending on the batch size:\n",
      "[0.21450113 0.15854522 0.10157831 0.08408723 0.06035723 0.05856826\n",
      " 0.05419021 0.04465598 0.03579469 0.03345471 0.02959608 0.03048616\n",
      " 0.03910919 0.04059562 0.04108077 0.0374901  0.03227728 0.0332325\n",
      " 0.03431917 0.01961375 0.01802264 0.0179603  0.01728265 0.02211609\n",
      " 0.01835649]\n",
      "Relative frequencies for failing and success after k\n",
      "[0.535 0.085 0.02  0.045 0.035 0.015 0.025 0.005 0.035 0.02  0.01  0.025\n",
      " 0.015 0.005 0.015 0.015 0.01  0.01  0.015 0.02  0.005 0.005 0.01  0.01\n",
      " 0.005 0.005]\n",
      "Using batch size   1, an average step would have taken 3.78343 = 0.04467 + 3.73875 seconds\n",
      "Using batch size   2, an average step would have taken 2.84765 = 0.01277 + 2.83488 seconds\n",
      "Using batch size   3, an average step would have taken 1.89163 = 0.00986 + 1.88177 seconds\n",
      "Using batch size   4, an average step would have taken 1.60213 = 0.00468 + 1.59745 seconds\n",
      "Using batch size   5, an average step would have taken 1.11971 = 0.00160 + 1.11812 seconds\n",
      "Using batch size   6, an average step would have taken 1.18566 = 0.00330 + 1.18236 seconds\n",
      "Using batch size   7, an average step would have taken 1.10963 = 0.00351 + 1.10612 seconds\n",
      "Using batch size   8, an average step would have taken 0.95965 = 0.00250 + 0.95715 seconds\n",
      "Using batch size   9, an average step would have taken 0.77979 = 0.00159 + 0.77819 seconds\n",
      "Using batch size  10, an average step would have taken 0.74328 = 0.00086 + 0.74242 seconds\n",
      "Using batch size  11, an average step would have taken 0.71859 = 0.00252 + 0.71606 seconds\n",
      "Using batch size  12, an average step would have taken 0.72776 = 0.00282 + 0.72494 seconds\n",
      "Using batch size  13, an average step would have taken 0.75075 = 0.00088 + 0.74987 seconds\n",
      "Using batch size  14, an average step would have taken 0.77940 = 0.00108 + 0.77832 seconds\n",
      "Using batch size  15, an average step would have taken 0.82784 = 0.00086 + 0.82698 seconds\n",
      "Using batch size  16, an average step would have taken 0.80050 = 0.00092 + 0.79958 seconds\n",
      "Using batch size  17, an average step would have taken 0.76731 = 0.00067 + 0.76663 seconds\n",
      "Using batch size  18, an average step would have taken 0.82492 = 0.00103 + 0.82389 seconds\n",
      "Using batch size  19, an average step would have taken 0.85516 = 0.00104 + 0.85412 seconds\n",
      "Using batch size  20, an average step would have taken 0.56530 = 0.00101 + 0.56429 seconds\n",
      "Using batch size  21, an average step would have taken 0.56927 = 0.00076 + 0.56851 seconds\n",
      "Using batch size  22, an average step would have taken 0.56536 = 0.00110 + 0.56425 seconds\n",
      "Using batch size  23, an average step would have taken 0.57136 = 0.00105 + 0.57032 seconds\n",
      "Using batch size  24, an average step would have taken 0.64850 = 0.00188 + 0.64662 seconds\n",
      "Using batch size  25, an average step would have taken 0.45924 = 0.00033 + 0.45891 seconds\n",
      "batch size was 25, optimal batch size would have been 25\n",
      "setting batch size to 25: expected step duration: 0.45924\n",
      "improvement compared to old batch size (25): 1.0x\n",
      "improvement compared to worst batch size (1): 8.2x\n",
      "improvement compared to smallest batch size (1): 8.2x\n",
      "improvement compared to largest batch size (25): 1.0x\n",
      "next batch size tuning in 400 steps, after step 4600\n",
      "  Success rate too high, increasing source step: 0.51 ( 75), 0.60 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.19 (100), 0.25 ( 4)\n",
      "  Boundary too non-linear, decreasing steps: 0.03 (100), 0.14 ( 7)\n",
      "Step 4300: 9.07102e-03, stepsizes = 1.3e-05/1.2e-07:  (took 0.61410 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.03 (100), 0.10 (10)\n",
      "Step 4330: 9.07102e-03, stepsizes = 8.8e-06/7.7e-08: \n",
      "Looks like attack has converged after 4331 steps for the first time. Resetting steps to be sure.\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.10 (10)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.09 (11)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.08 (12)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.08 (12)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.07 (14)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.06 (16)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.06 (17)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.06 (18)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.05 (19)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.05 (21)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.05 (21)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.05 (21)\n",
      "Step 4400: 9.07102e-03, stepsizes = 1.0e-05/1.0e-05:  (took 0.46576 seconds)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.05 (21)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.05 (22)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.04 (23)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.04 (25)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.04 (27)\n",
      "  Boundary too non-linear, decreasing steps: 0.01 (100), 0.04 (28)\n",
      "  Success rate too low, decreasing source step:  0.04 ( 50), 0.03 (30)\n",
      "  Boundary too non-linear, decreasing steps: 0.05 (100), 0.00 ( 3)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 5)\n",
      "  Boundary too non-linear, decreasing steps: 0.02 (100), 0.00 ( 7)\n",
      "  Boundary too non-linear, decreasing steps: 0.00 (100), 0.00 ( 7)\n",
      "  Boundary too non-linear, decreasing steps: 0.03 (100), 0.00 (10)\n",
      "Step 4442: 9.07102e-03, stepsizes = 1.2e-07/7.8e-08: \n",
      "Looks like attack has converged after 4443 steps, 100 remaining\n",
      "Step 4443: 9.07102e-03, stepsizes = 1.2e-07/7.8e-08: \n",
      "Looks like attack has converged after 4444 steps, 99 remaining\n",
      "Step 4444: 9.07102e-03, stepsizes = 1.2e-07/7.8e-08: \n",
      "Looks like attack has converged after 4445 steps, 98 remaining\n",
      "Step 4445: 9.07102e-03, stepsizes = 1.2e-07/7.8e-08: \n",
      "Looks like attack has converged after 4446 steps, 97 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.04 (100), 0.00 (14)\n",
      "Step 4446: 9.07102e-03, stepsizes = 7.8e-08/5.2e-08: \n",
      "Looks like attack has converged after 4447 steps, 96 remaining\n",
      "Step 4447: 9.07102e-03, stepsizes = 7.8e-08/5.2e-08: \n",
      "Looks like attack has converged after 4448 steps, 95 remaining\n",
      "Step 4448: 9.07102e-03, stepsizes = 7.8e-08/5.2e-08: \n",
      "Looks like attack has converged after 4449 steps, 94 remaining\n",
      "Step 4449: 9.07102e-03, stepsizes = 7.8e-08/5.2e-08: \n",
      "Looks like attack has converged after 4450 steps, 93 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.04 (100), 0.00 (18)\n",
      "Step 4450: 9.07102e-03, stepsizes = 5.2e-08/3.5e-08: \n",
      "Looks like attack has converged after 4451 steps, 92 remaining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4451: 9.07102e-03, stepsizes = 5.2e-08/3.5e-08: \n",
      "Looks like attack has converged after 4452 steps, 91 remaining\n",
      "Step 4452: 9.07102e-03, stepsizes = 5.2e-08/3.5e-08: \n",
      "Looks like attack has converged after 4453 steps, 90 remaining\n",
      "Step 4453: 9.07102e-03, stepsizes = 5.2e-08/3.5e-08: \n",
      "Looks like attack has converged after 4454 steps, 89 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.10 (100), 0.00 (28)\n",
      "Step 4454: 9.07102e-03, stepsizes = 3.5e-08/2.3e-08: \n",
      "Looks like attack has converged after 4455 steps, 88 remaining\n",
      "  Success rate too low, decreasing source step:  0.08 ( 25), 0.00 (30)\n",
      "Step 4455: 9.07102e-03, stepsizes = 3.5e-08/1.5e-08: \n",
      "Looks like attack has converged after 4456 steps, 87 remaining\n",
      "Step 4456: 9.07102e-03, stepsizes = 3.5e-08/1.5e-08: \n",
      "Looks like attack has converged after 4457 steps, 86 remaining\n",
      "Step 4457: 9.07102e-03, stepsizes = 3.5e-08/1.5e-08: \n",
      "Looks like attack has converged after 4458 steps, 85 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.09 (100), 0.00 ( 7)\n",
      "Step 4458: 9.07102e-03, stepsizes = 2.3e-08/1.0e-08: \n",
      "Looks like attack has converged after 4459 steps, 84 remaining\n",
      "Step 4459: 9.07102e-03, stepsizes = 2.3e-08/1.0e-08: \n",
      "Looks like attack has converged after 4460 steps, 83 remaining\n",
      "Step 4460: 9.07102e-03, stepsizes = 2.3e-08/1.0e-08: \n",
      "Looks like attack has converged after 4461 steps, 82 remaining\n",
      "Step 4461: 9.07102e-03, stepsizes = 2.3e-08/1.0e-08: \n",
      "Looks like attack has converged after 4462 steps, 81 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.06 (100), 0.00 (13)\n",
      "Step 4462: 9.07102e-03, stepsizes = 1.5e-08/6.9e-09: \n",
      "Looks like attack has converged after 4463 steps, 80 remaining\n",
      "Step 4463: 9.07102e-03, stepsizes = 1.5e-08/6.9e-09: \n",
      "Looks like attack has converged after 4464 steps, 79 remaining\n",
      "Step 4464: 9.07102e-03, stepsizes = 1.5e-08/6.9e-09: \n",
      "Looks like attack has converged after 4465 steps, 78 remaining\n",
      "Step 4465: 9.07102e-03, stepsizes = 1.5e-08/6.9e-09: \n",
      "Looks like attack has converged after 4466 steps, 77 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.18 (100), 0.00 (30)\n",
      "  Success rate too low, decreasing source step:  0.18 (100), 0.00 (30)\n",
      "Step 4466: 9.07102e-03, stepsizes = 1.0e-08/3.1e-09: \n",
      "Looks like attack has converged after 4467 steps, 76 remaining\n",
      "Step 4467: 9.07102e-03, stepsizes = 1.0e-08/3.1e-09: \n",
      "Looks like attack has converged after 4468 steps, 75 remaining\n",
      "Step 4468: 9.07102e-03, stepsizes = 1.0e-08/3.1e-09: \n",
      "Looks like attack has converged after 4469 steps, 74 remaining\n",
      "Step 4469: 9.07102e-03, stepsizes = 1.0e-08/3.1e-09: \n",
      "Looks like attack has converged after 4470 steps, 73 remaining\n",
      "Step 4470: 9.07102e-03, stepsizes = 1.0e-08/3.1e-09: \n",
      "Looks like attack has converged after 4471 steps, 72 remaining\n",
      "  Success rate too low, decreasing source step:  0.28 (100), 0.00 (30)\n",
      "Step 4471: 9.07102e-03, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 4472 steps, 71 remaining\n",
      "Step 4472: 9.07102e-03, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 4473 steps, 70 remaining\n",
      "Step 4473: 9.07102e-03, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 4474 steps, 69 remaining\n",
      "Step 4474: 9.07102e-03, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 4475 steps, 68 remaining\n",
      "Step 4475: 9.07102e-03, stepsizes = 1.0e-08/2.0e-09: \n",
      "Looks like attack has converged after 4476 steps, 67 remaining\n",
      "  Success rate too low, decreasing source step:  0.31 (100), 0.00 (30)\n",
      "Step 4476: 9.07102e-03, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 4477 steps, 66 remaining\n",
      "Step 4477: 9.07102e-03, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 4478 steps, 65 remaining\n",
      "Step 4478: 9.07102e-03, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 4479 steps, 64 remaining\n",
      "Step 4479: 9.07102e-03, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 4480 steps, 63 remaining\n",
      "Step 4480: 9.07102e-03, stepsizes = 1.0e-08/1.4e-09: \n",
      "Looks like attack has converged after 4481 steps, 62 remaining\n",
      "  Success rate too low, decreasing source step:  0.25 (100), 0.00 (30)\n",
      "Step 4481: 9.07102e-03, stepsizes = 1.0e-08/9.0e-10: \n",
      "Looks like attack has converged after 4482 steps, 61 remaining\n",
      "Step 4482: 9.07102e-03, stepsizes = 1.0e-08/9.0e-10: \n",
      "Looks like attack has converged after 4483 steps, 60 remaining\n",
      "Step 4483: 9.07102e-03, stepsizes = 1.0e-08/9.0e-10: \n",
      "Looks like attack has converged after 4484 steps, 59 remaining\n",
      "Step 4484: 9.07102e-03, stepsizes = 1.0e-08/9.0e-10: \n",
      "Looks like attack has converged after 4485 steps, 58 remaining\n",
      "Step 4485: 9.07102e-03, stepsizes = 1.0e-08/9.0e-10: \n",
      "Looks like attack has converged after 4486 steps, 57 remaining\n",
      "  Success rate too low, decreasing source step:  0.26 (100), 0.00 (30)\n",
      "Step 4486: 9.07102e-03, stepsizes = 1.0e-08/6.0e-10: \n",
      "Looks like attack has converged after 4487 steps, 56 remaining\n",
      "Step 4487: 9.07102e-03, stepsizes = 1.0e-08/6.0e-10: \n",
      "Looks like attack has converged after 4488 steps, 55 remaining\n",
      "Step 4488: 9.07102e-03, stepsizes = 1.0e-08/6.0e-10: \n",
      "Looks like attack has converged after 4489 steps, 54 remaining\n",
      "Step 4489: 9.07102e-03, stepsizes = 1.0e-08/6.0e-10: \n",
      "Looks like attack has converged after 4490 steps, 53 remaining\n",
      "Step 4490: 9.07102e-03, stepsizes = 1.0e-08/6.0e-10: \n",
      "Looks like attack has converged after 4491 steps, 52 remaining\n",
      "  Success rate too low, decreasing source step:  0.22 (100), 0.00 (30)\n",
      "Step 4491: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4492 steps, 51 remaining\n",
      "Step 4492: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4493 steps, 50 remaining\n",
      "Step 4493: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4494 steps, 49 remaining\n",
      "Step 4494: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4495 steps, 48 remaining\n",
      "Step 4495: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4496 steps, 47 remaining\n",
      "Step 4496: 9.07102e-03, stepsizes = 1.0e-08/4.0e-10: \n",
      "Looks like attack has converged after 4497 steps, 46 remaining\n",
      "  Success rate too low, decreasing source step:  0.22 (100), 0.00 (30)\n",
      "Step 4497: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4498 steps, 45 remaining\n",
      "Step 4498: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4499 steps, 44 remaining\n",
      "Step 4499: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4500 steps, 43 remaining\n",
      "Step 4500: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10:  (took 0.84592 seconds)\n",
      "Step 4500: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4501 steps, 42 remaining\n",
      "Step 4501: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4502 steps, 41 remaining\n",
      "Step 4502: 9.07102e-03, stepsizes = 1.0e-08/2.7e-10: \n",
      "Looks like attack has converged after 4503 steps, 40 remaining\n",
      "  Success rate too low, decreasing source step:  0.23 (100), 0.00 (30)\n",
      "Step 4503: 9.07102e-03, stepsizes = 1.0e-08/1.8e-10: \n",
      "Looks like attack has converged after 4504 steps, 39 remaining\n",
      "Step 4504: 9.07102e-03, stepsizes = 1.0e-08/1.8e-10: \n",
      "Looks like attack has converged after 4505 steps, 38 remaining\n",
      "Step 4505: 9.07102e-03, stepsizes = 1.0e-08/1.8e-10: \n",
      "Looks like attack has converged after 4506 steps, 37 remaining\n",
      "Step 4506: 9.07102e-03, stepsizes = 1.0e-08/1.8e-10: \n",
      "Looks like attack has converged after 4507 steps, 36 remaining\n",
      "Step 4507: 9.07102e-03, stepsizes = 1.0e-08/1.8e-10: \n",
      "Looks like attack has converged after 4508 steps, 35 remaining\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.00 (30)\n",
      "Step 4508: 9.07102e-03, stepsizes = 1.0e-08/1.2e-10: \n",
      "Looks like attack has converged after 4509 steps, 34 remaining\n",
      "Step 4509: 9.07102e-03, stepsizes = 1.0e-08/1.2e-10: \n",
      "Looks like attack has converged after 4510 steps, 33 remaining\n",
      "Step 4510: 9.07102e-03, stepsizes = 1.0e-08/1.2e-10: \n",
      "Looks like attack has converged after 4511 steps, 32 remaining\n",
      "Step 4511: 9.07102e-03, stepsizes = 1.0e-08/1.2e-10: \n",
      "Looks like attack has converged after 4512 steps, 31 remaining\n",
      "  Boundary too non-linear, decreasing steps: 0.15 (100), 0.00 (15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4512: 9.07102e-03, stepsizes = 6.9e-09/7.9e-11: \n",
      "Looks like attack has converged after 4513 steps, 30 remaining\n",
      "Step 4513: 9.07102e-03, stepsizes = 6.9e-09/7.9e-11: \n",
      "Looks like attack has converged after 4514 steps, 29 remaining\n",
      "  Success rate too low, decreasing source step:  0.32 ( 50), 0.00 (30)\n",
      "Step 4514: 9.07102e-03, stepsizes = 6.9e-09/5.3e-11: \n",
      "Looks like attack has converged after 4515 steps, 28 remaining\n",
      "Step 4515: 9.07102e-03, stepsizes = 6.9e-09/5.3e-11: \n",
      "Looks like attack has converged after 4516 steps, 27 remaining\n",
      "Step 4516: 9.07102e-03, stepsizes = 6.9e-09/5.3e-11: \n",
      "Looks like attack has converged after 4517 steps, 26 remaining\n",
      "  Success rate too low, decreasing source step:  0.39 (100), 0.00 (30)\n",
      "Step 4517: 9.07102e-03, stepsizes = 6.9e-09/3.5e-11: \n",
      "Looks like attack has converged after 4518 steps, 25 remaining\n",
      "Step 4518: 9.07102e-03, stepsizes = 6.9e-09/3.5e-11: \n",
      "Looks like attack has converged after 4519 steps, 24 remaining\n",
      "Step 4519: 9.07102e-03, stepsizes = 6.9e-09/3.5e-11: \n",
      "Looks like attack has converged after 4520 steps, 23 remaining\n",
      "Step 4520: 9.07102e-03, stepsizes = 6.9e-09/3.5e-11: \n",
      "Looks like attack has converged after 4521 steps, 22 remaining\n",
      "  Success rate too low, decreasing source step:  0.39 (100), 0.00 (30)\n",
      "Step 4521: 9.07102e-03, stepsizes = 6.9e-09/2.4e-11: \n",
      "Looks like attack has converged after 4522 steps, 21 remaining\n",
      "Step 4522: 9.07102e-03, stepsizes = 6.9e-09/2.4e-11: \n",
      "Looks like attack has converged after 4523 steps, 20 remaining\n",
      "Step 4523: 9.07102e-03, stepsizes = 6.9e-09/2.4e-11: \n",
      "Looks like attack has converged after 4524 steps, 19 remaining\n",
      "Step 4524: 9.07102e-03, stepsizes = 6.9e-09/2.4e-11: \n",
      "Looks like attack has converged after 4525 steps, 18 remaining\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.00 (30)\n",
      "Step 4525: 9.07102e-03, stepsizes = 6.9e-09/1.6e-11: \n",
      "Looks like attack has converged after 4526 steps, 17 remaining\n",
      "Step 4526: 9.07102e-03, stepsizes = 6.9e-09/1.6e-11: \n",
      "Looks like attack has converged after 4527 steps, 16 remaining\n",
      "Step 4527: 9.07102e-03, stepsizes = 6.9e-09/1.6e-11: \n",
      "Looks like attack has converged after 4528 steps, 15 remaining\n",
      "Step 4528: 9.07102e-03, stepsizes = 6.9e-09/1.6e-11: \n",
      "Looks like attack has converged after 4529 steps, 14 remaining\n",
      "  Success rate too low, decreasing source step:  0.30 (100), 0.00 (30)\n",
      "Step 4529: 9.07102e-03, stepsizes = 6.9e-09/1.0e-11: \n",
      "Looks like attack has converged after 4530 steps, 13 remaining\n",
      "Step 4530: 9.07102e-03, stepsizes = 6.9e-09/1.0e-11: \n",
      "Looks like attack has converged after 4531 steps, 12 remaining\n",
      "Step 4531: 9.07102e-03, stepsizes = 6.9e-09/1.0e-11: \n",
      "Looks like attack has converged after 4532 steps, 11 remaining\n",
      "Step 4532: 9.07102e-03, stepsizes = 6.9e-09/1.0e-11: \n",
      "Looks like attack has converged after 4533 steps, 10 remaining\n",
      "Step 4533: 9.07102e-03, stepsizes = 6.9e-09/1.0e-11: \n",
      "Looks like attack has converged after 4534 steps, 9 remaining\n",
      "  Success rate too low, decreasing source step:  0.33 (100), 0.00 (30)\n",
      "Step 4534: 9.07102e-03, stepsizes = 6.9e-09/7.0e-12: \n",
      "Looks like attack has converged after 4535 steps, 8 remaining\n",
      "Step 4535: 9.07102e-03, stepsizes = 6.9e-09/7.0e-12: \n",
      "Looks like attack has converged after 4536 steps, 7 remaining\n",
      "Step 4536: 9.07102e-03, stepsizes = 6.9e-09/7.0e-12: \n",
      "Looks like attack has converged after 4537 steps, 6 remaining\n",
      "Step 4537: 9.07102e-03, stepsizes = 6.9e-09/7.0e-12: \n",
      "Looks like attack has converged after 4538 steps, 5 remaining\n",
      "  Success rate too low, decreasing source step:  0.34 (100), 0.00 (30)\n",
      "Step 4538: 9.07102e-03, stepsizes = 6.9e-09/4.6e-12: \n",
      "Looks like attack has converged after 4539 steps, 4 remaining\n",
      "Step 4539: 9.07102e-03, stepsizes = 6.9e-09/4.6e-12: \n",
      "Looks like attack has converged after 4540 steps, 3 remaining\n",
      "Step 4540: 9.07102e-03, stepsizes = 6.9e-09/4.6e-12: \n",
      "Looks like attack has converged after 4541 steps, 2 remaining\n",
      "  Success rate too low, decreasing source step:  0.36 (100), 0.00 (30)\n",
      "Step 4541: 9.07102e-03, stepsizes = 6.9e-09/3.1e-12: \n",
      "Looks like attack has converged after 4542 steps, 1 remaining\n",
      "Time since beginning: 3089.00910\n",
      "   1.4% for generation (42.57724)\n",
      "   13.9% for spherical prediction (430.48540)\n",
      "   75.1% for prediction (2321.06994)\n",
      "   0.0% for hyperparameter update (1.03542)\n",
      "   9.5% for the rest (293.84109)\n"
     ]
    }
   ],
   "source": [
    "attack_params = {\n",
    "    'iterations': 10000,\n",
    "    'max_directions': 25,\n",
    "    'starting_point': None,\n",
    "    'initialization_attack': None,\n",
    "    'log_every_n_steps': 100,\n",
    "    'spherical_step': 0.5,\n",
    "    'source_step': 0.05,\n",
    "    'step_adaptation': 1.5,\n",
    "    'batch_size': 1,\n",
    "    'tune_batch_size': True, \n",
    "    'threaded_rnd': True, \n",
    "    'threaded_gen': True, \n",
    "    'alternative_generator': False\n",
    "}\n",
    "\n",
    "num = 1\n",
    "x_adv = np.zeros_like(x_test[:num].numpy())\n",
    "for i in range(num):\n",
    "    x_adv[i] = attack(x_test[i].numpy(), label=y_test[i].numpy(), \n",
    "                      unpack=True, verbose=True, **attack_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = dknn.classify(torch.tensor(x_adv))\n",
    "print((y_pred.argmax(1) == y_test[:num].numpy()).sum() / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6667733\n"
     ]
    }
   ],
   "source": [
    "dist = np.sqrt(np.sum((x_adv - x_test[:num].numpy())**2, (1, 2, 3)))\n",
    "print(dist.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f352498c588>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[0].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foolbox.criteria import Misclassification\n",
    "from foolbox.distances import MeanSquaredDistance, Linfinity \n",
    "\n",
    "criterion = Misclassification()\n",
    "distance = MeanSquaredDistance\n",
    "# distance = Linfinity\n",
    "\n",
    "attack = foolbox.attacks.SinglePixelAttack(\n",
    "    model=dknn_fb, criterion=criterion, distance=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10\n",
    "x_adv = np.zeros_like(x_test[:num].numpy())\n",
    "for i in range(num):\n",
    "    x_adv[i] = attack(x_test[i].numpy(), label=y_test[i].numpy(), \n",
    "                      unpack=True, max_pixels=784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "y_pred = dknn.classify(torch.tensor(x_adv))\n",
    "print((y_pred.argmax(1) == y_test[:num].numpy()).sum() / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "dist = np.sqrt(np.sum((x_adv - x_test[:num].numpy())**2, (1, 2, 3)))\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-pixel attack never succeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foolbox.criteria import Misclassification\n",
    "from foolbox.distances import MeanSquaredDistance, Linfinity \n",
    "\n",
    "criterion = Misclassification()\n",
    "distance = MeanSquaredDistance\n",
    "# distance = Linfinity\n",
    "\n",
    "attack = foolbox.attacks.LocalSearchAttack(\n",
    "    model=dknn_fb, criterion=criterion, distance=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10\n",
    "x_adv = np.zeros_like(x_test[:num].numpy())\n",
    "for i in range(num):\n",
    "    x_adv[i] = attack(x_test[i].numpy(), label=y_test[i].numpy(), \n",
    "                      unpack=True, r=1.5, p=10.0, d=5, t=5, R=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "y_pred = dknn.classify(torch.tensor(x_adv))\n",
    "print((y_pred.argmax(1) == y_test[:num].numpy()).sum() / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "dist = np.sqrt(np.sum((x_adv - x_test[:num].numpy())**2, (1, 2, 3)))\n",
    "print(dist.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f19e78a1278>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[5].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local-search attack only works 1/10 and noise is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers = ['relu1', 'relu2', 'relu3', 'fc']\n",
    "dknn = DKNNL2(net, x_train, y_train, x_valid, y_valid, layers, \n",
    "              k=75, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9607, 0.8579, 0.5358, 0.5033\n"
     ]
    }
   ],
   "source": [
    "# Verify Lipschitzness\n",
    "\n",
    "x = x_test.requires_grad_(True)[:1000]\n",
    "\n",
    "norms = compute_spnorm(x, dknn, layers)\n",
    "print(', '.join('%.4f' % i for i in norms.mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9607, 0.8579, 0.5358, 0.5033\n"
     ]
    }
   ],
   "source": [
    "print(', '.join('%.4f' % i for i in norms.mean(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0235, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3.297e+03, 2.081e+03, 1.830e+03, 1.407e+03, 8.290e+02, 3.950e+02,\n",
       "        1.240e+02, 3.300e+01, 3.000e+00, 1.000e+00]),\n",
       " array([5.9045851e-07, 1.1226014e-02, 2.2451438e-02, 3.3676863e-02,\n",
       "        4.4902287e-02, 5.6127708e-02, 6.7353129e-02, 7.8578554e-02,\n",
       "        8.9803979e-02, 1.0102940e-01, 1.1225483e-01], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gap = y_pred.max(1)[0] - y_pred.sort()[0][:, -2]\n",
    "print(gap.mean().data)\n",
    "plt.hist(gap.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_guide_samples(dknn, x, label, k=1, layer='relu1'):\n",
    "    \"\"\"\n",
    "    find k nearest neighbors of the same class (not equal to y_Q) but\n",
    "    closest to Q\n",
    "    \"\"\"\n",
    "    num_classes = dknn.num_classes\n",
    "    nn = torch.zeros((k, ) + x.size()).permute(1, 0, 2, 3, 4)\n",
    "    D, I = dknn.get_neighbors(\n",
    "        x, k=dknn.x_train.size(0), layers=[layer])[0]\n",
    "\n",
    "    for i, (d, ind) in enumerate(zip(D, I)):\n",
    "        mean_dist = np.zeros((num_classes, ))\n",
    "        for j in range(num_classes):\n",
    "            mean_dist[j] = np.mean(\n",
    "                d[np.where(dknn.y_train[ind] == j)[0]][:k])\n",
    "        # TODO: this may depend on the index used\n",
    "        mean_dist[label[i]] += 1e20\n",
    "        nearest_label = mean_dist.argmin()\n",
    "        # mean_dist[label[i]] -= INFTY\n",
    "        # nearest_label = mean_dist.argmax()\n",
    "        nn_ind = np.where(dknn.y_train[ind] == nearest_label)[0][:k]\n",
    "        nn[i] = dknn.x_train[ind[nn_ind]]\n",
    "\n",
    "    return nn\n",
    "\n",
    "\n",
    "def find_nn_diff_class(x_train, y_train, x, label, num_classes=10):\n",
    "    \"\"\"\n",
    "    find k nearest neighbors of the same class (not equal to y_Q) but\n",
    "    closest to Q\n",
    "    \"\"\"\n",
    "    nn = torch.zeros_like(x)\n",
    "\n",
    "    for i in range(x.size(0)):\n",
    "        d = ((x[i].unsqueeze(0) - x_train)**2).view(x_train.size(0), -1).sum(1)\n",
    "        sorted_ind = d.argsort()\n",
    "        for j in range(x_train.size(0)):\n",
    "            if label[i] != y_train[sorted_ind[j]]:\n",
    "                nn[i] = x_train[sorted_ind[j]]\n",
    "                break\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['relu3']\n",
    "dknn = DKNNL2(net, x_train, y_train, x_valid, y_valid, layers, \n",
    "              k=1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8868\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = dknn.classify(x_test)\n",
    "    print((y_pred.argmax(1) == y_test.numpy()).sum() / y_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rep_train = dknn.get_activations(x_train, requires_grad=False, device='cpu')[layers[0]]\n",
    "rep_test = dknn.get_activations(x_test, requires_grad=False, device='cpu')[layers[0]]\n",
    "rep_valid = dknn.get_activations(x_valid, requires_grad=False, device='cpu')[layers[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNL2(rep_train, y_train, rep_valid, y_valid, k=1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_nn = knn.find_nn_diff_class(rep_test, y_test)\n",
    "rep_adv = knn.get_min_dist(rep_test, y_test, rep_nn, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = ((rep_test - rep_nn)**2).view(rep_test.size(0), -1).sum(1).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4930)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 195., 1660., 3101., 2666., 1381.,  600.,  265.,  100.,   23.,\n",
       "           9.]),\n",
       " array([0.15380737, 0.26018655, 0.36656573, 0.47294492, 0.57932407,\n",
       "        0.6857033 , 0.7920824 , 0.89846164, 1.0048409 , 1.11122   ,\n",
       "        1.2175992 ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dist.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "perts = [1, 2, 3]\n",
    "a = []\n",
    "for pert in perts:\n",
    "    a.append((dist.cpu().detach().numpy() > math.sqrt(2) * pert).mean())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.knn import KNNL2\n",
    "\n",
    "knn = KNNL2(x_train, y_train, x_valid, y_valid, k=1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9683\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.classify(x_test)\n",
    "print((y_pred.argmax(1) == y_test.numpy()).sum() / y_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nn = knn.find_nn_diff_class(x_test, y_test)\n",
    "x_adv = knn.get_min_dist(x_test, y_test, x_nn, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = ((x_adv - x_test)**2).view(x_test.size(0), -1).sum(1).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8145)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 679.,  736., 1334., 2089., 2505., 1551.,  776.,  288.,   38.,\n",
       "           4.]),\n",
       " array([2.7532987e-03, 4.6384671e-01, 9.2494017e-01, 1.3860335e+00,\n",
       "        1.8471270e+00, 2.3082204e+00, 2.7693138e+00, 3.2304072e+00,\n",
       "        3.6915007e+00, 4.1525941e+00, 4.6136875e+00], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dist.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert = 3\n",
    "(dist.cpu().detach().numpy() > math.sqrt(2) * pert).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc7362b66a0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv[0].numpy().reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
